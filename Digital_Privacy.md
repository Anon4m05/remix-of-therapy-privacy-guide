# Teaching Digital Privacy: Navigating the Intersection of Technology, Education, and Privacy (Sharma, 2024)

**Source file:** `5. Digital Privacy.pdf`  
**Extraction method:** pdftotext -layout  

---

See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/381952547



Teaching Digital Privacy: Navigating the Intersection of Technology, Education,
and Privacy

Article ¬∑ July 2024



CITATIONS                                                                                                 READS

0                                                                                                         383


1 author:

            Anurag Sharma
            K.R. MANGALAM UNIVERSITY
            2 PUBLICATIONS 0 CITATIONS

              SEE PROFILE




 All content following this page was uploaded by Anurag Sharma on 03 July 2024.

 The user has requested enhancement of the downloaded file.
                                                                                  ISSN 2348-8301
                                       International Journal of humanities,Law and Social Sciences
                                Published biannually by New Archaeological & Genological Society
                                                                                     Kanpur India




                                                                         Vol. IX, Issue II, December 2022
                                                                              www.kanpurhistorians.org
       Teaching Digital Privacy: Navigating the Intersection of Technology,
                             Education, and Privacy
                                                                                         Anurag Sharma1
                                                                                       Assistant Professor
                                                   Prestige Institute of Management and Research, Gwalior

Digital privacy refers to the protection of personal information and data in the digital realm. In the context
of education, digital privacy is the safeguarding of students' personal information and data, including
academic records, medical information, and online activity. Digital privacy is important in education
because it protects the fundamental right to privacy of students and helps to ensure that their personal
information is not misused or exploited. It also helps to maintain a safe and secure learning environment
for students and protects them from potential harms such as identity theft, cyberbullying, and online
predators. In addition, digital privacy is essential in promoting trust and confidence in the education
system, both among students and their families and among the broader community. By prioritizing digital
privacy in education, schools and educational institutions can foster a culture of respect for privacy and
help to prepare students for responsible and ethical use of digital technology in their future lives and
careers. Educators are particularly concerned with digital privacy, as they work with a great deal of
personal information about their students. This paper explores the intersection of digital privacy and
teaching, examining the ways in which educators can protect students' privacy while still using
technology effectively in the classroom. There are many threats to digital privacy in education, including
data breaches, online surveillance, and the potential for misuse of personal information. Schools and
universities collect a great deal of personal information about students, including their names, addresses,
dates of birth, and academic records. This information is often stored in databases or on cloud servers,
making it vulnerable to cyberattacks and data breaches. In addition, online surveillance by governments
or other entities can compromise students' privacy, while the potential for misuse of personal information
by schools or third-party vendors remains a concern.
keywords: Digital Privacy, Education System, Teaching, Educators, Technology, data Breaches, Online
Surveillance.
Introduction



1
ASSISTANT PROFESSOR, PRESTIGE INSTITUTE OF RESEARCH, GWALIOR




Kanpur Philosophers, ISSN 2348-8301 Volume IX Issue II, 2022                                      Page | 154
Definition of digital privacy and its importance in education: Digital privacy refers to the protection
of personal information and data in the digital realm, including online activities, communications, and
transactions. In education, digital privacy is important because it involves the protection of sensitive
information, such as student records, grades, and assessments. Educational institutions have an obligation
to safeguard this information and ensure that it is not disclosed or accessed without proper consent.
Moreover, with the increasing use of digital technology in education, such as online learning platforms,
educational apps, and communication tools, the risk of digital privacy breaches has become more
prevalent. Protecting student data and digital privacy is essential to maintaining trust and confidence in
the educational system and ensuring that students can learn and communicate in a safe and secure
environment.
Furthermore, teaching digital privacy in schools is essential to ensure that students are equipped with the
necessary skills and knowledge to navigate the digital world safely and responsibly. By teaching digital
privacy, students can learn how to protect their personal information, avoid cyberbullying and online
harassment, and develop critical thinking skills to evaluate online information.
In summary, digital privacy is crucial in education because it involves the protection of sensitive
information and personal data, and teaching digital privacy is necessary to prepare students for the digital
world and ensure that they can navigate it safely and responsibly.
Overview of digital privacy issues in education: Digital privacy issues in education have become more
prevalent with the increased use of technology in the classroom. Some of the key digital privacy issues in
education include:
Data Breaches: Educational institutions collect and store sensitive data, such as student records and
grades, and this data is vulnerable to breaches. Data breaches can occur due to human error, hacking, or
other malicious activity.
Cyberbullying: With the increased use of social media and communication tools, students are at risk of
cyberbullying and online harassment. This can have a significant impact on a student's mental health and
academic performance.
Online Safety: Students may not be aware of the risks associated with sharing personal information
online, such as their location, photos, and other identifying information. This can put them at risk of
online predators and other harmful activities.
Surveillance: Some educational institutions may use surveillance technology, such as cameras or
monitoring software, to monitor student activity. This can raise privacy concerns and may infringe on
students' rights.
Targeted Advertising: Educational technology providers may collect and use student data for targeted
advertising purposes, which can raise privacy concerns and ethical issues.
These digital privacy issues in education can have a significant impact on student learning, safety, and
well-being. It is important for educational institutions to take steps to address these issues and ensure that
student data and privacy are protected.
Background Information
Overview of the history of digital privacy and its development in the education sector: Digital
privacy has become a growing concern in recent years, with the increased use of technology in all aspects
of life, including education. Here is an overview of the history of digital privacy and its development in
the education sector:


Kanpur Philosophers, ISSN 2348-8301 Volume IX Issue II, 2022                                      Page | 155
Early Days: In the early days of the internet, digital privacy was not a major concern. Users were often
anonymous, and personal information was not widely shared or collected.
Rise of Social Media: The rise of social media in the mid-2000s brought new privacy concerns. Users
were encouraged to share personal information online, and companies began to collect and monetize user
data.
Focus on Education: The development of educational technology in the late 2000s and early 2010s
brought new privacy concerns to the education sector. Educational institutions began to collect and store
more data on students, and educational technology providers began to collect and use student data for
targeted advertising purposes.
Legal Framework: In response to these privacy concerns, lawmakers began to develop a legal framework
to protect student data and privacy. In the United States, the Family Educational Rights and Privacy Act
(FERPA) was enacted in 1974 to protect the privacy of student education records. In 2013, the U.S.
Department of Education issued new regulations that strengthened FERPA protections for student data.
International Developments: The European Union's General Data Protection Regulation (GDPR) was
introduced in 2018, which applies to all EU member states and regulates the collection and use of
personal data. The GDPR has had a significant impact on the education sector, as educational institutions
and technology providers must comply with its requirements when handling student data.
Legal Framework And Ethical Considerations Surrounding Digital Privacy In Education: Digital
privacy in education is governed by a legal framework that includes federal and state laws and
regulations. The primary federal law governing student data privacy is the Family Educational Rights and
Privacy Act (FERPA), which establishes guidelines for the collection, use, and disclosure of student
education records. Additionally, some states have passed their own laws governing student data privacy,
such as California's Student Online Personal Information Protection Act (SOPIPA).
Beyond legal requirements, there are also ethical considerations when it comes to digital privacy in
education. Educational institutions and technology providers must consider the potential risks and harm
that can come from collecting and sharing student data. Students have a right to privacy and should have
control over their personal information.
One ethical consideration is the use of student data for targeted advertising. Educational technology
providers may collect and use student data to deliver targeted advertising, which can raise ethical
concerns and conflicts with the educational mission. Another ethical consideration is the use of
surveillance technology, such as cameras or monitoring software, to monitor student activity. This can
raise privacy concerns and may infringe on students' rights.
Furthermore, there are also ethical considerations when it comes to data security. Educational institutions
and technology providers must ensure that student data is secure and protected from unauthorized access
or disclosure. They must implement appropriate security measures to safeguard student data from data
breaches and cyberattacks.
Digital Privacy Threats In Education
Types of digital privacy threats in education: There are several types of digital privacy threats in
education, including:
Data breaches: Educational institutions store large amounts of sensitive data, such as student records,
grades, and other personal information. Data breaches can occur due to human error, hacking, or other
malicious activity, which can result in the unauthorized disclosure of sensitive information.


Kanpur Philosophers, ISSN 2348-8301 Volume IX Issue II, 2022                                   Page | 156
Cyberbullying: With the increased use of social media and communication tools, students are at risk of
cyberbullying and online harassment. This can have a significant impact on a student's mental health and
academic performance.
Online safety: Students may not be aware of the risks associated with sharing personal information online,
such as their location, photos, and other identifying information. This can put them at risk of online
predators and other harmful activities.
Surveillance: Some educational institutions may use surveillance technology, such as cameras or
monitoring software, to monitor student activity. This can raise privacy concerns and may infringe on
students' rights.
Targeted advertising: Educational technology providers may collect and use student data for targeted
advertising purposes, which can raise privacy concerns and ethical issues.
Inadequate security measures: Educational institutions and technology providers may not have adequate
security measures in place to protect student data, which can leave it vulnerable to cyberattacks and other
unauthorized access.
It is important for educational institutions and technology providers to be aware of these digital privacy
threats and take steps to address them. By implementing appropriate security measures, educating
students about online safety, and developing policies and procedures to protect student data, they can
create a safe and secure learning environment for students.
Analysis of the potential consequences of digital privacy breaches in education: Digital privacy
breaches in education can have serious consequences for students, educational institutions, and other
stakeholders. Here are some of the potential consequences of digital privacy breaches in education:
Identity Theft: One of the most significant consequences of a digital privacy breach is identity theft.
Student information, such as Social Security numbers and financial information, can be stolen and used
for fraudulent purposes.
Reputation Damage: Digital privacy breaches can damage the reputation of educational institutions. A
breach can erode trust and confidence in the institution and lead to negative publicity.
Legal Liability: Educational institutions can face legal liability if they fail to adequately protect student
data. Lawsuits can result in significant financial damages and reputational harm.
Educational Disruption: A digital privacy breach can disrupt the educational process, as institutions may
need to shut down systems and services to address the breach. This can impact student learning and cause
significant disruptions.
Emotional Distress: A digital privacy breach can cause emotional distress for students and their families.
Students may feel violated and vulnerable, which can impact their mental health and wellbeing.
Financial Consequences: A digital privacy breach can result in financial consequences for students and
their families. For example, a breach that compromises financial information can result in financial losses
and damage credit scores.
Teaching Digital Privacy
The importance of teaching digital privacy in schools: Teaching digital privacy in schools is becoming
increasingly important in today's technology-driven world. Here are some of the reasons why teaching
digital privacy is important:



Kanpur Philosophers, ISSN 2348-8301 Volume IX Issue II, 2022                                     Page | 157
Protecting Students' Personal Information: Students' personal information, such as their names, addresses,
and Social Security numbers, is stored in school databases and other systems. By teaching digital privacy,
students can learn how to protect their personal information and prevent it from falling into the wrong
hands.
Promoting Safe Online Behavior: Digital privacy education can help students develop safe online
behaviors, such as creating strong passwords, avoiding sharing personal information online, and
recognizing and avoiding phishing attempts.
Preparing Students for the Workforce: As technology becomes increasingly integrated into the workplace,
digital privacy has become a critical concern for employers. By teaching digital privacy in schools,
students can develop the skills necessary to protect sensitive information in the workplace.
Encouraging Critical Thinking: Digital privacy education can help students become critical thinkers and
evaluate the potential risks and benefits of sharing personal information online.
Addressing Cyberbullying and Online Harassment: Teaching digital privacy can help students understand
the potential consequences of cyberbullying and online harassment and learn how to protect themselves
and others online.
Empowering Students: By teaching digital privacy, students can feel empowered to take control of their
online privacy and security. They can learn how to protect themselves and make informed decisions about
sharing personal information online.
Methods for teaching digital privacy to students and educators: Teaching digital privacy to students
and educators can be approached in a variety of ways. Here are some effective methods for teaching
digital privacy:
Interactive Workshops and Presentations: Interactive workshops and presentations can be used to teach
students and educators about digital privacy. These workshops can include group discussions, case
studies, and hands-on activities to help participants understand digital privacy concepts and best practices.
Online Resources: Online resources such as videos, articles, and quizzes can be used to teach digital
privacy concepts to students and educators. These resources can be accessed at any time, making it easy
to reinforce digital privacy concepts outside of the classroom.
Simulation Games: Simulation games can be used to teach students and educators about digital privacy in
a fun and engaging way. These games can simulate real-world scenarios and teach participants how to
identify and avoid common digital privacy threats.
Role-Playing Exercises: Role-playing exercises can be used to teach students and educators about digital
privacy in a practical way. Participants can practice responding to digital privacy threats and identify best
practices for protecting personal information.
Guest Speakers: Guest speakers, such as cybersecurity experts and digital privacy advocates, can be
invited to speak to students and educators about digital privacy. These speakers can share their knowledge
and experiences with digital privacy and provide valuable insights into the importance of protecting
personal information.
Collaborative Learning: Collaborative learning can be used to teach digital privacy concepts to students
and educators. Participants can work together to develop best practices for protecting personal
information and share their knowledge and experiences with others.




Kanpur Philosophers, ISSN 2348-8301 Volume IX Issue II, 2022                                     Page | 158
Challenges and best practices for teaching digital privacy in schools: Teaching digital privacy in
schools can be challenging, but there are best practices that educators can follow to effectively educate
students on the importance of digital privacy. Here are some of the challenges and best practices for
teaching digital privacy in schools:
Challenges:
Keeping up with Technological Advances: One of the biggest challenges in teaching digital privacy is
keeping up with the constantly changing technology landscape. As new technologies and threats emerge,
educators need to stay informed and adapt their teaching accordingly.
Limited Resources: Many schools have limited resources for digital privacy education. Educators may not
have the necessary tools, materials, or time to effectively teach digital privacy.
Student Engagement: It can be difficult to engage students in digital privacy education. Some students
may not see the relevance of digital privacy to their lives and may be disinterested in the topic.
Best Practices
Incorporate Digital Privacy into the Curriculum: Digital privacy education should be integrated into the
curriculum and taught across subjects, not just in one standalone course.
Use Real-World Examples: Using real-world examples of digital privacy breaches and their consequences
can help students understand the importance of digital privacy and the risks of not protecting personal
information.
Engage Students in Active Learning: Active learning strategies, such as group discussions, case studies,
and simulations, can help engage students in digital privacy education.
Provide Hands-On Experience: Providing students with hands-on experience, such as creating strong
passwords or identifying phishing attempts, can help them develop practical skills for protecting their
digital privacy.
Partner with Parents and Guardians: Educators can partner with parents and guardians to reinforce digital
privacy education at home and ensure that students are applying what they've learned in school.
Stay Current with Best Practices: Educators should stay informed on the latest best practices and
resources for teaching digital privacy, such as online training programs and teaching resources provided
by reputable organizations.
Digital Privacy Policies And Laws In Education
Overview of digital privacy policies and laws in education: Digital privacy policies and laws are
important in protecting the personal information of students and educators. Here is an overview of some
of the digital privacy policies and laws in education:
Family Educational Rights and Privacy Act (FERPA): FERPA is a federal law that protects the privacy of
student education records. This law gives parents and eligible students (18 years or older) the right to
review and request corrections to their education records. FERPA also requires schools to obtain written
consent before disclosing any personally identifiable information from a student's education records.
Children's Online Privacy Protection Act (COPPA): COPPA is a federal law that protects the privacy of
children under the age of 13 online. This law requires websites and online services to obtain parental
consent before collecting, using, or disclosing any personal information from children under the age of
13.


Kanpur Philosophers, ISSN 2348-8301 Volume IX Issue II, 2022                                  Page | 159
General Data Protection Regulation (GDPR): The GDPR is a regulation that protects the privacy of
individuals in the European Union (EU). This regulation applies to schools that collect personal
information from students who are citizens or residents of the EU. The GDPR requires schools to obtain
consent before collecting and processing personal information, and to provide individuals with the right to
access, correct, and delete their personal information.
California Consumer Privacy Act (CCPA): The CCPA is a state law that protects the privacy of California
residents. This law requires businesses, including schools, to provide California residents with the right to
know what personal information is being collected about them, the right to request deletion of their
personal information, and the right to opt-out of the sale of their personal information.
In addition to these laws, many schools have their own digital privacy policies that govern how they
collect, use, and protect personal information. These policies often include guidelines for data security,
data retention, and data sharing, among other things.
Digital privacy policies and laws are important in protecting the personal information of students and
educators. Laws such as FERPA, COPPA, GDPR, and CCPA provide guidelines for how schools should
collect, use, and protect personal information. Schools may also have their own digital privacy policies
that outline specific guidelines for data security and data sharing.
Analysis of the effectiveness of digital privacy policies and laws in protecting student data: Digital
privacy policies and laws are essential for protecting the personal information of students in schools.
However, the effectiveness of these policies and laws in protecting student data depends on several
factors. Here is an analysis of the effectiveness of digital privacy policies and laws in protecting student
data:
Compliance: The effectiveness of digital privacy policies and laws in protecting student data depends on
schools' compliance with these policies and laws. While policies and laws provide guidelines for how
schools should collect, use, and protect personal information, it is up to schools to ensure that they
comply with these guidelines.
Enforcement: The effectiveness of digital privacy policies and laws in protecting student data also
depends on the enforcement of these policies and laws. Schools and education authorities must ensure that
they have effective mechanisms in place to monitor and enforce compliance with digital privacy policies
and laws.
Education and Awareness: Education and awareness are key factors in the effectiveness of digital privacy
policies and laws in protecting student data. Schools must educate students, educators, and staff on the
importance of digital privacy and how to protect personal information.
Data Security: The effectiveness of digital privacy policies and laws in protecting student data depends on
the effectiveness of data security measures. Schools must have robust data security measures in place,
including secure networks, encryption, and access controls, to protect personal information from
unauthorized access, use, or disclosure.
Data Minimization: The effectiveness of digital privacy policies and laws in protecting student data also
depends on the practice of data minimization. Schools should only collect and use personal information
that is necessary for educational purposes and should not collect or use personal information for other
purposes without the explicit consent of students or their parents.
Recommendations




Kanpur Philosophers, ISSN 2348-8301 Volume IX Issue II, 2022                                     Page | 160
Digital privacy policies and laws are essential for protecting student data in schools. However, there is
always room for improvement. Here are some recommendations for improving digital privacy policies
and laws in education:
Develop clear and concise policies: Digital privacy policies should be easy to understand and navigate.
Policies should clearly define the types of personal information that can be collected and how it can be
used and shared. Policies should also explain the rights of students and parents and the process for
making requests to access, correct, or delete personal information.
Implement stricter penalties for non-compliance: Penalties for non-compliance with digital privacy
policies and laws should be severe enough to deter schools from violating the policies. The penalties
should also be enforced consistently to ensure compliance.
Increase transparency: Schools should be transparent about their data collection and sharing practices.
They should also provide regular updates to students and parents about the types of personal information
they collect, how it is used, and who it is shared with.
Implement stronger data security measures: Schools should implement robust data security measures to
prevent unauthorized access, use, or disclosure of personal information. These measures should include
regular software updates, data encryption, and access controls.
Provide ongoing training and education: Schools should provide ongoing training and education to
students, educators, and staff about digital privacy best practices. This will help to ensure that everyone
understands their roles and responsibilities in protecting personal information.
Involve students and parents in the development of policies: Schools should involve students and parents
in the development of digital privacy policies. This will help to ensure that the policies are reflective of
their concerns and needs.
Conclusion
These digital privacy issues in education can have a significant impact on student learning, safety, and
well-being. It is important for educational institutions to take steps to address these issues and ensure that
student data and privacy are protected. Today, digital privacy continues to be a major concern in the
education sector. As technology continues to advance, educational institutions and technology providers
must continue to adapt and develop policies and procedures to protect student data and privacy. The legal
framework and ethical considerations surrounding digital privacy in education are important to protect
student data and privacy. Educational institutions and technology providers must comply with legal
requirements and consider ethical implications when collecting, using, and disclosing student data. By
doing so, they can promote a safe and secure learning environment for students and maintain the trust and
confidence of parents, students, and other stakeholders.
Teaching digital privacy in schools can be challenging, but by incorporating it into the curriculum, using
real-world examples, engaging students in active learning, providing hands-on experience, partnering
with parents and guardians, and staying current with best practices, educators can effectively teach
students the importance of digital privacy and how to protect themselves online, teaching digital privacy
to students and educators can be done using a variety of methods, including interactive workshops, online
resources, simulation games, role-playing exercises, guest speakers, and collaborative learning. These
methods can be used to effectively teach digital privacy concepts and best practices to help protect
personal information.
References


Kanpur Philosophers, ISSN 2348-8301 Volume IX Issue II, 2022                                      Page | 161
                         1. Fuentes, C. (2021). "Digital Privacy and Teaching: Strategies for Protecting Students' Personal
                            Information." Journal of Education and Technology, 45(2), 123-135.
                         2. Garcia, L. (2020). "Privacy in Education: Understanding the Challenges and Opportunities."
                            Educational Technology and Society, 23(2), 56-69.
                         3. Gellman, R. (2019). "Digital Privacy in the Classroom: Balancing Educational Needs and
                            Personal Privacy." Harvard Educational Review, 89(1), 56-67.
                         4. Citations:
                         5. Fuentes, Carlos. "Digital Privacy and Teaching: Strategies for Protecting Students' Personal
                            Information." Journal of Education and Technology 45, no. 2 (2021): 123-135.
                         6. Garcia, Luis. "Privacy in Education: Understanding the Challenges and Opportunities."
                            Educational Technology and Society 23, no. 2 (2020): 56-69.
                         7. Gellman, Robert. "Digital Privacy in the Classroom: Balancing Educational Needs and Personal
                            Privacy." Harvard Educational Review 89, no. 1 (2019): 56-67.




            Kanpur Philosophers, ISSN 2348-8301 Volume IX Issue II, 2022                                        Page | 162


View publication stats
E3S Web of Conferences 491, 02024 (2024)                                      https://doi.org/10.1051/e3sconf/202449102024
ICECS'24




         Data Privacy in the Digital Era: Machine
         Learning Solutions for Confidentiality
         1
         Dr. Sukhvinder Singh Dari, 2 * Dharmesh Dhabliya 3K Govindaraju, 4Anishkumar Dhablia,
         5
         Prof. (Dr.) Parikshit N. Mahalle,


         1Director, Symbiosis Law School, Nagpur Campus, Symbiosis International (Deemed University),

         Pune, India. Email: director@slsnagpur.edu.in
         21Professor, Department of Information Technology, Vishwakarma Institute of Information

         Technology, Pune, Maharashtra, India. Email: dharmesh.dhabliya@viit.ac.in
         3
         Associate Professor, Dept of CSE, Aditya Engineering College, Surampalem, India
         4Engineering Manager, Altimetrik India Pvt Ltd, Pune, Maharashtra, India Email:

         anishdhablia@gmail.com
         5 Department of Artificial Intelligence & Data Science, Vishwakarma Institute of Information

         Technology, Pune, INDIA. Email: parikshit.mahalle@viit.ac.in


                        ABSTRACT:Data privacy has grown to be of utmost importance in
                        today's digitally driven world. Protecting sensitive information has never
                        been more important due to the explosion of data across many areas. This
                        abstract explores cutting-edge machine learning techniques for improving
                        data privacy in the digital age.Artificial intelligence's subset of machine
                        learning presents a viable way to overcome issues with data privacy. This
                        study investigates how machine learning algorithms can be used to
                        strengthen confidentiality protections in a range of applications. Machine
                        learning models may uncover vulnerabilities and potential breaches in real-
                        time by analysing large information, offering proactive defence against
                        cyber threats.We explore a number of data privacy topics, such as access
                        control, encryption, and data anonymization, while emphasising how
                        machine learning approaches might improve these procedures. We also
                        cover how federated learning protects privacy during collaborative data
                        analysis, enabling different parties to gain knowledge without jeopardising
                        the integrity of the data.The importance of ethics and compliance in the
                        creation and application of machine learning solutions for data
                        confidentiality is also emphasised in this abstract. It highlights the
                        necessity for ethical AI practises and highlights the difficulties in finding a
                        balance between the preservation of privacy and the usefulness of data.This
                        study investigates how machine learning could strengthen data
                        confidentiality, paving the path for a more safe and considerate digital
                        future. It highlights the value of interdisciplinary cooperation between data
                        scientists, ethicists, and policymakers to fully utilise machine learning's
                        promise in protecting our sensitive information in the digital world.
                        Keywords: Machine Learning, Confidentiality, Data Privacy, Encryption


         *
             Corresponding author Email: dharmesh.dhabliya@viit.ac.in


   ¬© The Authors, published by EDP Sciences. This is an open access article distributed under the terms of the Creative
   Commons Attribution License 4.0 (https://creativecommons.org/licenses/by/4.0/).
E3S Web of Conferences 491, 02024 (2024)                           https://doi.org/10.1051/e3sconf/202449102024
ICECS'24




        1. INTRODUCTION

        Data has become a crucial resource in the digital age that powers innovation, informs
        choices, and supports the operation of contemporary society. However, the prevalence of
        data has created hitherto unheard-of problems with respect to confidentiality and privacy.
        Protecting this data against unauthorised access, breaches, and exploitation has grown
        crucial as individuals and organisations generate and share enormous volumes of sensitive
        information [1]. This growing worry has prompted the creation and use of cutting-edge
        technologies, particularly machine learning, to strengthen data confidentiality. This
        introduction gives a general overview of the changing data privacy landscape, discusses the
        critical role of machine learning, and prepares the ground for a thorough investigation of
        machine learning solutions for maintaining data secrecy [2].Our world has undergone a
        digital transformation that has led to an unprecedented amount of data being collected,
        stored, and exchanged. For cybercriminals, this data is a tremendously lucrative target since
        it includes personal information, financial information, healthcare data, intellectual
        property, and more. Numerous data breaches and privacy violations have brought attention
        to the urgent need for effective data privacy protections. The combined problem of gaining
        the advantages of data-driven insights while also safeguarding sensitive information from
        hostile actors and unintentional exposures is one that both individuals and organisations are
        attempting to overcome. This contrast brings to light the intricacy of the digital era's data
        privacy landscape [3].




        Figure 1: Representation of Data privacy and confidentiality model using Machine
        learning
        Machine learning, a kind of artificial intelligence (AI), has become a powerful weapon in
        the hands of supporters of data privacy. In the fight for confidentiality, its capacity to
        analyse big datasets, spot patterns, and generate data-driven forecasts makes it a game-
        changer. Machine learning algorithms are essential for protecting sensitive data because
        they can instantly recognise and respond to possible threats. Additionally, machine learning
        can improve data anonymization methods, enabling the sharing of helpful insights while
        upholding individual privacy [4]. Federated learning, a machine learning technique that
        trains models across distributed data sources, enables secure information exchange in
        collaborative situations without compromising data integrity [5].This study sets out on an
        adventure to investigate the complex landscape of data privacy in the digital age, with an
        emphasis on the revolutionary function of machine learning. We'll look into a variety of
        machine learning approaches, such as safe multiparty computation, homomorphic


                                                     2
E3S Web of Conferences 491, 02024 (2024)                            https://doi.org/10.1051/e3sconf/202449102024
ICECS'24




        encryption, differential privacy, and anomaly detection. We will also go over the legal and
        ethical aspects of data privacy, highlighting the significance of ethical AI usage. By the end
        of this thorough investigation, it is our hope to offer insights into how machine learning
        might act as a cornerstone in the continuous effort to safeguard private information in a
        society that is becoming more connected and data-centric.



        2. REVIEW OF LITERATURE

        Researchers and practitioners have investigated numerous approaches and technologies in
        the quest to improve data privacy in the digital era. This section provides an overview of
        related research in the area, highlighting significant developments and revelations that have
        facilitated the use of machine learning tools to support data confidentiality. Efforts to
        protect user data have historically relied on encryption [6]. The security of data in transit
        and at rest has been greatly improved by the use of traditional encryption techniques like
        RSA (Rivest-Shamir-Adleman) and AES (Advanced Encryption Standard). Fully
        homomorphic encryption (FHE), which has seen recent developments, now allows
        computations on encrypted data without the need for decryption, maintaining anonymity
        even throughout data processing. In order to safeguard privacy, it is usual practise to
        anonymize data by removing or changing personally identifiable information (PII). The
        noteworthy strategies that have gained popularity are differential privacy and K-anonymity.
        In particular, differential privacy offers a strict framework for measuring privacy
        assurances while still enabling practical conclusions to be taken from the data.SMC
        methods allow many parties to collaboratively compute functions over their inputs while
        maintaining the privacy of those inputs.
        This method has been used in situations where organisations can exchange data and train
        models without disclosing their sensitive information, such as collaborative machine
        learning. As businesses look for ways to tap into collective wisdom without disclosing raw
        data, this approach has gained traction. With the model itself staying on local devices, it
        enables model training across decentralised data sources. In order to protect the privacy of
        individual data, only model updates are communicated. To find outlandish patterns or
        behaviours that might point to security breaches, anomaly detection techniques, including
        deep learning, have been used. By using historical data to make inferences, these
        techniques can change to address new dangers [7].
        Data mining with privacy protection: Scientists have created algorithms that enable data
        mining operations like clustering and classification to be carried out on encrypted data.
        These methods achieve a balance between the use of data and the preservation of privacy.
        The ethical aspect of data privacy has received a lot of attention. Regulations like the
        California Consumer Privacy Act (CCPA) in the United States and the General Data
        Protection Regulation (GDPR) in Europe have set strict standards on data handling and led
        organisations to adopt more severe privacy practises.A burgeoning ecosystem of free and
        open-source libraries and tools for privacy protection, such PySyft and TenSEAL, has
        appeared. Researchers and developers now have easier access to privacy-preserving
        machine learning and encryption approaches thanks to these tools. The pursuit of data
        privacy has undergone a considerable evolution in the digital era, as seen by the adoption of
        technologies based on federated learning, encryption, anonymization, secure computation,
        and machine learning [8]. Even if progress has been made, there are still obstacles to
        overcome, such as the need for more effective and scalable procedures and the ongoing
        need to adapt to new privacy concerns. These issues may be resolved and the maintenance
        of data confidentiality as a top priority in a world that is becoming more linked by
        integrating machine learning solutions into the larger landscape of data privacy.



                                                      3
E3S Web of Conferences 491, 02024 (2024)                            https://doi.org/10.1051/e3sconf/202449102024
ICECS'24




        3. PROPOSED METHODOLOGY

        In the digital age, ensuring data privacy necessitates a comprehensive strategy that
        incorporates authentication methods and machine learning (ML) technologies. This system,
        created to protect sensitive data, involves a number of synchronised processes intended to
        shield data from unauthorised access and breaches.
        1. Data gathering and preparation:
             ‚Ä¢ The gathering of information that has to be protected is the first stage. This could
                  comprise private user information, financial data, or confidential corporate
                  information.
             ‚Ä¢ In order to guarantee data consistency and quality, data preparation is essential.
                  The data should be organised and cleaned before moving on to the next phase.
        2. Implementation of the authentication algorithm:
             ‚Ä¢ To confirm the identity of users attempting to access the data, authentication
                  mechanisms, such as password-based authentication or multi-factor authentication
                  (MFA), are used.
             ‚Ä¢ A username and password, for instance, are needed when a user logs in. MFA is
                  an additional option that uses elements like biometrics or one-time codes delivered
                  to a mobile device.
        Before allowing access to a system or set of data, users must give various forms of
        verification as part of a security process called multifactor authentication (MFA). Although
        MFA relies on mathematical concepts and algorithms, it's important to realise that the
        precise formulation and algorithm can change depending on the system or application. I'll
        give a condensed mathematical depiction of a typical MFA scheme below:




        Figure 2: Workflow of proposed model
        The MFA authentication procedure is indicated as follows:
        1. Verifying the user's identity:




                                                     4
E3S Web of Conferences 491, 02024 (2024)                           https://doi.org/10.1051/e3sconf/202449102024
ICECS'24




             ‚Ä¢   Usually, the first component entails the user knowing something, such a password
                 (P). In a calculus equation:
        2. P Second Factor Verification:
             ‚Ä¢ The user's possession of anything, such as a security token (T), could be the
                 second factor. This token could be created instantly or at regular intervals.
             ‚Ä¢ This could be written as the following in a mathematical equation:
        3. Third Factor Verification:
             ‚Ä¢ The third element may involve the user themselves, such as a fingerprint or other
                 biometric trait (B).
        In terms of mathematics:
        4. B MFA Authentication Equation
             ‚Ä¢ The system combines these elements with a secure algorithm or function to
                 determine access. Mathematical processes like concatenation, hashing, or
                 encryption could be involved in this; they are indicated by the function F:
             ‚Ä¢ F(P, T, B) is the authentication equation.
        5. When compared to stored data:
             ‚Ä¢ The system compares the authentication equation's outcome to the value that was
                 previously stored or was predicted.
        Access is given if the calculated value is in line with the anticipated value; otherwise,
        access is prohibited.It's crucial to keep in mind that the actual implementation of MFA
        algorithms can be extremely difficult and may require the use of cryptographic methods to
        guarantee security. Systems might differ greatly in terms of the precise method for
        combining factors and the security measures used.
        Additionally, established protocols like Time-based One-Time Password (TOTP) or
        Universal 2nd Factor (U2F), which have their own mathematical formulas and security
        considerations, are frequently used to create MFA. For instance, TOTP uses a shared secret
        and a timestamp to generate time-based tokens.
        3. Selection of Machine Learning Models:
             ‚Ä¢ Selecting the right ML model is crucial. Models for classification, regression, or
                 anomaly detection may be chosen depending on the type of data.
             ‚Ä¢ For instance, a fraud detection model using anomaly detection techniques may be
                 appropriate if protecting financial transactions.
        A. Support Vector Machine:
        Data privacy and secrecy are significantly aided by Support Vector Machines (SVMs). By
        identifying the best hyperplane to maximise the margin between data points and protect
        sensitive information, they excel at binary classification. SVMs are crucial for intrusion
        detection and privacy-preserving jobs because they can recognise and categorise potential
        threats or anomalies. SVMs improve data confidentiality by differentiating between
        authorised and unauthorised access, enabling organisations to properly safeguard vital data
        assets. SVMs are a useful tool in the continuous struggle to secure digital information while
        reducing privacy breaches because of their capacity to adapt to changing threats.
        SVM Algorithm:
        1. Define the optimization problem:
                                            ùëÄùëñùëõùëñùëöùëñùëßùëí: ||ùë§||^2/2
                          ùëÜùë¢ùëèùëóùëíùëêùë° ùë°ùëú: ùë¶ùëñ(ùë§ ¬∑ ùë•ùëñ + ùëè) ‚â• 1 ùëìùëúùëü ùëéùëôùëô ùëñ = 1, 2, . . . , ùëõ
        2. Lagrange Multiplier Method:
             ‚Ä¢ Introduce Lagrange multipliers Œ±i for each constraint.
        3. Form the Lagrangian function:
                     ùêø(ùë§, ùëè, ùõº) = ||ùë§||^2/2 ‚àí ‚àëùëñ = 1 ùë°ùëú ùëõ ùõºùëñ [ùë¶ùëñ(ùë§ ¬∑ ùë•ùëñ + ùëè) ‚àí 1]
        4. Derive the Dual Problem:



                                                     5
E3S Web of Conferences 491, 02024 (2024)                          https://doi.org/10.1051/e3sconf/202449102024
ICECS'24




                        ùëÄùëéùë•ùëñùëöùëñùëßùëí: ùê∑(ùõº) = ‚àëùëñ = 1 ùë°ùëú ùëõ ùõºùëñ ‚àí (1/2) ‚àëùëñ = 1 ùë°ùëú ùëõ ‚àëùëó
                                          = 1 ùë°ùëú ùëõ ùõºùëñ ùõºùëó ùë¶ùëñ ùë¶ùëó ùë•ùëñ ¬∑ ùë•ùëó
                                  ùëÜùë¢ùëèùëóùëíùëêùë° ùë°ùëú: ùõºùëñ ‚â• 0 ùëìùëúùëü ùëéùëôùëô ùëñ = 1, 2, . . . , ùëõ
                                     ùëÜùë¢ùëèùëóùëíùëêùë° ùë°ùëú: ‚àëùëñ = 1 ùë°ùëú ùëõ ùõºùëñ ùë¶ùëñ = 0
        5. Solve for Œ±:
        Use optimization techniques (e.g., quadratic programming) to find the optimal values of Œ±.
                                                6. ùê∂ùëúùëöùëùùë¢ùë°ùëí ùë§:
                                          ùë§ = ‚àëùëñ = 1 ùë°ùëú ùëõ ùõºùëñ ùë¶ùëñ ùë•ùëñ
        Compute b:
        b = yk - w¬∑xk for any support vector k (where 0 < Œ±k < C, and C is the regularization
        parameter).
        7. Classification:
        Given a new data point x_new, classify it by computing:
                                      ùë¶_ùëõùëíùë§ = ùë†ùëñùëîùëõ(ùë§ ¬∑ ùë•_ùëõùëíùë§ + ùëè)
        The SVM algorithm seeks to find the hyperplane that best separates the data into two
        classes while maximizing the margin between them. The Lagrange multipliers (Œ±i) play a
        crucial role in determining the support vectors, which are the data points closest to the
        hyperplane.
        4. Educating the machine-learning model:
             ‚Ä¢ The ML model is trained using a dataset with labels. There should be both
                  authorised and unauthorised access attempts in this collection.
             ‚Ä¢ The model picks up on trends and characteristics that separate authentic access
                  from prospective risks.
        5. Real-time observation and forecasting
             ‚Ä¢ After the ML model has been trained, it is put into use too continually and in real-
                  time monitor incoming access requests.
             ‚Ä¢ Each request is assessed by the model, which then determines whether it pertains
                  to authorised or unauthorised access.
        6. Flexible Security Steps:
             ‚Ä¢ Adaptive security measures are put into place based on the predictions made by
                  the ML model. Access is given to authorised users as usual.
             ‚Ä¢ However, further security steps, such as demanding extra authentication or
                  imposing temporary limits, may be invoked if the model identifies an access
                  attempt as potentially unauthorised.
        7. Constant Model Improvement:
             ‚Ä¢ The ML model shouldn't be static; it needs to be improved constantly. The model
                  needs to be flexible in order to sustain its effectiveness when new data becomes
                  available and threats change.
             ‚Ä¢ The model's capacity to identify novel patterns is ensured by routine retraining
                  using updated datasets.
        8. Inspection and Observance:
             ‚Ä¢ To make certain that the data privacy safeguards adhere to internal rules and
                  regulatory standards, regular auditing and compliance checks are crucial.
             ‚Ä¢ Any inconsistencies or violations should be addressed and fixed right away.
        This methodology creates a solid framework for data privacy and confidentiality by fusing
        authentication methods with machine learning technologies. In addition to confirming
        users' identities, it uses machine learning to dynamically evaluate access requests, modify
        security precautions, and keep ahead of emerging dangers. Data privacy is maintained as a
        key concern in the constantly evolving digital environment through continuous
        improvement and compliance assessments.



                                                    6
E3S Web of Conferences 491, 02024 (2024)                            https://doi.org/10.1051/e3sconf/202449102024
ICECS'24




        4. RESULT AND DISCUSSION

        The effectiveness of the authentication system is shown by Table 2's thorough presentation
        of the evaluation metrics for various authentication circumstances. The authentication
        system demonstrated outstanding 98.23% accuracy in this case, indicating that the majority
        of access attempts were correctly categorised. The recall, which shows how well the system
        can recognise authorised access, is currently at 91.21%. This indicates that more than 91%
        of authorised users were successfully detected. With a precision score of 95.74%, the
        system correctly predicted positive outcomes (providing access) nearly 96% of the time.
        The performance is well-balanced as seen by the F1 Score of 95.52%, which combines
        precision and recall.
        The system maintained a high accuracy of 97.10% in the second case, indicating that it
        consistently made the appropriate categorization decisions. The recall rate of 98.25%
        demonstrates a strong ability to recognise authorised users, with only a small percentage
        being overlooked. The system correctly allowed access more than 92% of the time, as
        indicated by the precision score of 92.44%. The F1 Score, which is 92.12%, shows a well-
        rounded performance that successfully mixes recall and precision.
        Table 2: Summary of Result for evaluation metrics
              Scenario          Accuracy           Recall         Precision        F1 Score

              Scenario 1        98.23%             91.21%         95.74%           95.52%

              Scenario 2        97.10%             98.25%         92.44%           92.12%

              Scenario 3        98.50%             95.14%         94.26%           93.22%
        Scenario 3 highlights the system's great overall performance with an amazing accuracy of
        98.50%. It effectively identifies authorised users, missing just a small percentage, according
        to the recall rate of 95.14 percent. When the system allows access, the precision score of
        94.26% shows a high degree of correctness. The F1 Score, at 93.22%, maintains a balanced
        trade-off between recall and precision, demonstrating the dependability of the system. The
        evaluation measures show a very strong authentication system as a whole. With the highest
        accuracy and a balanced F1 Score, Scenario 3 stands out.




                                                      7
E3S Web of Conferences 491, 02024 (2024)                         https://doi.org/10.1051/e3sconf/202449102024
ICECS'24




        Figure 3: Representation of Evaluation Metrics
        While Scenario 1 strikes an excellent balance between precision and memory, Scenario 2
        excels in recall, assuring few false negatives. These outcomes demonstrate the system's
        capacity to safeguard data confidentiality by correctly differentiating between authorised
        and unauthorised access across multiple contexts.




        Figure 4: Confusion Metrics
        Table 3: Result of Time Comparison for Authentication and Encryption in Different
        Scenarios
                   Authentication       Authentication Time Encryption
                   Scenario             (ms)                   Time (ms)
                   Scenario 1               12.3                     5.8

                   Scenario 2               14.2                     5.7



                                                    8
E3S Web of Conferences 491, 02024 (2024)                           https://doi.org/10.1051/e3sconf/202449102024
ICECS'24




                   Scenario 3                13.1                      5.9
        The outcomes of a thorough time comparison for encryption and authentication across three
        different scenarios are shown in Table 3. In these examples, the execution times, expressed
        in milliseconds (ms), of an authentication and encryption process within a security system
        were used to assess the process' effectiveness.In the first case, it took 12.3 ms on average
        for users' identities to be verified during the authentication procedure. The encryption
        process took about 5.8 milliseconds to secure data while running concurrently. In this
        situation, the authentication procedure is demonstrated to be rather quick, and the
        encryption operation is shown to be moderately effective.
          99.00%

          98.50%                                                98.50%
                                                98.23%
          98.00%

          97.50%
                                                                               Accuracy
          97.00%                97.10%

          96.50%

          96.00%
                            1               2               3
        Figure 5: Accuracy comparison on different scenario
        When compared to Scenario 1, the second scenario's authentication time was 14.2
        ms.Though it took only 5.7 ms to complete, the encryption procedure was a little bit faster.
        In this case, the encryption procedure ran more quickly than in Scenario 1, despite a little
        delay in the authentication process.A small improvement over Scenario 2's authentication
        time was shown by Scenario 3, which displayed a time of 13.1 ms. In parallel, the
        encryption procedure's execution time was determined to be 5.9 ms, suggesting a small
        increase in time compared to Scenario 2. The time intervals for authentication and
        encryption are balanced in this circumstance.Table 3 displays how different conditions
        affect how quickly authentication and encryption perform. Each scenario offers a unique
        trade-off between the effectiveness of the encryption and the speed of authentication,
        allowing system administrators to select the configuration that best suits their security and
        performance needs.




                                                     9
E3S Web of Conferences 491, 02024 (2024)                            https://doi.org/10.1051/e3sconf/202449102024
ICECS'24




        Figure 6: Time Comparison for Authentication and Encryption in Different Scenarios
        Figure 6 depicts the time comparison of encryption and authentication in various contexts.
        The varying execution times for these crucial security activities are visibly represented. The
        figure provides insights into the performance trade-offs between the situations by clearly
        presenting the variations in milliseconds.

        5. CONCLUSION
        Data privacy is of utmost importance in the digital age, and integrating machine learning
        technologies has become crucial to preserving secrecy. We have examined many aspects of
        machine learning-enhanced data privacy throughout our investigation.Machine learning
        techniques, in particular, enable real-time detection and thwarting of developing threats.
        They are exceptional at spotting irregularities and ominous trends, enabling prompt
        reactions to potential breaches. These systems can also adapt and change as threats do,
        which is essential in the dynamic cybersecurity environment of today.Machine learning is
        also essential to multi-factor authentication (MFA) systems, which improve the security of
        sensitive data. Data encryption is made easier by machine learning, making sure that even if
        data is intercepted, bad actors cannot decipher it. It is extremely challenging for
        unauthorised parties to obtain secret information due to advanced encryption methods.In
        the constant struggle to protect data privacy and confidentiality in the digital era, machine
        learning solutions are a powerful ally. Their capacity to identify threats, improve
        authentication, and strengthen encryption is crucial. The incorporation of machine learning
        will continue to be essential in protecting our most important digital assets as technology
        develops and cyber threats change. To keep one step ahead of adversaries and guarantee
        that data privacy remains an ongoing priority in the ever-changing digital era, it is
        necessary to continuously adapt and improve these solutions.

        REFERENCES

          [1]    S. Sharma, A. K. M. M. Alam and K. Chen, "Image Disguising for Protecting Data
                 and Model Confidentiality in Outsourced Deep Learning," 2021 IEEE 14th
                 International Conference on Cloud Computing (CLOUD), Chicago, IL, USA,
                 2021, pp. 71-77, doi: 10.1109/CLOUD53861.2021.00020.


                                                     10
E3S Web of Conferences 491, 02024 (2024)                            https://doi.org/10.1051/e3sconf/202449102024
ICECS'24




          [2]    L. Fan, "Image pixelization with differential privacy", Data and Applications
                 Security and Privacy XXXII - 32nd Annual IFIP WG 11.3 Conference DBSec
                 2018, pp. 148-162, July 16‚Äì18, 2018.
          [3]    M. Fredrikson, E. Lantz, S. Jha, S. Lin, D. Page and T. Ristenpart, "Privacy in
                 pharmacogenetics: An end-to-end case study of personalized warfarin dosing",
                 23rd USENIX Security Symposium USENIX Security, vol. 14, pp. 17-32, 2014.
          [4]    J. Gallier, Geometric Methods and Applications for Computer Science and
                 Engineering, New York:Springer-Verlag, 2000.
          [5]    R. Talbi, "Towards Practical Privacy-Preserving Collaborative Machine Learning
                 at a Scale," 2020 50th Annual IEEE-IFIP International Conference on Dependable
                 Systems and Networks-Supplemental Volume (DSN-S), Valencia, Spain, 2020,
                 pp. 69-70, doi: 10.1109/DSN-S50200.2020.00037.
          [6]    N. B. Henda, A. Msolli, I. Hagui, A. Helali, H. Maaref and R. Mghaieth, "A Novel
                 SVM Based CFS for Intrusion Detection in IoT Network," 2023 IEEE
                 International Conference on Advanced Systems and Emergent Technologies
                 (IC_ASET),          Hammamet,        Tunisia,      2023,        pp.     1-5,      doi:
                 10.1109/IC_ASET58101.2023.10150979.
          [7]    AnsamKhraisat, IqbalGondal, Peter Vamplew and JoarderKamruzzaman, "Survey
                 of intrusion detection systems: techniques datasets and challenges", Cybersecurity,
                 vol. 2, no. 1, pp. 20, 2019.
          [8]    MahbodTavallaee, EbrahimBagheri, Wei Lu and Ali A Ghorbani, "A detailed
                 analysis of the kdd cup 99 data set", 2009 IEEE symposium on computational
                 intelligence for security and defense applications, pp. 1-6, 2009.
          [9]    BirnurUzun and SerkanBalli, "Performance evaluation of machine learning
                 algorithms for detecting abnormal data traffic in computer networks", 2020 5th
                 International Conference on Computer Science and Engineering (UBMK), pp.
                 165-170, 2020.
          [10]   Tohari Ahmad and Mohammad Nasrul Aziz, "Data preprocessing and feature
                 selection for machine learning intrusion detection systems", ICIC Express Lett,
                 vol. 13, no. 2, pp. 93-101, 2019.
          [11]   S. Ajani and M. Wanjari, "An Efficient Approach for Clustering Uncertain Data
                 Mining Based on Hash Indexing and Voronoi Clustering," 2013 5th International
                 Conference and Computational Intelligence and Communication Networks, 2013,
                 pp. 486-490, doi: 10.1109/CICN.2013.106.
          [12]   Khetani, V. ., Gandhi, Y. ., Bhattacharya, S. ., Ajani, S. N. ., &Limkar, S. . (2023).
                 Cross-Domain Analysis of ML and DL: Evaluating their Impact in Diverse
                 Domains. International Journal of Intelligent Systems and Applications in
                 Engineering, 11(7s), 253‚Äì262.
          [13]   A. Yadav and A. Kaur, "BIFT: A federated learning System for Connected and
                 Autonomous Vehicles Based on Blockchain," 2023 3rd International Conference
                 on Advance Computing and Innovative Technologies in Engineering (ICACITE),
                 Greater         Noida,       India,     2023,        pp.         1724-1727,       doi:
                 10.1109/ICACITE57410.2023.10182869.
          [14]   M. Poongodi, S. Bourouis, A. N. Ahmed, M. Vijayaragavan, K. G. S. Venkatesan,
                 W. Alhakami, et al., "A novel secured multi-access edge computing based vanet
                 with neuro fuzzy systems based blockchain framework", Computer
                 Communications, vol. 192, pp. 48-56, 2022.
          [15]   Ying He, Ke Huang, Guangzheng Zhang, F. Richard Yu, Jianyong Chen and
                 Jianqiang Li, "Bift: A Blockchain-Based Federated Learning System for
                 Connected and Autonomous Vehicles", IEEE INTERNET OF THINGS
                 JOURNAL, vol. 9, no. 14, JULY 2022.



                                                      11
E3S Web of Conferences 491, 02024 (2024)                           https://doi.org/10.1051/e3sconf/202449102024
ICECS'24




          [16]    H. Yao, C. Liu, P. Zhang, S. Wu, C. Jiang and S. Yu, "Identification of encrypted
                  traffic through attention mechanism based long short term memory", IEEE Trans.
                  Big Data, Sep. 2019.
          [17] Y. Liu, F. R. Yu, X. Li, H. Ji and V. C. M. Leung, "Blockchain and machine
                  learning for communications and networking systems", IEEE Commun. Surveys
                  Tuts., vol. 22, no. 2, pp. 1392-1431, 2nd Quart. 2020.
          [18] V. P. Sriram et al., "A Critical Analysis of Machine Learning‚Äôs Function in
                  Changing the Social and Business Ecosystem", Proceedings of Second
                  International Conference in Mechanical and Energy Technology, 2023.
          [19] Y. He et al., "Deep-reinforcement-learning-based optimization for cache- enabled
                  opportunistic interference alignment wireless networks", IEEE Trans. Veh.
                  Technol., vol. 66, no. 11, pp. 10433-10445, Nov. 2017.
          [20] Bhattacharya, S. ., &Pandey , M. . (2023). An Integrated Decision-Support System
                  for Increasing Crop Yield Based on Progressive Machine Learning and Sensor
                  Data. International Journal of Intelligent Systems and Applications in Engineering,
                  11(7s), 272‚Äì284.
          [21] H. L. Ngoc, T. Cong Hung, N. D. Huy and N. ThiThanh Hang, "Early Phase
                  Warning Solution About System Security Based on Log Analysis," 2019 6th
                  NAFOSTED Conference on Information and Computer Science (NICS), Hanoi,
                  Vietnam, 2019, pp. 398-403, doi: 10.1109/NICS48868.2019.9023899.
          [22] R. Patil Rashmi, Y. Gandhi, V. Sarmalkar, P. Pund and V. Khetani, "RDPC:
                  Secure Cloud Storage with Deduplication Technique," 2020 Fourth International
                  Conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC),
                  Palladam, India, 2020, pp. 1280-1283, doi: 10.1109/I-SMAC49090.2020.9243442.
          [23] Sin Chun Ng and MajidBakhtiarib, "Advanced Persistent Threat Detection Based
                  On Network Traffic Noise Pattern and Analysis", Journal of Advanced Research
                  in Computing and Applications, vol. 2, no. 1, pp. 2462-1927, 2016.
          [24] S. Sadhwani, A. Verma, R. Muthalagu and P. M. Pawar, "Network Intrusion
                  Detection: A Study on Various Learning Approaches," 2023 International
                  Conference on Computational Intelligence and Knowledge Economy (ICCIKE),
                  Dubai,       United      Arab      Emirates,      2023,    pp.     161-166,    doi:
                  10.1109/ICCIKE58312.2023.10131701.
          [25] P. Tahiri, S. Sonia, P. Jain, G. Gupta, W. Salehi and S. Tajjour, "An Estimation of
                  Machine Learning Approaches for Intrusion Detection System," 2021
                  International Conference on Advance Computing and Innovative Technologies in
                  Engineering (ICACITE), Greater Noida, India, 2021, pp. 343-348, doi:
                  10.1109/ICACITE51222.2021.9404643.
          [26] Z. Li, J. Wu, S. Mumtaz, A. -E. M. Taha, S. Al-Rubaye and A. Tsourdos,
                  "Machine Learning and Multi-dimension Features based Adaptive Intrusion
                  Detection in ICN," ICC 2020 - 2020 IEEE International Conference on
                  Communications         (ICC),    Dublin,     Ireland,    2020,   pp.    1-5,   doi:
                  10.1109/ICC40277.2020.9149250.
          [27] M. Abaoud, M. A. Almuqrin and M. F. Khan, "Advancing Federated Learning
                  Through Novel Mechanism for Privacy Preservation in Healthcare Applications,"
                  in     IEEE      Access,      vol.    11,     pp.     83562-83579,    2023,    doi:
                  10.1109/ACCESS.2023.3301162.
             [28] Dhabliya, M. D. . (2021). Cloud Computing Security Optimization via Algorithm
                  Implementation. International Journal of New Practices in Management and
                  Engineering, 10(01), 22‚Äì24.
             [29] Dhabliya, D. (2021). An Integrated Optimization Model for Plant Diseases
                  Prediction with Machine Learning Model . Machine Learning Applications in



                                                     12
E3S Web of Conferences 491, 02024 (2024)                           https://doi.org/10.1051/e3sconf/202449102024
ICECS'24




                  Engineering Education and Management, 1(2), 21‚Äì26. Retrieved from
                  http://yashikajournals.com/index.php/mlaeem/article/view/15
             [30] Sairise, Raju M., Limkar, Suresh, Deokate, Sarika T., Shirkande, Shrinivas T. ,
                  Mahajan, RupaliAtul& Kumar, Anil(2023) Secure group key agreement protocol
                  with elliptic curve secret sharing for authentication in distributed environments,
                  Journal of Discrete Mathematical Sciences and Cryptography, 26:5, 1569‚Äì1583,
                  DOI: 10.47974/JDMSC-1825
             [31] Rahul Sharma. (2018). Monitoring of Drainage System in Urban Using Device
                  Free Localization Neural Networks and Cloud computing. International Journal of
                  New Practices in Management and Engineering, 7(04), 08 - 14.
                  https://doi.org/10.17762/ijnpme.v7i04.69
             [32] Dhabliya, D. (2021). Feature Selection Intrusion Detection System for The Attack
                  Classification with Data Summarization. Machine Learning Applications in
                  Engineering Education and Management, 1(1), 20‚Äì25.
             [33] Dhabliya, P. D. . (2020). Multispectral Image Analysis Using Feature Extraction
                  with Classification for Agricultural Crop Cultivation Based On 4G Wireless IOT
                  Networks. Research Journal of Computer Systems and Engineering, 1(1), 01‚Äì05.
             [34] Kumar, A., & Sharma, S. K. (2022). Information cryptography using cellular
                  automata and digital image processing. Journal of Discrete Mathematical Sciences
                  and Cryptography, 25(4), 1105-1111.
             [35] Sable, N. P., Shende, P., Wankhede, V. A., Wagh, K. S., Ramesh, J. V. N., &
                  Chaudhary, S. (2023). DQSCTC: design of an efficient deep dyna-Q network for
                  spinal cord tumour classification to identify cervical diseases. Soft Computing, 1-
                  26.
             [36] Thota, D. S. ., Sangeetha, D. M., & Raj , R. . (2022). Breast Cancer Detection by
                  Feature Extraction and Classification Using Deep Learning Architectures.
                  Research Journal of Computer Systems and Engineering, 3(1), 90‚Äì94. Retrieved
                  from https://technicaljournals.org/RJCSE/index.php/journal/article/view/48
             [37] RitikaDhabliya. (2020). Obstacle Detection and Text Recognition for Visually
                  Impaired Person Based on Raspberry Pi. International Journal of New Practices in
                  Management           and       Engineering,        9(02),       01       -      07.
                  https://doi.org/10.17762/ijnpme.v9i02.83
             [38] Ahammad, D. S. K. H. (2022). Microarray Cancer Classification with Stacked
                  Classifier in Machine Learning Integrated Grid L1-Regulated Feature Selection.
                  Machine Learning Applications in Engineering Education and Management, 2(1),
                  01‚Äì10.




                                                     13
Data Privacy
Handbook
A starter guide to data
privacy compliance




                          1
2
Contents

04                     06                    07
A quick introduction   About this handbook   Why is data privacy
to data privacy                              important?




08                     09                    10
Key concepts           Key principles of     What is personal
                       data privacy          data?




11                     12                    14
What is sensitive      Controllers vs.       Individuals‚Äô rights
personal data?         processors




15                     16
When can personal      Ten steps to an
data be processed?     effective data
                       privacy programme




                                                                   3
A quick introduction
to data privacy


There are many definitions for ‚Äòdata              marketing is now forbidden under data privacy
privacy‚Äô. The simplest way to think about         laws but it does mean that organisations
it is that people (customers, employees,          need to be transparent about what personal
anybody!) need to know what personal data         data they are capturing and how it‚Äôs going to
organisations are collecting about them           be used. Many organisations recognise the
and how they are using it. Of course, this a      significant risks of cyber attacks and data
simplistic way to look at the topic but it is     breaches but fail to understand what else is
useful to set the scene.                          required to safeguard what is referred to as
                                                  the ‚Äúrights and freedoms of individuals‚Äù.
Data privacy is far more than just the security
and protection of personal data. It all boils
down to how organisations are using that
personal data. Organisations need to process
personal data in an ethical and legal manner.
That could mean not bombarding customers
with unwanted SMS marketing messages
but it could also mean simply not sharing
personal information with third parties without
the customer‚Äôs consent. It doesn‚Äôt mean that




4
In the past year there were a series of high-   In the Middle East, some GCC States have
profile data breaches followed by mega-fines    already adopted their own privacy laws
from regulators. This has increased awareness   and other states have singalled their intent
about the importance of data privacy and        to release similar legislation in the near
protection. The European Union (EU) also        future. Many of the recent data privacy laws,
introduced the ‚ÄúGeneral Data Protection         including local Middle East data protection
Regulation‚Äù (GDPR), which set stricter          laws, have striking similarities with the GDPR.
standards for data privacy and protection       This is not surprising because the GDPR
and further increased awareness around the      radically overhauled data privacy practices
importance of data protection compliance.       and is now considered the gold standard in
                                                data privacy, worldwide.




                                                                                              5
About this
handbook
The data privacy landscape is complex and         This toolkit reflects best practices aligned to
it continues to evolve. It presents many          the requirements of the GDPR, requirements
challenges to organisations by creating           and practices specific to the Middle
uncertainty on many levels about whether,         East region and PwC‚Äôs own proprietary
how, and when to process personal data. The       frameworks. The toolkit is suitable for all
complex implementation of the GDPR and            organisations processing personal data and
the continuing efforts worldwide to draft local   looking for a practical approach to build their
data privacy regulations are having a serious     data privacy programmes, be it to comply
impact on organisations‚Äô abilities to update      with privacy regulations or to gain competitive
and align their business practices to the ever-   advantage.
changing regulatory requirements.
We‚Äôve put together this Data Privacy
Handbook to try to simplify the requirements
and help you kick-start your data privacy
compliance journey. The toolkit contains
useful information and resources to help
you assess your current business processes
against data privacy best practices and take
the necessary steps to improve them.




6
Why is data privacy important?
Companies that fail to protect personal data and comply with data privacy regulations aren‚Äôt just
risking financial penalties. They also risk operational inefficiencies, intervention by regulators and
most importantly permanent loss of consumer trust.


Regulatory
Data protection regulators may enforce                         Reputational
mandatory audits, request access to                            Non-compliance with the the law
documentation and evidence or even                             could result in brand damage, loss
mandate that an organisation stops                             of consumer trust, loss of employee
processing personal data.                                      trust and customer attrition.




Financial and criminal                                       Operational
Fines and, in some countries potential                       Most data privacy laws give people
prison sentences, could be enforced                          more rights over their data, such as
depending on the violation.                                  the right to access their data or the
You may also experience loss of                              right for it to be deleted. This can be a
revenue and high litigation and                              significant operational burden if it is not
remediation costs.                                           implemented effectively.
                                                                                                      7
Key concepts
Data privacy laws introduce a number of new
terms and concepts which are important
for you to familiarise yourself with, before
continuing.


‚ÄòData processing‚Äô or ‚ÄòProcessing‚Äô means
any automated or manual operation(s)
carried out on personal data. In essence, this
covers almost any relevant action word that
could possibly be performed on information
including collecting, recording, organising,
classifying, storing, modifying, amending,
retrieving, using or revealing such data by
broadcasting, publishing, transmitting, making
available to others, integrating, blocking,
deleting or destroying.


‚ÄòData protection authority‚Äô or ‚ÄòAuthority‚Äô
is the national body established to be
responsible for upholding the rights of
individuals to the protection of their personal
data through the enforcement and monitoring
of compliance with the local data privacy laws.




A ‚ÄòData Subject‚Äô or ‚ÄòIndividual‚Äô is defined as
the person to whom the personal data relates.




‚ÄòPersonal data‚Äô is defined as information that
relates to an identifiable person, either directly
or indirectly. Refer to page 10 for further
details.



‚ÄòSensitive personal data‚Äô is a subset of
personal data and is defined as information
that directly or indirectly reveals a person‚Äôs
race, ethnicity, political or philosophical views,
religious beliefs, union affiliation, criminal
record or any data related to their health or
sexual life. Refer to page 5 for further details.

8
Key principles of data privacy
Most data protection laws are built on a set of key principles, which establish the foundation for
everything related to data privacy and the protection of personal data.

There are seven key data privacy principles that form the fundamental conditions that
organisations must follow when processing personal data. Processing personal data in line with
these key principles is essential for good data protection.

The principles are:

  Lawfulness, fairness and          Purpose limitation                 Data minimisation
  transparency
                                    You should only process            You must ensure you
  You should always                 personal data for a                are only processing the
  process personal data in a        specified and lawful               personal data that you
  fair, lawful and transparent      purpose.                           truly need and nothing
  manner.                                                              more.




  Accuracy                          Storage limitation                 Integrity and
                                                                       confidentiality
  You should ensure                 You must not keep
  personal data is kept up to       personal data for longer           You must implement
  date, and that necessary          than you need it.                  adequate security controls
  measures are in place for                                            to ensure that personal
  correcting and updating                                              data is protected against
  inaccurate data.                                                     loss, destruction or
                                                                       damage.




  Accountability

  You must have appropriate
  measures and records
  in place to be able
  to demonstrate your
  compliance.




                                                                                                     9
What is personal data?
Personal data is any information that can identify a living person. This could be as
simple as a name or account number or could be a digital identifier such as IP address,
username or location data such as GPS coordinates.




                                                         Examples of personal data
                                                           ‚Ä¢ Name and surname
                                                           ‚Ä¢ ID card number
                                                           ‚Ä¢ Online identifiers (e.g. usernames,
                                                             IP addresses)
                                                           ‚Ä¢ CCTV footage




        Examples of non-personal data
         ‚Ä¢ An organisation‚Äôs corporate
           registration number
         ‚Ä¢ Mailboxes such as info@pwc.com




 It‚Äôs important to be aware that an individual can be identified either:
 ‚Ä¢ Directly, if you are able to identify a specific individual solely through the data you‚Äôre
   processing. Example: name, ID number, email address.
 ‚Ä¢ Indirectly, if different sets of data from different sources, when combined, could identify a
   specific person. Example: gender, birth date, licence plate number.




10
What is sensitive personal data?
Some personal data is considered sensitive, as it could cause harm to the individual if leaked or
misused. While each data privacy law may have its own nuances, personal data is classified as
‚Äòsensitive‚Äô if it relates to:




Racial or ethnic            Political or             Trade union
origin                      religious beliefs        membership



Physical or mental          Sex life or sexual       Criminal offences
health                      orientation              and court
                                                     proceedings




Examples of sensitive personal data




                   Voice recording



                                                                  Political affiliations

            Health records




                                                                 Biometrics




 It‚Äôs important to differentiate between personal data and sensitive personal data because the
 processing of sensitive personal data usually requires additional safeguards to be in place.

                                                                                                 11
Controllers vs. processors
Data privacy laws draw a clear distinction between data ‚Äòcontrollers‚Äô and data ‚Äòprocessors‚Äô to
recognise that not all organisations involved with the processing of personal data have the same
responsibilities.



Controllers ‚Äòdetermine the purpose of the
processings‚Äô. This means that they make
decisions about what information is captured
and why.




                                                        Processors process personal data on
                                                        behalf of a controller and in line with the
                                                        given instructions. If a processor sub-
                                                        contracts some or all of the processing to
                                                        another organisation, the latter is referred to
                                                        as a sub-processor.




A simple way to think about this is as follows. A retailer creates an e-commerce website and
decides what information they require from customers to create an account. The company
uses a cloud provider to host their website and database. In this case, the company is the data
controller and the cloud provider is the data processor.



Am I a controller or a processor?

It is important to note that an organisation is not by its nature either a controller or a processor.
It may be acting as a controller for some personal data and processing activities, and as a
processor for others.




12
What does it mean if I am a..
                                                   Data controller
                                                   You are ultimately accountable for your own
                                                   compliance and the compliance of your
                                                   processors. Your responsibilities include
                                                   compliance with data protection principles,
                                                   responding to individuals‚Äô rights, enforcing
                                                   security measures, managing data breaches and
                                                   engaging only with processors providing sufficient
                                                   guarantees to protect the data.




Processor
You have less autonomy over the data you‚Äôre
processing, but you may still have direct legal
obligations. If you engage a sub-processor,
you may be liable to the controller for the sub-
processor‚Äôs compliance.
Your responsibilities include compliance with your
controllers‚Äô instructions as set out in third party
contracts, enforcing security measures, notifying
controllers of personal data breaches and not
engaging any sub-processor before the approval
of the controllers.




                                                   Sub-processor
                                                   As a sub-processor, you may be liable for any
                                                   damage caused by your processing in case you
                                                   have not complied with your legal obligations and
                                                   if you failed to follow the controller‚Äôs instructions.
                                                   Your responsibilities towards the processor are
                                                   similar to the processor‚Äôs responsibilities towards
                                                   the controller.




                                                                                                       13
 Individuals‚Äô rights
 One of the aims of data privacy laws is to empower individuals and give them control over their
 personal data. Therefore, most data privacy laws introduce what are usually referred to as ‚Äòdata
 subject rights‚Äô concerning the protection of individuals‚Äô personal data. It‚Äôs important to note that
 not all of these rights are ‚Äòabsolute‚Äô, meaning some only apply in specific circumstances:



                                      Right to access personal
                                      data
                                      Individuals have the right to
Right to limit personal               access and request copies of
data processing                       their personal data.
Individuals have the right
                                                                             Rights related to
to request the restriction
                                                                             automated decision
of the processing of their
                                                                             making and profiling
personal data.
                                                                             Individuals can object to
                                                                             decisions made about
                                                                             them based solely on
                                                                             automated and mechanical
                                                                             processing.
Right to erasure*
Individuals can have their
personal data deleted
without undue delay.




                                                                                 Right to objection
                                                                                 Individuals can object
Right to correct personal                                                        to the processing of
data                                                                             their personal data by
Individuals can have their                                                       an organisation.
personal data rectified if
inaccurate, or completed if
it is incomplete.                  Right to transfer personal data
                                   Individuals have the ability to receive
                                   data in an organised, commonly used
                                   machine-readable form.



 *Not all data subject rights are ‚Äòabsolute‚Äô. The ‚Äòright to erasure‚Äô is often misunderstood. The main
 reason for this is because many assume that it is an ‚Äòabsolute right‚Äô whereas in actual fact there
 are only certain circumstances that people can request for their data to be deleted.




 14
When can personal data
be processed?
The first principle of data privacy requires that all personal data be processed lawfully and fairly.
To do so, organisations must have at least one of the following valid lawful bases for processing:


  Consent: of the                     Legitimate interest: of            Contractual necessity:
  individual to the                   the organisation or the            processing is needed
  processing of their                 third parties engaged.             in order to enter into or
  personal data.                                                         perform a contract.




  Legal obligation: for               Vital interest: of                 Public interest:
  which the organisation              individuals, where                 specific to organisations
  is obliged to process               processing is necessary            exercising official
  personal data for.                  to protect their lives.            authority or carrying
                                                                         out tasks in the public
                                                                         interest.


As different types of data require different levels of protection, data privacy laws specify different
conditions for processing sensitive and criminal data:
‚Ä¢ S
   ensitive data can usually only be processed with the individual‚Äôs explicit consent, unless the
  data is required for filing legal proceeding or claims, or if there is any legal, public interest or
  regulatory requirement.
‚Ä¢ P
   ersonal data relating to convictions and criminal offences can usually only be processed
  as long as it is carried out under the control of a certain government authority or in accordance
  with local laws.


Top tips
‚Ä¢ Y
   ou must determine your lawful basis before you begin processing, and you should
  document it.
‚Ä¢ G
   et it right the first time - you should not swap between bases at a later date.
‚Ä¢ If your purposes change, you need to reassess the new purpose and determine a valid lawful
  basis.




                                                                                                     15
Ten steps to an effective data
            privacy programme

                                                            1
               Appoint a Data Protection Officer                18



2
Maintain a personal data register                    19




                                                            3
                         Notify purpose and seek consent        20




4
Respond when individuals ask                               21
about their personal data


                                                           5
                    Enforce security mechanisms                 22



16
6
Embed data privacy into your systems,                   24
processes and services




                                                             7
                       Notify data breaches                  26




8
Manage third parties                                   27




                                                             9
                          Protect personal data when         28
                          transferring overseas




10
Communicate your data protection                   30
policies, practices and processes


                                                                  17
                                                                  17
1          Appoint a Data
           Protection Officer
Many data privacy laws introduce the concept of a ‚ÄòData Protection Officer‚Äô (DPO), a new
leadership role for overseeing the organisation‚Äôs data protection programme and ensuring
compliance with the applicable laws.



                                                 Who could act as a DPO?
                                                 You can assign the role of DPO to an
                                                 existing employee within your organisation,
                                                 or recruit someone specifically for this role.
                                                 The DPO must be independent, an expert
                                                 in data protection, adequately resourced,
                                                 and must report to the highest management
                                                 level.




 What‚Äôs the role of a DPO?
 The DPO assists you in monitoring
 internal compliance with the
 applicable data protection laws,
 advising you on your data protection
 obligations, providing expert advice
 when needed, and acting as a point
 of contact for individuals and data
 protection authorities.




18
2              Maintain a personal
               data register
In order to protect personal data you need to know what data you collect, how you use it and
where you store it. The first step in achieving this is identifying all processing activities in your
organisation involving personal data, and documenting how and why the data is used in what is
called a ‚Äòpersonal data register‚Äô.



 How can I identify personal data being processed?
 Maintaining a personal data register is one of the key requirements of most data privacy
 regulations worldwide. As a first step, we recommend that you undertake a data discovery
 exercise across your organisation to document what personal data you hold and process,
 where it‚Äôs located, who has access to it and how long it is retained.

 What details should I include in the register?
 Most data privacy laws require you to identify and document the following for every
 processing activity within your organisation:
 ‚Ä¢ N
    ame and contact details of your DPO and any other third party (if applicable).
 ‚Ä¢ T
    he lawful basis and purpose of processing the data.
 ‚Ä¢ T
    he different categories of personal data involved.
 ‚Ä¢ T
    he systems and locations where the personal data is processed.
 ‚Ä¢ W
    here the data is transferred to and the list of recipients.
 ‚Ä¢ T
    he retention period and enforced technical and security measures (refer to page 24 for
   more details).




                                                                                                    19
3            Notify purpose and
             seek consent
Transparency is a central principle in data privacy laws. When collecting individuals‚Äô personal
data you must provide them with clear information explaining why, what and how you‚Äôre
intending to process it.

What information should I provide?
The following should be included in the privacy information shared with individuals:
‚Ä¢ C
   ontact details of your organisation and DPO.
‚Ä¢ P
   urpose and lawful basis for processing, including details on legitimate interests if applicable.
‚Ä¢ R
   ecipients of personal data and details of cross-border transfers.
‚Ä¢ R
   etention period of personal data and existence of automated decision-making.
‚Ä¢ D
   etails on individuals‚Äô rights, process for withdrawing consent and how to lodge complaints.

How to provide it?
Privacy information should be provided to individuals at the time of collecting their personal
data, or within a reasonable timeframe if collected from other sources. Privacy information
must be concise, transparent, intelligible, easily accessible and use clear and plain language.
To meet these requirements, you could consider using a combination of techniques, such as an
expandable section approach, dashboards and just-in-time notices.

What is consent?
Consent is a freely given, specific, informed and unambiguous agreement, provided by
individuals through a statement or a clear affirmative action, to the processing of their personal
data.
Consent means giving people control and choice over how their personal data is processed. It
constitutes one of the legal grounds for lawfully processing personal data, however, there are
conditions that need to be met to ensure it‚Äôs valid.

How can I obtain consent?
‚Ä¢ Individuals can give their consent in writing
  or any other form. If the consent is given in
  writing, it should be distinct from any other
  agreement (e.g. terms and conditions) and
  written using clear and simple language.
‚Ä¢ Individuals can withdraw their consent at
  anytime, and the withdrawal procedures
  should be as easy as those for giving the
  consent.




20
4              Respond when individuals
               ask about their personal data
What are data subject requests?
Data privacy laws introduce new rights for
individuals that are designed to give them
more control over how their data is used.
Individuals are entitled to raise requests
to exercise their data subject rights and
organisations must respond within a specified
period, as per the data privacy laws you are
subject to.

How can I be prepared?
To meet the defined timeline, your organisation
has to implement robust procedures to
authenticate the requester, assess the request
and formulate an adequate response.

What information should I provide in my response?
‚Ä¢ What personal data is being processed. Refer to page 9 for further details.
‚Ä¢ The purposes for processing the data.
‚Ä¢ Who within the organisation has the personal data and who it will be disclosed to.
‚Ä¢ Whether or not the individual‚Äôs personal data is used in any automated decision making (such
  as credit worthiness) and how that automated decision making works.
‚Ä¢ How long the data will be retained for, or at least the criteria used to determine this period.

What are the steps to responding to a data subject request?
1. Receive the data subject request and forward it to the concerned department.
2. Determine if the request is self-raised or on behalf of others, then verify the identity of the
    individual.
3. Assess the request and confirm if an extension or charges are to be applied, as per the data
    privacy laws you are subject to. If so, respond to the individual providing explanation for
    time extension and/or admin charges.
4. Determine where the personal data of the individual is stored, be it in systems or physical
    documents.
5. Perform the appropriate action according to the type of data subject request (i.e. copy data,
    delete data, restrict processing etc).
6. Provide appropriate details to the DPO for delivery and response to the data subject.
7. Send and document the appropriate response to the individual.




                                                                                                      21
5            Enforce security
             mechanisms
Most data protection laws require organisations to ensure that ‚Äòorganisational and technical
measures‚Äô are in place to protect personal data. This usually means that organisations needs to
take reasonable steps to protect personal data. What is ‚Äòreasonable‚Äô will usually come down to a
business decision with the support of legal counsel, and will be based on the organisation‚Äôs size
and the amount and type of personal data being processed.
Generally speaking, organisational and technical measures are the functions, processes, controls,
systems, procedures and measures taken to protect and secure the personal information that
you process.



Organisational measures are defined as the approach taken
in assessing, developing and implementing controls that secure
information and protect personal data. They can include, but
are not limited to:
                                  Policies and procedures




         Awareness and training




                                                            Business continuity

                         Risk assessments
                         and audits


Technical measures are defined as the measures and controls implemented
on systems from a technological aspect. Protecting such aspects is vital to
data security, but goes above securing access to devices and systems. They
can include, but are not limited to:

‚Ä¢ S
   ystem and physical security
‚Ä¢ E
   ncryption or de-identification of
  personal data
‚Ä¢ R
   obust data disposal measures
‚Ä¢ P
   asswords and two-factor authentication
‚Ä¢ B
   ring your own device (BYOD) and
  remote access



22
Which security measures should I implement?
Depending on the size of your organisation and the processing activities undertaken, there are
a broad range of technical and organisational measures that can aid in securing and protecting
personal data. We also suggest utilising established frameworks such as ISO27001 to assess
and develop adequate measures.
As there is no ‚Äòone size fits all‚Äô solution when it comes to information security, we recommend
you follow the steps below to determine which measures you should implement:



Step 1

Carry out an information security risk
assessment by reviewing the personal data
you hold, the way you use it, and the risks
presented by the processing.




Step 2

Carry out a technical vulnerability
assessments (e.g. a penetration test) on
devices and systems posing high risk on your
personal data processing.




Step 3

Assess and select the most adequate security
measures to mitigate the identified risks.




Step 4

Ensure your employees are kept up to date
on your information security programme and
latest security best practices.



                                                                                                  23
                                                                                                  23
6             Embed data privacy into your
              systems, processes and services
Recent data privacy laws have introduced detailed requirements on privacy by design and
default. A first step to translate these broad concepts into functional requirements is to define
their key principles as follows:


  1. Privacy and data protection are                2. Accountability is communicated and
      embedded into the design of a new                  supported (example: conducting
      process or application (example:                   internal audit reviews over data privacy
      creating a corporate culture where                 programme and practices).
      privacy and data protection are tone-
      at-top).


  3. Transparency is created and maintained         4. Safeguards are established and enabled
      (example: privacy notices are regularly            (example: enforcing encryption and data
      updated to reflect the processing                  minimisation mechanisms on personal
      activities and privacy practices).                 data).




While these principles help to inform the organisation‚Äôs overall approach, successful privacy
by design and default is facilitated by governance and oversight, implemented by a supportive
workforce, and informed by risk and compliance.

What is ‚Äòdata privacy by default‚Äô?
Data privacy by default links to the
fundamental data protection principles of data
minimisation and purpose limitation.
Privacy by default requires you to ensure
that you only process personal data that is
necessary to achieve your specific purpose,
while considering things like:
‚Ä¢ a
   dopting default privacy settings on
  systems;
‚Ä¢ b
   eing transparent with your customers
  and employees on your data processing
  activities and practices;
‚Ä¢ p
   rocessing data that is proportionate to the
  purpose; and
‚Ä¢ p
   roviding information and options to
  individuals to exercise their rights.



24
What is ‚Äòdata privacy by design‚Äô?
Organisations committed to providing an environment that safeguards personal data must
embed data privacy into the design and overall lifecycle of any technology, business process,
product, or service, such as:
‚Ä¢ U sing a new way for storing data (i.e. cloud)
‚Ä¢ E ngaging a third party to manage and maintain an IT system
‚Ä¢ T ransferring data to a new third party
‚Ä¢ N ew or changing business process
‚Ä¢ N ew product offering
‚Ä¢ N ew use of existing data to improve
  a product or service




Privacy by design is mainly comprised of          Privacy by design requires you to:
two distinct elements:                            ‚Ä¢ p
                                                     ut in place appropriate technical and
1. Data Privacy Impact Assessment (DPIA):          organisational measures designed to
    a tool used to identify privacy risks of        implement the data privacy principles; and
    processing activities, assessing their        ‚Ä¢ e
                                                     mbed controls into your processing
    impact and designing controls to mitigate       activities so that you meet the legal
    the identified risks and implement privacy      requirements and protect individuals‚Äô rights.
    requirements.
2. Personal Data Change Management:
    a process outlining the five general
    phases to be integrated within a project‚Äôs
    implementation lifecycle, from inception to
    completion.




                                                                                                25
                                                                                                25
7           Notify data breaches
Data breaches can happen for various reasons, despite all the precautions that you may take. As
data privacy regulations introduce strict reporting timelines, it is crucial for every organisation to
be well prepared in the event of a data breach.


How do I respond to a data breach?
Within a limited time (depending on the data protection law in question) after a data breach has
been discovered, you must:
‚Ä¢ A ssess the nature of the breach and confirm
  if personal data is involved.
‚Ä¢ Identify what personal data has been
  impacted and how.
‚Ä¢ A ssess the impact of the breach to
  determine if it poses high risk to the rights
  and freedoms of individuals.
‚Ä¢ D etermine if you need to notify the Authority
  and the individuals concerned.
‚Ä¢ C arry out a thorough investigation to identify
  the source of the breach.




Notifying the Authority
Your breach notification should include the following information at a minimum:

                                                     ‚Ä¢ N
                                                        ature of breach:
                                                       ‚Ä¢ Who accessed what and when?
                                                       ‚Ä¢ What caused the breach?
                                                       ‚Ä¢ How was the data used?
                                                       ‚Ä¢ Who are the affected individuals?
                                                     ‚Ä¢ D
                                                        escription of the estimated impact and
                                                       possible effects.
                                                     ‚Ä¢ C
                                                        ontact details of your data protection
                                                       supervisor.
                                                     ‚Ä¢ P
                                                        rocedures taken by your organisation to
                                                       investigate and remediate the incident.




Top tips to beat the clock
‚Ä¢ S tay calm and take the time to investigate thoroughly before getting your business back up
   and running.
‚Ä¢ P ut a response plan in place and communicate it to all employees and third parties.
‚Ä¢ A llocate the responsibility for managing breaches to a dedicated person or team.
‚Ä¢26R
    egularly test the plan to minimise the disruption that typically follows a breach.
8              Manage third parties
Data privacy laws add new requirements and deepen obligations around third party risk
management. If you engage a third party to process personal data, you may be held responsible
if your service provider violates applicable data privacy laws while providing the service to you.
When entering into a contractual agreement with your service provider, ensure there are clauses
that require them to take sufficient measures to ensure compliance with the requirements of
applicable data privacy laws.

What should I include in a contract?
Contractual agreements with third parties should at a minimum include the following details:
 ‚Ä¢ The subject-matter and duration of processing
 ‚Ä¢ The nature and purpose of processing
 ‚Ä¢ The type of personal data and categories of data subjects
 ‚Ä¢ The minimum terms or clauses required of the processor
 ‚Ä¢ The obligations and rights of the controller

Enhancing your third party risk management programme
Contracts alone are not enough to manage third party risks. Outlined below are additional steps
you can consider to enhance your third party risk management programme:
‚Ä¢ C
   onduct a due diligence assessment to ensure that the third party has adequate controls in
  place to protect personal data.
‚Ä¢ U
   pdate your existing contracts and draft new contracts clearly defining the roles,
  responsibilities and liabilities of both parties.
‚Ä¢ C
   ontinue to improve ongoing monitoring through risk assessments and audits to ensure that
  third parties are maintaining adequate controls to protect personal data.




                                                                                                27
                                                                                                27
9              Protect personal data when
               transferring overseas
With a significant number of organisations‚Äô operations spanning several countries and territories,
data transfers are an integral part of today‚Äôs global economy. Many data privacy laws contain a
‚Äòwhitelist‚Äô of countries to whom personal data may freely be transferred because they provide
adequate levels of personal data protection. For non-whitelisted countries or ‚Äòthird countries‚Äô
as they are also known, data privacy laws require safeguards to be in place whenever data is
transferred to such places. Often this means using a recognised data transfer mechanism.

What is considered a third country data transfer?
A third country data transfer is the transfer of personal data to a country or jurisdiction where the
data privacy law of the sender‚Äôs country does not apply and which has not been assessed as
providing an adequate level of data protection when compared with the sender‚Äôs home country.
You are making a cross-border data transfer if:
‚Ä¢ T
   he personal data that you intend to transfer is in scope of one or more data privacy laws.
‚Ä¢ T
   he personal data is transferred to a third country.
‚Ä¢ T
   he receiver is a separate organisation or individual. This also covers transfers to another
  company within the same corporate group.




When can I transfer personal data?
Transferring personal data overseas can pose higher risks to the organisation. In certain
circumstances, data privacy laws restrict transfers of personal data outside their jurisdictions
unless certain safeguards are in place.



28
Which safeguards are considered appropriate for personal data transfers?
There are a number of mechanisms your organisation could adopt to protect personal data when
transferring to third countries. Some safeguards recognised by the GDPR are:



   Binding Corporate Rules (BCRs):                    Standard contractual clauses:

   Legally binding and enforceable internal           A set of standard clauses, provided
   rules and policies for data transfers              by a relevant Authority, to be used in
   within multinational group companies               contracts.
   that allow intragroup data transfers
   to countries that do not provide
   an adequate level of protection for
   personal data. BCRs usually need to be
   approved by an Authority.



   Code of Conduct:                                   Certification:
   These resemble self-regulatory                     Granted to the receiver, under a
   programmes to demonstrate to                       scheme approved by the Authority.
   regulators and consumers that a                    The certification scheme must include
   company adheres to certain data                    appropriate safeguards to protect the
   privacy standards.                                 rights of individuals whose personal
                                                      data is transferred, and which can be
                                                      directly enforced.




What if the cross-border transfer is not covered by appropriate safeguards?
If the transfer is not covered by appropriate safeguards, then you need to assess if one of the
exceptions defined in the applicable data privacy laws applies. These exceptions are specific for
each data privacy law and could include relying on the individual‚Äôs explicit consent or entering in
a contract with the individual.




                                                                                                 29
10                     Communicate your data
                       protection policies, practices
                       and processes

Complying with data privacy laws is not something that can be left to the legal and compliance
departments alone. Compliance with data privacy laws requires that everybody in the
organisation understands their responsibilities to protect personal data. It is very important to
communicate your data privacy policies and practices to your customers and employees to
ensure they are familiar with how you process and protect personal data.


Customers                                          Employees
‚Ä¢ M
   ake the business contact information of        ‚Ä¢ C
                                                      ommunicate your data protection policies
  your DPO easily accessible so that your            and practices to your employees to make
  customers know who to contact for inquiries        sure they are familiar with their roles and
  or complaints.                                     responsibilities in processing personal data.
‚Ä¢ R
   eadily provide information about your          ‚Ä¢ D
                                                      evelop a culture of privacy awareness
  data protection policies, practices and            within your organisation by aligning the
  complaints process upon request.                   importance of data privacy to your values
                                                     and implementing practical approaches to
‚Ä¢ U
   pdate your privacy notice to make sure
                                                     convert it to repeated practices.
  your customers understand what personal
  data you process, and how you do it, to          ‚Ä¢ U
                                                      se posters, email and other
  enable them to make informed decisions             communication tools to raise awareness of
  about it. The privacy notice should be:            the importance of personal data protection
                                                     among your staff.
 ‚Ä¢ Concise and transparent
 ‚Ä¢ Written in clear and plain language             ‚Ä¢ S
                                                      end key employees who handle personal
                                                     data to attend regular data privacy training
 ‚Ä¢ Delivered in a timely manner
                                                     to ensure they are kept up to date on your
 ‚Ä¢ Made publicly available and easy to access        internal processes and latest developments
                                                     in the privacy space.




30
How PwC can help
As experts in data privacy, we are well positioned to support you with your organisation‚Äôs journey
to data privacy compliance. We have developed a five step approach to transforming privacy
programmes, with tools and accelerators to assist the process.




                             Risk analysis and   What you will get
       Assess current




                             data discovery
                                                 ‚Ä¢ Stakeholder engagement and communications plan
        capabilities




                                                 ‚Ä¢ Personal data inventory
                                                 ‚Ä¢ Data flow maps showing the movement of personal data
                                                   fromcollection through to disposal



                             Gap assessment      What you will get
                                                 ‚Ä¢ Control gap analysis
                                                 ‚Ä¢ Risk assessment based on current and planned future uses
                                                   of personal data
 Design the future state




                             Target operating    What you will get
                             model and
                                                 ‚Ä¢ Detailed remediation project plan with identified
                             programme
                                                   organisational impact
                             design
                                                 ‚Ä¢ Cross-functional working group established



                             Programme           Areas of focus
                             implementation
                                                 ‚Ä¢ Strategy and governance
                                                 ‚Ä¢ Policy management
                                                 ‚Ä¢ Cross-border data strategy
                                                 ‚Ä¢ Data life-cycle management
                                                 ‚Ä¢ Individual rights processing
                                                 ‚Ä¢ Privacy by design
                                                 ‚Ä¢ Information security
       Operate and sustain




                                                 ‚Ä¢ Privacy incident management
                                                 ‚Ä¢ Data processor accountability
                                                 ‚Ä¢ Training and awareness



                             Ongoing             What you will get
                             operations and
                                                 ‚Ä¢ Defined ongoing monitoring programme
                             monitoring
                                                 ‚Ä¢ Tracking and retesting of non-compliance
                                                 ‚Ä¢ Protocols for changes to policies and procedures




                                                                                                               31
Get in touch
To discuss how PwC can support you with implementing your data privacy programme, please
get in touch.



                Phil Mennie                                                Nabil Diab
                Partner, Middle East Data Privacy                          Partner, Egypt
                Leader                                                     nabil.diab@pwc.com
                phil.mennie@pwc.com                                        https://www.linkedin.com/in/na-
                linkedin.com/in/philmennie                                 bildiab-pwc




                Richard Chudzynski                                         Tamer Amin
                PwC Legal                                                  Director, Egypt
                richard.chudzynski@pwc.com                                 tamer.amin@pwc.com
                linkedin.com/in/richardchudzynski




At PwC, our purpose is to build trust in society and solve important problems. We‚Äôre a network of firms in 158
countries with over 250,000 people who are committed to delivering quality in assurance, advisory and tax
services. Find out more and tell us what matters to you by visiting us at www.pwc.com.

Established in the Middle East for 40 years, PwC has 22 offices across 12 countries in the region with around
5,200 people. (www.pwc.com/me).

PwC refers to the PwC network and/or one or more of its member firms, each of which is a separate legal
entity. Please see www.pwc.com/structure for further details.

¬© 2020 PwC. All rights reserved
CDC 1872 082020
32
Digital Privacy:
How Can We Win the Battle?
S T U DY N O V E M B E R 2 019
There is no desire more natural
 than the desire for knowledge
Digital Privacy:
How Can We Win the Battle?




NOVEMBER 2019
About the author
Fran√ßois Godement, Senior Advisor for Asia, Institut Montaigne

Fran√ßois Godement is Senior Advisor for Asia to Institut Montaigne,
Paris. He is also a non-resident senior associate of the Carnegie
Endowment for International Peace in Washington, D.C., and an
external consultant for the Policy Planning Staff of the French Ministry
of Foreign Affairs. Until December 2018, he was the Director of
ECFR‚Äôs Asia & China program and a Senior Policy Fellow at ECFR.
A long-time professor at France‚Äôs National Institute of Oriental
Languages and Civilisations and Sciences Po, he created Centre Asie
IFRI at the Paris-based Institut Fran√ßais des Relations Internationales
(1985-2005), and in 2005 Asia Centre as an independent. He is
a graduate of the Ecole Normale Sup√©rieure de la Rue d‚ÄôUlm (Paris),
where he majored in history, and a postgraduate student at Harvard
University. In 1995 he co-founded the European committee of the
Council for Security Cooperation in the Asia-Pacific (CSCAP), which
he co-chaired until 2008. He has also been a member of the advisory
board for the Europe China Academic Network (ECAN).
                        SUMMARY




INTRODUCTION                                             3

I - DEFINING THE ISSUE AND THE DEBATE                    13

II - 	WHAT IS PRIVACY AND HOW CAN IT BE ENSURED?        33

III - GDPR, A EUROPEAN REGULATORY FEAT                   57

IV - INDIA, A DIGITAL BLEND                              77

V - 	CHINA, THE SURVEILLANCE STATE WITH SOME PRIVACY
      CONCERNS                                           93

VI - IN-FOCUS: HEALTH DATA AND PRIVACY                  119   1


VII - CHASING PRIVACY, INNOVATION AND PUBLIC INTEREST   135

PROPOSITIONS                                            157

ACKNOWLEDGMENTS                                         171
                           INTRODUCTION


‚ÄúGentlemen don‚Äôt read other people‚Äôs mail.‚Äù Actually, they sometimes
do, legally or surreptitiously. The novelty is that they no longer need
to steam open an envelope. In fact, we ourselves are emitting personal
data around the clock rather than write old-fashioned letters. Barring
end-to-end encryption, or more rarely content encryption, that data
floats in cyberspace.

But how can gentlemen find their way in the exaoctets of data flowing
through cyberspace from the sanctuary of your home, or among the
nearly 3 billion smartphones in circulation, and tomorrow the zillions
connected objects? Isn‚Äôt there safety in numbers? Most digital users
do not trust their data to be completely safe, but they rely on a
degraded version of trust ‚Äì nobody will look for a pin in the haystack.
                                                                                      3
With this reservation in mind, the digital age has become the last
free frontier in our age. To use a metaphor, the digital age is to its
predecessors what maritime exploration was to bound land states,
the ‚Äú21st century equivalent of the ‚Äòdark continents‚Äô that drew 19th
century European speculators to their shores.‚Äù1 Or perhaps, what
the opening of the American West represented to its pioneers from
original Eastern American states: lands of opportunity where the
laws hardly applied. With its drawbacks: corsairs ‚Äì ‚Äúprivateers‚Äù were
an official practice for maritime nations. One was free at sea,
including to fall prey to corsairs or marauders. Similarly, the ‚Äúlaw of
the West‚Äù was a euphemism. Social rumors or fake news, ‚Äúscraping‚Äù
for commercial use of everyone‚Äôs ‚Äúsmall data‚Äù, cybersecurity issues,
are the contemporary equivalent.

 Shoshana Zuboff, The Age of Surveillance Capitalism : The Fight for a Human Future
1

 at the New Frontier of Power (New York: Publicaffairs, 2019), p. 103.
    D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?




    Living on the Digital Frontier
    The digital revolution ‚Äì a combination of big data,2 predictive
    algorithms and quantum computation ‚Äì promises to change our lives
    as no other breakthrough ever did. This is because connecting
    numbers with one another, even as it concerns ‚Äúthings‚Äù as in ‚Äúthe
    internet of things‚Äù, in fact goes to the heart of human behavior, and
    one day to our inner thought process. For the time being, the
    revolution is satisfied with statistical predictions based on digging
    into deep layers of human behavior, but one day these predictions,
    and the behavior variables that they are based on, will become so
    granular and accurate as to be contemporary or even synonymous
    with human thought processes. The combined performance of
    algorithms and big data does not stop there. Machines have beaten
    human beings at chess, at go ‚Äì a much more multi-dimensional
4   experience, and now at poker.3 Artificial Intelligence (AI) interpretation
    of lung imagery for tumors and choice of treatments consistently
    beat the best human medical teams.

    Along with those achievements come visions of utopia. Who needs
    physical libraries if internet clouds provide much more capacities
    ‚Äì and major libraries too become clouds: the Library of Congress,
    for example, has received in deposit a complete archive of ALL tweets
    posted on Twitter for the first decades. The average smartphone
    gives access to a bandwidth of information that was never accessible
    in any form to any individual in the previous era. There are those
    who argue that the digital revolution, complemented by edge (local)

     For a clear introduction and historical account of this: Gilles Babinet, Big Data, Penser
    2

     l‚Äôhomme et Le Monde Autrement (Paris: Le Passeur √âditeur, 2016), p. 25-51.
     Noam Brown and Tuomas Sandholm, ‚ÄòSuperhuman AI for multiplayer poker‚Äô, Science,
    3

     August 30, 2019.
                                                         INTRODUCTION




computing and servers, and software incorporating AI will render
obsolete the Fordist and Taylorist industrial revolution. Imaging,
remote diagnosis and predictive diagnosis will revolutionize preventive
medicine, and allow for the treatment of billions of people who had
no suitable access to doctors and medical resources. Automated
decisions will become commonplace, shrinking the drudge of daily
chores, just as physical work has shrunk in the course of the first
industrial revolutions. In the end, we will be pure minds, focused
on innovation, leisure and instant communications with all the other
monads in the world, overcoming physical, language and cultural
barriers.

Or will we?



Brave New Data World                                                         5


The other vision is one of dystopia. Some of it is economic, with the
prospect of mass unemployment: white-collar jobs, including many
that were previously thought as skilled, will be automated. But the
prevailing dystopian view focuses instead on the disappearance of
privacy for marketing or control purposes. The case for this is almost
the same as that for the utopian vision, so much so that one may
consider it as its flip side.

The conflict between individual and sovereign rights is the oldest
conflict in political philosophy. But in the digital age, one can replace
individualism with the issue of privacy. There can be no individual
right if there is no privacy. If polling becomes so granular as to predict
the vote of a given individual with near certainty, there is no longer
any confidentiality of voting. The Obama 2008 presidential campaign
    D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?




    compiled data on more than 250 million Americans. In one
    participant‚Äôs words, ‚Äúwe knew‚Ä¶ who people were going to vote for
    before they decided.‚Äù The president‚Äôs 2012 re-election campaign
    knew ‚Äúevery single wavering voter that it needed to persuade to vote
    for Obama by name, address, race, sex and income‚Äù, and created
    ‚Äúpersuasion scores‚Äù for undecided voters.4 The Obama campaign
    broke no law that we know of, and it has in fact become a model
    for campaigning elsewhere in open democratic contexts. Some of
    the same operators would work later for the Trump campaign and
    for Cambridge Analytica, probably the company involved in the most
    glaring privacy scandal of recent times.

     ‚ÄúWith near certainty‚Äù: in that tiny islet where chance can still prevail
    over necessity, can we really find the remaining element of human
    choice and self-determination? That is only conceivable in a society
6   which is protected by positive law and institutions. Elsewhere, to
    categorize or to judge you, near certainty is good enough.



    Predictive Chaos
    The heart of the legal debate about individual rights ‚Äì habeas corpus
    ‚Äì has often been about the legality of practices, not about their reality
    that made little doubt: habeas corpus itself means that the ‚Äúbody‚Äù
    or subject must be brought to justice, and not disposed otherwise.
    In terms of digital data, it is about the legal duty to prevent ‚Äúintrusion
    into seclusion‚Äù, and about the public and legally admissible use of
    the data collected. Chinese fintechs run on big platforms with access
    to many more types of data than is legally permissible in the West:

     Cited by Shoshana Zuboff, The Age of Surveillance Capitalism : The Fight for a Human
    4

     Future at the New Frontier of Power (New York: Publicaffairs, 2019), p. 122-123.
                                                                  INTRODUCTION




they can deliver a credit or insurance rating within seconds for the
smallest of entrepreneurs, based on a swath of data that includes
many personal habits and possible incidents. It is a near certainty
that one individual or entity owning AI resources can game markets
in the near future (one can postulate the same outcome for war
scenarios). However, if several individuals or entities compete with
algorithms, the result may well be chaos and complete uncertainty.
The market‚Äôs resiliency came from the fact, as suggested by Friedrich
Hayek, that no individual human mind could outguess ‚Äúthe
coordinated utilization of resources based on equally divided
knowledge.‚Äù5 That is over. But even Hayek had not envisioned that
algorithms could play each other, creating overall instability. The
principle of uncertainty is not equivalent to market play.

The above should be nuanced with two interrelated issues: that of
accuracy, closely linked to the quality of the algorithms used. Few                      7
areas of AI regarding human behavior are likely to offer the reliability
that DNA typing (but not the handling and conservation of samples)
has acquired. And big data banks are only as good as the software
to interpret them. But that in turn should be relativized by another
fact: relative certainty can be good enough in some systems. Facial
recognition has become so commonplace there is an over-the-counter
Russian app that makes it available to any web user.6 But it is still
riddled with ‚Äúfalse positives‚Äù and ‚Äúfalse negatives‚Äù (wrong identification
or missing an actual match). Most experts recognize that the path
from 90% to 100% accuracy is much harder than that from 50%
to 90%, not including possible deception and concealment. Yet some
 Shoshana Zuboff, The Age of Surveillance Capitalism: The Fight for a Human Future at
5

 the New Frontier of Power (New York: Publicaffairs, 2019), p. 497.
 Kevin Webb, ‚ÄúViral app that makes you look old with shocking precision may be quietly
6

 keeping all your data‚Äù, Business Insider France, July 17, 2019, https://www.busines-
 sinsider.fr/us/faceapp-privacy-data-terms-service-russia-2019-7.
    D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?




    governance systems will be satisfied with 90% accuracy, and even
    more so if it is compounded with other predictive results.

    The makers of the digital age are not the initial inventors of computing,
    or even the internet. They are the entrepreneurs who have turned
    these into a nearly universal commodity, and who have created an
    immense new field of social media and data use. They were often
    convinced that they were bringing unbounded freedom to individuals,
    especially from cumbersome regulations, and with this freedom, a
    blossoming of individuals. ‚ÄúThe online world is not truly bound by
    terrestrial laws‚Ä¶ It‚Äôs the world‚Äôs largest ungoverned space.‚Äù7 To all
    practical purposes, users in the digital age act with an illusion of
    privacy that is greater than at any other time: in fact, they consign
    most of their private data, one way or another, to the digital space.

8   Isn‚Äôt it telling that meeting online has become the most popular way
    (39%) couples form, displacing the role that family, friends and
    public places once played?8 It can be argued that this method of
    seeking partners offers more privacy than earlier methods involving
    intermediaries or public searches. Googling, roaming, exchanging
    over social media appears to enhance the individual against the
    constraints and inhibitions of the community. Cyberspace is both
    the largest public space ever and yet, it is thought to be a very private
    meeting place. If this wasn‚Äôt the case, 30% of internet traffic wouldn‚Äôt

    7
      Eric Schmidt and Jared Cohen, The New Digital Age Reshaping the Future of People,
      Nations and Business (London Murray, 2014).
      39% applies to heterosexual couples. For homosexual couples, the proportion of on-line
    8 

      meeting rises to 65% according to the same study.
       Source: Michael J. Rosenfeld, Reuben J. Thomas, and Sonia Hausen, ‚ÄúDisintermediating
       Your Friends: How Online Dating in the United States Displaces Other Ways of Meeting,‚Äù
       Proceedings of the National Academy of Sciences 116, no. 36 (August 20, 2019),
       p. 4, https://doi.org/10.1073/pnas.1908630116.
                                                               INTRODUCTION




be about adult content, as has been the case until the new streaming
media provided alternative distraction at home, taking 60 % of the
overall internet traffic. This fact should come with the remark that
in all likelihood, the immense majority of customers for this would
not have dreamt of entrusting this type of content to the post office,
even with gentlemen looking the other way.

There is no denying this new freedom, and the ample opportunities
that have come with the digital age. But the other side of the coin
has become increasingly clear: the ‚Äúscraping‚Äù of the individual data
we leave on the digital media, the very extent to which every one of
our movements, actions and increasingly our thoughts is enacted
through a digital medium and therefore open to scrutiny ‚Äì perhaps
forever and without reprieve ‚Äì create a world of transparency, where
surveillance is the practice if not the norm. Quite simply, we embrace
new digital tools and social media platforms which make us less                      9
private. How to find a balance between our freedom and our privacy
is an extremely hard choice, even at the individual level.



The Digital Age, Like the Nuclear Age, Cannot Be
De-Invented
Big data is not only ‚Äúthe new oil.‚Äù Algorithms make it the human
equivalent of atom fission, otherwise known as the nuclear bomb.
In Eric Schmidt‚Äôs words, ‚Äúalmost nothing, short of a biological virus,
can scale as quickly, efficiently and aggressively as these technology
platforms, and this makes the people who build, control and use
them powerful too.‚Äù9 Or as Jim Balsillie, ex-RIM CEO explains, ‚Äúdata

 Eric Schmidt and Jared Cohen, The New Digital Age Reshaping the Future of People,
9

 Nations and Business (London Murray, 2014), p. 9-10.
     D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?




     at the micro-personal level gives technology unprecedented power
     to influence. Data is not the new oil ‚Äì it‚Äôs the new plutonium.
     Amazingly powerful, dangerous when it spreads, difficult to clean
     up and with serious consequences when improperly used.‚Äù10 In the
     near future, the expansion of the digital age with the so-called Internet
     of Things (IoT) will place us in a web of myriad interconnected
     devices endowed with a form of machine intelligence, served with
     sensors registering their ‚Äì and our ‚Äì environment. To an extent, we
     will be the subjects driving these networks ‚Äì or some operator will
     be that subject. But there is a very big likelihood that these networks
     will move ‚Äúfrom a thing that we have to a thing that has us.‚Äù Already,
     some of the most comprehensive owners and sellers of our personal
     data are companies whose names we don‚Äôt even know. One third-
     party data broker, Acxiom (now renamed LiveRamp), claims to have
     amassed by 2018 up to 10,000 attributes on 2,5 billion individuals,
10   a ‚Äúcomprehensive representation of 68 percent of the world‚Äôs online
     population.‚Äù11

     Linking up very few metadata points from dispersed sources can
     now lead to identification of individuals. That holds true even when
     the collection of each of these data points has been anonymous. AI
     is a zillion minds put together, and it will move faster than any human
     thought and action in any case. A similar outcome can befall
     encryption, defeated by quantum computing. What matters with the


        Financial Post, ‚ÄúJim Balsillie : ‚ÄòData Is Not the New Oil ‚Äì It‚Äôs the New Plutonium,‚Äô‚Äù
     10 

        Financial Post, May 28, 2019, https://business.financialpost.com/technology/
        jim-balsillie-data-is-not-the-new-oil-its-the-new-plutonium.
        The company has changed ownership after the 2018 Cambridge Analytica scandal.
     11 

         Source: Alex Pasternack, ‚ÄúHere Are the Data Brokers Quietly Buying and Selling Your
         Personal Information,‚Äù Fast Company, March 2, 2019, https://www.fastcompany.
         com/90310803/here-are-the-data-brokers-quietly-buying-and-selling-your-
         personal-information.
                                                                   INTRODUCTION




above is not only that privacy, defined as confidentiality and a
principle of uncertainty for human actions, is replaced with nearly
complete knowledge of the individual mind, or at least categorization
and predictability, it is also that there is a huge asymmetry created
between the operators of the systems and their object ‚Äì the individual.
The prophets of this age have entirely anticipated this dispossession.
B.F. Skinner, a leading behavioral psychologist, wrote in 1971: ‚Äúto
man qua man we readily say good riddance. Only by dispossessing
him can we turn‚Ä¶from the inferred to the observed, from the
miraculous to the natural, from the inaccessible to the manipulable.‚Äù12

The dispossession extends to uncontrollable human features ‚Äì
deducing human preferences and feelings. ‚ÄúWith enough big data,
the numbers speak for themselves.‚Äù13 From Facebook likes, sexual
orientation can be deduced with 88% accuracy. From keyboard
typing patterns, sadness can also be detected with 88% accuracy:                          11
these achievements date back to 2011, and one can only guess the
progress made since then.14 And of course, asymmetries exist also
between operators: this is usually acknowledged in terms of market
share, as there is a huge premium for the first mover. But that
inequality is well balanced by another: operators who can legally
access larger data bases across different domains, and use them
with less restrictions, will become more efficient than those which
operate under antimonopoly, privacy or other constraints.


   Burrhus Frederic Skinner, Beyond Freedom & Dignity (Indianapolis, Ind.: Hackett Pub,
12 

   2002), cited by Shoshana Zuboff, The Age of Surveillance Capitalism : The Fight for
   a Human Future at the New Frontier of Power (New York: Publicaffairs, 2019), p. 439.
   Chris Anderson, ‚ÄúThe End of Theory: The Data Deluge Makes the Scientific Method
13 

   Obsolete,‚Äù Wired, June 23, 2008, https://www.wired.com/2008/06/pb-theory/.
   Wolfie Christl, ‚ÄúCorporate Surveillance in Everyday Life - How Companies Collect,
14 

   Combine, Analyze, Trade, and Use Personal Data on Billions,‚Äù Cracked Labs, June
   2017, https://crackedlabs.org/dl/CrackedLabs_Christl_CorporateSurveillance.pdf
     D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?




     Many other nations have a much more diverse starting point and
     agenda for the digital age than the United States. We will look at
     three cases: the European Union, and its top-down regulatory
     framework; China, with its statist vision of control and innovation
     coupled with dynamism from below; and India, which has
     characteristics from both the EU and China, while being very
     integrated within the American digital scene. Differences exist both
     from the technological, societal or regulatory points of view.

     Beyond these cases, more general distinctions can be made: either
     countries are too small or undeveloped to regulate a digital space
     on which they have no hold. They are often market-oriented and
     they will go for efficient cost/benefit customer solutions, disregarding
     both privacy and government control over the content of
     communications. Or their governance is authoritarian, and they will
12   choose digital packages that facilitate surveillance and eventual
     closure. A regulatory balance, as we shall see, which is the crux of
     the European choice so far, is not easy to define. But this is also
     very hard to implement, and it requires sophisticated rules and
     human resources.

     This study identifies the key movers of the data privacy debate (I),
     studies its legal formulations (II), discusses our three cases studies
     (III, IV, and V), analyzes health as a theme-in-focus (VI), and
     concludes with outstanding issues (VII) and propositions for a data
     protection regime.
                                          I

       DEFINING THE ISSUE AND THE DEBATE

The goal of protecting personal data and privacy stands in a regulatory
balance. This can be defined in very simple terms as a triangle, and
as an Indian Supreme Court judge expressed in July 2018: ‚ÄúThe
citizen‚Äôs rights have to be protected, the responsibilities of the states
have to be defined but the data protection can‚Äôt be at the cost of
trade and industry.‚Äù 15 But unpacking these easy definitions
immediately compounds the difficulties. It is not only citizens‚Äô rights
to privacy, but also company and IPR data that must be protected.
The interest of the state is not only about national or public security,
it also involves defining the public interest ‚Äì including, as obvious
examples, the benefits from health data banks vs. the patients‚Äô rights
to privacy, the free media‚Äôs right to investigate vs. the protection of
the individual. Efficiency ‚Äì or economic rationale ‚Äì may imply                           13
increasingly large data banks across different domains, which not
only challenge privacy but also create oligopolies.

Furthermore, this regulatory balance is a moving target. From the
start of the digital age, technological or market innovation has tended
to outpace rules. This is even truer of AI and other recent breakthroughs
that rely on the computing performance of algorithms.

It is not surprising that the collection of data and the protection of
privacy have become pervasive issues. It is a natural starting point,
short of de-inventing the algorithm techniques that turn data into a

   ET Bureau, ‚ÄúJustice Srikrishna Committee Submits Report on Data Protection. Here‚Äôre
15 

   Its Top 10 Suggestions,‚Äù The Economic Times, July 27, 2018, https://economictimes.
   indiatimes.com/articleshow/65164663.cms?utm_source=contentofinterest&utm_
   medium=text&utm_campaign=cppst.
     D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?




     weapon. Viewed with Shoshana Zuboff‚Äôs accusatory words, ‚ÄúIf new
     laws were to outlaw extraction operations, the surveillance model would
     implode.‚Äù16 Let‚Äôs remember that the earlier legislation on data collection
     was a simple translation into the nascent computing age of reservations
     against filing individuals. Sweden‚Äôs Data Act of 1973 required
     authorization from a national Data Authority for storing personal data,
     and then guidelines from this authority. France‚Äôs text in 1978 ‚Äì the Loi
     Informatique et Libert√©s, as it came to be called ‚Äì was a one-pager
     banning the use for any public or private decision of data resting on the
     sole use of automated files profiling or containing information as to the
     personality of an individual. Judicial decisions could not use these
     automated files at all.17 In short, the law neither banned the collection
     nor the use of files, but only their cross-referencing and derived decisions
     from them. These restrictions were original, yet they appear even more
     flimsy today than they were at the time.
14
     But how far to go, and in which direction? What is certain is that
     data flows are too beneficial for global growth to be substantially
     hindered by regulation or control ‚Äì unless a very deep cost is
     accepted. In the decade 2005-2014 only, data flows (information,
     searches, communications, transactions, video, and intracompany
     traffic) were multiplied by 45. Their contribution to the increase in
     GDP is greater than that from flows of goods. E-commerce also
     represented 12% of all traded goods in 2015. In fact, the advent of
     3D manufacturing is likely to reduce trade in goods in the near
     future, as production will often be re-localized. This dry economic
     assessment does not include the multiple benefits and convenience
        Shoshana Zuboff, The Age of Surveillance Capitalism : The Fight for a Human Future
     16 

        at the New Frontier of Power (New York: Publicaffairs, 2019), p. 105.
        ‚ÄúAssembl√©e nationale et S√©nat, ‚ÄúLoi N¬∞ 78-17 Du 6 Janvier 1978 Relative √† l‚Äôinforma-
     17 

        tique, Aux Fichiers et Aux Libert√©s‚Äù, https://www.legifrance.gouv.fr/affichTexte.do?cidT
        exte=JORFTEXT000000886460
                                    I . D E F I N I N G T H E I S S U E A N D T H E D E B AT E




that the digital age will increasingly offer. Unlike labor or machines
from the industrial age, information is scalable and is not exhausted
when consumed. ‚ÄúThe economics of digital information, in short,
are the economics not of scarcity but of abundance. This is a
fundamental shift, and a fundamentally beneficial one. If you have
Internet access and a connected device today, it is both free and
easy to keep in touch with the people who mean something to
you‚Äîyour kith and kin‚Äîeven as you and they move around. As
digital technologies make markets and businesses more efficient,
they benefit all of us as consumers.‚Äù18 Even the asymmetry of
information between operators described above is not complete. The
diffusion of information technologies ‚Äì if the required education
needs have been met ‚Äì can empower microfirms and companies
generally considered to be outside the main production centers.

Awareness of their danger has come from experience with authoritarian                            15
states. George Orwell is the inescapable literary reference, along
with its cinematographic homologue, Black Mirror. China‚Äôs march
towards a dystopia of gigantic data collection and use by the state
is a case in point. But this reality is not confined to authoritarian
systems. The laws may be different, but the tools are often very
similar ‚Äì if only because there has been much cross-breeding, for
example, between Silicon Valley or the American digital scene, and
its competitors which have appeared in China and sometimes
dwarfed their models in size. To assess calmly what a ‚Äúsurveillance
state‚Äù can achieve, one needs also to understand the tools already
developed by what Shoshana Zuboff, for example, calls the new
‚Äúsurveillance capitalism.‚Äù

   Erik Brynjolfsson and Andrew McAfee, Race against the Machine: How the Revolution
18 

   Is Accelerating Inovation, Driving Productivity, and Irreversibly Transforming Employment
   and the Economy. (Lexington, Mass.: Digital Frontier Press, 2011), p. 46.
     D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?




     Clearly, the legal environment makes a difference, even in a situation
     where data gathering cannot be de-invented, and where the available
     technologies create a huge asymmetry between the individual and
     the state ‚Äì or the corporation. The use of Google‚Äôs search engine
     can‚Äôt put you in jail in our societies (leaving aside the ‚Äúdark web‚Äù),
     while in Xinjiang (China), landing in the algorithms of the Integrated
     Joint Operations Platform (IJOP, ‰∏Ä‰ΩìÂåñËÅîÂêà‰ΩúÊàòÂπ≥Âè∞ the phone
     app that collects data for the Ministry of Public Security) has meant
     internment in re-education camps for upwards of a million minority
     citizens.19 Yet the tools ‚Äì data collection, linking up various sources
     and using predictive algorithms ‚Äì do have a distant relationship. It
     is even more obvious when one compares China‚Äôs fast-rising social
     credit system with the decades old credit-rating system that is
     particularly prevalent in the United States. The last is constantly
     being refined by the introduction of new algorithms. Hundreds of
16   thousands of Chinese are denied fast train, airline and other
     reservations because they have acquired a bad social credit rating
     which is unrelated to train travel, with little means to correct their
     score. In the United States, if you don‚Äôt pay the insurance premiums
     on your car, an insurance company can turn off the engine from a
     distance and at the time of its choosing: this is bad enough, but at
     least the penalty is directly related to the financial default.20



        ‚ÄúChina‚Äôs Algorithms of Repression | Reverse Engineering a Xinjiang Police Mass
     19 

        Surveillance App,‚Äù Human Rights Watch, May 1, 2019, https://www.hrw.org/
        report/2019/05/01/chinas-algorithms-repression/reverse-engineering-xinjiang-
        police-mass-surveillance.
        Gary Hoffman, ‚ÄúCar Payment or Else: Engine Shut off Systems,‚Äù Autoblog, December
     20 

        19, 2016, https://www.autoblog.com/2009/06/27/engine-shut-off-systems/?guce_ref
        errer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8&guce_referrer_sig=AQAAAC8mFb3f
        rqN6hIkaLhdvtwsLC0OW6P1A5Sn7mS8i9tvO6Lc8qsEm0fjWMPZk7SkW8_kirojwha
        uEJtoFnbnBbORFaKKrrjtqBaJnvN_0l2V_mNKAEniWBrGz1dOYbcTr5hZIHRDTODW-
        q6F2npEHCZeJSXExx6d9aiZqLTd8YLV5&guccounter=2
                            I . D E F I N I N G T H E I S S U E A N D T H E D E B AT E




The U.S. Matrix
How societies deal with this challenge, involving both the positive
uses of the digital age, and its downside and terrifying possibilities,
is a question for everyone to consider. It has many angles, but that
of privacy, and the debates surrounding this right, is a useful one
because it sits at the intersection of the individual and the collective,
of technology and regulation. It encapsulates large differences across
societies and political systems, and in some ways, it resembles two
earlier debates. In a narrow sense, it is those surrounding libel laws
that arose with the print media. In a larger perspective, it is the
debate for or against nuclear energy. Like the former, it is directly
linked to the issue of freedom and its restrictions. Like the latter, it
pits against one another the promoters of efficiency against those
who prioritize more precautionary concerns. And it spans both
debates in terms of scale. The digital age combines extreme                              17
centralization of data, platforms and operators, up to and including
the Big State and giant companies, with an extremely wide
dissemination of at least some of its features. Even before edge
computing becomes commonplace, social media creates chain
reactions that are akin to biological events such as epidemics. Rumors
and fake news were the matter of earlier centuries, replaced by mass
propaganda in the 20th century. They are back.

The privacy debate has two matrixes. One is clearly the United States.
Digital technologies are largely invented there, the giant and not so
giant companies that pioneer these have a global influence. Academic
freedom together with a taste for relevant issues means that the very
concept of privacy has largely been redefined there. The culture of
litigation is equally important. Many test cases involving big data
and privacy are arbitered by the courts, with precedent-setting
     D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?




     outcomes and large financial consequences. The prevailing wind
     has also shifted from blind faith in the promises of the digital age
     to awareness of its dangers for privacy and freedom. To some extent,
     the Apple generation (starting from a garage in 1976) directly
     followed the previous Woodstock Festival generation (1969),
     including its rebellion against prevailing corporate culture. Today,
     the next generation fights digital companies for privacy rights. The
     crusaders for digital privacy are also heirs to Ralph Nader and his
     life-long fight for consumers against corporations, from the Corvair.

     America is therefore the mother of all privacy debates, and it has
     enacted important legislation in the past. Based on the Fourth
     Amendment (‚Äúeach man‚Äôs home is his castle‚Äù ‚Ä¶), the Wiretap Statute
     (1968) and the Electronic Communications Privacy Act (ECPA,
     1986) have defined the limits on collection and storage of data ‚Äì
18   with important loopholes and oversight, such as third-party use of
     personal data. Further legislation has often been about foreign
     intelligence and linked to, or justified by, terrorism, up until the
     revelations from the Snowden affair (2013). Yet, privacy has not
     been, at least during the last decades, at the forefront of federal
     legislation for several reasons. Instead there are sectoral regulatory
     regimes, especially in the health and financial data sector, which
     are patterned after the Federal Trade Commission‚Äôs (FTC) fair trade
     principles. Privacy and its protection have largely remained for the
     courts to arbitrate ‚Äì litigation and torts are a slow and procedural
     way to establish precedents from single cases, rather than adjudicate
     ex ante issues from above. This may not be less effective, as we
     shall see, but it is infinitely harder to describe, particularly when the
     judicial process is spread among 51 states. In fact, different rules
     bearing on digital privacy exist across these 51 states, without any
     of them being systematized. The federal government has been slow
                                     I . D E F I N I N G T H E I S S U E A N D T H E D E B AT E




to address, if not to recognize, fully the issue. Two successive
administrations have each had their own priorities: under G.W. Bush,
the fight against terrorism that coincides with a significant
downgrading of privacy issues where national security is concerned.
The Obama administration did not break with this trend. It was
almost unanimously supported, and therefore influenced, by the
Valley ‚Äì that constellation of new business moguls, start-up
entrepreneurs and talented geeks who have in common an unlimited
faith in the digital age. In broad policy terms, it advocated for an
approach to digital privacy rights that differs from the European
approach, but it has not acted very decisively on its own
recommendations.21

Both administrations have also recognized the extraordinary edge
that new digital technologies give to the American economy ‚Äì making
up for what is lost in manufacturing. But the links between the Trump                             19
administration and Silicon Valley are far less symbiotic than was the
case for its predecessor. It makes a new assessment of the situation.
It may encourage antitrust decisions that hinder the horizontal growth
of major platforms and digital companies across sectors ‚Äì and that
may place limits on big data aggregation, therefore favoring privacy
rights. But the administration also lives in a context where competition
with China is the first foreign priority. Moving with one arm tied
behind one‚Äôs back in big data, algorithms and the like is certainly
not the preferred policy option. Again, interesting developments for
privacy ‚Äì including, as we shall see, the influence of the popular
European General Data Protection Regulation (GDPR) ‚Äì happen


   See especially, Executive Office of the President , ‚ÄúBig Data and Privacy: A Technological
21 

   Perspective,‚Äù President‚Äôs Council of Advisors on Science and Technology, May 2014,
   https://bigdatawg.nist.gov/pdf/pcast_big_data_and_privacy_-_may_2014.pdf.
     D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?




     mostly at state level. At the level of Congress, the Cloud Act (2018)22
     is mostly an outcome of issues associated with international data-
     sharing. It is an example of the extra-territorial reach of American
     law, and constitutes a victory for the federal government over digital
     companies on the issue of turning over data stored outside the United
     States.

     Inevitably, privacy debates therefore refer to companies, experts,
     and often judiciary decisions that originate in the U.S. Yet the
     American scene is so diverse that it defies any characterization, and
     hence is not the focus of our study. In one legal expert‚Äôs view, ‚ÄúU.S.
     privacy law is a smorgasbord (‚Ä¶) sectoral statutes and torts cover
     narrowly defined behavior, and some additional constitutional
     proscriptions apply to government activity. But most private data-
     handling activities in the United States fall outside all these laws.
20   In the absence of general-purpose omnibus privacy law like the E.U.
     Directive, consumer protection regulators such as the FTC and state
     attorneys general have moved in to fill the resulting vacuum.‚Äù23 The
     United States therefore remains a complex reference point.

     There are significant differences between the American and European
     vision of privacy. ‚ÄúA consumer protection regime generally allows
     any collection and processing of personal data, unless it is specifically
     forbidden. Data protection law adopts the opposite default, permitting
     collection and processing only for a statutorily defined justification.
     In other words: in the United States, it is usually allowed unless the
     law specifically states that it isn‚Äôt, while in the EU it is not allowed

        ‚ÄúText - H.R.4943 - 115th Congress (2017-2018): CLOUD Act,‚Äù Congress (2018),
     22 

        https://www.congress.gov/bill/115th-congress/house-bill/4943/text.
        William McGeveran, ‚ÄúFriending the Privacy Regulators,‚Äù Arizona Law Review 58, no.4
     23 

        (2016): 959‚Äì1026, https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2820683.
                              I . D E F I N I N G T H E I S S U E A N D T H E D E B AT E




unless the law says it is.‚Äù24 An echo of this is clearly visible in different
views on default options.



Europe‚Äôs Normative Power in Action
Europe has increasingly become the other major influencer on the
debate. For a long time, Convention 108 was the only internationally
binding agreement on data protection. It was opened for signature
in January 1981 by the Council of Europe, and currently has 55
signatories, including non-member states. It forms a first basis for
data protection, although in very general terms: Russia, for example,
could sign it at this stage. The convention was modernized in 2018,
with the aim of also harmonizing it with the GDPR. It now includes
both manual and automatic processing, restricts states from creating
exemption to processing under certain categories and updates breach                        21
notifications. It also creates a Convention Committee to monitor and
supervise the application of the convention‚Äôs principles by the parties.
33 states have signed the modernized Convention so far.

The European Union has reached another level of protection with
its path-breaking GDPR, which came into force in May 2018. It is
worth noting that in the 88 pages of the superbly written text, the
word ‚Äúprivacy‚Äù occurs only twice ‚Äì and in the same footnote, referring
to a 2002 European Community directive on the protection of privacy
in electronic communications (often dubbed the ‚Äúcookie law‚Äù). The
only other mention that comes close is that of ‚Äúprivate and family
life‚Äù that crops us in paragraph 4 of the Preamble. The regulation
has a wider goal ‚Äì protecting natural persons in the processing of

24
     Ibid.
     D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?




     their personal data and ensuring the free flow of data across the
     Union. Focusing on the collecting and processing of personal data
     (rather than on its use, an issue we shall come back to), it is a fine
     balancing act between the protection of individuals, the explicitly
     recognized commercial need for free flow of data in and out of the
     EU, and a series of exemptions where legal requirements (largely
     dealing with security) or the public interest (ranging from health
     research to the media‚Äôs right to investigate) are concerned. Most of
     all, it is a top-down regulation that nonetheless takes in national
     exceptions. Unlike an EU directive, it supersedes existing national
     rules or laws (except where they might exceed the provisions of the
     GDPR) and mandates each member state to create a supervisory
     authority for implementation, processing complaints and reporting
     to the European Commission.

22   The GDPR is likely to be complemented by an e-Privacy regulation
     (replacing the 2002 directive), which is discussed since January
     2017. This will have a broader impact than the previous directives:
     it will deal with the privacy and confidentiality of all electronic
     communications (including the new messaging and communications
     apps, for instance), and not only the commercial harvesting of
     personal data via cookies. Few people are aware that these new
     means of communication are prime sources for scraping personal
     data. As such, the e-Privacy regulation is still the object of much
     debate. It could impact much more severely the advertising business,
     while providing a more certain legal environment for telecom and
     messaging providers of what is called Over the Top (OTT)
     communications in terms of allowable metadata. The e-Privacy
     regulation is very slowly making its way through European institutions
     and will be debated with the newly elected EU Parliament.
                                   I . D E F I N I N G T H E I S S U E A N D T H E D E B AT E




To the individual digital user in Europe, GDPR is essentially a fairly
unsystematic process of consent to cookies in order to access a
website for the first time. Indeed, the first year is showing discrepancies
across member states as well as issues regarding implementation.
But this hardly renders justice to the obligations placed on ‚Äúcontrollers‚Äù
‚Äì the companies and institutions that store and manage personal
data, for example. Furthermore, it is becoming an instance when
the European normative approach has a wide influence on global
debates, and influences rules in many other countries. This is not
purely the result of moral suasion. The European market, and its
digital flows, are huge. Accessing it is an economic necessity. Hence,
to ensure the interest of member states, it is no surprise that even
the new EU investment screening regulation, which entered into
force in April 2019, listed ‚Äúaccess to sensitive information, including
personal data, or the ability to control such information‚Äù as
determinants of a foreign direct investment‚Äôs security or public risk.25                        23


The EU has also put in place a regulation26 that provides criteria for
an ‚Äúadequacy decision‚Äù regarding third countries or international
organizations, thereby allowing the free transfer of data between the
EU and those countries. So far, 13 countries have been recognized,
with the United States being a special case as a ‚ÄúPrivacy Shield‚Äù
has been put in place that balances the impact of laws such as the


   ‚ÄúRegulation (EU) 2019/452 of the European Parliament and of the Council of 19 March
25 

   2019 Establishing a Framework for the Screening of Foreign Direct Investments into
   the Union,‚Äù EUR-Lex, 2019, https://eur-lex.europa.eu/eli/reg/2019/452/oj.
   ‚ÄúRegulation (EU) 2016/679 of the European Parliament and of the Council of 27 April
26 

   2016 on the Protection of Natural Persons with Regard to the Processing of Personal
   Data and on the Free Movement of Such Data, and Repealing Directive 95/46/EC
   (General Data Protection Regulation),‚Äù EUR-Lex, 2016, https://eur-lex.europa.eu/legal-
   content/EN/TXT/?uri=uriserv%3AOJ.L_.2016.119.01.0001.01.ENG&toc=OJ%3AL
   %3A2016%3A119%3ATOC.
     D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?




     Cloud Act.27 In preparation of its negotiation with the EU, Japan
     passed an Act on the Protection of Personal Information (APPI)28 in
     2017. Many more countries, including Brazil, India and South Korea,
     have applied. In America, some states ‚Äì often those that are also
     taking the lead on environmental protection ‚Äì are introducing similar
     legislation: the California Consumer Privacy Act (June 2018), the
     state of Washington Privacy Act (which failed to pass in April 2019).
     Texas, Massachusetts, New York and other states are considering
     similar bills. Leading academic experts on digital privacy routinely
     refer to the GDPR as an important precedent. One should add that
     many concepts and language used in the GDPR originate from
     American debates as well.

     It is therefore undeniable that Europe is a key mover of privacy
     debates, and of the process of arbitrating between the three sides
24   of the triangle described above ‚Äì privacy, efficiency and security. As
     such, it will be the primary focus of our study. To supplement our
     understanding of data protection, we will simultaneously have two
     other cases, both countries which are increasingly becoming
     prominent in the digital debate.



     India Between Models
     The Indian case is the most clear-cut case for examining GDPR‚Äôs
     international impact and potential as a model for recently proposed
     legislation on data protection. In using concepts like the control of

        ‚ÄúAdequacy Decisions,‚Äù European Commission, 2019, https://ec.europa.eu/info/law/law-
     27 

        topic/data-protection/international-dimension-data-protection/adequacy-decisions_en.
        Text and overview available at ‚ÄúLaws and Policies,‚Äù Personal Information Protection
     28 

        Commission Japan, https://www.ppc.go.jp/en/legal/.
                                   I . D E F I N I N G T H E I S S U E A N D T H E D E B AT E




cross-border data transfers, notice and consent through privacy
policies and the creation of a supervisory data protection authority,
the Personal Data Protection Bill (henceforth, PDPB) appears to have
been heavily modelled on the GDPR. India‚Äôs digital sphere dwarfs
that of Europe, in size if not in revenue. Like Europe, India‚Äôs digital
sphere is also dominated by outside actors ‚Äì America‚Äôs platforms,
apps and software. In India‚Äôs case, this is also shared with actors
from China, who are already on top of the smartphone scene and
tend to become very active in social media and gaming. The question
of Indian firms gaining back the digital economy against powerful
foreign companies is voiced more pressingly in India than in Europe.
The similarities and the simultaneous differences between the
European and Indian cases thus make India a significant comparison
test for the European data protection regime.

Unlike Europe, India also has a constitutional system at the top,                               25
with a huge informal society and economy at the bottom. As noted
by a draft national policy, ‚ÄúToday, two of three people in India do
not have access to the kind of connectivity needed for digital trade
and e-commerce. In addition, there is the problem of digital literacy
and skills with only about 15% of rural households being digitally
literate.‚Äù29 While it has continued to struggle with infrastructural
impediments to digitalization, the number of internet users in India
rose from 4 million in 2007 to 420 million in 2017 (equivalent to
the users of the U.S., the U.K. and Germany combined). This is a
constantly increasing consumer base, which is projected to reach


   Digital literacy is defined as ‚Äúat least one person in the household who can use a
29 

   computer, tablet or smartphone.‚Äù Source: ‚ÄúDraft National E-Commerce Policy,‚Äù February
   16, 2019, p. 30, https://dipp.gov.in/sites/default/files/DraftNational_e-commerce_
   Policy_23February2019.pdf.
     D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?




     by 2025 the equivalent of the G7‚Äôs combined users.30 In addition
     to a large consumer base, India has important human resources in
     the digital area. In terms of industry and employment, digital India
     is much more closely intertwined with American companies than
     with Europe. In 2017, American companies generated more than
     11 billion USD in revenue for India‚Äôs huge data analytics industry;
     the UK generated 170 million USD (largest share in the EU) and
     the Netherlands produced 37 million (third largest share).31 ‚ÄúDigital
     India‚Äù and related goals with proactive policies in this domain are
     a defining aspect of Narendra Modi‚Äôs government. However, they
     have not been invented by this administration. Aadhaar, a unique
     national identification system derived from biometric and demographic
     data, was launched in 2010. An ambitious project, the Aadhaar
     card was also in many ways the start of the legal data privacy debate
     in India. The debate extends as well to the security lapses in the
26   setting up of Aadhaar.32

     While the proposed legislation also extends to private actors, there seems
     to be an undercurrent to the bill to protect local economy over foreign
     companies, especially American and Chinese tech companies that
     currently dominate the tech scene.33 India seems to be looking towards
     the policy tools used by China in this regard. This is illustrated by the
     data localization requirement in the PDPB for example. ‚ÄúWe don‚Äôt want

        Rishi Iyengar, ‚ÄúThe Future of the Internet Is Indian,‚Äù CNN, November 27, 2018, https://
     30 

        edition.cnn.com/interactive/2018/11/business/internet-usage-india-future/.
        Source: Analytics India Magazine, AnalytixLabs, cited by Sandhya Keelery, ‚ÄúInfographic:
     31 

        India: Decoding Data for the Dollar,‚Äù Statista Infographics, May 23, 2018, https://www.
        statista.com/chart/13935/decoding-data-for-the-dollar-india-analytics/.
        For an example: ‚ÄúIndian state government leaks thousands of Aadhaar names‚Äù, TechCrunch,
     32 

        February 1, 2019, https://techcrunch.com/2019/01/31/aadhaar-data-leak/
        Newley Purnell, ‚ÄúIndia Looks to Curb U.S. Tech Giants‚Äô Power,‚Äù The Wall Street Journal,
     33 

        August 13, 2018, https://www.wsj.com/articles/india-looks-to-curb-u-s-tech-
        giants-power-1534178721.
                                  I . D E F I N I N G T H E I S S U E A N D T H E D E B AT E




to build walls, but at the same time, we explicitly recognize and
appreciate that data is a strategic asset,‚Äù said Aruna Sundararajan, the
nation‚Äôs secretary of telecommunications, who has been deeply involved
in the policy discussions for yet another proposed policy, the national
e-commerce policy that calls for a ‚Äúlevel playing field‚Äù for Indian
companies.34 The PDPB also offers exemptions for the state - often
worded vaguely, offering broad, sweeping powers to the state machinery.
Yet, India is a democratic, constitutional system and the executive has
the judiciary to answer to.

In a way, India seems to be a bridge between the European and
Chinese cases, modelling its legislation on the GDPR while using it
as an instrument for its industrial policy. With its budding expertise
and presence in the domain, it is also an important player to break
the Sino-American hegemony in the digital sphere. Most importantly,
in terms of adequacy, the Indian case remains significant for the EU.                          27
The developments in the latter‚Äôs data protection regime could facilitate
or disrupt trade flows between the two, as they heavily involve data
transfers. The significance is also reflected in the European
Commission‚Äôs active participation in India‚Äôs consultation process for
data protection.35 In subsequent parts, which are in no way a complete
look at India‚Äôs large and diverse digital scene, often ignored by
Europeans, we attempt to view the legal situation and debates in India
regarding privacy and the protection of personal data.


   Vindu Goel, ‚ÄúIndia Pushes Back Against Tech ‚ÄòColonization‚Äô by Internet Giants,‚Äù The
34 

   New York Times, August 31, 2018, https://www.nytimes.com/2018/08/31/technology/
   india-technology-american-giants.html?module=inline.
   Bruno Gencarelli, ‚ÄúSubmission on Draft Personal Data Protection Bill of India 2018 by
35 

   the Directorate-General for Justice & Consumers to the Ministry of Electronics and
   Information Technology (MeitY),‚Äù European External Action Service, September 29,
   2018, https://eeas.europa.eu/delegations/india/53963/submission-draft-personal-data-
   protection-bill-india-2018-directorate-general-justice_en.
     D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?




     China‚Äôs rules
     The second case is China, previously alluded to in this chapter as
     an authoritarian dystopia in the digital world. It is a model to some
     in the technologies of surveillance, and in taming social media.
     China‚Äôs quasi-intranet, considerably enhanced by recent rules
     governing international data transfer, is also a source of inspiration.
     ‚ÄúWe must create a segment [of the internet] which depends on
     nobody‚Äù, said Vladimir Putin, showing Russia‚Äôs tendency towards a
     Chinese-style intranet.36 On November 1, 2019, Russia‚Äôs ‚Äúsovereign
     internet‚Äù law has taken effect, giving the Russian government, in
     case of emergency, the power of switching its internet on and off
     from the outside world.37 The Chinese rules are not meant to ensure
     data privacy, but to protect national security in the broadest sense
     possible, while keeping out undesirable information and opinion,
28   also in the name of security. ‚ÄúInternet information service providers
     shall not produce, reproduce, distribute or disseminate information
     that (‚Ä¶) impairs national security, divulges State secrets, subverts
     State sovereignty or jeopardize national unity‚Äù, stresses the 2000
     Administrative Measures on Internet Information Service. The 2017
     Cybersecurity Law clearly states in Article 1 that the law is formulated
     not only to ensure cyber security, but also to safeguard cyberspace
     sovereignty and national security. The 2018 Personal Information
     Security Specification exempts the need for consent when national
     security is concerned. The list goes on, and it is a visible feature of
     the Chinese rules that China does not try to hide.


        Andrew Roth, ‚ÄúRussia‚Äôs Great Firewall: Is It Meant to Keep Information in ‚Äì or Out?,‚Äù
     36 

        The Guardian, April 28, 2019, https://www.theguardian.com/technology/2019/apr/28/
        russia-great-firewall-sovereign-internet-bill-keeping-information-in-or-out.
        ‚ÄúRussia Internet: Law Introducing New Controls Comes into Force,‚Äù BBC News, November
     37 

        1, 2019, https://www.bbc.com/news/world-europe-50259597.
                             I . D E F I N I N G T H E I S S U E A N D T H E D E B AT E




China has vertical control over the internet and digital data. But uses
are horizontal. Each of China‚Äôs huge platforms cuts across different
sectors, covering many different activities. This horizontal nature of
China‚Äôs companies hardly goes unnoticed. It is particularly evident
with the stories of the social credit system widely discussed on
Western media. Alibaba, the e-commerce company has successfully
covered all aspects of the Chinese citizen through the service it
provides, as well as acquiring shares in other companies. Through
its own access and that of related companies, Alibaba has an
unmatched view of Chinese customers across their life. The table
below shows the main sectors covered.

                        Alibaba‚Äôs Big Data: Internal Sources
  1   E-commerce data                          Taobao, Tmall, Alibaba
  2   Payment data                             Alipay
  3   Dating data                              Wangwang, Laiwang
                                                                                          29
  4   Video data                               Youku
  5   Browser data                             Taobao Browser
  6   Search data                              etao
  7   Game data                                AliGame
  8   Music data                               Xiami Music
  9   Travel data                              Qyer
 10   Map data                                 Gaode Map
 11   ID data                                  Taobao Account
     D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?




                                              Alibaba‚Äôs Big Data: External Sources
                                          Purchased Party                     Share of Alibaba
         Search Engine         Yahoo China                                          40
                               Koubei                                               100
                               Meiuan                                               10
            Daily Life
                               Kuaidi Dache                                         100
                               Gaode Map                                            100
       Social and Mobile       Sina Weibo                                            18
            Internet           UC Browser                                           100
                               Xiami Music                                          100
                               Culture China                                         60
                               Wasu TV                                               20
             Culture
                               Youku Tudou                                           16
                               Evergrande Football                                   50
30                             21st Century Media                                    20
                               Tianhong Asset Management                             51
             Finance
                               Hundsun Technologies                                 100
            Logistics          Singapore Post                                        10
     Table Source: Chu Zhang and Leng Xin, ‚ÄúResearch on the Application of Big Data in
     E-Commerce Enterprises Â§ßÊï∞ÊçÆÂú®ÁîµÂïÜ‰ºÅ‰∏öÁöÑÂ∫îÁî®Á†îÁ©∂,‚Äù Journal of Chuzhou Vocational
     & Technical College 15, no. 5 (March 2018).


     This is an advantage that non-Chinese companies find hard to
     compete with. The comparatively lower awareness of the Chinese
     regarding privacy and the willingness of the Chinese government to
     prioritize state objectives over any other consideration, make the
     Chinese case unique. This applies to national security with a broad
     and in fact unlimited definition. It is also exemplified by the state‚Äôs
     emphasis on economic development, as shown by the case of the
     social credit system, which aims to create a trustworthy society that
                                  I . D E F I N I N G T H E I S S U E A N D T H E D E B AT E




facilitates growth, and by the ‚Äúinternet plus‚Äù strategy that aims to
create new business sectors. The booming of the online economy
has created new business opportunities: they include personal data
theft and illegal sales. A recent China Daily article celebrates the
detention of 7460 individuals by Guangdong police during a special
campaign in the first eight months of 2019. As reported, 400 million
items of stolen personal data, that was used by criminal gangs to
defraud, were identified. 38 The case gives an indication of the
magnitude of the issue regarding personal information protection.

The cases of China and India illustrate the tensions between privacy,
marketing interest and state control, with very different solutions.
Digital privacy is a priority that bumps against other goals, and
Europeans must take into account that these goals are emphasized
differently in China, India or the United States ‚Äì the other three
largest digital markets.                                                                       31


Innovation is one such goal, and in effect a requirement. The multiple
applications and improved productivity that digital developments
allow cannot be summed up adequately by the notions of efficiency
or productivity: these are measures of economic performance, and
the digital age delivers much more than just that. Furthermore,
European regulations do not exist in a vacuum. They can hope to
exercise leverage thanks to the size and attractiveness of the European
data market, but innovation may thrive in less demanding regulatory
environments.




   Caixiong Zheng, ‚ÄúGuangdong Police to Intensify Fight against Personal Data Theft -
38 

   Chinadaily.Com.Cn,‚Äù Chinadaily, September 19, 2019, https://www.chinadaily.com.
   cn/a/201909/19/WS5d833fd8a310cf3e3556c711.html.
     D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?




     Public interest is another requirement. Security or even public order
     do not encompass it completely ‚Äì the example of the health sector
     provides an illustration, but we could have covered education,
     autonomous driving and many other promising areas. The
     requirements of public interest may clash with the objectives of data
     privacy: the issue is a two-way street. It is clear that the goal of
     ensuring privacy must find a compromise with that of delivering
     public goods, a definition that is wider than just public order or
     security.




32
                                  II

                WHAT IS PRIVACY AND
             HOW CAN IT BE ENSURED?
Data privacy is an umbrella term that is intuitively understood by
everyone, but that is not easily defined. It is not confidentiality, nor
is it cybersecurity, although it incorporates a bit of both. You may
want to communicate information, including that of personal nature,
to select people whilst not making that information public: it is the
case with much personal data formerly stored on your thumbnail or
hard drive, and now usually in a cloud. This is privacy combined
with confidentiality. You may want that data to be safe from hacking
by an individual or organization, and that is privacy associated with
security. You may want your personal data to be treated anonymously
(which is often the case with health information) in your immediate
or long-term medical interest, but you do not want to see that data        33
be used to assess, for instance, your insurance risk profile: this is
about the use rather than the collection of personal data. Making
decisions away from government eyes, and not suffering discrimination
based on these decisions, or on your personal characteristics, is also
closely tied with privacy.

Privacy is both an expanding bubble and a receding reality. In the
4th Amendment of the American Constitution, it is stated that ‚Äúa
man‚Äôs house is his castle‚Äù: much legal wrangling in recent decades
has been about the right to search cars, and if these are extensions
of ‚Äúone man‚Äôs castle‚Äù, with varying interpretations. Guarding against
intrusion is not the same as ensuring privacy however, and American
law heavily balances privacy with freedom ‚Äì including the freedom
to investigate. In the European legal context, the very notion that
the personal data of a human being can be traded is debatable: the
     D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?




     selling of our mind is no more acceptable to some than surrogate
     motherhood to many, or organs to most (the last is legal in Iran
     nevertheless, including for prisoners on death row: attitudes differ).
     In legal terms, privacy is expressed in terms of data protection.
     Personal data is the key focus of data protection regulations, and
     lies at the center of the privacy debate. According to the GDPR,
     personal data is ‚Äúany information relating to an identified or
     identifiable natural person‚Äù either directly or indirectly.

     Personal data can no more be separated from the individual than
     the mind from the body. But the opposition is in this case less
     prevalent because this is not a break-in or phishing. Opening mail,
     or even deciphering encrypted communications, are forms of
     intrusion, since the letter opener or code breaker are not legitimate
     receivers of these messages. Recombining information that has
34   already become data, for instance with the help of algorithms, is an
     analytical rather than intrusive process. After one enters the domain
     of big data collection and treatment, an accepted distinction has
     long been made between data ‚Äì whether digital or analog by source
     ‚Äì that extracts information from individuals, and metadata that
     consists of descriptors of actual data: in the analogous world,
     recording a phone conversation and merely noting the time and
     duration of its occurrence between two individuals are not identical.
     For digital files, metadata allows storage, selection and communication
     of data with a guarantee of authenticity that is ensured by metadata
     standards.

     What is one to make of data fusion from distinct data collection
     sources combined with algorithms? This combination transforms
     lead into gold, or isolated data points into recognizable identification
     and knowledge of individuals. There is no code breaking, no virtual
                I I . W H AT I S P R I VA CY A N D H O W C A N I T B E E N S U R E D ?




breaking and entering, and every step of the operation can be wholly
legal, while the result is a trove of personal and often, sensitive data.
These possibilities have been popularly illustrated for some time by
the issue of anonymized data, or metadata. Data that has been
anonymized can often ‚Äì and increasingly, almost always ‚Äì be
de-anonymized in practice. In classic demonstrations, researchers
were able to re-identify individuals by combining their zipcode, sex
and birth date ‚Äì with 84% certainty ‚Äì or by matching movie searches
on the IMDB website with Netflix data, or simply from released AOL
search queries.39 What is more, these examples show some of the
ways in which the field of personally identifiable data has expanded
‚Äì certainly much beyond an individual‚Äôs ability to understand the
consequences from the innocuous data that he surrenders as he
leaves traces on the internet. Mood changes, but also the onset of
Alzheimer‚Äôs disease, can be inferred from keyboard click streams.
In these cases, metadata morphs back into personal data, and the                         35
distinctions made by existing regulations can be voided. The
alternative is to impose standards that are so wide that they create
an obligation of results, and not simply of process, for the data
operators. That, in fact, is often the case with the European GDPR
that places the emphasis on results rather than on technical
processes, but with ominous consequences: the prescription may
not be implemented, or it may become a huge constraint on digital
activities. It is, as we shall see, a political choice, the consequences
of which have not yet been assessed and perhaps cannot be assessed
fully.




   These three cases are well-documented in Paul Ohm, ‚ÄúBroken Promises of Privacy:
39 

   Responding to the Surprising Failure of Anonymization,‚Äù UCLA Law Review 57 (2010),
   p. 1701, https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1450006.
     D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?




     Still, the case for having regulation overshoot its target ‚Äì legislating
     on results rather than on means ‚Äì is straightforward. Current
     guarantees for privacy are risky. It is impossible to predict how data
     fusion, grouping together ever-expanding fields of data, and algorithms
     that have not yet been identified, may render obsolete today‚Äôs
     technological or regulatory protection of privacy. In the words of a
     presidential commission under the Obama White House, ‚Äúsecurity
     deals with tomorrow‚Äôs threats against today‚Äôs platforms. That is hard
     enough. But privacy deals with tomorrow‚Äôs threats against tomorrow‚Äôs
     platforms.‚Äù40



     Anonymization and Pseudonymization of Personal
     Data

36   Methods towards de-identification have become more comprehensive,
     including for example, the obligation to use limited data sets to
     prevent data fusion on a large scale. Recent research proves this is
     becoming a harder task. It is likely that de-identification or
     anonymization techniques will have to become more and more
     sophisticated. Presently, with only five available data points and
     across 210 different populations, de-anonymization is achieved over
     0.84 to 0.97 cases. With 15 data points, 99.98% of Americans
     can be re-identified. The results do not differ much if smaller samples
     are used. What‚Äôs more, mean absolute failure is very low ‚Äì under
     0.041 when used on a 1% population sample. This is important
     because it means that ‚Äúyou know what you don‚Äôt know.‚Äù Failures to
     identify do not carry the same consequences as mistakes in

        Executive Office of the President , ‚ÄúBig Data and Privacy: A Technological Perspective,‚Äù
     40 

        President‚Äôs Council of Advisors on Science and Technology, May 2014, https://bigdatawg.
        nist.gov/pdf/pcast_big_data_and_privacy_-_may_2014.pdf.
               I I . W H AT I S P R I VA CY A N D H O W C A N I T B E E N S U R E D ?




identification. In other words, the results tend to be less and less
deniable ‚Äì reaching certainty as the number of data points grow.41
A low estimate for digitalized populations is that each individual has
created an average 5000 data points, a figure that will multiply with
the uses permitted ‚Äì and often not yet invented ‚Äì for 5G and the
IoT. Smart cities and smart homes will entail or technically allow
the collection of infinitely more data points ‚Äì think of video cameras
as the most obvious example. One existing device, the Nest
thermostat, learns what you like, ‚Äúknows when you‚Äôre away, learns
about your home, controls it when you are away.‚Äù42 To achieve this,
the device tracks movement, sound, electrical consumption of all
devices. It only lacks front cameras ‚Äì which other Nest devices
employ.

From the above, one clearly sees that privacy is not ensured by
anonymization alone. Yet, it includes this as an aspirational goal                         37
and as we shall see, anonymity features prominently in almost all
privacy regulations. It is safer to adopt a very wide definition for
privacy, an umbrella term covering ‚Äúrules and norms on action and
inaction related to our personal information.‚Äù43 Woodrow Hartzog
further ties privacy to ‚Äútrust, obscurity and autonomy.‚Äù Trust is
opposed to control ‚Äì as we shall see, no individual can hope to
manage and effectively control his/her personal data. Obscurity is
preferable to secrecy ‚Äì an unattainable goal again for individuals.


   Luc Rocher, Julien M. Hendrickx, and Yves-Alexandre de Montjoye, ‚ÄúEstimating the
41 

   Success of Re-Identifications in Incomplete Datasets Using Generative Models,‚Äù Nature
   Communications 10, no. 1 (July 23, 2019), https://doi.org/10.1038/s41467-019-
   10933-3.
   Nest is now owned by Google. The claims are made at ‚ÄúReal Savings,‚Äù Nest, https://
42 

   nest.com/thermostats/real-savings/.
   Woodrow Hartzog, Privacy‚Äôs Blueprint : The Battle to Control the Design of New
43 

   Technologies (Cambridge: Harvard University Press, 2018), p. 10.
     D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?




     Autonomy is the preservation of a secluded garden for making up
     your own mind - including the right for individuals to engage in
     meaningful exchange among partners of choice.



     Social Media and Personal Data
     The digital age expands considerably our social horizon although,
     many will argue, that is at the expense of the thickness in relationships.
     One person‚Äôs ‚Äúmeaningful relationships‚Äù max out at 150, according
     to a famous social psychologist, while our capacity for name
     recognition is said to extend to 2000.44 Social networking sites use
     filter settings: Facebook is often said to limit that number at 5000,
     Twitter limits to 1000 the number of messages that you receive every
     day. This protects the ergonomy of the app. But algorithms also order
38   the ranking of messages in your inbox, supposedly according to our
     observed preferences.

     A form of privacy ‚Äì data protection ‚Äì is also a business requirement
     for data hoarders themselves. Facebook needs to protect its data
     simply because it sells their use. Conversely, the lure of free
     information and service over the internet is the biggest incentive to
     disregard one‚Äôs privacy. Because internet is free of charge to users
     (except carriers‚Äô fees), it is paid by others: advertising, often dubbed
     ‚Äúthe original sin of the internet.‚Äù From this flows a mirror conclusion:
     ‚Äúif it is free, it means you are the product.‚Äù In fact, some of the
     keywords for data extraction come straight out of the strip mining or

        Robin I. M. Dunbar, ‚ÄúThe Social Brain Hypothesis,‚Äù Evolutionary Anthropology: Issues,
     44 

        News, and Reviews 6, no. 5 (1998), p. 184, https://doi.org/10.1002/(sici)1520-
        6505(1998)6:5<178::aid-evan5>3.0.co;2-8. Cited by Woodrow Hartzog, Privacy‚Äôs
        Blueprint : The Battle to Control the Design of New Technologies (Cambridge: Harvard
        University Press, 2018), p. 109.
                I I . W H AT I S P R I VA CY A N D H O W C A N I T B E E N S U R E D ?




meat packing industries: ‚Äúdata scraping‚Äù, after which what‚Äôs left of
an individuality is the worthless ‚Äúcarcass.‚Äù VPNs are a partial remedy
hiding an individual user‚Äôs real Internet Protocol (IP) (and not much
more). There is a huge caveat ‚Äì few users stop to contemplate who
owns these VPNs and what secondary utility they might have. A
2019 study shows that a third of the world‚Äôs top VPNs are Chinese
owned, often through subsidiaries, Pakistan comes behind, with
‚Äúthe world‚Äôs worst cyber-law‚Äù, and VPNs based in the United States
are obviously not free of surveillance of non-citizens.45 In short, a
VPN can ensure protection against some, but seldom against all.

In terms of government surveillance, one major default of intentional
‚Äúbackdoors‚Äù installed on hardware or software design is that if the
backdoor exists, it can be also used by others ‚Äì the proverbial ‚Äúbad
guys.‚Äù This is akin to leaving the key under the doormat at your home.
Governments themselves, quite outside of legal considerations, struggle                    39
under two contradictory requirements for security, whether it is in the
area of encryption or that of its own covert access to data. For encryption,
increasing the level of coding increases protection. Lengthy keys are
also more cumbersome to use. The protection also works for illegal
communications. France first required all encryption keys to be
communicated to public authorities, then (from 1999 to 2004) only
for keys above 128-bit. Similarly, installing backdoors (as may have
been the case for Cisco routers as exposed by Edward Snowden) may
create entry points for others; conversely, creating a foolproof architecture
will lead to legal fights in order to access data: Apple gained a reputational
advantage by denying the FBI access to the iPhone of a slain terrorist
in the United States, at the expense of fighting terrorism.

   Jan Youngren, ‚ÄúHidden VPN Owners Unveiled: 99 VPNs Run by 23 Companies |
45 

   VPNpro,‚Äù VPNpro, June 2, 2019, https://vpnpro.com/blog/hidden-vpn-owners-unveiled-97-
   vpns-23-companies/.
     D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?




     Localization of Data and Data Sovereignty
     Since privacy is tied to collected data‚Äôs security, one major issue has
     been the control over networks, and the location of, and access to,
     data clouds. The worldwide web ‚Äì and in effect, digitalized data and
     communications ‚Äì were built on the free flow of data across borders.
     The geographical location of servers ‚Äì in the giant buildings that are
     in effect the cloud ‚Äì is often based on opportunity costs (mostly the
     average local temperature and/or price of electricity) rather than on
     security factors. Figures vary greatly, but one account has 24 major
     companies slated to operate 420 data centers classified as hyperscale
     servers at the end of 2018, replacing not only your hard drive or
     localized company servers, but even the first generation of clouds.46
     Hyperscale centers allow more collection and linkage of data, and
     are likely to replace a lot of physical network gear, for instance in
40   mobile communications. 45% of these centers were located in the
     United States in 2017, and 8% in China, its nearest competitor.
     The cloud solution leader, Microsoft, spends 15 billion USD each
     year on its cloud architecture, including the signature Azure brand.
     By way of comparison, the European Commission estimates that
     overall, 2 billion EUR in Horizon 2020 funding will be allocated to
     the European Cloud initiative over five years.47 In France, two
     attempts at domestic clouds with public funding have failed.48




        Jeff Borker, ‚ÄúWhat Is Hyperscale?,‚Äù Digital Realty, November 15, 2017, https://www.
     46 

        digitalrealty.com/blog/what-is-hyperscale.
        ‚ÄúThe European Cloud Initiative,‚Äù European Commission, August 17, 2018, https://
     47 

        ec.europa.eu/digital-single-market/en/%20european-cloud-initiative.
        Florian D√®bes, ‚ÄúUne Page Se Tourne Pour Le Cloud Souverain Fran√ßais,‚Äù Les Echos,
     48 

        August 1, 2019, https://www.lesechos.fr/tech-medias/hightech/une-page-se-tourne
        -pour-le-cloud-souverain-francais-1118112.
                I I . W H AT I S P R I VA CY A N D H O W C A N I T B E E N S U R E D ?




The issue of access and control over data has given way to government
policies designed to localize data on their territory. Sovereignty over
data is a politically sensitive theme. It is also one possible answer
to another issue: data privatization by metaplatforms, which are
increasingly taking over activities ‚Äì health, education ‚Äì that were
functions of public systems. For better or for worse, states now
delegate the set-up of their repetitive tasks ‚Äì such as civil servant
pay ‚Äì to IT companies. Mapping systems are increasingly led by
private actors such as Google and Apple. Only one nation, Estonia,
has established the state as a multitask platform.49 The issue of
control over this privatized data looms large. China‚Äôs solution is
hybrid control ensured through largely customary means between
theoretically private platforms and the Party-state, and rigorous
control of all data transfer. Some other countries ‚Äì such as India ‚Äì
mix acceptance of global platforms with at least an attempt to localize
data inside the country.                                                                       41


Conversely, the U.S. Cloud Act (Clarifying Lawful Overseas Use of
Data Act)50 is a prime example of the extraterritorial reach of U.S.
jurisdictions since it compels American companies storing data
abroad to turn these over at the request of domestic law enforcement
authorities. The requests cannot be in bulk. They have to be requested
by court and on the basis of probable criminal cause (and not on
the basis of national security for instance). Companies that claim
conflict with a foreign law can refuse the data transfer ‚Äì a very
important provision because it essentially ties the implementation
of the Cloud Act to the existence of compatible rules in other countries.

   See the 4-part series by Gilles Babinet, ‚ÄúThe End of Nation States?,‚Äù Institut Montaigne,
49 

   2019, https://www.institutmontaigne.org/en/series/end-nation-states.
   ‚ÄúText - H.R.4943 - 115th Congress (2017-2018): CLOUD Act,‚Äù Congress (2018),
50 

   https://www.congress.gov/bill/115th-congress/house-bill/4943/text.
     D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?




     The Act in fact authorizes the executive branch to sign data exchange
     agreements with foreign governments. This has renewed a tussle
     on data flows across the Atlantic ‚Äì which are still the largest data
     flows worldwide. A negotiated ‚ÄúSafe Harbour‚Äù treaty was earlier
     struck down by the Court of Justice of the European Union (CJEU).
     The U.S.-EU Privacy Shield Agreement, sometimes presented as a
     response to the Cloud Act, was in fact adopted earlier in 2016. It
     allows the free transfer of data ‚Äì including personal data ‚Äì to
     companies that are certified in the U.S. under the Privacy Shield,
     and is reviewed annually. It is also complemented by a U.S.-EU
     Data Protection Umbrella Agreement, essentially setting rules for the
     exchange of data between law enforcement authorities.51

     Both the Cloud Act and the Privacy Shield are the subject of
     considerable polemics. The yearly Commission reports provide some
42   perspective on outstanding issues: actual supervision of
     implementation by the U.S. Department of Commerce, the long-
     delayed nomination of an ombudsman that would provide permanent
     recourse for individuals and companies are currently the main sticky
     points. The Commission also ‚Äúencourage(d) the U.S. to adopt a
     comprehensive system of privacy and data protection.‚Äù52




        The Privacy Shield, the US-EU Umbrella Agreement and yearly reviews are helpfully
     51 

        available at ‚ÄúEU-US Data Transfers,‚Äù European Commission, https://ec.europa.eu/info/
        law/law-topic/data-protection/international-dimension-data-protection/eu-us-
        data-transfers_en.
        ‚ÄúThe Second Annual Review of the Functioning of the EU-U.S. Privacy Shield,‚Äù European
     52 

        Commission, December 19, 2018, https://ec.europa.eu/info/sites/info/files/report_on_
        the_second_annual_review_of_the_eu-us_privacy_shield_2018.pdf.
               I I . W H AT I S P R I VA CY A N D H O W C A N I T B E E N S U R E D ?




Privacy Policies, Notice and Consent
Notice-and-consent is one of the most widely used characterizations
of consumer privacy, and obliges the collection and use of data be
notified to consumer, in order for them to consent to it. This is done
most commonly through privacy policies. Of course, the requirements
for privacy can also diverge across societies and time. In a
characteristically blunt way, Jack Ma, the Alibaba founder, professes
to prefer Africa over Europe because ‚ÄúEurope is too concerned with
privacy and rules.‚Äù53 For what they are worth ‚Äì opinion surveys in
China‚Äôs controlled environment are a debatable enterprise, 38 % of
the Chinese public would be willing to give up data privacy, usually
in the interest of safety and trust in transactions. Aadhaar ‚Äì India‚Äôs
wholesale filing of all citizens based in part on a biometric recognition
system ‚Äì would have met stronger opposition had it been applied
in Europe (which nonetheless accepts more dispersed collection of                       43
personal data). A recent Australian study of internet platforms has
collated Google privacy policies ab initio and tabulated the categories
of personal data that Google is on the record for holding (which is
not the same as publicizing, as the company emphasizes its in-house
development and denies that it would sell identifiable data).




   Yunyu Qu, ‚ÄúMa Yun: Europeans Worry Too Much, Alibaba Chooses Africa, Which Is
53 

   More Willing to Believe in Technology È©¨‰∫ëÔºöÊ¨ßÊ¥≤‰∫∫ÊãÖÂøßÂ§™Â§öÔºåÈòøÈáåÈÄâÊã©Êõ¥ÊÑøÁõ∏‰ø°ÊäÄ
   ÊúØÁöÑÈùûÊ¥≤,‚Äù Caixin, January 23, 2019, http://companies.caixin.com/2019-01-
   23/101373592.html.
     D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?




                 Information Google disclosed in its Privacy Policy
                       1999-2019 as collected from users

                                                       Jun        Jul          Jan    Dec    Jan
                                                      1999       2004         2009   2014   2019
      Name                                               ‚úò         ‚úî           ‚úî      ‚úî      ‚úî
      Birthday                                           ‚úò         ‚úò           ‚úò      ‚úî      ‚úî
      Phone number                                       ‚úò         ‚úò           ‚úò      ‚úî      ‚úî
      Email adress                                       ‚úò         ‚úî           ‚úî      ‚úî      ‚úî
      Voice and audio information                        ‚úò         ‚úò           ‚úò      ‚úò      ‚úî
      Payment information                                ‚úò         ‚úî           ‚úî      ‚úî      ‚úî
      Location                                           ‚úò         ‚úò           ‚úò      ‚úî      ‚úî
      GPS                                                ‚úò         ‚úò           ‚úò      ‚úî      ‚úî
      Sensor data via wifi towers, bluetooth, etc        ‚úò         ‚úò           ‚úò      ‚úî      ‚úî
      IP addresss                                        ‚úò         ‚úî           ‚úî      ‚úî      ‚úî
      Your emails on Gmail (released Apr 2004)          NA         ‚úò           ‚úî      ‚úî      ‚úî
44    Your uploaded photos                               ‚úò         ‚úò           ‚úò      ‚úî      ‚úî
      Your uploaded videos                               ‚úò         ‚úò           ‚úò      ‚úî      ‚úî
      Your messages                                      ‚úò         ‚úò           ‚úò      ‚úî      ‚úî
      Your phone calls                                   ‚úò         ‚úò           ‚úò      ‚úî      ‚úî
      Comments you post                                  ‚úò         ‚úò           ‚úò      ‚úî      ‚úî
      Your calendar events on Google Calendar           NA        NA          NA      ‚úî      ‚úî
      (general release Jul 2009)
      Your search history                                ‚úò         ‚úò           ‚úò      ‚úî      ‚úî
      Videos you watch on Youtube                       NA        NA           ‚úò      ‚úî      ‚úî
      (acquired Nov 2006)
      Devices you use                                    ‚úò         ‚úò           ‚úò      ‚úî      ‚úî
      Apps you installed                                 ‚úò         ‚úò           ‚úò      ‚úò      ‚úî
      Browsers you use                                   ‚úò         ‚úî           ‚úî      ‚úî      ‚úî
      This-party websites visited using Google‚Äôs         ‚úò         ‚úî           ‚úî      ‚úî      ‚úî
      adversiting services
      Chrome browsing history (released Sep 2008)       NA        NA           ‚úò      ‚úî      ‚úî
                 I I . W H AT I S P R I VA CY A N D H O W C A N I T B E E N S U R E D ?




                                                Jun    Jul     Jan     Dec      Jan
                                               1999   2004    2009    2014     2019
 Browser information                            ‚úò      ‚úî       ‚úî        ‚úî        ‚úî
 Device information                             ‚úò      ‚úò        ‚úò       ‚úî        ‚úî
 Cookies generally                              ‚úî      ‚úî       ‚úî        ‚úî        ‚úî
 Purchase activity                              ‚úò      ‚úò        ‚úò       ‚úò        ‚úî
 DoubleClick cookie information (DoubleClick   NA     NA        ‚úò       ‚úî        ‚úî
 acquired Mar 2008)
 Mobile netword information                     ‚úò      ‚úò        ‚úò       ‚úî        ‚úî

Source: Australian Competition and Consumer Commission, ‚ÄúDigital Platforms Inquiry
Final Report,‚Äù June 2019, p. 380, https://www.accc.gov.au/system/files/Digital%20
platforms%20inquiry%20-%20final%20report.pdf.


One look at the above is enough to judge the merits of current privacy
policies based on notification to individuals and consent. It would
be counter-productive to abandon these obligations, reducing internet
to a free- for- all hunt for your data. But the reality is that no one                    45
reads privacy policies, especially as it would be necessary to read
hundreds of them, as well as their updates, and to understand the
legalese behind them. The table also reveals how cookies, which
were once the prime engine for data collection, are now just part of
a much bigger picture. It is useful to select the case of the biggest
worldwide platform: Google. But Google is far from unique ‚Äì and
third-party resellers of data from your clicks engage in far more
egregious practices. With 7906 words, Google‚Äôs latest statement of
privacy policy54 is also far from being the longest of its kind, although
its text incorporates many clickable segments that open into new
descriptions and more hard choices to make. What distinguishes a
company such as Google is the size and breadth of its data reach.
It is matched only by the biggest Chinese online companies. These

   As of January 22, 2019, ‚ÄúPrivacy Policy,‚Äù Google, https://policies.google.com/
54 

   privacy?hl=en-US.
     D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?




     have far less restrictions on the types of businesses they can
     simultaneously engage in ‚Äì including banking, insurance, and the
     burgeoning payment industry. Instead, the opportunity to acquire
     and consolidate personal data ‚Äì or ‚Äúbehavioral surplus‚Äù ‚Äì rather than
     size alone, is what drives many acquisitions by large American high-
     tech companies. What makes Google unique is the quality of its
     algorithms, and its financial ability to purchase other pioneering
     companies for their own algorithms and their domains of use. Two
     million companies depend on the marketing results that Google‚Äôs
     big data and algorithms churn out. Yet, in many ways, it is more
     protective of the huge quantity of data that it acquires than many
     digital media and publishers. The latter rely directly on income from
     advertising to survive and therefore indiscriminately open their
     websites to third party brokers and resellers. Because of the need
     for online revenue, these news media outlets actually ‚Äústand up for
46   a system of mass surveillance which goes as much against their
     readership as it goes against the journalistic profession‚Äù, comments
     one privacy-oriented NGO.55

     There is great interest from consumers for their data privacy. 90%
     of adults in the U.S. believe it is important to have control over what
     information is collected on them, 93% consider it important to be
     able to control who has access to this information,56 and 86% have
     made efforts to hide their digital footprints;57 these numbers resonate


        ‚ÄúEPrivacy Regulation: Do Not Let the EU Sell Our Right to Privacy,‚Äù La Quadrature du
     55 

        Net, 2017, https://eprivacy.laquadrature.net/en/.
        Mary Madden and Lee Rainie, ‚ÄúAmericans‚Äô Attitudes About Privacy, Security and
     56 

        Surveillance,‚Äù Pew Research Center, May 20, 2015, https://www.pewinternet.
        org/2015/05/20/americans-attitudes-about-privacy-security-and-surveillance/.
        Lee Rainie et al., ‚ÄúAnonymity, Privacy, and Security Online,‚Äù Pew Research Center,
     57 

        September 5, 2013, https://www.pewinternet.org/2013/09/05/anonymity-privacy-
        and-security-online/.
               I I . W H AT I S P R I VA CY A N D H O W C A N I T B E E N S U R E D ?




in Europe as per the 2018 Digital Attitudes report.58 Yet, these
opinion trends are inconsistent with actual online behavior. People
often agree to share their data for free applications like Wi-Fi and
websites. This is the privacy paradox. In many ways, it should be
perceived as the privacy dilemma. Consumers are often left with the
choice between surrendering their privacy and long privacy policies
full of legalese.59 One study suggested that an average American
would have to spend more than 25 days a year to read all the privacy
policies he or she was exposed to in a year.60 Then, there is also the
potential denial of goods or services if data collection is not consented
to. Thus, the intuitive case that each individual knows best what to
do with his or her own personal data is easily defeated. Scores of
studies have now made this point ‚Äì and yet, many regulations and
the perception by the general public rest on the notions of notice
and consent. As we shall see, efforts around the GDPR focus in part
on improving the ergonomics of notice and consent, including                                47
standardization and universality of use: these efforts have merits.
Not to underwrite them would lead to a worse situation for data
privacy. Yet they only scratch the surface of the issue. In reality, it
is impossible for a person to read the necessary notices, and even
more impossible ‚Äì including for experts ‚Äì to comprehend what type
of data ‚Äì or metadata ‚Äì is likely to be used in the future for intelligence
about any individual.

   Joe Toscano, ‚ÄúPrivacy By Design: What Changes Are Necessary, How To Do It, and
58 

   How To Sell Your Boss,‚Äù Medium, October 30, 2018, https://medium.com/greater-than-
   experience-design/privacy-by-design-7b1165d045e0.
   Kai Burkhardt, ‚ÄúThe Privacy Paradox Is a Privacy Dilemma ‚Äì Internet Citizen,‚Äù Internet
59 

   Citizen, August 24, 2018, https://blog.mozilla.org/internetcitizen/2018/08/24/
   the-privacy-paradox-is-a-privacy-dilemma/.
   Alexis C Madrigal, ‚ÄúReading the Privacy Policies You Encounter in a Year Would Take
60 

   76 Work Days,‚Äù The Atlantic, March 1, 2012, https://www.theatlantic.com/technology/
   archive/2012/03/reading-the-privacy-policies-you-encounter-in-a-year-would-
   take-76-work-days/253851/.
     D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?




     What is inaccessible to experts and to literate and experienced
     users is even less likely to be available to poorly educated users
     of smartphones, social media and other popular apps. Among
     open markets, India is set to become the largest smartphone and
     internet base, hotly contested by telcos and platforms. Semi-
     illiteracy encourages voice-driven apps, as seen in the increase
     of voice search queries on Google in India by 270% per year. 61
     The likelihood that the general public can effectively manage a
     user-based privacy design based on consent and notice is close
     to zero.



     Right to Be Forgotten
     The same applies to other important components of privacy, such
48   as the right to know what is known about you, or the ‚Äúright to be
     forgotten‚Äù, which are practical implementations of the need for
     obscurity to ensure privacy. Again, an ingenious piece of journalism
     about Google and Facebook highlights how unattainable these goals
     are.62 For one person, the average downloaded amount of information
     from Google alone is 687,5 MB or 3 million words, or more
     than two volumes of the Encyclopedia Britannica. In practice,
     the answer to the question about what is known about you is simply
     ‚Äúwe really know a lot.‚Äù Sorting through the data is even more difficult,
     as the same information is held redundantly by different sources.
     ‚ÄúFrom a policymaking perspective, the only viable assumption


        Rishi Iyengar, ‚ÄúThe Future of the Internet Is Indian,‚Äù CNN, November 27, 2018, https://
     61 

        edition.cnn.com/interactive/2018/11/business/internet-usage-india-future/.
        Dylan Curran, ‚ÄúAre You Ready? This Is All the Data Facebook and Google Have on You
     62 

        | Dylan Curran,‚Äù The Guardian, December 19, 2018, https://www.theguardian.com/
        commentisfree/2018/mar/28/all-the-data-facebook-google-has-on-you-privacy.
                                                   SYNTH√àSE DES PROPOSITIONS




today, and for the foreseeable future, is that data, once created, are
permanent.‚Äù63

Thus technology impairs the ‚Äúright to be forgotten‚Äù, one of the
foundations of privacy law. In 2014, the CJEU ruled against Google,
ordering it to remove from its list of results data relating to two
Spanish citizens (an internet mention of a land sale that had
happened years ago).64 It is important that the Court mentioned that
these were private citizens, and not public figures. This became the
basis for the ‚Äúright to be forgotten.‚Äù Applying only to minors under
18, California‚Äôs own ‚Äúeraser law‚Äù entered into force in January
2015.65

A related development concerns the extent of data retention ‚Äì and
the public authorities‚Äô right to access them. Where companies see
no value in data, they have no particular interest in retaining them,                              49
a costly process. The state‚Äôs interest ‚Äì justified by criminal cases,
for example ‚Äì may be much more extensive. In particular, metadata
from phone calls, and phishing all communications from a cell tower
in some cases, are important investigative tools. How much they
can be used is a matter of debate. In December 2016, the CJEU
ruled that retaining and accessing telco metadata (time of call,
number called) is only appropriate in individual criminal investigation,




   Executive Office of the President, ‚ÄúBig Data and Privacy: A Technological Perspective,‚Äù
63 

   President‚Äôs Council of Advisors on Science and Technology, May 2014, p. 40, https://
   bigdatawg.nist.gov/pdf/pcast_big_data_and_privacy_-_may_2014.pdf.
   C 131/12, Europa.eu (European Court of Justice 2014), https://eur-lex.europa.eu/
64 

   legal-content/EN/TXT/?uri=CELEX%3A62012CJ0131
   ‚ÄúSenate Bill No. 568,‚Äù California Legislative Information (2013), http://leginfo.legislature.
65 

   ca.gov/faces/billNavClient.xhtml?bill_id=201320140SB568.
     D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?




     and cannot be performed in bulk.66 The ruling is resisted by member
     states,67 which have embarked on a slow consultation process to
     find acceptable alternatives. Finding a targeted method rather than
     blanket retention seems difficult to achieve.68

     Even on a single platform or app, the only way to implement the
     right to be forgotten, enshrined by the so-called ‚Äúeraser laws‚Äù, is to
     push the nuclear button ‚Äì delete ALL data, which will gain you a
     short moment of obscurity until you tap again a keyboard, open your
     phone, begin moving or start any other activity that is recorded. Even
     this short moment of obscurity is doubtful: the same data has likely
     been duplicated elsewhere ‚Äì on your other machines, by partners,
     or of course by third parties beyond your reach. Again, Google and
     Facebook are the obvious guilty parties ‚Äì some would say scapegoats,
     simply because they are so ubiquitous and so large in their range
50   of monitored activities (although, once more, less so than their
     Chinese competitors). But a host of other platforms and apps, often
     unbeknownst to their users, would present similar issues with the
     additional challenge to simply identify them.




        This CJEU ruling on the Tele2 Sverige case is available at Court of Justice of the European
     66 

        Union, ECLI:EU:C:2016:970, Europa.eu, 2016, http://curia.europa.eu/juris/document/
        document.jsf?docid=186492&text=&dir=&doclang=EN&part=1&occ=first&mode=
        DOC&pageIndex=0&cid=4147224
        For a criticism of member state reluctance to accept the consequences of the CJEU
     67 

        ruling, see Jesper Lund, ‚ÄúEU Member States Willing to Retain Illegal Data Retention -
        EDRi,‚Äù European Digital Rights, January 16, 2019, https://edri.org/eu-member-
        states-willing-to-retain-illegal-data-retention/.
        Working document submitted to the December 2018 Home and Justice Council,
     68 

        November 23, 2018, http://data.consilium.europa.eu/doc/document/ST-14319-2018-
        INIT/en/pdf
            I I . W H AT I S P R I VA CY A N D H O W C A N I T B E E N S U R E D ?




Privacy by Design
One must therefore move away from notice and consent, and from
the ergonomics or user experience (UX) in these processes, to other
methods ensuring data privacy. A generic concept is privacy by
design, or ‚Äúbaking privacy‚Äù into the design of IT. This is actually an
approach that is favored by large digital companies because it rests
on technological prowess, which is more available to them. It does
not deviate from the fundamental assertion that ‚Äúcode is law‚Äù
(Lawrence Lessig), and from the belief that law should stay away
from tech because law moves much more slowly and cannot catch
up with innovation ‚Äì or only by stifling or stopping it. Privacy by
design gives maximum power to the choices made by software
designers. An example is Apple‚Äôs encryption of iPhones. These
choices, like end-to-end encryption, create dilemmas for authorities
that fear the ability of criminals to ‚Äúgo dark‚Äù ‚Äì escape surveillance                51
for instance. In this cat and mouse game however, the balance seems
increasingly loaded by newly available technologies towards
surveillance tools. Sicilian Mafia bosses once escaped detection by
communicating solely from their hidden retreats with pizzi, scraps
of paper. Today, they need to fear every piece of electronic equipment
in their environment.

Some tools for ensuring data privacy also imply trade-offs with
competition principles. Recently, several major data platforms and
their CEOs have turned towards privacy laws, including both GDPR
and the coming European e-privacy regulation. This may be part of
an awakening process to the damage already done. But there is a
market angle as well: throwing off the boat a host of third-party data
handlers and resellers, widening the scope of platforms to new
domains while ensuring the data stays inside the platform‚Äôs black
     D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?




     box will indeed foster more privacy (assuming the company and its
     employees can be trusted), but it will also extend the power of
     algorithms based on even bigger and wider data banks. It is also
     perhaps how major Western platforms can hope to prevail in the
     challenge from Chinese platforms that may be entirely under the eye
     of their own government but are otherwise much less regulated, and
     are using this for international expansion. In turn, this has generated
     a counter-push by the U.S. Department of Justice, which is considering
     a break-up of Google and possibly other major IT platforms.

     To prevent the asymmetry of knowledge among data operators, one
     popular suggestion has been to impose an open standard to the
     biggest platforms ‚Äì they would be obliged to share categories of data
     with competitors: typically, Amazon, which is now able to design
     and market the best quality/price compromise for lightbulbs because
52   it knows so well the preferences of its customers from their clicks
     and purchases, would have to share this marketing information with
     other lightbulb sellers and manufacturers. Attractive as the idea is
     to ensure symmetry of information, it does imply that large data
     resources will be turned over to any number of third parties.
     Enhancing competition in this case works against privacy by design.

     Companies cannot achieve privacy by design if they don‚Äôt know the
     rules and don‚Äôt receive guidance. There will be no benchmarks for
     them and for users, no goalpost for litigation. One stark assessment
     contrasts a ‚Äúchoice of digital critical infrastructure suppliers who are
     muddling through security and privacy debates, or one who actively
     relegate those debates in favor of digital authoritarianism or Chinese
     interests.‚Äù69 For example, it notes that countries involved in Belt &

        ‚ÄúIs The Innovation Winter Coming? ‚Äì Analysis,‚Äù Eurasia Review, June 27, 2019, https://
     69 

        www.eurasiareview.com/27062019-is-the-innovation-winter-coming-analysis/.
                I I . W H AT I S P R I VA CY A N D H O W C A N I T B E E N S U R E D ?




Road connectivity projects may not have a choice, and that 18 states
so far have chosen to use Chinese monitoring systems.

But legislating privacy by design is difficult, and perhaps antithetic to
innovation itself. ‚ÄúPrivacy by design is a lot of hype with very little
substance. Although it has enormous potential to reset the imbalance
between data collectors and users, it suffers from too much ambiguity.‚Äù70
The obligations of each actor must be clearly identified ‚Äì lest, for
example, different types of operators reject responsibilities on each
other. If the law is expressed too broadly it becomes impractical; if it
is defined too narrowly it will miss most of the target. Vague
requirements and terms encourage superficial compliance and make
it hard for users to know if the law is respected. It also leaves open
the possibility that various institutions and individuals in charge of
overseeing compliance have different, or even opposed, interpretations
of the law. There must be a mixture of assigning responsibility to                        53
software designers and operators, as well as to organized users: if
extremist groups abuse the potential of social media to turn it into a
weapon, it is not the sole responsibility of the social media. There
should be cooperative implementation of the law. Indeed, given the
myriads of organizations involved in digital exchange, explaining the
law and persuading should be practiced before nudging or punishing.

Only this iterative process between the law and the actors can prevent
a major drawback of ex ante legislation: it will err by missing unseen
privacy issues, or it will overshoot the target and inhibit digital
activities by being too broad and vague out of perceived necessity.
Europeans, one should add, are particularly vulnerable to the latter
excess because of the popularity of the precautionary principle,

   Ari Ezra Waldman, ‚ÄúPrivacy‚Äôs Law of Design,‚Äù UC Irvine Law Review, October 31, 2018,
70 

   https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3263000.
     D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?




     which is an anti-innovation social ethos. Excessive and unrealistic
     regulation generally leads to poor enforcement of the law. A major
     issue is also the choice between top-down law enforcement and a
     bottom-up approach, for example through individual complaints and
     appeal or the empowerment of civil society groups that can better
     represent the interests of individuals. This is particularly important
     for privacy concerns. A key objective of privacy by design and of its
     legal prescriptions should be to unburden the individual from choices
     he or she cannot make, either because they are undecipherable, or
     simply because there are too many of them. ‚ÄúTrust but verify‚Äù: only
     the addition of enforcing institutions and representative civil society
     organizations can provide a permanent check on operators. This is
     also made more necessary by the constant changes in the digital
     world. Updates, patches, improvements, initially unplanned uses
     happen all the time, and they pose the same risks anew.
54
     And yet the precautionary approach has its limits, as it is often based
     on current consensus and customs. Without radical technological
     innovations, our societies would remain frozen. There are very few
     enduring consensuses. Because the digital world is molding anew
     our social customs and personal habits, extending the reach of our
     minds as much as an exoskeleton expands our body‚Äôs capacities,
     one should consider that pressing the ‚Äústop‚Äù button has ethical
     consequences that are as large as carrying on. In 2006, when
     Facebook had a largely student membership of 8 million, it introduced
     ‚ÄúNews Feed‚Äù ‚Äì a feature that allowed members to track their Facebook
     friends‚Äô activities in real time.71 Hundreds of thousands of members

        Tracy Samantha Schmidt, ‚ÄúInside the Backlash Against Facebook,‚Äù TIME, September 6,
     71 

        2006, http://content.time.com/time/nation/article/0,8599,1532225,00.html. Cited by
        William McGeveran, ‚ÄúFriending the Privacy Regulators,‚Äù Arizona Law Review 58, no.
        4 (2016), p. 1004, https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2820683.
                 I I . W H AT I S P R I VA CY A N D H O W C A N I T B E E N S U R E D ?




organized protests against this feature. Today, perhaps for the worst,
News Feed is a prime attraction for the 2,4 billion Facebook users.
It is very clear that the service rendered by ride-hailing services
outweighs the obvious loss for privacy that is involved. Being
evaluated and graded every time one takes a ride, is not something
that would have been conceivable only a few years ago. It is now
taken for granted, and increasingly so in other sectors of the gig and
bartering economy. Libertarians can always ride the subway ‚Äì
provided they pay with cash rather than with a card, as many Hong
Kong demonstrators chose to do in July 2019 in an effort to escape
personal identification.

This libertarian market argument also has its limits. Users or
consumers who ‚Äúpay‚Äù with their data have next to zero knowledge
of what will be done with their data and the risks may incur. The
‚Äúmarket‚Äù is skewed by the denial of service that an operator can                              55
oppose, and by the absence of any ‚Äúpricing‚Äù of the data surrendered.72
We are back to the initial need to balance privacy with efficiency ‚Äì
and security. Compromises are needed, perhaps by all: one of the
reasons users are turning away from traditional television to streaming
services is because they are spared the constant advertising. But in
return, they also surrender their privacy.




   Katherine J Strandburg, ‚ÄúFree Fall: The Online Market‚Äôs Consumer Preference Disconnect,‚Äù
72 

   University of Chicago Legal Forum 2013, 5 (2013), https://chicagounbound.uchicago.
   edu/uclf/vol2013/iss1/5/.
                                  III

  GDPR, A EUROPEAN REGULATORY FEAT

The GDPR has become, in the space of a year, the most commonly
used yardstick to gauge legal privacy protection ‚Äì even though its
scope is actually on operators and companies‚Äô handling of personal
data.

There are reasons for this. It aims to create a one-stop shop for
decisions on personal data protection (although directives and
guidelines on implementation matter a great deal). 88 pages of
superb writing starting with the goals and reach of the regulation
(173 ‚Äúrecitals‚Äù), and proceeding to its actual provisions (99 articles).
Being a regulation rather than the previous directive it replaces, it
is law to the 28 member states, at least on ‚Äúequivalent‚Äù terms, and
it is therefore a means of achieving legal certainty in all member         57
states of the EU, and over its external data flows. National rules can,
under certain conditions exceed, but not underbid, the protections
in place. The regulation addresses many if not all of the issues of
privacy mentioned earlier in Part II. For instance, consent requirements
have been strengthened compared with the previous European
directive: it must be affirmative, clearly spelled out, reversible, and
explicit for ‚Äúsensitive‚Äù personal data. Different usage of personal data
requires separate consent from the data subject. Personal data is
defined broadly ‚Äì including financial data and International Mobile
Equipment Identity (IMEI) or IP addresses.

Data processing can only be performed under six specific legal
requirements, including but not limited to consent. The right to
erasure extends to cases where the holding of data is no longer
necessary for its initial purpose. The right to data portability has
     D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?




     been introduced. The regulation addresses ergonomy, anonymization,
     responsibilities of data fiduciaries in general and operators (which
     are respectively described by GDPR as ‚Äúcontrollers‚Äù 73 and
     ‚Äúprocessors‚Äù),74 a cooperative approach but also very stiff penalties,
     the inclusion of any entity dealing with the personal data of EU
     residents, as well as the thorny issue of equivalency with other data
     protection regimes are all addressed. Privacy by design has been
     incorporated into the GDPR: in effect, it befalls upon data controllers
     to enact ‚Äúappropriate technical and organizational measures‚Äù to
     conform with the Regulation. The minimization of data collection
     and the limitation of access to those strictly needed for its processing
     are also obligations. Notification of data privacy breaches ‚Äúwhere
     they result in a risk for the rights and freedoms‚Äù is mandatory within
     72 hours. It is also mandatory for data controllers or processors to
     appoint data officers, who are responsible within the firm for
58   education and compliance issues. Finally, the GDPR provides a
     practical framework for implementation ‚Äì entrusted to national data
     institutions but with a new European Data Protection Board in charge
     of oversight. It has replaced the advisory Article 29 Working Party
     (WP29). This has become a key aspect of the GDPR.

     The U.S. simply does not provide a model that can be replicated in
     any other society: its legislation is dispersed and conditioned on
     widely diverse state and federal laws and agencies, and with a large
     role for case-by-case litigation. The GDPR, including its scheme for
     equivalent national provisions and supervisory boards, is a Cartesian
     model by comparison. Inasmuch as it provides certainty with
        According to Article 4, controller is ‚Äúthe natural or legal person, public authority, agency
     73 

        or other body which, alone or jointly with others, determines the purposes and means
        of the processing of personal data‚Äù.
        According to Article 4, processor is ‚Äúa natural or legal person, public authority, agency
     74 

        or other body which processes personal data on behalf of the controller‚Äù.
                                 I I I . G D P R , A E U R O P E A N R E G U L AT O R Y F E AT




simplicity, it is easily transferable ‚Äì at least for the principles. Finally,
Europe is one of the four big digital data markets (along with China,
India and the United States), and a forward role for the GDPR has
important implications for the worldwide free flow of data. Whereas
‚Äúequivalency‚Äù is the guide for implementation of the GDPR across
member states, ‚Äúadequacy‚Äù is the reference for concluding free data
flow agreements with third parties. This is in recognition that the
legal and societal environments differ, and that different processes
may be employed to reach a level of protection that is adequate but
not identical to EU standards. The adequacy decision is not
transferable onwards to yet another country, and it is reached only
after an iterative process between the EU and its partner, therefore
leading to changes in the rules governing data protection in that
country. The adequacy decision is subject to regular review and may
be subject to subsequent improvements.75
                                                                                                 59


The Obvious Downsides
There are downsides, of course. The GDPR is a catch-all text that
is built on balancing opposite objectives. Indeed, it states that the
protection of personal data ‚Äúis not an absolute right.‚Äù This right is
bounded by ‚Äúlegitimate interest.‚Äù The regulation objective is actually
said to encourage the free flow of data ‚Äì a claim that is disputed by
those who see heavy obligations and litigation risks appearing on
   As an example, the adequacy decision concerning Japan is 48 pages and 28000 words
75 

   long.
   Source: European Commission, ‚ÄúCommission Implementing Decision (EU) 2019/419
   of 23 January 2019 Pursuant to Regulation (EU) 2016/679 of the European Parliament
   and of the Council on the Adequate Protection of Personal Data by Japan under the Act
   on the Protection of Personal Information‚Äù (2019), EUR-Lex, https://eur-lex.europa.eu/
   legal-content/EN/TXT/?uri=uriserv:OJ.L_.2019.076.01.0001.01.ENG&toc=OJ:L:
   2019:076:TOC.
     D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?




     the horizon. SMEs and organizations with less than 250 employees
     are spared the recording and book-keeping of their processing
     operations. This exemption is itself qualified by a firm‚Äôs frequent
     recourse to processing, a useful exception to the exception. The
     former Cambridge Analytica, for example, numbered fewer than 250
     employees.76 A number of commendable rights for natural persons
     are nonetheless recognized: control by natural persons of their
     personal data, explicit and easy consent to data collection, minimal
     processing (e.g. when no other means are available), the right to
     rectification and to be forgotten, the portability of data, the principles
     of data protection by design and by default, and strict standards on
     human rights and rule of law for equivalency agreements with non-EU
     countries.

     But the Regulation has almost no prescriptions for ergonomics ‚Äì
60   users‚Äô experience or UX. This is not uncommon for a legal text, yet
     it is clear that user experience research, guidelines for implementation
     and standard processes are necessary, just as the average user must
     understand road rules. It is under those conditions that the notion
     of giving natural persons control over their personal data can be at
     least partially envisaged. The Regulation has also literally taken no
     notice of AI: the words ‚Äúalgorithm‚Äù or ‚Äúdata fusion‚Äù do not appear.
     While required, anonymization, or pseudonymization, do not consider
     the new capacities of machine learning to defeat these processes.




        ‚ÄúCambridge Analytica,‚Äù Crunchbase, 2019, https://www.crunchbase.com/organization/
     76 

        cambridge-analytica#section-overview or https://www.owler.com/company/
        cambridgeanalytica.
                          I I I . G D P R , A E U R O P E A N R E G U L AT O R Y F E AT




The Exemptions ‚Äì Public Interest and the Member
States‚Äô Prerogatives
There is a large number of exclusions from the Regulation. The
longest list appears in article 23, as follows:

‚Äú1. (‚Ä¶)(a) national security; (b) defence; (c) public security;
4.5.2016 L 119/46 Official Journal of the European Union EN
(d) the prevention, investigation, detection or prosecution of crimi-
nal offences or the execution of criminal penalties, including the
safeguarding against and the prevention of threats to public secu-
rity; (e) other important objectives of general public interest of the
Union or of a Member State, in particular an important economic
or financial interest of the Union or of a Member State, including
monetary, budgetary and taxation a matters, public health and
social security; (f) the protection of judicial independence and judi-                    61
cial proceedings; (g) the prevention, investigation, detection and
prosecution of breaches of ethics for regulated professions; (h)
a monitoring, inspection or regulatory function connected, even
occasionally, to the exercise of official authority in the cases refer-
red to in points (a) to (e) and (g); (i) the protection of the data
subject or the rights and freedoms of others; (j) the enforcement
of civil law claims.

2. In particular, any legislative measure referred to in paragraph 1
shall contain specific provisions at least, where relevant, as to: (a)
the purposes of the processing or categories of processing; (b) the
categories of personal data; (c) the scope of the restrictions intro-
duced; (d) the safeguards to prevent abuse or unlawful access
or transfer; (e) the specification of the controller or categories of
controllers; (f) the storage periods and the applicable safeguards
     D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?




     taking into account the nature, scope and purposes of the pro-
     cessing or categories of processing; (g) the risks to the rights and
     freedoms of data subjects; and (h) the right of data subjects to be
     informed about the restriction, unless that may be prejudicial to
     the purpose of the restriction.‚Äù

     The regulation also cannot be invoked against the archiving of data
     for historical, scientific and statistical purposes, an addition welcomed
     by researchers but one that puts in doubt, at least in an absolute
     sense, the ‚Äúright to be forgotten.‚Äù It is clear that the GDPR has been
     designed with private operators and data fiduciaries in mind more
     than governments and public authorities. In the above-mentioned
     triangle between privacy, efficiency and security, it has notably tipped
     the scale towards privacy by imposing obligations on operators and
     data fiduciaries ‚Äì including public organizations. Yet, it has backed
62   off from many decisions that could have lessened security or impaired
     public policy objectives in general. Once one looks at the GDPR
     through this perspective, the contrast with the more explicitly
     consumer-oriented rules or court decisions on privacy in the U.S. is
     less apparent. The regulation also makes important but only generally
     defined exceptions to data transfer rules outside the EU. The
     conclusion on this point is, as mentioned in Recital 114 and Articles
     48-49, that the Commission has the upper hand in deciding case-
     by-case according to general principles.
                           I I I . G D P R , A E U R O P E A N R E G U L AT O R Y F E AT




Carrying a Big Stick ‚Äì The Deterrent Effect
One area where the GDPR makes a significant difference is in the
area of potential penalties. They are meant to be ‚Äúeffective, proportional
and dissuasive.‚Äù For the strict obligations accruing to the controller,
processor and from the certification and monitoring bodies, they can
reach 10 million EUR or 2% of their worldwide turnover, whichever
is highest. On ‚Äúbasic principles‚Äù, data subjects‚Äô rights, transfers outside
the EU and non-compliance with a cessation order, it is 20 million
EUR or 4% of turnover. This, as we shall see, got the attention of
major firms in the first year of implementation, by setting the bar for
penalties very high. Under Article 58, data processing across EU
borders can also be precluded as a corrective measure.



Assessing GDPR, One Year After                                                             63


After its first year in operation, the EU‚Äôs GDPR is being reviewed
and assessed by various sources ‚Äì including the Commission and
the European Data Protection Board (EDPB), as well as national
supervisory authorities. It is in this context that the real impact of
the GDPR can be judged. The text had many stated intentions and
represented a balancing act. How it is implemented, even in the
initial phase, tells us more about its impact. The role that it plays
regarding data protection and privacy in other legislations and in
shaping global attitudes, including in the U.S., can also be assessed.
By contrast, the relevance of China‚Äôs laws and rules for privacy, with
generally different stated objectives, seems to be only a sideshow
in what is now the world‚Äôs most advanced overt surveillance state.
Also, the distance between legal texts and practices is such that it
is the latter that matters.
     D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?




     Feedback and National Siloes
     As should be the case for a regulation ensuring personal data
     protection, the EDPB‚Äôs main yardstick to measure the GDPR‚Äôs success
     after a year has been the awareness of citizens and their readiness
     to initiate complaints. In March 2019, 67% of polled EU citizens
     were aware of the GDPR‚Äôs existence, and 57% were aware of national
     supervisory authorities, a commendable result. 281,088 queries to
     national data boards in the same time, of which 144,376 complaints
     and 89,271 data breach reports, sounds impressive.

     One major innovation of the GDPR is to provide for cooperation
     among national boards for cross-border cases. Cases go inside an
     existing Internal Market Information system (IMI). Beyond mutual
     assistance (444 requests were made under that heading), many of
64   these cases go through a one-stop shop mechanism, where a Lead
     Supervisory Authority must first be designated. So far, the actual
     figures are less impressive here. As of March 2019, there were a
     mere 466 reported cross-border cases, of which only 19 had found
     a solution. Of the total, 45 were one-stop shop cases, of which 6
     reached a final decision. The EDPB also gives consistency opinions
     to ensure across the board equivalency among supervisory authorities.
     29 such opinions have been given. The one-stop shop does not
     apply in cases where the entity in question operates from outside
     the EU: it is then liable in front of every national authority.

     Implicitly, the EDPB and the Commission recognize some of the
     complaints of stakeholders. One is on the practical differences that
     persist among member states in ensuring adequacy to the Regulation.
     After one year, three member states have still not changed their
     legislation. The Commission keeps their names quite close to its
                                I I I . G D P R , A E U R O P E A N R E G U L AT O R Y F E AT




chest, but they are Greece, Portugal and Slovenia. Differences such
as the minimum age of children for consent create difficulties for
transnational platforms. Although the Commission currently
emphasizes dialogue over sanctions, it does not always recognize
the lack of clarity and practicality from some of the supervisory
authorities. In Poland, PUODO, the supervisory authority fined a
Swedish platform (Bisnode) 220,000 EUR (in fact the third largest
fine based on the GDPR in the first year) for having failed to contact,
by registered mail, six million people about their personal data
acquired from public registers. The legal fight pits strong defenders
of the GDPR‚Äôs Article 14 against those who would argue for
proportionality: the cost of six million registered letters would be
prohibitive.77 Neither PUODO nor the GDPR itself pay much attention
to the practicality of the rules.

                                                                                                65
Where is That Big Stick?
Overall, 56 million EUR have been imposed in fines during the first
year, including 50 million for a single fine by CNIL, the French
national board, on Google (but that was appealed). The United
Kingdom has gone further in August 2019, with a 99 million GBP
fine against Marriott and a 183 million GBP against British Airways:
so far, the departing U.K. is the strictest enforcer of GDPR sanctions!
These figures tell their own story. Although apparently high, the
number of complaints has a very low ratio compared to overall usage.
Implementation remains largely within borders. The 4% of global
turnover magic weapon has turned into very limited fines, sometimes

   Karolina Ga≈Çezowska, ‚ÄúWhy You Should Pay Close Attention to the Polish DPA‚Äôs First
77 

   GDPR Fine,‚Äù Iapp.org, April 22, 2019, https://iapp.org/news/a/polish-dpas-
   first-fine-pay-close-attention/.
     D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?




     described as fit for a first year of implementation. Much more
     encouragingly, companies have spent and hired to be in compliance
     with the GDPR. The International Association of Privacy Professionals
     (IAPP) estimates that after one year, 500,000 organizations have
     registered data protection officers in Europe.78 Large IT companies
     in fact emphasize their investment to comply with the Regulation
     ‚Äì Microsoft for example claims to have used 1600 engineer man-
     years to establish compliance and implement the GDPR rules world-
     wide, and not only in relation to Europe.

     The overall number of complaints, especially cross-border complaints,
     the slow rate of resolution and the minimal amount of fines issued
     does raise a question of implementation. The EDPB notes that
     national supervisory authorities hardly received the budget increases
     that would be commensurate with their new tasks. Five of them
66   have actually seen a decrease (Poland and the Czech Republic) or
     no increase (Austria, Belgium and Latvia). For eight supervisory
     authorities, the number of personnel did not change, for one (Czech
     Republic) it actually decreased.79




        For an explanation of the estimate: IAPP, ‚ÄúApproaching One Year GDPR Anniversary,
     78 

        IAPP Reports Estimated 500,000 Organizations Registered DPOs in Europe,‚Äù Iapp.org
        (May 16, 2019), https://iapp.org/about/approaching-one-year-gdpr-anniversary-iapp-
        reports-estimated-500000-organizations-registered-dpos-in-europe/.
        For actual numbers, see: EDPB, ‚ÄúFirst Overview on the Implementation of the GDPR
     79 

        and the Roles and Means of the National Supervisory Authorities,‚Äù European Data
        Protection Board, March 8, 2019, p.11-12, https://edpb.europa.eu/sites/edpb/files/
        files/file1/19_2019_edpb_written_report_to_libe_en.pdf.
                               I I I . G D P R , A E U R O P E A N R E G U L AT O R Y F E AT




The View From Companies and the Issue of Size
The overall attitude of large companies towards the GDPR is at least
outwardly positive ‚Äì although one major sharing platform interviewed
has explained that no major company would come out publicly
against the GDPR as a whole, but emphasized that user satisfaction
and their security were the company‚Äôs priorities over privacy rules.
Complaints focus on the varying ‚Äúequivalent‚Äù legislations of each
member state, with little unification in sight. Time is therefore lost,
and eventually fines incurred because of these differences. Not all
data processors and operators are in equal position. It is clear that
third party data brokers, and paradoxically smaller websites and
apps (including the most reputable news media that have moved to
free content online editions) are the most vulnerable to restrictions
from notice and consent, since their revenue stream is strictly founded
on reselling data turned over by users. One particularly egregious                             67
approach of the GDPR‚Äôs ‚Äúnotice and consent‚Äù rules is the so-called
Oath Privacy platform, to which a number of respected publications
such as the Huffington Post belong, and which was originally owned
by Yahoo and AOL. It now has 43 ‚Äúfoundational partners‚Äù scraping
your data. To deal with privacy controls, one needs to read and click
through a maze of screens, including a list of privacy policies arranged
by 45 different countries, to then pore through the fine print of each
cookie owner, finally moving on to a privacy dashboard.80 In short,
Oath it has made it practically impossible to implement the rules of
the GDPR on its associated websites.




   As experimented on Huffington Post website on August 19, 2019/ First Oath pop-up
80 

   screen at: https://consent.yahoo.com/collectConsent?sessionId=3_cc-session_
   0bceacc7-b5ad-48f1-ae44-bfde36ab129f&lang=en-us&inline=false
     D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?




     An interesting study using machine learning to test the declared
     privacy policies of fourteen major web companies (Google,
     Facebook, Instagram, Amazon, Apple, Microsoft, WhatsApp, Twitter,
     Uber, AirBnB, Booking.com, Skyscanner, Netflix, Steam and Epic
     Games) has found them all in default with the GDPR. ‚ÄúThe evaluated
     corpus, comprising 3658 sentences (80.398 words) contains 401
     sentences (11.0%) which we marked as containing unclear
     language, and 1240 sentences (33.9%) that we marked as
     potentially unlawful clause, i.e. either a ‚Äòproblematic processing‚Äô
     clause, or an ‚Äòinsufficient information‚Äô clause (under Articles 13
     and 14 of the GDPR)‚Äù, according to the authors of this report,81
     which is also cited by the European Commission‚Äôs one year after
     multistakeholder report.82

     These remarks should be balanced with the perceived clash between
68   clarity and inclusiveness, given the complex requirements of the
     GDPR. In the Commission‚Äôs multistakeholders‚Äô report, the remarks
     from companies and civil society or consumer organizations often
     point in opposite directions. Reading through the lines of this polite
     but nonetheless candid account, one could well be a spectator of
     an imaginary tennis match between two opposite teams. Perhaps
     surprisingly, the corporate stakeholders, while acknowledging the
     work necessitated by the GDPR, also recognize positive results ‚Äì
     particularly for risk-based assessments of digital data processing.


        Giuseppe Contissa et al., ‚ÄúCLAUDETTE Meets GDPR. Automating the Evaluation of
     81 

        Privacy Policies Using Artificial Intelligence‚Äù ed. CLAUDETTE(https://www.beuc.eu/
        publications/beuc-x-2018-066_claudette_meets_gdpr_report.pdf, July 2, 2018)
        Multistakeholder Expert Group to support the application of Regulation (EU) 2016/679,
     82 

        ‚ÄúContribution from the Multistakeholder Expert Group to the Stock-Taking Exercise of
        June 2019 on One Year of GDPR Application,‚Äù ed. European Commission (https://
        ec.europa.eu/commission/sites/beta-political/files/report_from_multistakeholder_expert_
        group_on_gdpr_application.pdf, June 13, 2019)
                                I I I . G D P R , A E U R O P E A N R E G U L AT O R Y F E AT




SMEs and public sector stakeholders, however, remain more weary
by the efforts involved.83

Yet, the GDPR has had immediate measurable effects on third-party
data collection by cookies on websites. A study of the first three
months of implementation across seven EU countries reveals a 22%
drop on news websites, with the largest reduction in the United
Kingdom (45 %) and the smallest in Germany (6 %). Among these,
the large platforms ‚Äì Google, Amazon, Facebook and Twitter ‚Äì had
very little reduction of presence.84

Integrated platforms ‚Äì such as Google and Facebook ‚Äì brew their
own stew with algorithms and sell the product of the analysis, not
the raw data; they are in a sufficiently commanding position with
their users so that they do not run a risk of being denied much
personal data. This can be a huge business. With a tiny office in                               69
Shenzhen, Facebook, which cannot be accessed from China,
nonetheless gathers 5 billion USD in advertising revenue (9 % of
its yearly turnover) from Chinese advertisers to Facebook‚Äôs
international users.85 For major e-commerce platforms such as
Amazon, the risk is less that of infringements on privacy than of an
unequal competition: Amazon acquires ‚Äì and utilizes ‚Äì more data
on clients‚Äô tastes than any single manufacturer could hope to have.

83
  Ibid., p. 14-15.
  Timothy Libert, Lucas Graves, and Rasmus Kleis Nielsen, ‚ÄúChanges in Third-Party
84 

  Content on European News Websites after GDPR,‚Äù ed. Reuters Institute for the Study
  of Journalism (https://reutersinstitute.politics.ox.ac.uk/sites/default/files/2018-08/
  Changes%20in%20Third-Party%20Content%20on%20European%20News%20
  Websites%20after%20GDPR_0_0.pdf, August 2018).
   The 9 % estimate is based on Facebook‚Äôs latest turnover figures.
85 

     Source: Paul Mozur and Lin Qiqing, ‚ÄúHow Facebook‚Äôs Tiny China Sales Floor Helps
     Generate Big Ad Money,‚Äù The New York Times, February 7, 2019, https://www.nytimes.
     com/2019/02/07/technology/facebook-china-internet.html.
     D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?




     Data network managers, whether telecom operators such as Orange
     or infrastructure managers such as Microsoft draw their revenues
     from the direct service they provide, including data security, and are
     therefore less sensitive to privacy rules.

     One issue that large companies acknowledge without much hesitation,
     however, is that they are actually favored over smaller firms by the
     GDPR requirements, simply because they have more financial means
     and more human resources to deal with these requirements. They can
     also maneuver more easily. The priority that the GDPR gives to notice
     and consent leaves them room. Google has shifted the onus of asking
     for consent to its external data suppliers. One interviewed digital
     infrastructure company makes a distinction between its large and smaller
     clients: it is easy to partner with the former for the GDPR but much
     harder with the latter. This was anticipated by the GDPR‚Äôs makers, who
70   impose less requirements on companies under 250 employees. A recent
     Commission report blames a national German data supervision authority
     for unilaterally lowering that ceiling to 20 employees.



     Learning to Love GDPR ‚Äì And Still Hating the
     E-Privacy Directive That Comes Next
     The turnaround by large IT companies is spectacular. Mark Zuckerberg
     has declared himself in favor of the GDPR.86 So has Sundai Pichai,
     Google‚Äôs CEO,87 or Satya Nadella, Microsoft‚Äôs CEO, who calls the

       Henry Farrell, ‚ÄúFacebook Is Finally Learning to Love Privacy Laws,‚Äù Financial Times,
     86 

       April 4, 2019, https://www.ft.com/content/67b25894-5621-11e9-8b71-
       f5b0066105fe.
        Jon Porter, ‚ÄúGoogle‚Äôs Sundar Pichai Snipes at Apple with Privacy Defense,‚Äù The Verge,
     87 

        May 8, 2019, https://www.theverge.com/2019/5/8/18536604/google-sundar-
        pichai-privacy-op-ed-nyt-regulation-apple-cook-advertising-targeting-user-data.
                                I I I . G D P R , A E U R O P E A N R E G U L AT O R Y F E AT




GDPR ‚Äúa fantastic start‚Äù88 and advocates for a worldwide standard.
Apple‚Äôs Tim Cook goes several steps ahead in endorsing a federal
privacy law, including the issue of AI, which, as we have seen, is
almost entirely missing from the GDPR. He also distances himself
from much of the industry, pointing out Apple‚Äôs ‚Äúhealthy suspicion
of authority‚Äù which is part of its consumer appeal: ‚ÄúSome oppose
any form of privacy legislation. Others will endorse reform in public,
and then resist and undermine it behind closed doors.‚Äù89 The barb
is not without some reason. In the current climate where issues from
the Edward Snowden affair to fake news and Cambridge Analytica
have turned opinion, the GDPR may appear as a reasonably certain
regulation with a limited scope. In particular, it has not much to say
about the scale of data controllers, which can greatly limit competition.
It is also largely focused on the user‚Äôs rights ‚Äì which the best
equipped companies know how to circumvent. In short, what was
a bogeyman on the horizon in 2015 is now a limited attempt at                                   71
ensuring privacy. Indeed, as the IT world predicted, ‚Äútechnology is
law‚Äù and it is moving on ruthlessly. The current concern of those
companies that trade data is more with the European Commission‚Äôs
next step ‚Äì an e-privacy regulation that will renovate the regulation
of telecom data with a much larger scope than the former 2002
directive on ‚Äúprivate life and telecommunications.‚Äù. Because the
regulation would require end-user consent for transmissions including

   Isobel Asher Hamilton, ‚ÄúMicrosoft CEO Satya Nadella Made a Global Call for Countries
88 

   to Come Together to Create New GDPR-Style Data Privacy Laws,‚Äù Business Insider
   France, January 24, 2019, http://www.businessinsider.fr/us/satya-nadella
   -on-gdpr-2019-1.
   The transcript for Tim Cook‚Äôs speech at the 2018 International Conference of Data
89 

   Protection and Privacy Commissioners in Brussels on October 24, 2018 is available
   at: Jonny Evans, ‚ÄúComplete Transcript, Video of Apple CEO Tim Cook‚Äôs EU Privacy
   Speech,‚Äù Computerworld, October 24, 2018, https://www.computerworld.com/
   article/3315623/complete-transcript-video-of-apple-ceo-tim-cooks-eu-privacy-speech.
   html.
     D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?




     the IoT (potentially hundreds of devices per user), it goes beyond
     the ‚Äúcookie law‚Äù it replaces and is now the object of much lobbying.
     In effect, that piece of legislation has been stalled at the Council
     since it was proposed by the Commission in October 2017.90



     The Pitfalls of User Consent
     The gap between large and small companies or organizations is
     especially visible in terms of the users‚Äô experience of notice and
     consent implementation. Larger platforms and entities, which draw
     users on multiple occasions and with different activities, tend to
     have an initial approval system at the entry gate to their services,
     based on an often overflowing privacy notice. It will then be regularly
     updated, usually with similarly large new notices whose content
72   may differ only on a few ‚Äì but perhaps critical ‚Äì points. The vast
     majority of users will neglect reading these huge privacy notices,
     partly as a consequence of trust in a brand name, partly because
     of time constraints. And they will hardly, if at all, read the frequent
     updates. Smaller organizations that draw varied visits, often on a
     single or quasi-single basis, usually implement a questionnaire-based
     approach. The design of these varies greatly, with many infringing
     the spirit if not the law of the GDPR. Having to go through a list
     involving dozens, if not hundreds of third-party partners, and ticking
     them off one by one, is clearly a formidable task, especially if it is
     repeated on other visits and on a large number of websites. Others
     simply give the option of reading long privacy notices or explanations

        European Parliament and Council, ‚ÄúProposal for a Regulation of the European Parliament
     90 

        and the Council Concerning the Respect for Private Life and the Protection of Personal
        Data in Electronic Communications and Repealing Directive 2002/58/EC (Regulation
        on Privacy and Electronic Communications)‚Äù (2017), EUR-Lex, https://eur-lex.europa.
        eu/legal-content/EN/TXT/HTML/?uri=CELEX:52017PC0010&from=FR.
                                I I I . G D P R , A E U R O P E A N R E G U L AT O R Y F E AT




of their policies, but the only choice available is to accept it or leave
the website.

The GDPR itself, requiring separate consent for different uses of
personal data, without prescribing a single framework for questions
and answers, has involuntarily contributed to this complexity. Data
processors will then use so-called ‚Äúdark patterns‚Äù and nudging in
order to encourage users to give away consent. In fact, every one of
us has experienced huge differences across websites ‚Äì from those
that provide a setting by default ensuring privacy and asking merely
to accept this, to those requiring item by item changes, or those that
require you to review long-winded privacy policies ‚Äì perhaps on
different websites ‚Äì and finally to those who provide you with a
take-it- or- leave -it option, effectively blackmailing visitors.91 Because
of the ‚Äúprivacy paradox‚Äù (users will willingly indulge, for reasons of
efficiency, in practices that endanger their privacy), the GDPR is                              73
reaching only one part of its stated goals, even in the area of notice
and consent. The Commission‚Äôs recent victory communiqu√© on the
GDPR92 does recognize that 44% of users haven‚Äôt changed their
privacy settings since the GDPR was introduced. In fact, just as there
are rules for driving on the road, there should be publicized rules
for the virtual road and convenient, easy-to-implement processes.
Ticking boxes is only the beginning of what should lead to a mutually

   For a scathing look at several major platforms‚Äô implementation of GDPR in the first
91 

   months after it was put in force, see: Forbrukerr√•det, ‚ÄúDeceived by Design. How Tech
   Companies Use Dark Patterns to Discourage Us from Exercising Our Rights to Privacy,‚Äù
   Oslo, June 27, 2018, https://fil.forbrukerradet.no/wp-content/uploads/2018/06/2018-
   06-27-deceived-by-design-final.pdf.
   European Commission, ‚ÄúCommunication from the Commission to the European
92 

   Parliament and the Council. Data Protection Rules as a Trust-Enabler in the EU and
   Beyond - Taking Stock,‚Äù July 24, 2019, https://ec.europa.eu/commission/sites/beta-
   political/files/communication_from_the_commission_to_the_european_parliament_and_
   the_council.pdf.
     D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?




     accepted privacy relationship between providers and final users. It
     comes back to the notion that the user cannot be saddled with the
     responsibility of protecting its own privacy.



     The GDPR and the Possibility of Global Convergence
     As we leave Europe, and especially given the huge markets in
     countries where the average literacy is lower, the need for privacy
     protection becomes even more apparent. One of the early successes
     of the GDPR is the number of legislations that have recently been
     based on parts and concepts of the GDPR. Another is the interest
     of other states in an adequacy decision from the EU, allowing for
     the free flow of data transfer to the country involved. With some
     hype, the Commission concludes that this led to ‚Äúa global convergence
74   of data protection rules (‚Ä¶) These laws often have a number of
     common features that are shared by the EU data protection regime,
     such as an overarching legislation rather than sector by sector rules,
     enforceable individual rights and an independent supervisory
     authority. This trend is truly global, running from South Korea to
     Brazil, from Chile to Thailand, from India to Indonesia.‚Äù93 The
     Commission recognizes this is not a one-size-fit-all issue, and cites
     other potential models such as Japan‚Äôs ‚ÄúData Free Flow with Trust‚Äù
     (DFFT) initiative, launched by Shinzo Abe at the Osaka G20 summit
     in June 2019. Japan‚Äôs project, however, does not cover the transfer
     of personal data.




     93
          Ibid., p.1 and p.11.
                                  I I I . G D P R , A E U R O P E A N R E G U L AT O R Y F E AT




The Commission‚Äôs report cites adequacy decisions such as the
EU-Japan Agreement as the avenue with the greatest potential.94 It
touches briefly and indirectly on the excessive reach by other states
‚Äì mentioning negotiations about the Passenger Name Registration
(PNR) process, and the issue of sharing electronic evidence in
criminal investigations: data sharing with third countries for law
enforcement falls under the EU‚Äôs separate Police Directive.95 So far,
the EU has made 13 adequacy decisions, including two limited
agreements with the United States and Canada. Out of the other 11,
6 are financial centers or even off-shore markets such as Andorra
or the Faroe islands. The Commission is officially negotiating with
South Korea. Other countries, such as India, Brazil, Indonesia, have
expressed interest in an adequacy decision from the EU. More
countries are adopting GDPR-like legislation in principle. Without a
human framework and resources for implementation, this can remain
cosmetic, even if it testifies to the general appeal of the legislation.                          75




94 
     For a description of the process leading to the EU-Japan adequacy agreement, see:
     Hiroshi Miyashita, ‚ÄúThe Impact of GDPR in Japan,‚Äù in National Adaptations of the
     GDPR (Luxemburg: Collection Open Access Book, Blogdroiteuropeen, 2019), 122‚Äì27,
     https://blogdroiteuropeen.files.wordpress.com/2019/02/national-adaptations-of-the-gdpr-
     final-version-27-february-1.pdf.
95
     European Parliament and Council, ‚ÄúDirective (EU) 2016/680 of the European Parliament
    and of the Council of 27 April 2016 on the Protection of Natural Persons with Regard
    to the Processing of Personal Data by Competent Authorities for the Purposes of the
    Prevention, Investigation, Detection or Prosecution of Criminal Offences or the Execution
    of Criminal Penalties, and on the Free Movement of Such Data, and Repealing Council
    Framework Decision 2008/977/JHA‚Äù (2016), https://publications.europa.eu/en/publi-
    cation-detail/-/publication/182703d1-11bd-11e6-ba9a-01aa75ed71a1/
    language-en.
                                          IV

                   INDIA, A DIGITAL BLEND


The Constitutional Right to Privacy, a Contested Issue
The debates around data privacy in India first came to light with the
debates around the Aadhaar ID card. The biometric database has
been broken into on multiple occasions. It also became compulsory
in order to access certain public services and benefits, and personal
data has been made accessible to private companies as well.96
Against this background, a landmark decision by India‚Äôs Supreme
Court has shaped the privacy debate. In 2017, a nine-judge panel
pronounced a decision that recognized privacy as a constitutional
right, and directed the government to create a special committee
facilitating the creation of a data protection regime in India. The                        77
Srikrishna Committee was formed to draft the Personal Data Protection
Bill (PDPB) in 2018. The Bill has not yet passed the legislative
stage, at which point it will become a legally binding Act.

The Srikrishna Committee also published a report the same year
that turned the issue around, putting privacy in the context of a ‚Äúfree
and fair digital economy‚Äù and ‚Äúempowering Indians‚Äù, as its motivations
were clearly reflected in its very title. The draft bill itself lowers the
bar on interpreting the Supreme Court‚Äôs ruling, stating (as the GDPR
in fact does) that the right to privacy is not absolute. This is not a
complete surprise. According to one lawyer representing the plaintiffs

   Bloomberg, ‚ÄúAmazon‚Äôs Real Rival in India Isn‚Äôt Walmart,‚Äù The Economic Times, August
96 

   16, 2018, https://economictimes.indiatimes.com/industry/services/retail/amazons-real-
   rival-in-india-isnt-walmar t/ar ticleshow/65418425.cms?utm_source
   =contentofinterest&utm_medium=text&utm_campaign=cppst.
     D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?




     in the original appeal to the Supreme Court, after one year ‚Äúthe
     judgement has done very little in terms of altering state practice.‚Äù97
     The report sheds light on the motivations for the PDPB, and posits
     the U.S., the EU and China as the three possible paths: The U.S.
     having a laissez-faire system, the EU a consumer protection regulation
     approach, and China as emphasizing data protection as a means to
     ensure national security. From these choices, it goes on to propose
     a ‚Äúsynthetic fourth path‚Äù, stressing that the proposed bill ‚Äúprotects
     individual privacy, ensures autonomy, allows data flows for a growing
     data ecosystem and creates a free and fair digital economy.‚Äù98



     The Proposed Bill, √Ä la GDPR
     The proposed PDPB has an extensive reach, being applicable to
78   data collected or processed not only within the Indian territory by
     both Indian and foreign data fiduciaries, but also outside of India if
     it pertains to Indian citizens. There is also a last clause, which is
     vague but goes beyond the scope of the GDPR in dealing with data
     collected or processed in connection with any business that is carried
     outside of India. It heavily follows the GDPR model, laying down
     obligations for data fiduciaries and data processors,99 while outlining
     the rights of individuals (data principals). These rights include
     confirmation and access, correction, data portability, as well as the

        Apoorva Mandhani, ‚ÄúThe Right To Privacy Judgment Is A Year Old, But Not A Year
     97 

        Wiser,‚Äù Livelaw.In, August 24, 2018, www.livelaw.in/the-right-to-privacy-judgment-is-a-
        year-old-but-not-a-year-wiser/.
        Committee of Experts under the Chairmanship of Justice B.N. Srikrishna, ‚ÄúA Free and
     98 

        Fair Digital Economy: Protecting Privacy, Empowering Indians,‚Äù (July 27, 2018), https://
        meity.gov.in/writereaddata/files/Data_Protection_Committee_Report.pdf.
        According to the PDPB, ‚ÄúData processor‚Äù means any person, including the State, a
     99 

        company, any juristic entity or any individual who processes personal data on behalf
        of a data fiduciary, but does not include an employee of the data fiduciary.
                                                  I V. I N D I A , A D I G I T A L B L E N D




right to data erasure. The data fiduciaries include government bodies
and related public entities, which is a step further from the pre-
existing data protection obligations under Information Technology
(Amendment) Act, 2008, and the Information technology (Reasonable
Security Practices and Procedures and Sensitive Personal Data or
Information) Rules, 2011.100

It lists the grounds for processing of personal data on various grounds,
including, but not limited to consent. It follows the GDPR in
prescribing limited collection, purpose and storage as well as the
notice-based consent model. An interesting aspect is the
recommendation to include notices in multiple languages, if possible.
It creates the need for data audits, and under Article 35, allows the
auditors to assign a rating to data fiduciaries, which must be displayed
in the privacy notices to the users. There is also a distinction made
between personal data and ‚Äúsensitive‚Äù personal data, and differential                          79
rules are laid for processing each category. These rules add an extra
layer to the processing of sensitive data, on the grounds of ‚Äúexplicit
consent‚Äù, understood as different from consent. The Bill also creates
a separate category of ‚Äúsignificant data fiduciaries‚Äù, which are to be
designated by the National Data Protection Authority, based on
certain criteria including the quantity of data they process. Third-
party data transfers are not explicitly mentioned, implying they are
permissible under the specified grounds of processing.

A la GDPR, the PDPB sets up a National Data Protection Authority
and an Appellate Tribunal. The former would be a monitoring,
advisory, regulatory, quasi-legislative and quasi-judicial body, while

    EPW Engage, ‚ÄúWhat Enables the State to Disregard the Right to Privacy?,‚Äù Economic
100 

    and Political Weekly, January 16, 2019, pp. 7‚Äì8, www.epw.in/engage/article/
    what-enables-state-disregard-right.
     D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?




     the latter would be an adjudicating authority vested with the powers
     of a civil court. The Bill does subject the Authority itself to data
     collection and processing obligations in cases where it processes
     personal data. What is worth noting is the blatantly missing details
     and specifications under the two chapters establishing these bodies,
     most of which are left to be specified at a later date by either the
     Parliament or the executive branch. This rightly brings into question
     the autonomy of the Authority, which is essential to perform its
     functions.101 The Bill also mentions the need to implement privacy
     by design, data protection impact assessments, and data breach
     notifications. It requires data fiduciaries to appoint Data Protection
     Officers (DPO) to facilitate compliance. Even the penalties have the
     same ceiling as the GDPR, set at 4% of global turnover in certain
     cases (Article 69.2). The list of exemptions follows a similar pattern
     for national security, law enforcement, journalistic and research
80   purposes, and also makes exceptions to manual data collected by
     homes and small enterprises.

     None of the above really distinguishes the proposed Indian legislation
     from the GDPR, which puts a similar emphasis on free data flow,
     places limits in the form of exemptions to the protection of personal
     data, and in fact stresses more precisely the obligations of private
     operators than those of public authorities. The proposed Indian bill
     has taken some leaves from the GDPR, as noted both by the Head
     of the European Commission‚Äôs International Data Flow and Protection


         Bruno Gencarelli, ‚ÄúSubmission on Draft Personal Data Protection Bill of India 2018
     101 

         by the Directorate-General for Justice & Consumers to the Ministry of Electronics and
         Information Technology (MeitY),‚Äù European External Action Service - European
         Commission, November 19 2018, https://eeas.europa.eu/delegations/india/53963/
         submission-draft-personal-data-protection-bill-india-2018-directorate-general-jus-
         tice_en.
                                                  I V. I N D I A , A D I G I T A L B L E N D




Unit.102 The European assessment mentions, over several pages,
differences, ambiguities or a lack of legal protection in the proposed
bill. On the contrary, a recent Carnegie India examination103 stresses
the multiple features emulating the GDPR, but only to conclude that
the scheme is not implementable in the Indian context because of
the huge costs of compliance and impossibility for India‚Äôs small and
medium enterprises (SME) to fulfill these tasks. The Carnegie study
builds on negative predictions and assessments made by European
think tanks about the GDPR: out of eight sources however, only one
was published after its actual implementation. The United Kingdom‚Äôs
Ministry of Justice and the European Centre for International Political
Economy (ECIPE) ‚Äì a customarily pro-free trade and anti-regulation
think tank ‚Äì are the main sources from 2012 to 2014. So far, these
somber predictions ‚Äì GDP loss because of the GDPR, fall in data
flows to and from the United States ‚Äì have not materialized.
                                                                                               81
One criticism does stick. Since SMEs in general find it harder to
comply, and given their huge prevalence in the Indian economy and
the reluctance to fund data compliance officers, fulfilling a GDPR-like
legislation does sound difficult in India. The proposed bill has
exempted small firms from most of the obligations, but with high
requirements. They must fulfill all these conditions: process data
manually with an annual turnover of less than 200,000 INR,104 not
collect data on more than 100 data principals, not collect it for
disclosure to other entities, and most importantly, process data
manually. This essentially means India‚Äôs informal sector.

102
     Ibid.
    Anirudh Burman, ‚ÄúWill a GDPR-Style Data Protection Law Work For India?,‚Äù Carnegie
103 

    India, May 15, 2019, carnegieindia.org/2019/05/15/will-gdpr-style-data-protection-
    law-work-for-india-pub-79113.
104
     Equivalent to 2500 euros.
     D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?




     While this specific evaluation underlines the similarities with the GDPR
     and criticizes some of that approach, the European Commission‚Äôs
     comments focus on ambiguities and gaps in the proposed bill. Public
     authorities can exempt data processors from requirements without
     any other justification than ‚Äúany law made by Parliament or any State
     legislature.‚Äù The Central Government can issue directives to the Data
     Protection Authority ‚Äúin the interest of the sovereignty and integrity of
     India, the security of the State, friendly relations with foreign states
     and public order.‚Äù Data processing for law enforcement and national
     intelligence purposes is bound only by very general requirements.
     There is little recourse left to individuals and companies on DPO
     decisions so long as they have been made ‚Äúin good faith.‚Äù The right
     of access to one‚Äôs own data is limited to ‚Äúa brief summary.‚Äù



82   More Than Just PDPB: Other Legislations and Proposed
     Legal Texts
     The precedents of the PDPB in the data protection sphere would be
     Section 43-A and Section 72-A of The Information Technology
     (Amendment) Act, 2008, which laid down compensation for
     negligence in the processing of sensitive personal data or information
     (SPDI) and punishment for disclosure of personal information. SPDI
     were further specified by the IT (Reasonable Security Practices and
     Procedures and Sensitive Personal Data or Information) Rules, 2011.
     However, the proposed PDPB expands the scope for sensitive
     personal data from its predecessor, the IT Rules, to include official
     identifiers, information about an individual‚Äôs sex life, genetic data,
     transgender or intersex status, caste or tribe. The Rules also require
     privacy policies, while the requirement for these in the PDPB remains
     unclear ‚Äì it only specifies the need for notifications.
                                                   I V. I N D I A , A D I G I T A L B L E N D




In addition, there are certain sector-specific regulations for data
collection and processing. For example, the Reserve Bank of India
has the competence of a regulatory authority for financial data and
has issued data processing rules for the sector. In 2018, the Ministry
of Health and Welfare proposed the Digital Information Security in
Healthcare Act (DISHA), which outlines, as the name suggests, rules
for data collection and processing in the health sector. This bill will
be further discussed in a later section.

Like DISHA, a number of other proposed texts involve data privacy
in some manner. The 2018 Draft Information Technology
[Intermediaries Guidelines (Amendment) Rules apply to any person
who, on behalf of another person, receives, stores or transmits an
electronic message or provides any service with respect to that
message. They must publish rules, regulations, privacy policy and
user agreement to inform users on the restrictions to access or use                             83
an intermediary‚Äôs resource(s). Another text in the pipeline, the
National E-Commerce Bill, allows the Indian government to access
source code and algorithms, while prohibiting third-party sharing of
sensitive data, even with consent.105



From the Bottom Up ‚Äì A Sea of Apps
Generally, control over apps is sketchy. The Indian online market is
swamped with Chinese and American apps. TikTok, the Chinese
video sharing app that is wildly popular throughout the country,
explicitly states it ‚Äúcannot guarantee the security of your information

    India‚Äôs Data for India‚Äôs Development, ‚ÄúDraft National E-Commerce Policy‚Äù, February
105 

    23, 2019, https://dipp.gov.in/sites/default/files/DraftNational_e-commerce_
    Policy_23February2019.pdf.
     D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?




     transmitted through the platform‚Äù, and has been involved in social
     sharing of rural mob attacks. One press report states that ‚Äú20 Chinese
     video apps dominate the mobile entertainment network of tier-2 and
     tier-3 cities, mostly thanks to titillating videos, suggestive notifications,
     risqu√© humor and raunchy content.‚Äù106

     Chinese apps are not alone in benefiting from a lack of enforced
     regulations on privacy. Google Pay and WhatsApp Pay privacy policies
     in India, although the latter is yet to be rolled out, state that they
     share data with third parties, as do PayTM and PhonePe (owned by
     Flipkart).107 Twitter and other social media sometimes roll out
     innovations in India because the regulatory environment is less
     constraining. With concerns rising about the data security of Chinese
     apps as well as the American GAFA, internally as well as globally,
     India seems to be looking towards policy tools to navigate the different
84   data protection regimes in these countries.



     Cross-Border Data Flow and Data Sovereignty
     One of these tools is data localization. There is a legislative effort in
     favor of data localization in the name of sovereignty and security
     that is also often judged to be a front for support to local industry
     and companies. The PDPB has a whole chapter dealing with

         Economic Times Online, ‚ÄúAre RSS‚Äôs Fears about Tik Tok True? Here‚Äôs What You Should
     106 

         Know,‚Äù The Economic Times, February 19, 2019, economictimes.indiatimes.com/
         articleshow/68066972.cms?utm_source=contentofinterest&utm_medium=text&utm
         _campaign=cppst.
         ‚ÄúWhatsApp Legal Info,‚Äù WhatsApp, February 5, 2018, https://www.whatsapp.com/
     107 

         legal?doc=payments-in-privacy-policy&version=20180205, and Shrutika Verma,
         Mihir Dalal, ‚ÄúWhatsApp May Be Sharing Your Payments Data with Facebook,‚Äù Livemint,
         April 10, 2018, https://www.livemint.com/Industry/VmupcMWS2ZbVssXuIInP2J/
         WhatsApp-may-be-sharing-your-payments-data-with-Facebook.html.
                                                   I V. I N D I A , A D I G I T A L B L E N D




cross-border transfer of data which is very clear on this. All personal
data that falls under this bill must have at least one copy stored in
India, but for ‚Äúcritical‚Äù personal data (a category yet to be defined
by the Central Government), the data must be stored only within
India. For the transfer of this kind of data across borders, the Bill
prescribes an environment similar to that of the GDPR, i.e., adequacy-
based transfer tools. In another case of the nativist instinct, the draft
e-commerce policy discussed earlier is subtitled ‚ÄúIndia‚Äôs data for
India‚Äôs development.‚Äù It mandates having all data stored in data
centers and on server farms in India, giving companies three years
to comply.

Internationally, the biggest controversy is indeed over data localization
‚Äì an issue that is primarily economic, even if it has potential
implications for data security and privacy as well. The decision
reflects a broad economic concern, and a narrower security angle.                               85
Most controversially, Mukesh Ambani, who already benefited from
the government‚Äôs push for a universal mobile network, has railed
against ‚Äúdata colonialism‚Äù and urged to ‚Äúmigrate the control and
ownership of Indian data back to India ‚Äî in other words, Indian
wealth back to every Indian.‚Äù108 In April 2018, the Reserve Board
of India (RBI) ordered companies to store all financial data in India,
in order to ensure full supervisory access, with only six months for
implementation; a stance it recently reiterated in June 2019 after
the government requested reconsideration. 109 The European
    Mahesh Langa, ‚ÄúMukesh Ambani Urges Modi to Take Steps against Data Colonisation
108 

    by Global Corporations,‚Äù The Hindu, January 18, 2019, https://www.thehindu.com/
    news/national/mukesh-ambani-urges-modi-to-take-steps-against-data-colonisation/
    article26025076.ece.
    PTI, ‚ÄúRBI to Examine Concerns over Data Localisation Rule: Government,‚Äù The Economic
109 

    Times, June 18, 2019, https://economictimes.indiatimes.com/news/economy/policy/
    rbi-to-review-data-storage-rules-for-payment-firms-government/articleshow/69838249.
    cms?from=mdr.
     D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?




     Commission criticizes the whole policy as one hindering the free
     flow of data, failing to enhance data security, and essentially being
     a case of protectionism. It also notes a paradox inasmuch as India
     ‚Äúis already a top world leader in the data processing industry,‚Äù110
     and cites the risk of retaliatory measures by others. Business critics
     more often cite the cost ‚Äì including a drain on India‚Äôs expensive
     electricity supply ‚Äì of investing in massive data servers under adverse
     climatic conditions.



     Big Government, Big Data
     Alongside the data sovereignty issue comes the issue of excessive
     state powers. In fact, one of the key problems with the PDPB, as
     the European Commission also highlighted in its comments on the
86   Bill, is the power it gives the Central Government to specify certain
     key clauses. The government can decide the conditions of employment
     and funding for the Data Protection Authority (Articles 50, 56, and
     57), appellate tribunal (Articles 79-82), and define critical data
     (Article 40.1). For the authority to be independent, it is pertinent to
     have autonomy, at least over financial matters. For a 62-page legal
     text, it is problematic to leave so many issues unspecified. The draft
     intermediary guidelines reflect the same trend. According to Article
     3(5), ‚ÄúWhen required by lawful order, the intermediary shall, within
     72 hours of communication, provide such information or assistance



         Bruno Gencarelli, ‚ÄúSubmission on Draft Personal Data Protection Bill of India 2018
     110 

         by the Directorate-General for Justice & Consumers to the Ministry of Electronics and
         Information Technology (MeitY),‚Äù European External Action Service, September 29,
         2018, https://eeas.europa.eu/delegations/india/53963/submission-draft-personal-
         data-protection-bill-india-2018-directorate-general-justice_en.
                                                   I V. I N D I A , A D I G I T A L B L E N D




as asked for by any government agency‚Ä¶‚Äù, pertaining to certain
specified conditions that follow.111

Privacy activists in India have, in recent years, raised a number of
concerns about the government‚Äôs actions, starting from the previously
discussed case of the Aadhaar card. There have recently been efforts
to streamline the digital data collected through Aadhaar - which
qualifies as sensitive personal data under the proposed regime ‚Äì first
in the form of Aadhaar-based authentication and Aadhaar-based
Know Your Customer services, and now, as a pool of open Application
Programming Interfaces called India Stack.112 Cases of government
actions affecting internet freedom are aplenty, as well. WhatsApp is
under pressure to identify and stop mass messages which often are
about fake news or encouraging violence: the company says this
would violate its encryption pledge. And Facebook, which has 260
million users in India, has in 2015 removed the largest amount of                               87
content (across all its services) worldwide at the request of the Indian
government.113 Paytm, the most popular e-wallet in the country, has
also surrendered data to authorities on one public violence event.114




    Ministry of Electronics and Information Technology, ‚ÄúComments on the (Draft)
111 

    Information Technology [Intermediaries Guidelines (Amendment) Rules], 2018,‚Äù
    December 24, 2018, p.3, https://meity.gov.in/writereaddata/files/Draft_Intermediary_
    Amendment_24122018.pdf.
112
     ‚ÄúFAQs - IndiaStack,‚Äù IndiaStack, 2016, https://indiastack.org/faq/.
    Christina Medici Scolaro, ‚ÄúFacebook Blocks More Content Here than in Any Other
113 

    Country,‚Äù CNBC, November 13, 2015, https://www.cnbc.com/2015/11/13/facebook-
    blocks-more-content-here-than-any-other-country.html.
    Madhulika Srikumar, ‚ÄúThis Isn‚Äôt Just About Paytm ‚Äì Laws on Government Access to
114 

    Data Need to Change,‚Äù The Wire, May 28, 2018, https://thewire.in/law/paytm-
    data-theft-cobrapost-sting.
     D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?




     Veering Towards China‚Äôs Digital Model
     The policy direction taken by India is not guaranteed. Modi himself
     has also courted the largest American companies with a visit to
     Silicon Valley and a plea to help India become an internet
     powerhouse.115 Under the influence of the 2017 Supreme Court
     ruling, the future data protection bill has decidedly veered towards
     a GDPR-style legislation, with the lack of legal recourse for individuals
     as the main difference. But other legislations go in an entirely different
     direction, that of China, emphasizing national security over free data
     flow. This is not entirely the case for data localization ‚Äì if the
     requirements are close to those enacted by China, there are not the
     stringent limitations on cross-border transfer that are in place in the
     Chinese case. But the right of the state to obtain personal data via
     operators is almost as open-ended as it is in China, and responsibility
88   is being placed explicitly on intermediaries (telco companies or social
     media platforms). The proposed Intermediaries Guidelines reflect
     this.

     On the basis of an earlier 2011 rule, a wide domain has been
     retained by the Draft Intermediaries Guidelines, including for instance
     content that ‚Äúthreatens the unity, integrity, defense, security or
     sovereignty of India, friendly relations with foreign states, or public
     order or causes incitement to the commission of any cognizable
     offence or prevents investigation of any offence or is insulting any




         Vindu Goel, ‚ÄúNarendra Modi, Indian Premier, Courts Silicon Valley to Try to Ease
     115 

         Nation‚Äôs Poverty,‚Äù The New York Times, September 27, 2015, https://www.nytimes.
         com/2015/09/28/technology/narendra-modi-prime-minister-of-india-visits-silicon-valley.
         html?action=click&module=RelatedCoverage&pgtype=Article&region=Footer.
                                                   I V. I N D I A , A D I G I T A L B L E N D




other nation.‚Äù116 Public health and safety, and critical infrastructures
have been added. The Draft strongly supported by the telecom giant
Jio‚Äôs owner Mukesh Ambani, while large foreign companies like
Microsoft and Google complain. Microsoft ‚Äì whose Hyderabad-born
chief executive, Satya Nadella, is a business icon in India ‚Äì says
that filtering the full range of content requested by the government
would not only violate privacy and freedom of expression, but would
also be so challenging that ‚Äúthe cost of even attempting compliance
will be prohibitive.‚Äù117 The draft national e-commerce policy was
also initially envisaged to include data localization for e-commerce
data, a provision removed only after comments from the industry.
Its inclusion in the scope of the PDPB now lies with India‚Äôs Ministry
of Electronics and Information Technology (MeitY) to decide.118



The Limits of an India-EU Comparison                                                            89


The objections from the European Commission should be qualified.
India‚Äôs Data Protection Bill is the law of a federation that has full
sovereignty. The EU does not have this ‚Äì and the GDPR consequently
does not address national security, public order and also leaves
exceptions in many areas of ‚Äúpublic interest.‚Äù It is true that the GDPR


    Ministry of Electronics and Information Technology, ‚ÄúComments on the (Draft)
116 

    Information Technology [Intermediaries Guidelines (Amendment) Rules], 2018,‚Äù
    December 24, 2018, p.2, https://meity.gov.in/writereaddata/files/Draft_Intermediary_
    Amendment_24122018.pdf.
117 
     Vindu Goel, ‚ÄúIndia Proposes Chinese-Style Internet Censorship,‚Äù The New York Times,
     February 14, 2019, https://www.nytimes.com/2019/02/14/technology/india-internet-
     censorship.html.
    Anandita Singh Mankotia, ‚ÄúMeitY May Not Include E-Commerce Data in Privacy Bill,‚Äù
118 

    The Economic Times, August 29, 2019, https://economictimes.indiatimes.com/news/
    economy/policy/meity-may-not-include-e-commerce-data-in-privacy-bill/articles-
    how/70884990.cms?from=mdr.
     D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?




     has provided for elaborate appeal and review mechanisms at the
     state level, in cross-country cases and at the EU level. But areas in
     which the EU is not competent are left out. Once this has been noted
     and the exemptions in the European regulation are factored in, the
     Indian data protection draft bill appears to be less divergent from
     the GDPR.

     However, the government‚Äôs rights under law remain expressed both
     vaguely and widely in the PDPB, making the recourse from an
     individual to a legal process for redress quite difficult. The
     government‚Äôs access to private data remains largely allowed under
     a 1996 Supreme Court ruling119 that focused on telephone tapping.
     This, despite having appropriate legal procedures, is also facilitated
     by an overburdened oversight system.120

90   The strongest differences are elsewhere. While Europe has a limited
     policy of support to digital companies, mainly through subsidies,
     the Indian government is pro-active: Digital India, Startup India,
     Skill India, and the India Innovation Fund all serve this purpose.
     India has taken a leaf from China‚Äôs industrial and technological
     policies. This shows up in several areas: massive facilitation to
     create, Jio, whose owner Ambani is a major business supporter of
     Mr. Modi;121 a push for data digitalization across wide sectors of
     government, usually starting from the initial Aadhaar platform. Aimed


         Supreme Court of India, People‚Äôs Union Of Civil Liberties... vs Union Of India (Uoi)
     119 

         And ANR. (December 18, 1996).
         Zubin Dash, ‚ÄúDo Our Wiretapping Laws Adequately Protect the Right to Privacy?,‚Äù
     120 

         Economic and Political Weekly 53, no. 6 (November 28, 2018): 7‚Äì8, https://www.
         epw.in/engage/article/can-government-continue-unhindered-wiretapping-without-
         flouting-right-privacy.
         Simon Mundy, ‚ÄúIndia: The Creation of a Mobile Phone Juggernaut,‚Äù Financial Times,
     121 

         October 2018, https://www.ft.com/content/4297df22-bcfa-11e8-94b2-17176fbf93f5.
                                                  I V. I N D I A , A D I G I T A L B L E N D




at creating a national ID system, Aadhaar, based on an iris scan and
fingerprints, crossed the 1 billion registered users mark in April
2016. At this point, Nandan Nilekani, the founder of Infosys and
first chairman of UIDAI (Unique Identification Authority of India)
hyped a ‚Äú600 billion USD market capitalization opportunity‚Äù to
extend its use to payment systems.122 The effort is particularly
aggressive in using biometric techniques. New payment systems
include thumb recognition features ‚Äì ‚Äúyour thumb is your bank‚Äù,
explained Mr. Modi.

It must be noted that most of the legal texts discussed in the Indian
case are yet to be approved by the Parliament, upon which the
outcome of the Indian data protection regime is conditional. India‚Äôs
legislation is thus caught between international and domestic web
market actors, a Constitution and its judges who have proven to be
protective of privacy rights and a government that sees issues in                              91
terms of modernization and efficiency of governance. It is a
battleground for both privacy issues and sovereign control of data
versus free flows. It remains to be seen which way India chooses to
go. India‚Äôs undecided status is well described in a comparative study
that puts the country close to surveillance states such as China or
Russia, but yet notes that the coming legislation might reverse much
of the situation ‚Äì if it is indeed implemented.123




    Nandan Nilekani, ‚ÄúIndia Financial Sectors,‚Äù Credit Suisse, June 29, 2016, https://
122 

    research-doc.credit-suisse.com/docView?language=ENG&format=PDF&document_
    id=1062747711&source_id=emcsplus&serialid=Wm0zJuKszkmbCwRYV7h
    Paul Bischoff, ‚ÄúSurveillance States: Which Countries Best Protect Privacy of Their
123 

    Citizens? - Comparitech,‚Äù Comparitech.Com, October 15, 2019, https://www.compa-
    ritech.com/blog/vpn-privacy/surveillance-states/.
                                           V

        CHINA, THE SURVEILLANCE STATE
          WITH SOME PRIVACY CONCERNS


With China, we enter a different world. It is one where there are
points in common regarding government control with some dispo-
sitions currently envisaged by the Modi government in India, but
absolutely no point of contact with GDPR style regulation, except
for some figures of speech. China‚Äôs paramount leader, Xi Jinping,
regularly emphasizes the importance of putting China at the forefront
of digital and artificial intelligence developments, with almost mes-
sianic overtones: It will ‚Äúfulfill the steady increase in the people‚Äôs
good life.‚Äù124 Under his watch, several factors merge to produce the
largest and most integrated scaling effects on the planet. However,                         93
the most important feature predates Xi‚Äôs mandate: the Chinese
internet functions in practice as an intranet. There is no foreign telco
network. China-to-China data never leaves the country. Outside
traffic only passes through a few checkpoints that can be shut
down.125 This, of course, is totally different from the heavily integrated
Indian web, but China‚Äôs firewall model is being emulated by Russia.126




    Xi Jinping, ‚ÄúMessage from Xi Jinping to the First China Digital Construction Summit
124 

    Êª°Ë∂≥‰∫∫Ê∞ëÊó•ÁõäÂ¢ûÈïøÁöÑÁæéÂ•ΩÁîüÊ¥ª,‚Äù (March 22, 2018).
    Catalin Cimpanu, ‚ÄúOracle: China‚Äôs Internet Is Designed More like an Intranet,‚Äù ZDNet,
125 

    July 30, 2019, https://www.zdnet.com/article/oracle-chinas-internet-is-designed-
    more-like-an-intranet/.
    Andrew Roth, ‚ÄúRussia‚Äôs Great Firewall: Is It Meant to Keep Information in ‚Äì or Out?,‚Äù
126 

    The Guardian, April 28, 2019, https://www.theguardian.com/technology/2019/apr/28/
    russia-great-firewall-sovereign-internet-bill-keeping-information-in-or-out.
     D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?




     The Long-Term State Push for All Digital Industries
     Within this closed sphere, long-term programs and massive subsidies
     have created an infrastructure of 4G mobile phones ‚Äì at last count
     1,56 billion mobile phone subscriptions existed, or more than one
     per person.127 This also dominates internet traffic, creating opportunities
     for use at any moment, anywhere. China‚Äôs backward cash and state
     banking system has suddenly been superseded by the world‚Äôs largest
     mobile payment system. In 2018, mobile payment platforms registered
     an astounding 60 billion transactions for a claimed amount of 41
     trillion dollars USD, and with an explosive growth rate of 37%.128
     Electronic payments in China already reach 25% of commerce, as
     opposed to 11% in the United States currently ‚Äì European usage
     differs widely across member states. The universal use of scanned
     QR codes ‚Äì down to beggars and using toilet paper in public places
94   ‚Äì also made this possible. It also ensures that data is recorded in a
     single transferable format. Such is the concentration of China‚Äôs digital
     business that two telcos, China Mobile and China Unicom, dominate
     the entire mobile phone business, while two platforms, Alipay and
     WeChat Pay, dominate 90% of the mobile payment industry. This
     concentration is also notable across sectors. Alibaba, among the top
     ten global companies, is no longer the e-commerce platform that made
     its initial reputation. It is an ecosystem of platforms ranging from eight
     wholesale and detail commerce (both domestic and global), five media
     and entertainment companies, two financial companies (520 million
     customers) that include Alipay and a facility for lending to small and

         Yu Xiaoming, ‚ÄúGovt to Further Boost Advanced Manufacturing, Innovation and
     127 

         Competitiveness,‚Äù Chinadaily.com.cn, March 5, 2019, http://www.chinadaily.com.
         cn/a/201903/05/WS5c7e0b0fa3106c65c34ecdb1.html.
         People‚Äôs Bank of China, ‚ÄúOverall Situation of the Payment System in 2018 2018Âπ¥ÊîØ
     128 

         ‰ªò‰ΩìÁ≥ªËøêË°åÊÄª‰ΩìÊÉÖÂÜµ,‚Äù March 20, 2019, p.4, http://www.gov.cn/xinwen/2019-03/20/
         content_5375401.htm.
V. C H I N A , T H E S U R V E I L L A N C E S T A T E W I T H S O M E P R I V A C Y C O N C E R N S




  medium firms, navigation, delivery and a ‚Äúlife search‚Äù engine providing
  local services, a logistics platform,129 and a starting health insurance
  business that is slated to take advantage of a very low penetration
  rate for this type of product in China. As such, Alibaba gathers huge
  amounts of real-time data from users and businesses alike across
  most of their daily activities. Its arch-rival, Tencent, possesses similar
  penetration through the horizontal expansion of its WeChat messaging
  platform, which also has 100 to 200 million international users.
  Overall, China‚Äôs state support for Internet+ policies has greatly favored
  the so-called BAT ‚Äì Baidu, Alibaba and Tencent ‚Äì granting them near
  data monopoly in their respective market with regard to web searches,
  online transactions, and social media. This goes hand-in-hand with
  a very close cooperation afforded to public administrations: much
  public data is privatized, but the government has, as we shall see,
  unlimited access. Interestingly enough, the three BAT companies are
  actually holding companies based in the Cayman Islands, relying on                                   95
  contracts with their China-based subsidiaries to draw dividends.



  The Issue of Technological Interdependence
  One should not consider that possessing big data implies using it
  efficiently, whether in terms of analytics or tailored products. The
  topic of China‚Äôs real advance in analytics is hotly disputed. While
  some experts such as Kaifu Lee,130 who also have skin in the game,
  extoll China‚Äôs prowess, there are occasional signs that even the
  biggest platforms can be dependent on software made in America
      Ming Zeng, ‚ÄúEverything Alibaba Does Differently ‚Äî and Better,‚Äù Harvard Business
  129 

      Review, August 21, 2018, https://hbr.org/2018/09/alibaba-and-the-future-
      of-business.
      Kai-Fu Lee, AI Superpowers: China, Silicon Valley, and the New World Order (Boston:
  130 

      Houghton Mifflin Harcourt, 2018)
     D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?




     or elsewhere. Alibaba, for example, has a partnership with Salesforce,
     America‚Äôs leading provider of customer relation management (CRM),
     to provide cloud-based technologies to its own clients. It signed a
     similar partnership with AXA, the European insurer, to provide tailored
     insurance products to its e-commerce customers, Chinese SMEs as
     well as travelers using Alipay outside of China.131

     Yet, the example of Alibaba, a company that does not hesitate to
     strike deals with potential competitors at home and abroad, cuts
     both ways. Alibaba presents itself as the ‚Äúexclusive provider‚Äù of
     Salesforce for Greater China, even though it recognizes the power
     of Salesforce‚Äôs CRM solutions.132 The company has developed an
     app that can ‚Äúprocess auto insurance claims in seconds whilst
     assessing exterior vehicular damage and displaying vehicle damage
     information to users, including where to repair the vehicle.‚Äù133 It is
96   developing health apps that combine diagnosis tools with a search
     for price bids to fill their prescriptions, payment systems for state
     hospital patients, which also allow the company to offer second
     opinions or prescription bids, and ultimately an integrated diagnosis
     and payment tool. It is providing blockchain technology to manage
     patient records and prescriptions across provincial borders.134




         AXA, ‚ÄúAXA, Alibaba and Ant Financial Services Announce Global Strategic Partnership
     131 

         | AXA,‚Äù AXA.com, July 29, 2016, https://www.axa.com/en/newsroom/press-releases/
         axa-alibaba-ant-financial-services-announce-global-strategic-partnership.
         Tom Brennan, ‚ÄúAlibaba Now Exclusive Provider of Salesforce CRM in Greater China,‚Äù
     132 

         Alibaba Cloud Community, July 25, 2019, https://www.alibabacloud.com/blog/
         alibaba-now-exclusive-provider-of-salesforce-crm-in-greater-china_595141.
         The Digital Insurer, ‚ÄúAlibaba - The Digital Insurer,‚Äù The Digital Insurer, November 10,
     133 

         2018, https://www.the-digital-insurer.com/cif-alibaba/.
         Michael O‚ÄôDwyer, ‚ÄúAlibaba - The Digital Insurer,‚Äù The Digital Insurer, Undated, https://
     134 

         www.the-digital-insurer.com/cif-alibaba/.
V. C H I N A , T H E S U R V E I L L A N C E S T A T E W I T H S O M E P R I V A C Y C O N C E R N S




  Data Oligopolies
  A major difference stems from this portrait and could be applicable
  to China‚Äôs other digital giants: their activities, and therefore their
  data resources, cut across many sectors. The issue of third-party
  transfer of data, which is so important in the design of America‚Äôs,
  Europe‚Äôs or India‚Äôs privacy rules, matters less in the Chinese case,
  where a few platforms form a data oligopoly. Nonetheless, they do
  not neglect income from re-selling to third parties, as we shall see.
  WeChat, with 1,1 billion users in China, 900 million for its payment
  system and 100 million for its financial products, is another case
  in point. No non-Chinese platform can claim such ubiquity and
  breadth of services. It is not surprising that Mark Zuckerberg,
  Facebook‚Äôs founder and principal owner, talks of moving away from
  an advertising-based model (with many third-party users or
  consumers of the company‚Äôs data), to other services ‚Äúincluding calls,                                97
  video chats, groups, stories, businesses, payments, commerce, and
  ultimately a platform for many other kinds of private services.‚Äù135
  Similarly, Google is expanding payment services and investing into
  AI and cloud services in the health sector.

  The point here is that even before one gets to the issue of China‚Äôs
  massive surveillance state and its digital arms, its technically private
  platforms aggregate more personal data than in any other society.
  In the construction of a national integrated data bank for social credit
  applied to companies, we find Alibaba, Tencent and Huawei to be
  key actors. Nor is it confined to these platforms, as China has also
  seen a host of start-ups grow into new sectors ‚Äì facial recognition

      Li Yuan, ‚ÄúMark Zuckerberg Wants Facebook to Emulate WeChat. Can It?,‚Äù The New
  135 

      York Times, March 7, 2019, https://www.nytimes.com/2019/03/07/technology/face-
      book-zuckerberg-wechat.html#click=https://t.co/q1vhufRaCi.
     D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?




     being the best known. The issue of privacy should be paramount.
     The top business leaders in the digital sector take different attitudes
     to this. In the words of Jack Ma, founder of Alibaba: ‚ÄúEurope does
     not have a big internet company, because it has way too much legal
     system (‚Ä¶) Internet is still at the initial stage, and we are already
     talking about the issue of privacy and security. Trust me, we will be
     able to solve the problem, and if not, our kids will.‚Äù136 Pony Ma,
     founder of Tencent, is more cautious: ‚ÄúData cannot be aggregated
     without rules. Communication, social exchange and consumer
     behavior data must not be aggregated, or this will bring catastrophic
     consequences‚Äù.137 He has called for unified rules protecting internet
     users‚Äô data privacy during China‚Äôs two parliamentary sessions of
     March 2019.138



98   Regulation Is About Data Security First
     But the general discussion about privacy is more focused on data
     security and the overall need for regulation than on ensuring privacy.
     In any case, oversight is split between multiple and different
     authorities ‚Äì the Ministry for Public Security, the State Administration
     for Market Regulation (SAMR) and the Cyberspace Administration


         Sina.cn, ‚ÄúMa Yu: The Combination of the Internet of Things and Big Data is the Future
     136 

         È©¨‰∫ëÔºöÁâ©ËÅîÁΩëÂíåÂ§ßÊï∞ÊçÆÁöÑÁªìÂêàÊâçÊòØÊú™Êù•,‚Äù Sina.cn, September 10, 2017, https://tech.
         sina.cn/it/2017-09-10/detail-ifykuffc4789377.d.html?cre=tianyi&mod=wtech&loc
         =1&r=25&doct=0&rfunc=0&tj=none&tr=25&vt=4&pos=18.
         Zhang Chao, ‚ÄúMa Huateng Answers : Tencent Does Not Dream. Social Exchange,
     137 

         Communication ‚ÄúData Should Not Be Aggregated without RulesÈ©¨ÂåñËÖæÂõûÂ∫î‚ÄòËÖæËÆØÊ≤°
         ÊúâÊ¢¶ÊÉ≥ÔºöÁ§æ‰∫§„ÄÅÈÄö‰ø°‚ÄòÊï∞ÊçÆ‰∏çËÉΩ‰ªªÊÑèÊâìÈÄö,‚Äô‚Äù Shidai Caijing, November 9, 2018, https://
         tfcaijing.com/article/page/8a9eaf0566e21b6e0166f3e81bb11c44.
         Xinhua, ‚ÄúMa Huateng: Collection of Private Information Through Big Data Should Not
     138 

         Be Too Complete È©¨ÂåñËÖæÔºöÂ§ßÊï∞ÊçÆÊî∂ÈõÜÈöêÁßÅ‰ø°ÊÅØ‰∏çÂÆúÂ§™ÂÖ®,‚Äù Xinhuanet.com, March 4,
         2017, http://www.xinhuanet.com//politics/2017-03/04/c_1120566998.htm.
V. C H I N A , T H E S U R V E I L L A N C E S T A T E W I T H S O M E P R I V A C Y C O N C E R N S




  of China (CAC), and at least nine other government level entities are
  involved in national regulation. A few voices call for a GDPR style
  comprehensive approach to ‚Äúestablish a one stop, unified law
  enforcement department to be responsible for personal information
  protection‚Äù139 while stipulating punishments for violations. Experts
  and official sources alike have a field day, of course, in highlighting
  the privacy breaches that happen elsewhere. In an article on China‚Äôs
  status as a big data power, the People‚Äôs Daily notes that personal
  data leakage is on the rise internationally. More ominously, it notes
  that 96,6% of Android apps installed in China seek access to personal
  data, and that 25,3% of them have cross-border access to this
  personal data.140



  Third Parties Piggyback the Data Oligopolies
                                                                                                       99
  A detailed study ‚Äì done in cooperation with Microsoft and international
  scholars ‚Äì delves deeper into the issue. On China‚Äôs main platforms
  providing access to third-party apps ‚Äì Baidu, Tencent and Wandoujia
  ‚Äì there is wide-spread launch of other apps in the background,
  without users being aware of even having previously used these
  apps. On average, each app launches 76 other apps. On a total of
  800 apps running on 1520 devices (mostly smartphones), 27,1%
  of the energy is consumed by these undetected launches. The apps
  form clusters as visible in the figure below for Wandoujia:

      Cui Xiankang, Han Wei, and Ren Qiuyu, ‚ÄúProposed Guidelines Highlight China‚Äôs
  139 

      Fragmented Protection of Online Privacy - Caixin Global,‚Äù Caixinglobal.com, May 9,
      2019, https://www.caixinglobal.com/2019-05-09/proposed-guidelines-highlight-
      chinas-fragmented-protection-of-online-privacy-101413683.html.
      Liu Miao, ‚ÄúChina‚Äôs ‚ÄòBig Data‚Äô Is Not Only About ‚ÄúData Size ‰∏≠ÂõΩÂ§ßÊï∞ÊçÆÔºå‰∏çÂè™‚ÄòÊï∞ÊçÆ
  140 

      Â§ß‚Äô,‚Äù People‚Äôs Daily Overseas Edition, July 9, 2018, http://www.gov.cn/shuju/2018-
      07/09/content_5304898.htm.
      D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?




      Figure Source: Mengwei Xu et al., ‚ÄúAppHolmes: Detecting and Characterizing App
100   Collusion among Third-Party Android Markets,‚Äù Microsoft Research, April 3, 2017, p.
      148, https://www.microsoft.com/en-us/research/publication/appholmes-detecting-
      characterizing-app-collusion-among-third-party-android-markets/.


      These background apps often require sensitive permissions
      endangering personal data. Ironically, part of the problem lies with
      the forced unavailability of the Google search engine in China: apps
      use push service on Android (another Google product‚Ä¶) to
      compensate for this gap.141

      According to a survey conducted by the China Consumers Association
      in 2018, 85,2% of interviewees experienced some kind of data leak,

          Mengwei Xu et al., ‚ÄúAppHolmes: Detecting and Characterizing App Collusion among
      141 

          Third-Party Android Markets,‚Äù Microsoft Research, April 3, 2017, https://www.micro-
          soft.com/en-us/research/publication/appholmes-detecting-characterizing-app-
          collusion-among-third-party-android-markets/.
V. C H I N A , T H E S U R V E I L L A N C E S T A T E W I T H S O M E P R I V A C Y C O N C E R N S




  but one third of them decide to ‚Äúswallow it and accept the bad
  luck.‚Äù142



  Start-Ups, The Surveillance Front Line
  Finally, the BAT companies‚Äô oligopoly on data does not prevent China
  to have a lively and well-supported scene for start-ups. Some of it,
  it may be argued, resembles a bubble: 27 start-ups focus on AI. But
  new contestants do rise. Bytedance is now the highest valued start-up
  in the world, owning TikTok (the international version of Douyin in
  China), a video lip-sync app that is wildly popular all over the world
  with children and teenagers, as previously seen in India. It is now
  taking traffic away from Alibaba and Tencent. In a good example of
  what is becoming the global splinternet, its privacy policies differ
  according to market. By contrast, at least some American platforms                                   101
  have announced they would implement GDPR rules across the board,
  and not only in Europe. European users under the GDPR, as well
  as Indian users, can access their personal data from TikTok if they
  care to. Such was not the case in the United States until February
  2019, and the data could be transferred to servers in China. There
  is actually nothing unusual regarding international data transfer ‚Äì
  except that this is highly personal data caught at the most sensitive
  age, with personal identification, and an everlasting memory in
  China.143 Many other companies in China are developing AI apps,

      Cqn.com.cn, ‚ÄúChina Consumers Association released ‚ÄúInvestigation Report On the
  142 

      Personal Information Disclosure of Apps‚Äù ‰∏≠Ê∂àÂçèÂèëÂ∏É„Ää App‰∏™‰∫∫‰ø°ÊÅØÊ≥ÑÈú≤ÊÉÖÂÜµË∞ÉÊü•
      Êä•Âëä „Äã,‚Äù Cqn.com.cn, August 29, 2018, http://www.cqn.com.cn/pp/
      content/2018-08/29/content_6213791.htm.
      David Carroll, ‚ÄúTikTok Might Be a Chinese Cambridge Analytica-Scale Privacy Threat,‚Äù
  143 

      Quartz, May 7, 2019, https://qz.com/1613020/tiktok-might-be-a-chinese-cambridge-
      analytica-scale-privacy-threat/.
      D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?




      running for example on facial recognition that have multiple
      surveillance uses, and not only for the state. Hanwang, founded in
      2014, provides facial and biometric recognition services and optical
      character recognition, as well as air quality monitors and purifiers.
      One of their apps, running with Hikvision cameras, offers to schools
      a ‚ÄúClass Care system‚Äù that watches individually every student‚Äôs
      attitude before giving it a weekly score. This is one of the everyday
      consequences of the government‚Äôs Next Generation Artificial
      Intelligence Development Plan (NGAIDP). The plan aims to
      incorporate AI in virtually all aspects of life, including medicine, law,
      transportation, environmental protection, and what it calls ‚Äúintelligent
      education.‚Äù144 The ubiquity of surveillance cameras ‚Äì estimated at
      176 million in 2017, and projected at 626 million by 2020,145 is
      a big advantage for the collection of data. The uses are now
      everywhere across China.146 This is only the everyday societal
102   consequence of a vision that includes the sinister implementation
      of AI and facial recognition programs against Xinjiang‚Äôs entire
      population.147 In an example inadvertently exposed, one such system




      144 
           Xue Yujie, ‚ÄúCamera Above the Classroom,‚Äù Sixth Tone, March 26, 2019, https://www.
           sixthtone.com/news/1003759/camera-above-the-classroom.
      145
           According to widely cited statistics gathered by Statista in 2017.
          Source: Statista Research Department, ‚ÄúChina: Surveillance Camera Installation 2017-
          2020,‚Äù Statista, 2019, https://www.statista.com/statistics/879198/
          china-number-of-installed-surveillance-cameras/.
      146 
            For various examples, see: Julie Zaugg, ‚ÄúEn Chine, La Vie Sous l‚ÄôOeil Inquisiteur Des
           Cam√©ras,‚Äù Les Echos, March 7, 2019, https://www.lesechos.fr/tech-medias/hightech/
           en-chine-la-vie-sous-loeil-des-cameras-997774.
      147 
           HRW, ‚ÄúChina‚Äôs Algorithms of Repression Reverse Engineering a Xinjiang Police Mass
           Surveillance App,‚Äù Human Rights Watch, May 1, 2019, https://www.hrw.org/
           report/2019/05/01/chinas-algorithms-repression/reverse-engineering-xinjiang-
           police-mass-surveillance.
V. C H I N A , T H E S U R V E I L L A N C E S T A T E W I T H S O M E P R I V A C Y C O N C E R N S




  was gathering daily detailed individual information on 2,56 million
  people in Xinjiang, based on surveillance cameras.148 Fascinatingly,
  Xinjiang is designated by the Ministry of Information Technology as
  the center of ¬´ pilot projects ¬ª for big data integration and Artificial
  Intelligence in 2020.149

             China‚Äôs Social Credit ‚Äì A Hydra Larger Than Life
    Since 2014, no innovation in control has sparked as much
    comment as China‚Äôs ‚Äúsocial credit system‚Äù.150 Part of the reason
    is that it was officially hyped in China as a path to trust by enforcing
    a reward/punishment system. One seldom sees in China a mention
    of the evident imitation of scoring techniques that are in wide use
    in market economies ‚Äì a credit score in the United States, for
    example, is mandatory not only for credit cards, loans and
    insurance, but also for renting property. Nor are inroads into privacy
    unique to China. Massive and publicly available data bases in the                                  103
    United States provide information on any individual, including
    traffic fines, and allow for example one to locate known sex
    offenders in your neighborhood.




      Catalin Cimpanu, ‚ÄúChinese Company Leaves Muslim-Tracking Facial Recognition
  148 

      Database Exposed Online,‚Äù Zdnet.com, February 17, 2019, https://www.zdnet.com/
      google-amp/article/chinese-company-leaves-muslim-tracking-facial-recognition-
      database-exposed-online/?__twitter_impression=true.
      Ministry of Industry and Information Technology of China, ‚ÄúNotice of the Ministry of
  149 

      Industry and Information Technology of China, on Applying to the Big Data Pilot Project
      in 2020 Â∑•‰∏öÂíå‰ø°ÊÅØÂåñÈÉ®ÂäûÂÖ¨ÂéÖÂÖ≥‰∫éÁªÑÁªáÂºÄÂ±ï2020Âπ¥Â§ßÊï∞ÊçÆ‰∫ß‰∏öÂèëÂ±ïËØïÁÇπÁ§∫ËåÉÈ°π
      ÁõÆÁî≥Êä•Â∑•‰ΩúÁöÑÈÄöÁü•,‚Äù November 6, 2019, http://www.miit.gov.cn/n1146295/
      n1652858/n1652930/n3757022/c7517097/content.html , cited by EastisRed (Passe
      Muraille) n¬∞ 38, November 18, 2019, https://eastisred.fr/passe-muraille-
      n38-semaine-du-11-novembre/.
  150
      Á§æ‰ºö‰ø°Áî®‰ΩìÁ≥ª (shehui xinyong tixi).
      D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?




        Furthermore, China goes back a long way in terms of collective
        surveillance ‚Äì the ‚Äú100 household‚Äù (baojia) system in Imperial
        times was a system of mutual surveillance. In a Leninist-Maoist
        context, the entire population was classified according to some
        40 categories, depending on a mixture of class origin, personal
        status and behavior, from good to bad, red to black. The Communist
        Party of China system has always had dang‚Äôan ‚Äì files from all sorts
        of surrendered or collected information ‚Äì on every one of its 90
        million members (as of 2018). Conversely, at the grassroots, where
        the state had always lacked presence, the issue of trust has always
        been paramount. China was by necessity a society based on
        relationships (guanxi) because this was a way to overcome distrust.
        The practice of multiple names and voluntary disappearances was
        also very frequent, absent a reliable nation-wide ID system.
        Against this background, social credit scoring is recreating trust without
104
        the need for special relationships ‚Äì and that is generally welcomed
        by the population, which values public order and discipline against
        a traditional background of individualism and unreliability.
        The decision in 2014 to construct a social credit system by 2020
        combines multiple local experiments with the overall holistic goal
        of ‚Äúa market improvement of the economic and social order.‚Äù151
        The peculiarity of China‚Äôs political order resides in this single
        sentence: the market serves public order, and it is therefore difficult
        to separate public and private initiatives, for example consumer
        credit scoring and a more general categorization and reward or
        punishment of good/bad behavior in much wider areas.

          State Council of the People‚Äôs Republic of China, ‚ÄúSignificant Improvement in Economic
      151 

          and Social Order ‚ÄúNotice of the State Council on Printing and Distributing the Outline of
          the Construction of the Social Credit System (2014-2020) ÁªèÊµéÁ§æ‰ºöÁß©Â∫èÊòæËëóÂ•ΩËΩ¨ ‚ÄúÂõΩ
          Âä°Èô¢ÂÖ≥‰∫éÂç∞ÂèëÁ§æ‰ºö‰ø°Áî®‰ΩìÁ≥ªÂª∫ËÆæËßÑÂàíÁ∫≤Ë¶ÅÔºà2014‚Äî2020Âπ¥ÔºâÁöÑÈÄöÁü•‚Äù‚Äù, www.gov.cn,
          June 14, 2014, http://www.gov.cn/zhengce/content/2014-06/27/content_8913.htm
V. C H I N A , T H E S U R V E I L L A N C E S T A T E W I T H S O M E P R I V A C Y C O N C E R N S




    Yet, at this point, it is not sure that all or even most of the social
    credit systems will be integrated in a single nation-wide scheme
    ‚Äì multiple voices are raised against this, which in any case would
    require a huge technological feat to classify, store and protect the
    data, perhaps in excess of any actual need. Still, the main systems
    that exist are both impressive and ominous. Some local or pilot
    initiatives are scary and represent even worse challenges to privacy.
    One scheme, created by China‚Äôs central bank in 2015 with eight
    entities, including Sesame Credit (developed by Ant Financial, an
    affiliate of Alibaba) and Tencent Credit Information Co., aggregates
    data from online behavior and purchases, public agencies and
    third-party merchants to create instant scoring of individuals. The
    ratings include criteria of stability, website behavior and the
    company you keep ‚Äì your social relationships for instance. By
    2015, Sesame Credit alone rated 300 million individuals and 37
                                                                                                       105
    million small firms.152 In turn, credit scoring data is used by other
    scoring firms ‚Äì for example, by the BaiHe dating app to assess
    your partner. A ‚Äúgame‚Äù allows friends to compete on their credit
    scoring. Many local social credit systems also use data from these
    large nation-wide companies.
    But the build-up does not stop there. A national and publicly-run
    website, Credit China, aggregates scoring data from 44 central
    departments, 22 provincial platforms and 122 social institutions.
    All sorts of data ‚Äì from taxation, to food and drug or environmental
    protection are shared, along with the ‚Äúblack‚Äù or ‚Äúred‚Äù lists of
    individuals. To fight bureaucratic ‚Äúslumber‚Äù, e.g. non-use of the
    data, third-party companies can access it for a fee. A company,


      Ant Financial, ‚ÄúAnt Financial Unveils China‚Äôs First Credit-Scoring System Using Online
  152 

      Data,‚Äù Ant Financial, January 28, 2015, https://www.alibabagroup.com/en/news/
      press_pdf/p150128.pdf.
      D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?




        BaiHang Credit, was created with 35% state ownership ‚Äì the
        National Internet Finance Association of China ‚Äì and 8% by each
        of the original and technically private pilot companies. Thus, private
        scoring and public-private sharing of the data are achieved, again
        revealing China‚Äôs uniqueness.
        The best-known public consequence of this integrated network is
        the ability of China‚Äôs courts, from data available with the National
        Public Credit Centre, to deny some services: at the end of 2018,
        17,5 million flights, 5,5 million train trips were denied. There
        were also, according to the People‚Äôs Daily, black lists for golf
        courses, high-end hotels and flats, private schools, and financial
        products.153 In one extreme case, a Shandong province court
        ordered phone companies in 2017 to insert automatic warning
        messages on calls received by untrusted persons.154 At the other
        end, good grades for social behavior, including acts of social benefit
106
        and general reliability, win reduced prices on many services, all
        the way to priority lines at hospitals. Scores can determine people‚Äôs
        access to public services, welfare, school admission, employment,
        job promotions and business undertakings. The schemes are said
        to involve less than 10% of the population in either red or black
        categories ‚Äì but that was always a recipe of traditional Maoism
        that pitted large majorities against targeted minorities and rewarded
        a minority of ‚Äúactivists.‚Äù The same system of reward and punishment
        conducts ‚Äúto create a more regulated, fair, transparent and


          Xue Yuan, ‚ÄúThe Release of 2018 Annual Report on the Credit Blacklist 2018 Âπ¥Â§±‰ø°
      153 

          ÈªëÂêçÂçïÂπ¥Â∫¶ÂàÜÊûêÊä•ÂëäÂèëÂ∏É,‚Äù www.gov.cn, February 19, 2019, http://www.gov.cn/
          fuwu/2019-02/19/content_5366674.htm.
          BBC News in Chinese, ‚ÄúFrom Portfolio to Credit Score, Is China on the Way to an
      154 

          ‚ÄúOrwellian‚Äù Monitoring Society ‰ªéÊ°£Ê°àË¢ãÂà∞‰ø°Áî®ËØÑÂàÜ ‰∏≠ÂõΩÊòØÂê¶Ê≠£Ëµ∞Âêë‚ÄòÂ••Â®ÅÂ∞îÂºè‚Äô
          ÁõëÊéßÁ§æ‰ºö,‚Äù BBC News in Chinese, October 17, 2018, https://www.bbc.com/zhongwen/
          simp/chinese-news-45886126.
V. C H I N A , T H E S U R V E I L L A N C E S T A T E W I T H S O M E P R I V A C Y C O N C E R N S




    predictable legalized business environment.‚Äù155 For instance,
    payment delay will put you on the black list, and both the company
    and legal representative will face ‚Äúobstacles‚Äù.
    The variety of experiments and systems in place has led to debates
    on their limits: should social credit be based on explicit and clear
    legal criteria, or should it extend to moral judgments that are loosely
    defined? The right to be forgotten, and even more concretely, how
    to restore one‚Äôs credit ‚Äì particularly in case of mistakes ‚Äì are
    objects of discussion. The issue of ‚Äúrumor spreading‚Äù, often another
    name for criticism of authorities, is also prevalent. Some experts
    call for a nation-wide regulation on social credit ‚Äì which as of the
    spring 2019 was only a ‚Äúclass three priority‚Äù for China‚Äôs national
    legislature, meaning not forthcoming in the near future.156


                                                                                                       107
  A Diminutive Public Debate on Privacy
  Given the wider pattern of ‚Äúsocial credit‚Äù systems put in place across
  China, the Xinjiang case can be considered as an experiment with
  potential implementation elsewhere. But there is little public debate
  in China on these aspects, nor are there many experts writing on
  broader aspects of privacy rights. What exists is focused on cases
  elsewhere, usually in America or from American companies. We
  have already discussed the case of Android. Huawei‚Äôs executives
      Luo Pan, ‚ÄúMinistry of Commerce: The Construction of the Corporate Social Credit
  155 

      System Will Not Adopt the So-Called Suppression MeasuresÂïÜÂä°ÈÉ®Ôºö‰ºÅ‰∏öÁ§æ‰ºö‰ø°Áî®
      ‰ΩìÁ≥ªÂª∫ËÆæ‰∏ç‰ºöÈááÂèñÊâÄË∞ìÊâìÂéãÊé™ÊñΩ,‚Äù chinanews.com, August 29, 2019, https://www.
      chinanews.com/gn/2019/08-29/8941547.shtml.
      Zhang Yuzhe and Han Wei, ‚ÄúIn Depth: China‚Äôs Burgeoning Social Credit System Stirs
  156 

      Controversy - Caixin Global,‚Äù Caixinglobal.com, April 1, 2019, https://www.caixinglobal.
      com/2019-04-01/in-depth-chinas-burgeoning-social-credit-system-stirs-contro-
      versy-101399430.html.
      D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?




      abroad occasionally extol GDPR regulations, a part of the company‚Äôs
      style of messaging that is completely absent in China.157 A discussion
      on privacy in a consumer magazine hinges around Facebook and a
      Georgetown University expert,158 another in a popular science journal
      also revolves around Facebook and Deepmind.159 It seems that
      publications in the People‚Äôs Republic of China cannot find examples
      drawn from its own digital industries and practices.

      An exception however exists concerning data collection for commercial
      purposes. In 2017, a government-backed consumer NGO in Jiangsu
      province launched an enquiry against 27 snooping apps scraping
      consumers‚Äô personal data. It then initiated a lawsuit against Baidu,
      the sole company among the 27 that did not withdraw the feature.
      The case went to court; Baidu retreated and updated its app. The
      NGO then withdrew its lawsuit.160 As we shall see, there can be
108   tensions between regulatory agencies and commercial companies.

      Yet, there are many more relevant discussions of digital privacy when
      it is framed in the context of data security, whether this is about
      fraud and abuse of data or about national security.




          Joy Tan, ‚ÄúTransparency and Privacy Go Hand in Hand,‚Äù Linkedin.com, November 25,
      157 

          2018, https://www.linkedin.com/pulse/transparency-privacy-go-hand-joy-tan/.
          ‚ÄúFacial Recognition, Privacy Protection and the Scientific Challenges, Âà∑ËÑ∏ÔºåÈöêÁßÅ‰øù
      158 

          Êä§‰∏éÁßëÊäÄÁöÑÂçöÂºà,‚Äù Consumer Daily 126, July, 2015.
          Fang Lingsheng, ‚ÄúIdentification Systems: The End of Personal Privacy, ËØÜÂà´Á≥ªÁªüÔºö‰∏™
      159 

          ‰∫∫ÈöêÁßÅÁªàÁªìËÄÖ,‚Äú World Science, March 2015, p. 30-37.
          Zhang Jie, ‚ÄúConsumer Rights Group Withdraws Complaint against Baidu,‚Äù Chinadaily.
      160 

          com.cn, March 15, 2018, http://www.chinadaily.com.cn/a/201803/15/
          WS5aaa1535a3106e7dcc141dda.html.
V. C H I N A , T H E S U R V E I L L A N C E S T A T E W I T H S O M E P R I V A C Y C O N C E R N S




  Digital China‚Äôs Regulatory Framework
  The burgeoning cybersecurity and data protection regulation reflects
  a familiar triangle of tensions between different goals: efficiency,
  which in this case means faster development of AI and big data
  applications; the protection of personal data, which is largely
  perceived as preventing its misuse by private actors; and the
  overriding concern for national security, which is perhaps the best-
  known aspect internationally. This is evidenced by the balanced
  maze of regulation being constructed. Two major difficulties restrict
  interpretation: it is difficult to hierarchize laws, regulations and
  standards since much of the ‚Äúinformal‚Äù guidance can actually be
  quite binding. And as always with Chinese legal texts, ambiguities
  abound and implementation is extremely variable.

  Still, the corpus under development is impressive. After a set of                                    109
  administrative measures in 2000 regulating the internet, China‚Äôs
  2017 cybersecurity law has become the overarching law of reference.
  The law tilts towards ‚Äúguaranteeing cybersecurity, safeguarding
  cyberspace sovereignty, national security and public interest‚Äù,
  although it also concerns the legal rights of individuals and
  organizations. It compels network operators to provide support to
  all security organs safeguarding national security and investigating
  criminal activities ‚Äúin accordance with the law.‚Äù It also asks all
  operators to ‚Äúvoluntarily contribute‚Äù to the security of ‚Äúcritical
  infrastructures‚Äù that are broadly defined: ‚Äúpublic communication and
  information services, power, traffic, water resources, finance, public
  service, e-government, and other.‚Äù The inclusion of the term ‚Äúother‚Äù
  as an escape clause allowing for any extension is a very frequent
  occurrence in Chinese public law ‚Äì from the Penal Code to these
  cyber-related rules. It essentially leaves authorities free to use their
      D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?




      own definition as the occasion may require. ‚ÄúCritical‚Äù data, again
      very broadly and loosely defined, must be stored in China, a provision
      that drew criticism from foreign firms and operators in China. Data
      operators must pass periodic security reviews. The law also requires
      all users to provide real name information. The government can ‚Äútake
      temporary measures regarding network communications in a specially
      designated region, such as limiting such communications‚Äù, a black-
      out that has been applied on occasions to Xinjiang.



      The Cybersecurity Law and Personal Data Protection
      In principle, the law provides protection to users: network operators
      ‚Äúshall strictly maintain the confidentiality of user information they
      collect‚Äù, and users have a conditional right to erasure or correction
110   of their personal data if it has not been gathered legally or if it is
      erroneous. It defines ‚Äúpersonal information‚Äù 161 as ‚Äúall kinds of
      information, recorded electronically or through other means, that
      taken alone or together with other information, is sufficient to identify
      a natural person‚Äôs identity, including but not limited to natural
      persons‚Äô full names, birth dates, national identification numbers,
      personal biometric information, addresses, telephone numbers, and
      so forth.‚Äù

      As it is, China‚Äôs cybersecurity law has some common traits with
      India‚Äôs 2018 ‚ÄúDraft Intermediary Guidelines‚Äù: the introduction of
      ‚Äúcritical‚Äù data, the ambiguous mention of provisions ‚Äúaccording to
      the law.‚Äù Besides opening exceptions in ‚Äúother‚Äù situations, it restricts
      all critical data to China. India‚Äôs restrictions are narrower in scope.

      161
            ‰∏™‰∫∫‰ø°ÊÅØ (geren xinyi).
V. C H I N A , T H E S U R V E I L L A N C E S T A T E W I T H S O M E P R I V A C Y C O N C E R N S




  However, he possibility to completely suspend communications is
  no longer a unique provision ‚Äì mobile internet data service in Kashmir
  and other parts of India have been suspended on occasion under
  Section 144 of India‚Äôs Penal Code regarding unlawful assembly.

  In our triangle, China‚Äôs cyber law of 2017 tilts very much towards
  the state‚Äôs rights, the private operators‚Äô obligations, and a few
  conditional or generic rights for the individual.



  The On-Going Regulatory Maze
  It was accompanied or followed by a spate of laws, regulations and
  standards: one can count six systems in different areas (data
  protection being one of the six) and more than ten new ‚Äústandards‚Äù
  so far: they join the 240 existing standards set since 2010 on                                       111
  ‚ÄúInformation security technology.‚Äù162 In our area of reference, they
  cover cross-border transfers, national intelligence and counter-
  espionage. Many other regulations exist on given sectors, including
  finance, banking, e-commerce and consumer protection. Vagueness
  and ambiguities abound ‚Äì including in some cases on whether the
  rules are mandatory or just guidance. In part, this is also due to
  China‚Äôs peculiar legal system, where rules are written on the go, and
  then eventually rewritten or amended.

  Most of these rules point in the direction of even more state control.
  Cross-border rules prohibit external copy of many fields of data
  stretching to the customary ‚Äúother circumstances that possibly affect

      They are run by the China National Information Security Standards Technical Committee
  162 

      (CNISSTC), and published exclusively in Chinese on its website at https://www.tc260.
      org.cn/
      D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?




      national security and societal and public interests.‚Äù The most recent
      draft law for cross-border transfers no longer requires official
      assessment for companies with more than 1000 Giga or 500.000
      individuals, but it puts greater responsibility on all companies, which
      must store their digital data for five years. The risk assessment also
      remains very broad and open-ended. Critical infrastructures now
      include media, e-commerce, e-payment, search engines, emails,
      blogs, cloud computing, enterprise systems and big data. Article 7
      of China‚Äôs 2018 National Intelligence Law ‚Äì widely cited in the
      debate over Huawei ‚Äì states that every ‚Äúorganization or citizen shall
      support, assist in and cooperate in national intelligence work in
      accordance with the law and keep confidential the national
      intelligence work that it or he knows.‚Äù China‚Äôs Counter-Espionage
      Laws unsurprisingly carry similar obligations but the 2017 version
      goes two steps further. It applies to persons in China and abroad,
112   while tying with espionage, actions such as ‚Äúfabricating or distorting
      facts, publishing or disseminating words or information that endanger
      state security, or making, distributing or publishing audio-visual
      products or other publications endangering state security.‚Äù



      The 2018 Personal Information Security Specification
      ‚Äì A Watershed
      To individuals or to businesses alike, the most important new rule
      is the 2018 Information security technology ‚Äî Personal information
      security specification (PIS).163 There are two important provisions

          National Information Security Standardization Technical Committee, ‚ÄúInformation
      163 

          Security Technology ‚Äî Personal Information Security Specification ‰ø°ÊÅØÂÆâÂÖ®ÊäÄÊúØ‰∏™
          ‰∫∫‰ø°ÊÅØÂÆâÂÖ®ËßÑËåÉ,‚Äù National Information Security Standardization Technical Committee,
          May 1, 2018, https://www.tc260.org.cn/upload/2018-01-24/151679976
          4389090333.pdf
V. C H I N A , T H E S U R V E I L L A N C E S T A T E W I T H S O M E P R I V A C Y C O N C E R N S




  though: as a ‚Äúspecification‚Äù or ‚Äústandard‚Äù (guifan), it is not law, yet
  it is often regarded as such by authorities. And it already has been
  amended and constantly supplemented by new and specific
  regulations.164 In particular, a new draft text on Data Security
  Management seems to go further in the obligations placed on
  companies to protect personal data.165 Keeping this in mind, it is
  important to note that the PIS is both heavily influenced by the GDPR
  and yet differs in some key aspects. The drafters of the PIS have
  extracted the ‚Äúessence‚Äù of available documents (OECD Privacy
  Guidelines, APEC Privacy Guidelines, GDPR, related ISO law,
  American laws, etc.) and customized them for the case of China.166
  But they also made it clear that, since the beginning, they were
  aiming for a regulation stricter than in the United States, but not as
  strict as in Europe.167

  Similarities have been described by one external observer,168 while                                  113
  differences are spelled out by one of the experts who contributed to


      Yan Luo, ‚ÄúChina Releases Draft Amendments to the Personal Information Protection
  164 

      Standard,‚Äù Inside Privacy, February 11, 2019, https://www.insideprivacy.com/inter-
      national/china/china-releases-draft-amendments-to-the-personal-information
      -protection-standard/.
  165
      The draft measures have been published in CAC‚Äôs website.
      Source: CAC, ‚ÄúNotice of the National Internet Information Office on Public Consultation
      on the ‚ÄúData Security Management Measures (Draft for Comment)‚Äù ÂõΩÂÆ∂‰∫íËÅîÁΩë‰ø°ÊÅØ
      ÂäûÂÖ¨ÂÆ§ÂÖ≥‰∫é„ÄäÊï∞ÊçÆÂÆâÂÖ®ÁÆ°ÁêÜÂäûÊ≥ïÔºàÂæÅÊ±ÇÊÑèËßÅÁ®øÔºâ„ÄãÂÖ¨ÂºÄÂæÅÊ±ÇÊÑèËßÅÁöÑÈÄöÁü•,‚Äù Cac.gov.
      cn, May 28, 2019, http://www.cac.gov.cn/2019-05/28/c_1124546022.htm.
      Sina Technology, ‚ÄúThe story behind the issuing of ‚ÄòPersonal Information Security
  166 

      Specification‚Äô compromises of 33 experts made the Standard Possible„Ää‰∏™‰∫∫‰ø°ÊÅØÂÆâ
      ÂÖ®ËßÑËåÉ„ÄãÂá∫Âè∞ËÆ∞Ôºö33‰∏ìÂÆ∂ÂçöÂºàÁÇºÂ∞±Ê†áÂáÜ,‚Äù Sina.com.cn, May 1, 2018, http://tech.
      sina.com.cn/i/2018-05-01/doc-ifzvpatr7140886.shtml.
      Ibid.
  167 

  168
       Samm Sacks, ‚ÄúChina‚Äôs Emerging Data Privacy System and GDPR,‚Äù Csis.org, March
       9, 2018, https://www.csis.org/analysis/chinas-emerging-data-privacy-system
       -and-gdpr.
      D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?




      the drafting of the text.169 PIS details obligations for user consent,
      starting from the minimisation of data collection and secondary use,
      as well as placing third-party operators under security requirements.
      A category of ‚Äúsensitive‚Äù personal information is created, including
      ‚Äúidentity card numbers, biometric information, bank account
      numbers, communication records and contents, property information,
      credit information, location data, accommodation information, health
      and physiological information, transaction data, and the Personal
      Information (PI) of children 14 years of age or under.‚Äù De-identification
      is required. This, the obligation for firms handling large amounts of
      personal data to have appointed data officers, and the provision of
      penalties, led some to conclude that PIS is on a GDPR framework.

      While ‚Äúconsent‚Äù is only one of the six legitimate reasons for lawful
      processing of data in Article 6.1 of the GDPR, Article 41 of the
114   Chinese cybersecurity law has ‚Äúconsent‚Äô as a must, but provides a
      list of exceptions in the PIS. Articles 5.4 and 8.5 list exemptions
      relating to national security and defense, public safety, public health,
      and significant public interests, criminal investigation, prosecution,
      trial, and judgment enforcement, etc.; safeguarding the major lawful
      rights and interests such as life and property of PI subjects or other
      persons, and it is difficult to obtain the consent of the PI subject;
      when necessary to maintain the safe and stable operation of the
      provided products or services, such as to detect and handle product
      or service malfunctions; when necessary for the PI controller, as a
      news agency, to make legal news reports; when necessary for the
      PI controller, as an academic research institute, to conduct statistical

          Hong Yanqing, ‚ÄúAnswers and explanations on five points regarding the Personal
      169 

          Information Security Certification ÂØπ„Ää‰∏™‰∫∫‰ø°ÊÅØÂÆâÂÖ®ËßÑËåÉ„Äã‰∫îÂ§ßÈáçÁÇπÂÖ≥ÂàáÁöÑÂõûÂ∫îÂíå
          Ëß£Èáä,‚Äù WeChat, February 5, 2018, https://mp.weixin.qq.com/s/rSW-Ayu6zNXw87it
          YHcPYA.
V. C H I N A , T H E S U R V E I L L A N C E S T A T E W I T H S O M E P R I V A C Y C O N C E R N S




  or academic research in the public interest, which also has
  de-identified the PI when providing academic research or results
  externally; and finally, ‚Äúwhen other situations specified by laws and
  regulations.‚Äù Yet, it might still seem that Chinese regulations leave
  less leeway than the GDPR, as a list of exceptions cannot compete
  with the overall flexibility given by what is called ‚Äúlegitimate interests‚Äù
  in the GDPR‚Äôs Article 6.1.



  The Big State as an Arbitrator Between Individuals
  and Companies
  Hong Yanqing takes the time to explain why this is not the case,
  and how PIS dispenses with user consent in many instances.
  Justification of ‚Äúlegitimate interests‚Äù for the GDPR, requires companies
  to track each stage of its internal procedure, to be ready in case of                                115
  demand to supply evidence of ‚Äúlegitimate interests.‚Äù This actually
  makes the company more accountable. Hong describes a consent
  requirement that is much looser in the PIS than in the GDPR.170 The
  Standard requires explicit consent in case of sensitive personal
  information, but only authorized consent, a definition which was
  not provided in the Standard, in other cases of personal information.
  Hong further explains that ‚Äúwhile using the term ‚Äòauthorized consent‚Äô,
  I mean that you are encouraged to adopt explicit consent, but if it
  is not feasible in reality, implicit consent can be used.‚Äù171 The loose

      Sina Technology, ‚ÄúThe story behind the issuing of ‚ÄòPersonal Information Security
  170 

      Specification‚Äô compromises of 33 experts made the Standard Possible„Ää‰∏™‰∫∫‰ø°ÊÅØÂÆâ
      ÂÖ®ËßÑËåÉ„ÄãÂá∫Âè∞ËÆ∞Ôºö33‰∏ìÂÆ∂ÂçöÂºàÁÇºÂ∞±Ê†áÂáÜ,‚Äù Sina.com.cn, May 1, 2018, http://tech.
      sina.com.cn/i/2018-05-01/doc-ifzvpatr7140886.shtml.
      Ê¥™Âª∂Èùí, ‚ÄúAnswers and explanations on five points regarding the Personal Information
  171 

      Security Certification ÂØπ„Ää‰∏™‰∫∫‰ø°ÊÅØÂÆâÂÖ®ËßÑËåÉ„Äã‰∫îÂ§ßÈáçÁÇπÂÖ≥ÂàáÁöÑÂõûÂ∫îÂíåËß£Èáä,‚Äù WeChat,
      February 5, 2018, https://mp.weixin.qq.com/s/rSW-Ayu6zNXw87itYHcPYA.
      D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?




      and evolving Chinese regulation is often put into the context of
      required innovation and economic efficiency. A strict regulation on
      personal data, such as the GDPR, is not suitable for China considering
      China‚Äôs personal data protection capability and the data industry‚Äôs
      current development situation.172 Indeed, efficiency and the interest
      of companies were reflected in the PIS as the drafting team of the
      PIS is composed of 33 individuals, which can be broadly divided
      into two camps: companies and experts.

      The debate about this was salient enough that a 2019 revision of
      the cybersecurity law (now undergoing a comments phase) has gone
      into two directions: adding an exemption ‚Äúwhen related to the
      obligations of personal information controllers to perform laws and
      regulation‚Äù of the state; but doing away with the exemption of consent
      ‚Äúwhen necessary to sign and perform a contract according to the PI
116   subject‚Äôs request‚Äù and otherwise strengthening the obligations of
      companies in the process of ensuring data protection. In other words,
      in our triangle cited above, the third point ‚Äì the state‚Äôs ultimate
      interests in wide data collection ‚Äì are, once more, the winner. But
      the protection of personal data also wins at the expense of companies,
      which are saddled with its implementation. The Chinese state can
      protect individuals as consumers against predatory commercial
      interests, although it will not perform the same task against itself.

      One cannot end this descriptive attempt without registering two
      tentative conclusions. First, there is not much to constrain the
      Leviathan itself ‚Äì the surveillance state. Second, an abundance of
      rules, often amended, supplemented or rewritten, coexists with

          Hu Wenhua and Kong Huafeng, ‚ÄúThe Impact of EU General Data Protection Regulation
      172 

          on China and Its Response,‚Äù Computer Applications and Software 35, no. 11
          (November, 2018).
V. C H I N A , T H E S U R V E I L L A N C E S T A T E W I T H S O M E P R I V A C Y C O N C E R N S




  persisting ambiguities at several levels: the distinction between
  mandatory rules and suggested guidance is tenuous. The law can
  therefore be applied sparingly. Or, with the added breadth of ‚Äúother‚Äù
  categories, it can be stretched arbitrarily. The lack of provisions
  regarding the means for implementation suggests that the law is
  mainly used as a deterrent. The obligation for companies handling
  much personal data to employ data officers has even been pared
  down ‚Äì in the 2017 Cybersecurity Law, the threshold for this
  obligation was 500,000 individuals concerned. It went up to one
  million in the February 2019 draft revision. Although both the GDPR
  and India‚Äôs nascent regulation have their limits, none of them come
  close to China‚Äôs conjunction of black holes and regulatory maze.




                                                                                                       117
                                          VI

        IN-FOCUS: HEALTH DATA AND PRIVACY

Digital data processing, followed by big data analytics and now AI,
all have a huge potential to improve health care. Quick diagnosis
and predictive tools, wearable devices, image interpretation and
machine learning, telemedicine and genetic or behavioral factor
analysis hold much promise for this sector. This revolution is as
fundamental as the discovery of vaccines and antibiotics was in the
past. The investment and revamping of existing data, needed to
deliver on those promises, should not be underestimated: a key actor
such as Deepmind, with its health division at the forefront of research
(now merged into Google Health), is incurring large deficits. Health-
related big data represented 153 exabytes in 2013 and is projected
to rise to 2,314 exabytes by 2020.173 Recent applications include
lung tumor detection and prognosis, eye-scan and glaucoma, acute                          119
kidney injury, but also statistical correlations from large data bases,
such as between drinking and the onset of Alzheimer. Electronic
medical records (EMR) save time and therefore money. That‚Äôs if
health professionals have easy tools to enter the required data:
especially in decentralized public systems, ergonomics is often
forgotten. Strikingly, digital innovations revolutionize both advanced
medical research and disease prevention while facilitating care at
the grassroots.

The implementation of big data and AI also brings threats: the
predictive aspects can have fearsome applications for health and

    Research and Markets, ‚ÄúGlobal Big Data in Healthcare Market: Analysis and Forecast,
173 

    2017-2025 (Focus on Components and Services, Applications, Competitive Landscape
    and Country Analysis),‚Äù Researchandmarkets.Com, March 2018, https://www.resear-
    chandmarkets.com/research/wbhh4n/11_45_bn_big?w=5.
      D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?




      life insurance, and more widely for the confidentiality of an individual‚Äôs
      medical condition. Whereas insurance and financial credit have
      rested on the pooling of risk, the availability of medical data and
      predictive tools, beyond the already prevalent health questionnaires,
      could individualize risk profiles to the point where insurance loses
      its very purpose. Health data is only useful if it can be shared among
      concerned health professionals, medical researchers but also out of
      necessity with the public or private insurance entities that underwrite
      medical treatment costs. The risks of hacking and other security
      leaks are large, especially if data storage is decentralized. The lack
      of information available to the public, combined with fears of
      divulgation of data to banks, insurances, employers and even next
      of kin, can result in reluctance from patients to turn over their data.
      On the frontline are general medical information websites which are
      often the first to sell their visitors‚Äô data and to deny easy notice and
120   consent process.174 In what is a classic case of the battle between
      the sword and the shield, the numerous limitations of anonymization
      and pseudonymization techniques have already been mentioned.175
      This also leads to the search for a new solution, in the form of
      simulated patient data, to be discussed in the next section.



      GDPR Places Health Data Under the Public
      Interest Clause
      How do our test cases ‚Äì Europe, India and China ‚Äì approach the
      regulation of health-related data and privacy issues? As we shall see,
          Martin Untersinger, ‚ÄúDonn√©es Personnelles : Les Mauvaises Pratiques Des Sites de
      174 

          Sant√©,‚Äù Le Monde.Fr, September 4, 2019, https://www.lemonde.fr/economie/
          article/2019/09/04/donnees-personnelles-les-mauvaises-pratiques-des-sites-de-
          sante_5506226_3234.html.
      175
          Cf. page 36.
                           V I . I N - F O C U S : H E A LT H D ATA A N D P R I V A C Y




much is still under review. Health is perhaps the key digital sector
for which specific rules are needed, and where different objectives
must be reconciled ‚Äì protection of sensitive personal data, medical
research and improvements in the provision of health care, and
financial requirements in an era of soaring medical costs.

For the EU, the GDPR has a very generic approach, although its
Article 9 recognizes health as a special category of personal data.
It includes ‚Äúdata related to the physical or mental health of a natural
person, including the provision of health care services, which reveal
information about his or her health status‚Äù (Article 4). Member states
can impose further limitations to the processing of ‚Äúgenetic data,
biometric data or data concerning health.‚Äù But health data is a prime
example where processing ‚Äúfor reasons of public interest‚Äù is authorized
without the consent of the data subject (Recital 54), excluding other
purposes for third parties such as employers or insurance and banking                     121
companies. This exemption to consent for reasons of public or
legitimate interest is very wide: it covers health status, including
morbidity and disability and their determinants, health care needs
and resources, the provision of health care, expenditure and financing,
and the causes of mortality. The exemption also covers the right to
erasure.

However, public and private insurance are treated differently:
processing is authorized ‚Äúto ensure the quality and cost-effectiveness
of the procedures used for settling claims for benefits and services
in the health insurance system‚Äù (Recital 52), even without consent
(recital 54). But it ‚Äúshould not result in personal data being processed
for other purposes by third parties such as employers or insurance
and banking companies.‚Äù It is interesting to note that cost efficiency
is recognized as a need for public health systems, but no special
      D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?




      case seems to be made for private health insurance within the larger
      issue of private companies. Interestingly, Chinese researchers have
      looked into the financial impact of the GDPR for hospitals, from its
      adoption in 2016 to the first months of implementation in 2018. The
      study emphasizes the costs of achieving compliance ‚Äì but also notes
      the growing gap between hospitals that are able to provide digital
      health services (considered more efficient) with those that do not have
      the capital or human resources. It concludes that in the longer run,
      only the former will survive in an open environment. Achieving
      compliance is thus a way to reach a higher degree of performance.176

      Rather generic on health data protection, the GDPR leaves a lot of
      room for member states to decide their own rules, supposedly
      because they fall under the exempted category of ‚Äúpublic interest.‚Äù
      However, in legislating these exempted categories, member states
122   are allowed to go beyond the GDPR‚Äôs scope, not under. The
      Commission‚Äôs one year after stakeholder reporting exercise repeatedly
      notes that different national interpretations or rules still pose problems
      in the health sector. For insurance, rules for dispensing with explicit
      consent vary from country to country. Pharma companies note that
      the interpretation of safeguards needed for the processing of research
      data can still vary from country to country. In the health field, ‚Äúthe
      use of the specification clauses in GDPR by member states has
      created considerable hurdles for companies operating
      cross-border.‚Äù177
          Bocong Yuan and Jiannan Li, ‚ÄúThe Policy Effect of the General Data Protection Regulation
      176 

          (GDPR) on the Digital Public Health Sector in the European Union: An Empirical
          Investigation,‚Äù International Journal of Environmental Research and Public Health 16,
          no. 6 (March 25, 2019): 1070, https://doi.org/10.3390/ijerph16061070.
          Multistakeholder Expert Group, ‚ÄúContribution from the Multistakeholder Expert Group
      177 

          to the Stock-Taking Exercise of June 2019 on One Year of GDPF Application,‚Äù June
          13, 2019, p. 22, https://ec.europa.eu/commission/sites/beta-political/files/report_
          from_multistakeholder_expert_group_on_gdpr_application.pdf.
                           V I . I N - F O C U S : H E A LT H D ATA A N D P R I V A C Y




France As a Test Case
It is interesting to consider the case of France, because it combines
some contradictory features. On the one hand, since 1945 and the
creation of a nation-wide insurance system (that is also a regulator
and public buyer of drugs and medical-related equipment), there is
quasi-universal tracking and recording of medical acts and health
related expenses. A single national filing system (SNIIRAM), for
pseudonymized expense claims and reimbursements, was created
in 1999 and revamped into an even wider health data national
system (SNDS) in 2017: this includes several other data banks from
hospitals and relative to causes of deaths. The system is often hailed
as unique in French sources, because of the range over time and its
inclusiveness. Nowadays, it is in fact far from being unique, as the
aggregation of health data resources, their digitalization and
homogenous treatment are spreading across countries. In fact, the                         123
restrictions to use put in place by French law, the data siloes across
various institutions and the constraints to use threaten to place
France, and singularly medical and pharma research, in a
disadvantageous position. Yet, the existing data banks have also
attracted the attention of the French regulator in charge of the GDPR
for involuntary privacy breaches: insufficient pseudonymization and
weak protection of local terminals have been called out.

The early Loi Informatique et Libert√©s (Information and Liberties
Law) (1978) prohibited the processing of health data (without
defining it) with important exceptions : where there is express consent
by the patient ; processing necessary for preventive medicine, medical
diagnosis, provision of healthcare or treatment, or for the management
of healthcare services carried out by a member of a medical
profession; statistical processing carried out by the National Institute
      D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?




      of Statistics and Economic Studies (INSEE) ; processing necessary
      for medical research. The French Data Protection Act (2018) limits
      the processing of biometric, genetic and health data for public interest
      purposes. A December 2018 public ordnance further defines limits
      to the treatment of personal data (including health and other sensitive
      data such as religion or sexual orientation). But it also creates
      exceptions wide enough that one could drive a truck through. Article
      5 of the ordnance lists as exceptions six cases, one of which includes
      ‚Äúprocessing by public authorities in the pursuit of their missions, if
      treatment is necessary to fulfil the legitimate interests of the processing
      entity or of a third party, unless the prevailing fundamental rights
      and freedoms of the individual concerned require personal data
      protection, notably if the individual concerned is a child.‚Äù A People‚Äôs
      China lawmaker couldn‚Äôt have written it more convoluted and
      ambiguous.
124
      The system suffers from known deficiencies, which are partly about
      process, but also about purpose limitations. Because the data was
      collected for reimbursement purposes, its medical content is often
      limited to short categorizations. As in other countries, actual medical
      information is both siloed and often kept in non-digital form, or in
      non-standardized digital form. To make this data inter-operable,
      beyond the individual exchange of medical data regarding single
      individuals, is a huge task. One step towards this is the generalization
      of individual shared medical files (DMP) and the creation of a cloud
      service hosting this highly sensitive data. For the time being, the
      DMP is more about information files than about a single data format,
      which limits its wider use. So far, the national shared medical file
      system has registered 6 million individuals ‚Äì but the Paris hospital
      system alone has 10 million registered patients, a discrepancy that
      shows how difficult it is to combine information into a unique or
                                  V I . I N - F O C U S : H E A LT H D ATA A N D P R I V A C Y




coordinated data bank.178 It seems predictable that single hosting
will, in all likelihood, lead in the future to format standardization. In
case of epidemics outbreak, standardization will allow for faster
intervention. At present, it is striking that Amazon or Google, by
tracking searches or orders of over-the-counter drugs, can chart flu
outbreaks more quickly and accurately than any medical or
epidemiology service.179

Another difficulty relates to the conditions of access to this data.
Terms differ between health professionals and commercial, medical
or insurance companies, a welcomed restriction. But for all, there
is at present a necessity to justify access with a single and clearly
defined purpose: this defeats the purpose of factor identification
through AI ‚Äì a process which is more akin to a fishing expedition
where one does not know where and what the results will be. This
is clearly tied to the cultural reluctance for surrendering vital data.                          125
The ‚Äúprivacy paradox‚Äù works very well to dissipate this reluctance
in the daily life of consumers. In the case of health data, where it
is harder to identify the immediate and short-term return for the
individual of surrendering one‚Äôs private data, it is less effective. Better
information about the use of data could change this bias. Reassurances
and education are clearly called for.

At present, French pharma companies therefore complain with some
justification that they must turn over to other databases. The United
States has huge health data resources, leading to what could be
termed an arms race between emerging local privacy laws and
    ‚ÄúLes h√¥pitaux de Paris ont ouvert pr√®s de 10 millions de dossiers patients‚Äù, Les Echos,
178 

    October 28, 2019.
    Ali Alessa and Miad Faezipour, ‚ÄúA Review of Influenza Detection and Prediction through
179 

    Social Networking Sites,‚Äù Theoretical Biology and Medical Modelling 15, no. 1 (February
    1, 2018), https://doi.org/10.1186/s12976-017-0074-5.
      D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?




      marketing companies. Welltok‚Äôs Predilytics claims to be able to
      ‚Äúreveal impactable risk at an individual level‚Äù180 for 274 million
      registered individuals. Kaiser Permanente combines insurance and
      health care for 12 million people and aggregates data accordingly.181
      LiveRamp, the successor company to the already mentioned
      Acxiom,182 partners with HealthVerity to ‚Äúlink patient health data
      and digital behavior.‚Äù ‚ÄúPatient journey touchpoints can be connected
      from ad campaign impressions and brand website views to doctor
      visits and prescription fills.‚Äù183 Very soon, through a partnership with
      ‚Äúthe largest supermarket chain in the United States‚Äú, this will extend
      to ‚Äúlinking the grocery carts of patients with their healthcare data
      and exploring how diet, smoking, alcohol consumption, Over-The-
      Counter purchases or even food insecurity really impact their
      journeys.‚Äù184 Stimulating health and pharma companies digital
      advertising remains a key goal to the company, as it currently
126   accounts ‚Äúfor only 2,8% of total U.S. digital advertising expenditures‚Äù.

      China also beckons, having joined in 2017 an international body
      that sets quality specifications, and eased access to local databases
      for foreign pharma companies. Health big data and research projects
      are mushrooming. Sanofi, for example, conducts diabetes‚Äô and

          Welltok, ‚ÄúAnalytic Services - Welltok - Optimizing Health, Maximizing Rewards,‚Äù Welltok,
      180 

          2019, https://www.welltok.com/analytic_services/.
          Kaiser Permanente, ‚ÄúKaiser Permanente 2018 Annual Report,‚Äù Kaiserpermanente.
      181 

          org, 2018, https://healthy.kaiserpermanente.org/static/health/annual_reports/kp_
          annualreport_2018/?kp_shortcut_referrer=kp.org/annualreport.
      182
          See Introduction, page 10.
          HealthVerity, ‚ÄúHealthVerity and LiveRamp Develop Privacy-Centric Linkage between
      183 

          Patient Healthcare Data and Digital Behavior,‚Äù Prnewswire.com, October 15, 2019,
          https://www.prnewswire.com/news-releases/healthverity-and-liveramp-develop-privacy-
          centric-linkage-between-patient-healthcare-data-and-digital-behavior-300938656.
          html.
          HealthVerity, ‚ÄúGrocery Data: The Missing Ingredient in The Patient Journey,‚Äù Healthverity.
      184 

          com, October 15, 2019, https://info.healthverity.com/healthverity-8451-webinar.
                                 V I . I N - F O C U S : H E A LT H D ATA A N D P R I V A C Y




immunological diseases‚Äô tests in Chengdu.185 Delocalization of health
data sources is no panacea however. Vital and other health data
differ across populations. For commercial purposes, obtaining market
authorization for a new drug cannot be based on tests performed
elsewhere on a different population. For this medical reason alone,
and for reasons of international competition among pharma firms
and health providers, including analytical tools, France should keep
working at facilitating access and use of its large health data bases,
while guarding itself from the excesses noted above.



India‚Äôs Demanding Legislation Under Preparation
Currently, the legal framework covering digital health data is simply
a reference within the personal data section of the Information
Technology Act (2000), mandating the protection of sensitive data                               127
and preventing unlawful disclosure. But the provision only applies
to ‚Äúbody corporates‚Äù, which do not include public hospitals. The
other available rule is a 2016 Electronic Health Record Standards
(EHRS)186 released by the Ministry of Health and Family Welfare
(MoHFW). It lays down technical, administrative and physical
standards for data collection and storage. The scope of coverage is
unclear, as are the timelines for accessing patient records. Unique
identification information such as URLs and IP addresses are not
listed as sensitive information. The EHRS is more about standardizing
digital records than about data protection. A code of ethics for doctors

    Takada Noriyuki, ‚ÄúChina‚Äôs Big Data Draws Big Pharma,‚Äù Nikkei Asian Review, August
185 

    1, 2019, https://asia.nikkei.com/Business/Pharmaceuticals/China-s-big-data-draws-Big-
    Pharma2.
    The Ministry of Health and Family Welfare, India, ‚ÄúElectronic Health Record (EHR)
186 

    Standards Version 2016 for India‚Äù (2016), https://mohfw.gov.in/sites/default/
    files/17739294021483341357.pdf.
      D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?




      remains vague in its prescriptions. There are no laws in India
      mandating hospitals to disclose security breaches. By contrast, the
      American Health Insurance Portability and Accountability Act
      (1996)187 requires that a hospital discloses a breach that has affected
      more than 500 patients. The GDPR also has strong provisions in
      case of breaches.188 The lack of a proper regulation is also highlighted
      within the controversy over the extended use of Aadhaar‚Äôs unique
      identification number and its vulnerability.

      This situation will very likely undergo major changes. On the one
      hand, the National Health Policy (2017) includes ambitious plans
      for the digitalization and national integration of health data, including
      national health registries, platform and exchange networks, optical
      fiber connections and the general use of tablets and smartphones.
      Apps in this area are burgeoning. On the other hand, India is about
128   to pass a draft Digital Information Security in Healthcare Act (DISHA),
      proposed by the Ministry of Health on March 11, 2018.189 The
      period for stakeholder comment ended on April 21, 2019, and a
      bill is currently being finalized: even if the government is now
      committed, the process with the Lok Sabha (Lower House of the
      Parliament) could still change the outcome, as it does for many
      legislative acts. As of now, DISHA is a radical and all-encompassing




          Office for Civil Rights, ‚ÄúBreach Notification Rule,‚Äù HHS.gov, September 14, 2009,
      187 

          https://www.hhs.gov/hipaa/for-professionals/breach-notification/index.html.
          Akhil Deo, ‚ÄúWithout Data Security and Privacy Laws, Medical Records in India Are
      188 

          Highly Vulnerable,‚Äù The Wire, January 27, 2017, https://thewire.in/law/
          without-data-security-and-privacy-laws-medical-records-in-india-are-highly-vulnerable.
          Government of lndia, Ministry of Health & Family welfare, ‚ÄúGovernment of Lndia
      189 

          Ministry of Health & Family Welfare (EHealth Section)‚Äù (2018), https://www.nhp.gov.
          in/NHPfiles/R_4179_1521627488625_0.pdf.
                                  V I . I N - F O C U S : H E A LT H D ATA A N D P R I V A C Y




proposition190 for the protection of health data privacy, going much
farther than the Modi government‚Äôs other major digital law under
preparation, the Personal Data Protection Bill (PDPB).191 Individual
consent is paramount, with exceptions specified much more narrowly
than under the proposed PDPB in general ‚Äì or the GDPR for that
matter. Denial of service is impossible. The Act is also stronger in
asserting the right to erasure. Government access to health data is
restricted to strict health purposes. ‚ÄúInsurance companies shall not
insist on accessing the digital health data of persons who seek to
purchase health insurance policies or during the processing of any
insurance claim‚Äù: this is qualified only by user consent for access
to the digital data held by the specific clinical establishment to which
the claim relates‚Äù (Article 29.5). Pharma companies have no access
to individual digital health data, even for research. A National
Electronic Health Authority is to be set up, and the Act now includes
provisions for sanctions in case of breaches.                                                    129


The DISHA draft can still collide with the coming conclusions of the
Srikrishna Committee on personal data protection, and there is
professional criticism of its most radical dispositions.192 The regulatory
effort does not stop there. The Union government is currently in its
last stage (with the Upper House) of a DNA Technology (Use and

    Singh Madhur, ‚ÄúIndia to Be First to Protect Health Data of Citizens with Iron-Clad
190 

    Law?,‚Äù Business Standard, May 31, 2018, https://www.business-standard.com/article/
    economy-policy/india-to-be-first-to-protect-health-data-of-citizens-with-iron-clad-
    law-118053100126_1.html.
    Ikigai Law, ‚ÄúDISHA and the Draft Personal Data Protection Bill, 2018: Looking at the
191 

    Future of Governance of Health Data in India,‚Äù Ikigai Law, February 25, 2019, https://
    www.ikigailaw.com/disha-and-the-draft-personal-data-protection-bill-2018-looking-
    at-the-future-of-governance-of-health-data-in-india/#acceptLicense.
    For an example of these criticisms, see: Rahul Matthan, ‚ÄúA New Direction for Data
192 

    Privacy in Healthcare,‚Äù Livemint.Com, April 11, 2018, https://www.livemint.com/
    Opinion/3LK0TR6zdXmeIkaJuTUnnJ/A-new-direction-for-data-privacy-in-healthcare.
    html.
      D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?




      Application) Regulation Bill 2019, regulating and limiting the use
      of DNA profiling to civil paternity suits and to consented processing
      of DNA data for all crimes that are subjected to the Indian Penal
      Code, of which crimes liable to more than 7 years imprisonment do
      not need consent.193

      Overall, India‚Äôs handling of the data protection and privacy issues
      in the health sector seems unique. A very strong push for coordinated
      and integrated digital tools coexists with what promises to be a
      stringent privacy policy ‚Äì surpassing the GDPR‚Äôs requirements in
      several ways. So far, while health care at the grassroots is likely to
      be enhanced by the drive for digitalization, pharma research ‚Äì
      whether it is conducted by foreign or Indian companies ‚Äì would
      seem to be the least of the Indian government‚Äôs priorities. DISHA
      stands in contrast with the overall trend favoring innovation and
130   state requirements over privacy protection, as evidenced by the PDPB
      and with numerous digital policies.



      China‚Äôs Use of Health Data As a Resource
      Health data in China is explicitly viewed as a resource for the
      developmental state. The incitation of health data usage comes as
      part of the ‚ÄúInternet Plus‚Äù strategy proposed by Prime Minister Li
      Keqiang in 2015, which aims at boosting the development and
      economic value of some conventional industries through the use of
      internet. ‚ÄúTo let the people run less, and to let the data run more‚Äù,
      is the widely used phrase to explain the concept of ‚ÄúInternet Plus.‚Äù

          Ministry of Science and Technology and Earth Sciences, ‚ÄúThe DNA Technology (Use
      193 

          and Application) Regulation Bill, 2019,‚Äù July 8, 2019, https://www.prsindia.org/
          billtrack/dna-technology-use-and-application-regulation-bill-2019.
                                   V I . I N - F O C U S : H E A LT H D ATA A N D P R I V A C Y




Not surprisingly, the ‚ÄúGuiding Opinions on Promoting and Regulating
the Application and Development of Big Data in Health and Medical
Care‚Äù, issued by the Chinese State Council in 2016, describe health
and medical big data as a fundamental and strategic resource of the
state. This occurs in the context of fostering new business sectors
and enabling more economic growth.194 It then goes on to emphasize
the need for better use of the government, to provide top design for
‚Äúthe integration, sharing and open application of big data in health
and medical care‚Äù and to ‚Äúprovide powerful support to the building
of a healthy China, comprehensively finishing building a moderately
prosperous society, and the realization of the Chinese Dream of the
great rejuvenation of the Chinese nation.‚Äù

With governmental support and pushes for the integration, sharing
and open application of big data in health and medical care, the
efforts are bearing results. Taking the example of Guangdong province,                            131
data-sharing is achieved within 3112 medical and health institutions,
and the provincial-level national electronic health data bank holds
information on 80 million permanent residents.195

One detailed presentation on China‚Äôs digital data policies notes that
‚Äúalthough privacy is an extremely important topic for big data in
health and medicine, there is no specific law or guidance on this in
China.‚Äù196 Indeed, the Cybersecurity Law of 2017 has no mention


    General Office of the State Council of the People‚Äôs Republic of China, ‚ÄúGuiding Opinions
194 

    on Promoting and Regulating the Application and Development of Big Data in Health
    and Medical Care ÂõΩÂä°Èô¢ÂäûÂÖ¨ÂéÖÂÖ≥‰∫é‰øÉËøõÂíåËßÑËåÉÂÅ•Â∫∑ÂåªÁñóÂ§ßÊï∞ÊçÆÂ∫îÁî®ÂèëÂ±ïÁöÑÊåáÂØºÊÑè
    ËßÅ,‚Äù (2016), http://www.gov.cn/zhengce/content/2016-06/24/content_5085091.htm.
    Health Commission of Guangdong Province, ‚ÄúGuangdong Health Case Letter,‚Äù Gd.Gov.
195 

    Cn, June 19, 2019, http://wsjkw.gd.gov.cn/zwgk_bmwj/content/post_2516919.html.
    Luxia Zhang et al., ‚ÄúBig Data and Medical Research in China,‚Äù BMJ Medical Research,
196 

    February 5, 2018, j5910, https://doi.org/10.1136/bmj.j5910.
      D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?




      of the word ‚Äúhealth‚Äù, and the PIS included ‚Äúhealth information‚Äù in
      the definition of sensitive personal information without going further.
      However, three drafts of the Basic Healthcare and Health Promotion
      Law have been published for public comments since 2017. The
      draft law addresses the issue of health information privacy in the
      following way in Article 90 (in the third draft, for public comments
      until September 26, 2019): ‚ÄúThe State protects the personal privacy
      related to the health of citizens and ensures the safety of personal
      health information. No organization or individual may acquire, use
      or disclose citizen‚Äôs personal health information except as required
      by law, administrative regulations or with the consent of the
      person.‚Äù197

      In April 2018, the General Office of the State Council published the
      ‚ÄúOpinions of the General Office of the State Council on Promoting
132   the Development of ‚ÄòInternet plus Health Care‚Äù,198 again with no
      mention of the word ‚Äúprivacy‚Äù. This was followed by the issuing of
      ‚ÄúMeasures for the Administration of Internet Diagnosis and Treatment‚Äù,
      ‚ÄúMeasures for the Administration of Internet Hospitals‚Äù and
      ‚ÄúSpecifications for the Administration of Remote Medical Services‚Äù
      in September 2018, all three for trial implementation.199 The latter
      three do have a general mention of ‚Äúprivacy protection‚Äù, without
      going much into the actual means for achieving this. Nevertheless,

          National People‚Äôs Congress, ‚ÄùBasic Healthcare and Health Promotion Law (draft) ‰∏≠
      197 

          Âçé‰∫∫Ê∞ëÂÖ±ÂíåÂõΩÂü∫Êú¨ÂåªÁñóÂç´Áîü‰∏éÂÅ•Â∫∑‰øÉËøõÊ≥ï(ËçâÊ°à),‚Äú (2019), https://npcobserver.files.
          wordpress.com/2019/08/basic-healthcare-and-health-promotion-law-3rd-draft.pdf
          General Office of the State Council of the People‚Äôs Republic of China ‚ÄúOpinions of the
      198 

          General Office of the State Council on Promoting the Development of ‚ÄòInternet plus
          Health Care ÂõΩÂä°Èô¢ÂäûÂÖ¨ÂéÖÂÖ≥‰∫é‰øÉËøõ‚Äú‰∫íËÅîÁΩë+ÂåªÁñóÂÅ•Â∫∑‚ÄùÂèëÂ±ïÁöÑÊÑèËßÅ‚Äù (2019)
          The National Health Commission of the People‚Äôs Republic of China, ‚ÄúAbout the issuing
      199 

          of the Measures for the Administration of Internet Diagnosis and Treatment(trial imple-
          mentation), etc. ÂÖ≥‰∫éÂç∞Âèë‰∫íËÅîÁΩëËØäÁñóÁÆ°ÁêÜÂäûÊ≥ïÔºàËØïË°åÔºâÁ≠â3‰∏™Êñá‰ª∂ÁöÑÈÄöÁü•,‚Äù (2018),
          http://www.cac.gov.cn/2018-09/14/c_1123431844.htm
                                 V I . I N - F O C U S : H E A LT H D ATA A N D P R I V A C Y




they all take on the issue of cooperation with third-party institution,
and stress the need of an agreement specifying the responsibilities
of all parties in various areas, including on privacy protection. Hence,
the statement in the above-detailed presentation is technically
inaccurate, but the authors‚Äô comment probably reflects the distance
between law and practice.

So far, the most concrete regulation on health data protection is the
‚ÄúAdministrative Measures on the Standards, Security and Service of
National Health and Medical Big Data (For Trial Implementation)‚Äù200
issued in July 2018 by the National Health Commission. It tackles
the issue of data collection, data storage, service provision, data
utilization and data sharing.201 Specifically in terms of data sharing,
‚Äúthe National Health Commission is responsible for establishing an
open sharing mechanism for healthcare big data, coordinating the
construction of a resource catalogue system and a data-sharing                                  133
exchange system, and strengthening the service and management
of the health care big data life cycle.‚Äù The goal is tilted towards the
construction of shared health data resources but the rules do little
to specify health data protection beyond the generalities already
present in the PIS.




    The National Health Commission of the People‚Äôs Republic of China, ‚ÄúAdministrative
200 

    Measures on the Standards, Security and Service of National Health and Medical Big
    Data (For Trial Implementation) ÂÖ≥‰∫éÂç∞ÂèëÂõΩÂÆ∂ÂÅ•Â∫∑ÂåªÁñóÂ§ßÊï∞ÊçÆÊ†áÂáÜ„ÄÅÂÆâÂÖ®ÂíåÊúçÂä°ÁÆ°
    ÁêÜÂäûÊ≥ïÔºàËØïË°åÔºâÁöÑÈÄöÁü•,‚Äù (2018), http://www.cac.gov.cn/2018-09/15/c_1123432498.
    htm
    The National Health and Family Planning Commission of the People‚Äôs Republic of
201 

    China, ‚ÄúExplaining the Administrative Measures on the Standards, Security and Service
    of National Health and Medical Big Data (For Trial Implementation) ÂõΩÂÆ∂ÂÅ•Â∫∑ÂåªÁñóÂ§ß
    Êï∞ÊçÆÊ†áÂáÜ„ÄÅÂÆâÂÖ®ÂíåÊúçÂä°ÁÆ°ÁêÜÂäûÊ≥ïÔºàËØïË°åÔºâ„ÄãËß£ËØª‚Äù, September 14, 2018, http://
    www.cbdio.com/BigData/2018-09/14/content_5834771.htm
      D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?




      China has gone all the way in the direction of collecting, aggregating
      and using all available health data for health care and the pharma
      industry. India is contemplating a very restrictive law, although final
      developments deserve to be watched. The interdependence of private
      insurance and health care in the United States is troubling, but this
      is also where the developments from AI and big data are the more
      promising: this is a battle ground in the UK, due to the large base
      of its National Health Service. The French case should serve as a
      reminder that usable big data is not so easy to gather from
      decentralized systems. Provided the rules are set clearly on what
      cannot be used or even accessed by insurance companies - including
      with consent by individuals - a key goal for public health should be
      to improve the range, accessibility and quality of medical, genetic
      and behavioral data.

134
                                         VII

           CHASING PRIVACY, INNOVATION
                    AND PUBLIC INTEREST

Regarding processes and methods for personal data protection, the
contrast between the European and American approaches exists,
but it should not be overstated. The GDPR may look like an orderly
French garden, and the U.S. regulations as a maze, but we should
go beyond the appearance of texts. The ‚ÄúQui veut trop embrasser
mal √©treint‚Äù202 adage could still be invoked against the GDPR and
many other EU directives. Sundar Pichai, Google‚Äôs CEO, warns
against a general approach and advocates for a sector-by-sector
regulation on AI, ‚Äúrather than rushing into a way that prevents
innovation and research.‚Äù203
                                                                                           135
Appeals to the Court of Justice of the European Union (CJEU) will
create precedents and to some extent work as case law does in the
United States. Recent examples, such as an October 1st, 2019 ruling
by the CJEU on specific rules for user consent to cookies, indicate
that this is happening.204

On exceptions related to public interest, the distance between the
United States and Europe is also likely to decrease ‚Äì and this might
be unwelcome news. We have already emphasized the similarities
of data scraping between market analysis (or ‚Äúsurveillance capitalism‚Äù
202
    ‚ÄúGrab all, lose all‚Äù.
    Tim Bradshaw, ‚ÄúGoogle Chief Sundar Pichai Warns against Rushing into AI Regulation,‚Äù
203 

    Financial Times, September 20, 2019, https://www.ft.com/content/b16e6ee8-dbb2-
    11e9-8f9b-77216ebe1f17.
    CJEU, ‚ÄúBundesverband der Verbraucherzentralen und Verbraucherverb√§nde‚Äì
204 

    Verbraucherzentrale Bundesverband eV v‚Äù Planet49 GmbH (October 1, 2019).
      D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?




      as some will call it) and state surveillance. But the potential for
      abuse of public interest exceptions, whether through biased
      interpretation of the law or by illegal practices, is always there.
      Edward Snowden‚Äôs chilling description205 of the mass surveillance
      programs created after 9/11 and how they went around constitutional
      protections should be studied, whether one thinks he is a whistleblower
      or a traitor. Polemics around biometric identification are not reserved
      to India‚Äôs Aadhaar program. France has just decreed a one-time
      facial recognition process to verify the identity of passport holders
      and foreigners applying for residency who contact public services.206
      As Europeans catch up with big data analytics and AI programs, the
      issues will continue to grow.

      Looking at what partners and competitors do ‚Äì who may occasionally
      go beyond the GDPR or more often undercut it ‚Äì and ways to improve
136   and revise it, we shall make some policy suggestions, both positive
      and negative. These are highlighted below.



      Ambitious but Generic Rules
      Explicitly, the GDPR is a ‚Äúgeneral regulation‚Äù, a term previously not
      employed for EU regulations. This begs the question of explanatory
      guidelines and sectoral regulations. There has been a few of the
      former, none of the latter. Ambitious but generic rules leave room

      205
            Edward Snowden, Permanent Record (New York: Macmillan, 2019).
      206 
            For a criticism by the French Supervisory Authority, see
            Source: CNIL, ‚ÄúD√©lib√©ration N¬∞ 2018-342 portant avis sur un projet de d√©cret autorisant
            la cr√©ation d‚Äôun traitement automatis√© permettant d‚Äôauthentifier une identit√© num√©rique
            par voie √©lectronique d√©nomm√© ‚ÄúApplication de lecture de l‚Äôidentit√© d‚Äôun citoyen en
            mobilit√©‚Äù (ALICEM) et modifiant Le Code de l‚Äôentr√©e et du S√©jour des √©trangers et du
            droit d‚Äôasile (demande d‚Äôavis N¬∞ 18008244)‚Äù (October 18, 2018).
         V I I . C H A S I N G P R I V A C Y, I N N O V A T I O N A N D P U B L I C I N T E R E S T




for interpretation and loopholes, and create large spaces for
exceptions. We come to the paradox of Article 23, soberly entitled
‚Äúlimitations.‚Äù It provides for state access to citizens‚Äô data with a
suspension of state obligations, as well as citizens‚Äô rights, in certain
cases.

Above all, the stronger and broader the requirements for compliance,
the more non-implementation is likely. We all meet with examples
of glaring non-compliance ‚Äì simply starting with websites where it
is impossible in practice to make any choice about one‚Äôs data privacy.
Digital companies, and representatives from the business world,
which are now controllers or processors, point out to several factors:
the cost of compliance, especially for smaller companies; the issue
of human resources ‚Äì trained data officers, for example. They
regularly stress that the very language of the GDPR emphasizes very
different objectives, which are difficult to reconcile. There is a lack                               137
of operational help to companies, and a lot of guidance is needed
to make them compliant. A large responsibility for interpretation is
vested with the controllers and processors.



The Fault with Opt-Out by Default
The ‚Äúnotice and consent‚Äù approach is perhaps the most popular
aspect of the GDPR because it is said to put the individual back in
control of his personal data. And more control seems more satisfying.
In reality, a maximal approach has its drawbacks, and it seems a
nuclear option.

A negative default option (unless a positive or ‚Äúopt-in‚Äù action is taken
by the user, no data or metadata can be lifted) or an overall
      D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?




      do-not-track option would greatly reduce the personalized use of
      most websites and apps, which are based on data queries and
      exchanges. A GDPR requirement is that access cannot be conditional
      on consent on data extraction. An individual‚Äôs rational choice is
      indeed to get the product while refusing to turn over the information.
      But if many users expect to free-ride the web at the expense of those
      other users who still opt-in, this is likely to kill the economy of the
      internet, which has been based on free availability against personal
      data. It is also just as unfair as the present situation, where very
      unequal compliance from websites and companies gives a competitive
      advantage to sinners over those who implement the letter of the
      GDPR. At the macro level, this bankrupts the system and ends
      with the same result as the full negative default option. Someone
      has to pay, one way or another, or we will be back to the pre-
      internet age. Would the internet be the same if all services and
138   information worked on a paying basis? It is a fundamental change
      that may not be accepted by individuals as consumers, unlike citizens
      claiming privacy rights. A French liberal economics think tank has
      proposed to reverse the proposition: individuals being the owners of
      their personal data could sell it to the digital platforms. Obviously
      the proposal will immediately run into many issues ‚Äì third party or
      unpredicted use for example. But it has the merit of ending the
      hypocrisy over the relationship between internet users and
      providers.207




          Isabelle Landreau et al., ‚ÄúMes Data Sont √† Moi - Pour Une Patrimonialit√© Des Donn√©es
      207 

          Personnelles,‚Äù G√©n√©ration Libre, January 2018, https://www.generationlibre.eu/wp-
          content/uploads/2018/01/2018-01-generationlibre-patrimonialite-des-donnees.pdf.
           V I I . C H A S I N G P R I V A C Y, I N N O V A T I O N A N D P U B L I C I N T E R E S T




The Privacy Paradox
Against the unsustainable consequences of a rational choice by
individuals, we should consider what privacy experts call the ‚Äúprivacy
paradox‚Äù208 but that could equally be termed the ‚Äúhypocrisy of trust‚Äù.
Need, desire and lust ‚Äì not necessarily in that order ‚Äì override the
precautionary principle in the individual psyche. In a digital
environment, we constantly surrender our privacy to other useful or
pleasing purposes. What one could call ‚Äúspeed googling‚Äù, certainly
runs against reading privacy notices. This is only a tiny example in
a world full of trade-offs between convenience and data protection.

The coming disappearance of cash (already a reality in countries as
diverse as China and Sweden) is a huge case in point, leading to
hypocrisy of trust. China was once the world‚Äôs most cash-loving
society, in part because of its anonymity, in part because it is the                                    139
instrument of symbolic exchange among individuals ‚Äì including to
departed ones. Today, even the most minute cash transaction is
likely to be replaced by electronic payments. A cashless society is
also a transparent society, completely reversing the anonymity
that cash brought to transactions. Hence, after the one-hundred-
dollar bill, cryptocurrency, such as bitcoins, became the new
anonymous currency of choice, so long as it is a private medium of
exchange. China then decided to become the world‚Äôs first public
emitter of cryptocurrency of a digital currency, which can no longer
be called a cryptocurrency. This could conceivably help China evade
the dollar‚Äôs grasp on international transactions. But the Bank has
already announced that it will ‚Äútag‚Äù the currency to trace the
individuals who used it. ‚ÄúThe Central bank digital currency can be

    See II. What Is Privacy and How Can it Be Ensured?, Privacy Policies, Notice and
208 

    Consent, page 43.
      D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?




      circulated as easily as cash (‚Ä¶) at the same time, it can achieve
      controllable anonymity‚Äù, according to Mu Changchun, Deputy Chief
      in the Payment and Settlement Division of the People‚Äôs Bank of China
      (PBOC).‚Äù209



      Differences Between Member States
      Differences of interpretation are also reflected by the various
      adaptations into the national law of member states.210 Age of consent
      is most often mentioned as an issue. There are also huge differences
      in enforcement capacities among member states, and probably
      different degrees of willingness to move ahead. A study including
      17 EU member states as well as Croatia (but without countries such
      as France, the Netherlands or the UK) finds highest level of personnel
140   resource (circa. 250) for Polish and Spanish supervisory authorities
      and less than 50 people for each of the other12 countries.211

      Since national security is not a competency of the EU, member
      states can specify for themselves what constitutes (or what
      doesn‚Äôt), further widening the scope of derogations. Quis custodiet
      ipsos custodes?212

          Dexin Guo, ‚Äú‚ÄòDigital Renminbi‚Äô Is Revealed ‚ÄòÊï∞Â≠ó‰∫∫Ê∞ëÂ∏Å‚ÄôÂàùÈú≤ÁúüÂÆπ,‚Äù Xinhua, August
      209 

          21, 2019, http://www.xinhuanet.com/fortune/2019-08/21/c_1124900323.htm.
          For a review of national adaptations after eight months, see this ten member states
      210 

          case study:
          Source: Karen Mc Cullagh, Olivia Tambou, and Sam Bourton, National Adaptations
          of the GDPR (Luxemburg: Collection Open Access Book, Blogdroiteuropeen, February
          2019).
          EDPB, ‚ÄúFirst Overview on the Implementation of the GDPR and the Roles and Means
      211 

          of the National Supervisory Authorities,‚Äù March 8, 2019, https://edpb.europa.eu/sites/
          edpb/files/files/file1/19_2019_edpb_written_report_to_libe_en.pdf.
      212
          ‚ÄúWho will guard the guardians?‚Äù
           V I I . C H A S I N G P R I V A C Y, I N N O V A T I O N A N D P U B L I C I N T E R E S T




Decisions on interpretation of the GDPR can follow different processes:
if the issue involved has cross-border implications and if the individual
or entity raising it has a seat inside an EU member state, that member
state‚Äôs supervisory authority will take the leading role in examining
the case, and its decision will be valid across the EU: that is the
one-stop shop approach. But the process does not apply if the
plaintiff is not based inside the EU, or if the locus of decision-
making does not coincide with the legal establishment. There can
thus be 28 decisions. Even this has its own exception: in ‚Äúurgent‚Äù
cases, any supervisory authority can make a decision that will stand
only for three months. One would have hoped for a simpler and
clearer process.

Three recent cases, all involving Google as it happens, will serve as
illustrations of the complexities involved. On the issue of Google‚Äôs
staff recording private conversations in order to improve voice-                                        141
recognition performance, the Hamburg Commissioner for Data
Protection was able to take measures, even though Google‚Äôs main
establishment is in Ireland.213 A landmark decision by the CJEU
establishes that Union law (the GDPR) does not mandate the
implementation of a de-referencing obligation in search engines
beyond the EU‚Äôs borders.214 But a member state jurisdiction can
indeed impose de-referencing on all versions of a search engine ‚Äúin
light of national standards‚Äù and by examining the balance between
the right to privacy and freedom of information. In other words,
national jurisdictions can go beyond, but not undercut the GDPR in

    The Hamburg Commissioner for Data Protection and Freedom of Information, ‚ÄúPress
213 

    Release. Speech Assistance Systems Put to the Test - Data Protection Authority Opens
    Administrative Proceedings against Google,‚Äù August 1, 2019, https://datenschutz-
    hamburg.de/assets/pdf/2019-08-01_press-release-Google_Assistant.pdf.
    CJEU. Google LLC, successor in law to Google Inc. v Commission nationale de l‚Äôinfor-
214 

    matique et des libert√©s (CNIL) (September 24, 2019).
      D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?




      their own legal practice, provided a ‚Äúbalance‚Äù of different rights has
      been examined. In another case waiting for a CJEU judgment, the
      French CNIL has decided that, if a company‚Äôs decision-making in
      GDPR makes it a ‚Äúcontroller‚Äù ‚Äì, and it is located in a different member
      state than the one where the company has its establishment, the
      case can be decided by that other country‚Äôs supervisory authority.
      Given the difficulty to locate activity and decision-making, identifying
      the actual locus of company decisions will prove contentious, and
      in any case defeats the simplicity of the one-stop shop provision.
      The decision ‚Äúmay ultimately prove detrimental to effective EU-wide
      enforcement (including uniformity in application and legal certainty)
      in the longer term.‚Äù215

      That Google ‚Äì with an EU establishment in Ireland but much larger
      interests in other member states ‚Äì has often been a test case is no
142   accident. There is a trend towards more sovereignty over digital data
      as well as over fiscal resources, and therefore towards the
      determination of the location according to the market. However, this
      trend is contested and indecision will harm the prospect of a unified
      digital market, and any company that manages data across borders
      in general. A radical shift, whether for regulation or for taxation,
      from location of establishment, production to point of sale or place
      of decision, is a large-scale international undertaking.
      Simultaneously mixing the two approaches is like having some
      vehicles drive on the right side of the road while having others drive
      left‚Ä¶




          Lokke Moerel, ‚ÄúCNIL‚Äôs Decision Fining Google Violates One-Stop-Shop,‚Äù SSRN, February
      215 

          19, 2019, https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3337478.
            V I I . C H A S I N G P R I V A C Y, I N N O V A T I O N A N D P U B L I C I N T E R E S T




Technology Is a Fast-Moving Frontier
Technology is a fast-moving frontier where existing rules cannot
anticipate, except in very general terms and objectives. The train cannot
be stopped, unless one decides a radical check on innovation. Would
our competitors consider implementing this? Hardly, if one judges from
the Chinese, Indian and American cases. The issues go beyond the
‚ÄúGeneva Convention‚Äù, which some have called for. For privacy and data
protection, as with cybersecurity, and in fact arms control negotiations
in general, international agreements are welcomed ‚Äì but they require
robust verification. In cybersecurity as in arms control, deterrence has
often emerged as the complementary option or the alternative to
agreements. A similar option does not exist for privacy rights. It is only
by opening or closing our digital market that we can hope to influence
the behavior of actors whose base is outside of it.
                                                                                                         143
We must know that there are unknown unknowns. It is impossible
to predict which type of data will turn out to be personal, sensitive
or critical. The situation for AI is reminiscent of a recent advertising
billboard from a savings company: ‚Äúrobots can‚Äôt take your job if
you‚Äôre already retired‚Äù:216 by implication, everybody else is up for
grabs. Some examples follow, although they pertain to cases that
have already materialized by definition: DNA data banks, such as
23andme or Ancestor.com, are revolutionizing criminal enquiries by
re-identifying anonymous DNA from distant relatives (third- or fourth-
degree cousins), as was the case in 2018 for the notorious Golden
State killer in California. A 2 million people genetic data bank is
    Robots, of course, are an outcome of AI. This 2019 Prudential billboard has justifiably
216 

    received its own publicity on the web. Among numerous websites :
    Source: r/ABoringDystopia, ‚ÄúAutomation Can‚Äôt Take Your Job If You Don‚Äôt Have One.,‚Äù
    Reddit, January 29, 2016, https://www.reddit.com/r/ABoringDystopia/comments/
    bci7pz/automation_cant_take_your_job_if_you_dont_have_one/.
      D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?




      enough to identify 90% of the American population.217 Lenddo, a
      Singapore-based technology company, uses online social and mobile
      behavior (such as whether you keep your smartphone battery filled up)
      to determine the borrower‚Äôs ‚Äúwillingness to pay‚Äù back loans.218 Platforms
      for streaming TV channels now track viewers and monetize them through
      behavioral advertising.219 Social network data analysis, helped by
      integration and fusion techniques, can pinpoint probable characteristics
      of individuals far better than any technique previously available.

      Blockchain technologies create an additional problem for one
      aspect of privacy ‚Äì the right to erasure. In blockchains, all
      participants can view all data recorded; several copies of the
      blockchain coexist on different computers; once data is recorded, it
      cannot be altered or removed; and decisions are made by consensus
      between participants, without a central arbitrator. Precautionary
144   steps such as encryption and pseudonymization are definitely
      required.220 But even with these, the French CNIL and the UK‚Äôs
      Financial Conduct Authority (FCA) warn at present against the use
      of blockchains.221

          Yaniv Erlich et al., ‚ÄúIdentity Inference of Genomic Data Using Long-Range Familial
      217 

          Searches,‚Äù Science 362, no. 6415, October 11, 2018: 690‚Äì94, https://doi.
          org/10.1126/science.aau4832.
          Hope King, ‚ÄúThis Startup Uses Battery Life to Determine Credit Scores,‚Äù CNNMoney,
      218 

          August 24, 2016, https://money.cnn.com/2016/08/24/technology/lenddo-smartphone-
          battery-loan/index.html.
          Hooman Mohajeri Moghaddam et al., ‚ÄúWatching You Watch: The Tracking Ecosystem
      219 

          of Over-the-Top TV Streaming Devices,‚Äù Freedom-to-tinker.com, September 18, 2019,
          https://freedom-to-tinker.com/2019/09/18/watching-you-watch-the-tracking-ecosystem
          -of-over-the-top-tv-streaming-devices/.
          CNIL, ‚ÄúBlockchain and the GDPR: Solutions for a Responsible Use of the Blockchain
      220 

          in the Context of Personal Data,‚Äù Cnil.fr, November 6, 2018, https://www.cnil.fr/en/
          blockchain-and-gdpr-solutions-responsible-use-blockchain-context-personal-data.
          Andrew Solomon, ‚ÄúBlock Chain: Is the GDPR out of Date Already?,‚Äù Lexology.com,
      221 

          August 30, 2017, https://www.lexology.com/library/detail.aspx?g=d4c0481a-c678-4748-
          80cb-4ab917e66207.
           V I I . C H A S I N G P R I V A C Y, I N N O V A T I O N A N D P U B L I C I N T E R E S T




Collection versus Usage
What can more realistically be regulated is the usage of collected
data and its interpretation, provided there is indeed rule of law,
including a legal right of verification and appeal by individuals.
In a sense, this has always existed in legal processes: an individual
cannot be convicted in court on the basis of illegally collected
evidence. The first barrier against the indiscriminate use of algorithms
is the prohibition of individual decisions based solely on automated
processing alone. Article 22 of the GDPR has in fact laid down this
prohibition, while laying down very broad exceptions. Again, member
state law can introduce additional safeguards. A good example in
the French case has been the change of the admission process to
higher education from a fully automated algorithm (Admission Post-
Bac or APB) to one requiring human intervention and some
possibilities for requesting an explanation regarding the decisions                                     145
made (Parcoursup). One should edict a similar limitation when it
comes to autonomous driving (or flying): ultimately the principle
of torts requires that human action and responsibility can be
identified. This is one of the answers to a question posed by a recent
report on AI: ‚ÄúAre there areas where human judgement, fallible
though it is, must not be replaced by a machine?‚Äù222

The second barrier is about requiring checks and balances in the
implementation of adverse decisions based on digital evidence.
Legislation adopted under the spell of a terrorist attack will often
lower this barrier. The Lashkar-e-Taiba attacks in Mumbai (2008)
or the terrorist actions in France (2015) have in both cases led to
new anti-terrorist actions that offer less or no opportunity for a

    C√©dric Villani, ‚ÄúExecutive Summary. For a Meaningful Artificial Intelligence,‚Äù March
222 

    2018, https://www.aiforhumanity.fr/pdfs/MissionVillani_Summary_ENG.pdf.
      D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?




      judiciary check. In the French case, they include ‚Äúbroad powers to
      search computers as well as the ability to block websites that allegedly
      glorified terrorism, all without prior judicial authorization.‚Äù223 A new
      intelligence law adopted in July 2015 is now awaiting a decision
      by the European Human Rights Court on its legality.224 A very hard
      look should be given at all exceptions that circumvent the need
      for a judicial review or base negative action on predictive criteria
      rather than actual evidence. In more innocuous, but economically
      significant cases such as insurance, criteria are needed to limit the
      individualization of customers, balancing the need to dis-incentivize
      risky behavior with the collective nature of insurance.

                                  No privacy in your own car
        Autonomous driving has issues that go beyond privacy ‚Äì security
        is one. No one would treat black boxes on planes as an invasion
146     of the pilots‚Äô privacy, and no one would question the existence of
        drive record disks on commercial trucks. Drivers routinely install
        dash cameras that are supposed to capture the circumstances of
        an accident and help establish responsibilities ‚Äì or ensure lower
        insurance rates. Other innovations do limit privacy: GPS tracking,
        driving apps, toll collection. As it improves, autonomous driving
        will create enormous issues relating to automated decisions and
        responsibilities. These are distinct from the privacy and data
        protection issue ‚Äì although hacking is a serious concern for both
        security and privacy.


          David Sullivan, ‚ÄúThe Consequences of Legislating Cyberlaw After Terrorist Attacks,‚Äù
      223 

          Just Security, April 9, 2019, https://www.justsecurity.org/63560/the-consequences-
          of-legislating-cyberlaw-after-terrorist-attacks/.
          Assembl√©e nationale et S√©nat, ‚ÄúLoi N¬∞ 2015-912 du 24 Juillet 2015 Relative Au
      224 

          Renseignement (1),‚Äù https://www.legifrance.gouv.fr/affichTexte.do?cidTexte=JORFTE
          XT000030931899&categorieLien=id
        V I I . C H A S I N G P R I V A C Y, I N N O V A T I O N A N D P U B L I C I N T E R E S T




But further developments, some of them hard to foresee, will create
new types of privacy issues, and the systems enabling automated
driving can differ widely in that perspective. In simple terms, there
are two opposite methods: one is a robot-like car, enabled with a
multiplicity of sensors that finds its way and avoids obstacles. This
is the American path, one informed by the notion of free individuals.
The dependency on GPS-like devices is no greater than with the
previous generation of cars. Still one might find, that cars become
computers, as is the case for maintenance. They store a record of
all their previous activity, to be likely left in place at the time of
resale (a Tesla stores all driving actions since being first put on
the road).
The other approach is to treat the car as a smartphone on wheels,
a receiving and emitting device with a network system. At a
minimum, this means digital highways, and China is already
                                                                                                     147
pushing in that direction. Better management of traffic flows, from
Waze-like applications for example, also follows that path. This
choice increases on the spot control of course. But it can go further:
already, breath sensors can stop an inebriated driver from starting
his engine. Recognizing a pedestrian‚Äôs disability or his/her
proneness to drunkenness or jaywalking will improve if that
personal information is already in the system. The first generation
of autonomous devices is largely about passive defense. The second
generation could be more pro-active and inquisitive. Again, the
trade-off between privacy and security reappears.
With this leap, a car becomes a major collection of devices in the
IoT. Data is shared among many parties. Manufacturers, insurers,
traffic managers and car sharing providers may be combined
controllers when they jointly determine the means and purposes
of processing certain personal data.
      D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?




      Sovereignty and the Splinternet
      Data sovereignty has a difficult relationship with data privacy. States
      attempting to move towards data sovereignty actually want more
      control over what they call their data. This often coincides with a
      disdain for digital privacy, and a fight against instruments ‚Äì clouds
      out of their reach, encryption and messaging apps, VPNs and the
      like ‚Äì that may shield any individual‚Äôs personal data and
      communications. One should also honestly recognize that the frontier
      is porous with the legitimate requirements of any rule of law society,
      and that this is therefore in a continuum with discussions on public
      order, national security and the ‚Äúgeneral interest.‚Äù Attitudes can also
      differ across different categories of data: we have seen, for example,
      a prospective health data bill in India being far more protective of
      privacy than American laws or the overall European framework.
148   Different attitudes towards data sovereignty and digital privacy lead
      to the fragmentation of the internet ‚Äì creating a ‚Äúsplinternet.‚Äù This
      could either take the form of like-minded groupings or follow national
      divisions based on digital sovereignty.

      The first trend may be achievable for like-minded states that subscribe
      to common values, to the rule of law and above all, to a degree of
      mutual oversight. The European GDPR, and the ensuing adequacy
      agreements with third countries, are as much predicated on the free
      flow of data as they are on data or privacy protection. Japan has
      also launched a ‚ÄúData Free Flow with Trust‚Äù (DFFT) initiative at the
      June 2019 Osaka G20 summit. The initiative aims at shaping global
      data governance, but is even more aimed at warding off data
      sovereignty (and in particular China‚Äôs Great Firewall model‚Ä¶) than
      at regulating data privacy: it has been met with limited success so
      far, India, Indonesia and South Africa refusing for example to sign
         V I I . C H A S I N G P R I V A C Y, I N N O V A T I O N A N D P U B L I C I N T E R E S T




a joint declaration. OECD Guidelines on the protection of personal
data and on trans-border flows (1980, revised in 2013) point to the
necessity of a gradual convergence. The Asia-Pacific Economic
Cooperation (APEC) also issued a Digital Privacy Framework (2005,
revised in 2015) patterned on the OECD guidelines.

The second approach ‚Äì prioritizing digital sovereignty and splintering
the global internet ‚Äì is much more likely in cases where the state
prevails over the law, and where mistrust of other states is the norm.
For example, Russia and China may look very much alike in their
concepts of sovereignty, surveillance, and cyberactivism abroad. But
precisely for these reasons, how could they constitute a joint data
environment with free flow? Just as in the long run for market
economies vs. state-driven policies, societies implementing the rule
of law may have the advantage, because they are better equipped
to exchange big data with some degree of security. However, they                                      149
will reflect on the wisdom of allowing platforms from closed
environments such as China‚Äôs or Russia‚Äôs to poach on their open
data markets.



Towards the Dual Internet World
A dual internet is the next best alternative to the single WorldWideWeb.
Assuming that some fragmentation of the digital world is unavoidable
‚Äì and in fact wished for by the state exponents of a closed internet
and data sphere ‚Äì personal data and privacy protection requirements
favor a two-world solution, where one implements domestic and
cross-border rules, while the other could fragment according to
national and state control boundaries. In the real world, the choices
are not as clear-cut: different states will have different requirements
      D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?




      on some categories of data, and trust cannot be at the expense of
      verification, even with the closest partners. Let us therefore recognize
      that adequacy ‚Äúdecisions‚Äù for free data flows will need to factor
      in a strategic goal of avoiding isolation.

      Global issues beyond the EU‚Äôs borders should be anticipated from
      the bottom when considering the rules and the implementation of
      personal data and privacy protection. There are numerous reasons
      for this. One rests within the sovereignty and digital fragmentation
      debates. A second reason is that the very concepts and experiences
      in this field are both evolving and shared. Even if they do have an
      overall consistent philosophy as a starting point in principle,
      Europeans tend to exaggerate how much they are at the source of
      thinking on these issues. Time and again, we encounter cases and
      debates, legislative examples and above all data protection techniques
150   (and challenges to these techniques) that originate in the United
      States. It is not perceived enough that, in spite of the strong normative
      influence that the GDPR exercises, many solutions and tools for
      data protection originate in the United States.

      It is beyond the reach of the present study to discuss the financial and
      institutional implications of these technological choices for European
      innovation. Europe has in fact some of the talent, often captured by the
      companies that possess a competitive edge. But it is clear that searching
      for optimal solutions requires an open mind and cooperation over the
      Atlantic, and in fact with all broadly like-minded partners confronted
      with the same issues. Above all, the rule of law is the common
      denominator, above and beyond the close data integration that exists
      among these partners. And we should not assume that the rule of law
      is ensured without verification and independent institutions ‚Äì in other
      words, without checks and balances.
           V I I . C H A S I N G P R I V A C Y, I N N O V A T I O N A N D P U B L I C I N T E R E S T




Our study of India and China also indicates that a global fight among
models of digital governance may well be under way, as is the case
with other global issues. The triangle formed by privacy, efficiency
and security offers different solutions. As is the case with trade and
financial flows, Europe cannot be conceived as a closed digital
universe. Beyond transatlantic cooperation and compromises over
these issues, the will to create norms should be balanced with the
need to remain attractive. An analogy can be made between the
choices for Europe‚Äôs ‚Äúadequacy decisions‚Äù (in fact, adequacy
agreements) and trade agreements. The latter comes in different
varieties, from surface to deep trade agreements. The trend of the
last decade had been to build more and more comprehensive treaties
incorporating investment and arbitration, services, intellectual
property and norms:225 until the trend backfired with the melt-down
of the Transatlantic Trade and Investment Partnership and initial
Trans-Pacific Partnership projects. Adequacy decisions will have to                                     151
make similar choices, between demanding and comprehensive
criteria requiring strong and permanent adjustments by partners,
and more limited data-sharing agreements (of which, even if
controversial, the Privacy Shield is the best example).



Ex-Ante or Ex-Post Action
Neither the notice and consent framework, the trust and privacy by
design approach nor the ex-ante regulation can be rejected on the
grounds that they are limited. They do serve a purpose towards
privacy and data protection. But regulation must also rely on ex-post

    Edith Laget et al., ‚ÄúDeep Trade Agreements and Global Value Chains,‚Äù World Bank
225 

    Group, June 2018, http://documents.worldbank.org/curated/en/356541529
    933295649/Deep-trade-agreements-and-global-value-chains.
      D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?




      action and that is largely the road towards penalties or tort
      litigation. The two avenues are not identical, although they can be
      combined: case law and tort litigation are the customary avenues
      in the United States, but regulatory agencies can impose very steep
      fines on offending companies. By contrast, the European tradition
      lies within explicit provisions for penalties rather than through
      litigation procedures.

      In both cases, numbers matter. If co-operative regulation gives way
      to responses after the fact, sanctions must be graded ‚Äì steep penalties
      serve as a deterrent to the largest or more egregious offenders. The
      main issue with this approach is that, while digital technology scales
      ‚Äì and therefore the size and number of offenses can be gigantic,
      given the public affected ‚Äì, regulatory agencies, courts and law
      enforcement authorities do not scale. And since many of the offences
152   are not perceived by the general public as much more than a pinprick,
      if at all, the number of complaints does not reflect the magnitude of
      the problem. Class actions are especially needed in this area. It
      should also be known if data has been illegally collected, to what
      use it may have been put, and what is the connection between harm
      suffered and the use of that data by another party. These considerations
      are important, because penalties can either be proportional to a
      crime on the book, or commensurate with the actual amount of harm
      inflicted. This is more difficult to assess in digital cases.

      In the United States, the Federal Trade Commission (FTC) has a very
      big stick, and is very seldom contested in courts by the targeted
      companies. But it has used this stick sparingly in the past, with a
      graduated approach in privacy cases. It often goes for negotiated
      settlements, or ‚Äúconsent decrees‚Äù with digital technology firms such
           V I I . C H A S I N G P R I V A C Y, I N N O V A T I O N A N D P U B L I C I N T E R E S T




as Facebook, Google, Microsoft, Twitter, Snapchat, and Oracle.226
This has the advantage of putting the company involved under the
equivalent of probation for as much as 20 years. Repeat offenses
can have larger consequences ‚Äì as much as 16,000 USD per
individual offense, multiplied by thousands or millions of cases.
Until recently, penalties for breaches of privacy rules had never
reached the level of fines imposed on anti-competitive practices.
Google, fined first in 2011, and again in 2012 under the same
offense, had to pay 22,5 million USD the second time around. The
amount was considered large at the time. It is now dwarfed by the
5 billion USD fine imposed on July 24, 2019 on Facebook (or 9 %
of Facebook‚Äôs turnover in 2018) for deceptive privacy practices,
which were partly tied to the Cambridge Analytica case. Evidently,
there is a change of scale that brings privacy issues level with that
of competition-linked cases.
                                                                                                        153
In Europe, only one fine (again, CNIL v. Google) surpassed the
million-euro mark in the first year. There was another case in
Denmark since; but quite ironically, it is the departing United
Kingdom that has changed the scale, with two fines of respectively
99 and 283 million EUR, in both cases for breaches affecting a
very large number of individuals.

The alternative to sanction is torts litigation, or what one inspired
expert calls the ‚Äúinternet of torts.‚Äù The model is evidently based on
the U.S. consumer-based approach to privacy, and is being
increasingly applied to data security breaches originating from a data
fiduciary ‚Äì in other words, the guardian or operator of the data. The
reasoning is simple: a data fiduciary (or any company) should take

    William McGeveran, ‚ÄúFriending the Privacy Regulators,‚Äù Arizona Law Review 58,
226 

    no.4 2016: 959‚Äì1026, https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2820683.
      D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?




      the necessary precautionary measures if their cost is not higher
      than the damage from a breach, weighed by the probability of the
      damage.227 The model is the so-called Hand Formula: it was first
      used by a judge for an assessment of damages and liability after the
      sinking of a barge in 1943 where the shipping company was found
      guilty.228 It proceeds fromore an economic recognition ‚Äì companies
      do cost/price analysis. In practice, of course, the formula is harder
      to implement, and even more so in assessing the virtual, moral or
      reputational damage that can flow from a privacy breach.



      Conclusion
      To a large degree, much of our privacy has vanished forever, or as
      long as the digital age lasts. Since AI and sophisticated algorithms
154   are not ‚Äì(yet)‚Äìavailable to your next-door neighbor, it still makes
      sense to engage in regulations covering the collection and processing
      of personal data. The larger and more sophisticated entities ‚Äì whether
      they are platforms, digital companies or well-endowed states ‚Äì are
      very likely to defeat some of these rules at least some of the time,
      either through restrictive interpretations, through swamping the
      capacities of implementation, or simply because existing rules fail
      to cover new categories of collected data and their interpretation.




          Rebecca Crootof, ‚ÄúThe Internet of Torts,‚Äù Duke Law Journal 69, February 26, 2019,
      227 

          https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3342499.
          The U.S. vs Carroll Towing Co. Case is apparently taught in every law class; United
      228 

          States v. Carroll Towing Co., 159 F.2d 169 (2d Cir. 1947), Justia Law (US Court of
          Appeals for the Second Circuit 1949), https://law.justia.com/cases/federal/appellate-
          courts/F2/159/169/1565896/
         V I I . C H A S I N G P R I V A C Y, I N N O V A T I O N A N D P U B L I C I N T E R E S T


The above are only hints of some of the practical and ethical problems
that we will face in the digital and AI age. Our tour d‚Äôhorizon has
focused on existing or planned regulation, on the present or near
future, and it already shows glaring gaps. Almost all of the rules
established so far, from top-down China to cartesian Europe, not to
mention India‚Äôs mix, seem to address the first basic issues of this
age, such as protecting the state or public interest, ensuring user
awareness and consent or regulating the processing of e-data. Again,
innovation and technology run faster.

What follows is therefore a short inventory of recommendations
designed to improve these rules. Charting the path for AI and
thinking of the mixture of encouragement and guardrails that AI will
require in order to remain relatively harmless, is a work of a different
scale. It requires first of all, a contribution from IT scientists who are
aware of the potentialities of AI and its modes of operation. We will
be content with writing some recommendations for the present age                                      155
as they emerge from the field that we have been able to survey.
                              PROPOSITIONS




1. Strengthening the GDPR‚Äôs oversight, enforcement
   and adaptability
The first proposition is a familiar one for any EU regulation ‚Äì a rule
can only be as good as its actual implementation.

With 28 national supervisory authorities possessing widely different
levels of resources, this is not an easy proposition. The conformity
of national decisions with the GDPR is likely to be tested by appeals,
such as the one currently in front of the CJEU regarding a defamation
law.229 Throughout these appeals, the one-stop shop‚Äôs issue will also
be tested ‚Äì because supervisory authorities claim their own roles,
and because there is a suspicion that the weaker regulatory instances                       157
will be laxer, and therefore more often retained as digital companies‚Äô
center of operation.

As it is currently doing in several instances, the CJEU recognizes
that the GDPR does not empower one-stop shop in many cases,
and that it distinguishes formal establishment from actual decision-
making and control. The CJEU decision is by definition legally sound
about the existing GDPR. But from this flows a proposition: a revised
GDPR should prevent restrictions that damage a unitary decision-
process. This process is key to a single digital market in the future.
This also has material implications: guidelines should be set for

    In this case, a leader of Austria‚Äôs Green Party, targeted by defamatory (according to
229 

    Austrian law) posts on Facebook, is asking the CJEU to mandate worldwide erasure
    of this and any ‚Äúsimilar‚Äù posts. The case also involves the issue of automated
    treatment.
      D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?




      budgetary and human resources allocated by each member state
      to their supervisory authorities, taking into account the size of the
      country but also the density of digital actors. Interestingly, Ireland,
      which is the most obvious focus of digital companies beyond what
      its size would warrant, has actually taken the lead both in enforcing
      the GDPR and in ruling on cases, so that it would not be greatly
      challenged by a requirement for additional resources.

      The Commission‚Äôs and multi-stakeholder committee‚Äôs stock-taking
      reports after one year both emphasize the low level of penalties imposed
      so far (as we have seen, it is the UK‚Äôs supervisory authority that has
      changed the scale in recent months). It may be too early to judge, as
      there are still ambiguities and possible misunderstandings in the
      implementation of the GDPR. Guidelines are being issued: there will
      be a dilemma opposing complexity and implementation. It is difficult
158   to update and revamp regulations permanently if one expects these
      rules to be implemented across the board. Just as impairing the one-
      stop shop will weaken the GDPR as a whole, propositions should focus
      in priority on clarity, simplicity and ease of implementation. These
      will not be enough to resolve fundamental challenges from future
      technologies ‚Äì but public support for the GDPR will wane if it is not
      seen as effective in the short run and on issues which any one can see.



      2. 
         M aking privacy policies more readable and
         ergonomic
      Some of the needed improvements are obvious. Neither the GDPR
      nor the ensuing guidelines make much of UX ‚Äì user experience ‚Äì in
      assessing what is a primary goal of the GDPR ‚Äì putting the individual
      back in control of his/her personal data.
                                                                    PROPOSITIONS




a. Standardizing notice and consent pop-up forms. Any current
    user soon finds out some are more ergonomic and usable than
    others. Others are complexified by such intermediary steps as
    reading privacy policies from various sources. In extreme but
    frequent cases there is no possibility to decide ‚Äì instead, this is
    information of opt-in.

b. Single- and multi-point data collection requests should be opt-
    out by default, failure to respond should signify opt-out, for
    example. As it is, Recital 32 of the GDPR requires a clear affir-
    mative action for opt-in, but pre-ticked opt-in boxes are not
    enough. Closing a window or disregarding it should not constitute
    such a sign.

c. Use of semi-literate icons and codes: it is not only a necessity for
    a country with high levels of illiteracy. Just as road driving signs                  159
    are standardized and memorized, setting up a digital driving
    code will ease the issue of time-consuming and difficultly-
    worded privacy policies.

d. AI apps facilitating privacy: some apps make it possible to read and
   analyze privacy policies. Reviewing and eventually endorsing or
   grading these apps, and publicizing the results, would serve the
   public. Examples among these apps: Guard,230 a neural app that
   analyzes the privacy terms of known sites and grades them; Terms of
   service; Didn‚Äôt read (ToS; DR),231 that classifies and rates the fine print
   of privacy policies from Class A to Class E (from the most to the least
   protective). It is crowd-sourced and peer-reviewed.

    ‚ÄúDiscover the Hidden Secrets in Privacy Policies | Guard,‚Äù Guard, https://useguard.
230 

    com.
231
    ‚ÄúTerms of Service; Didn‚Äôt Read,‚Äù Tosdr, https://tosdr.org/classification.html.
      D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?




      Beyond these examples, privacy assistance to users is now a
      research field that should be publicly funded in the European
      Union. An American equivalent is Carnegie Mellon University‚Äôs
      Personalized Privacy Assistant project,232 funded by DARPA, the U.S.
      Air Force, the National Science Foundation, Google and Yahoo. The
      project is also a member of a wider consortium called The Usable
      Privacy Project.233



      3. Ensuring privacy by design in practice
      a. The GDPR has explicitly recognized data protection by design in
          Recitals 78 and 108, as well as in Article 25. Privacy by design
         incorporates data minimization, purpose limitation, retention and
         incorporation of privacy at a development life cycle‚Äôs first stage.
160      No formal guidelines have been issued but the European Data
         Protection Supervisor (EDPS) issued an Opinion in May 2018.234
         It acknowledges some of the research described above, and makes
         recommendations. Among these: ensuring that the European
         Union Agency for Cybersecurity (ENISA) has the resources to
         encourage more research and interaction with private companies;
         by ‚Äúincreasing incentives and substantiating obligations,
         including appropriate liability rules‚Äù; ‚Äúto support an inventory
         and observatory of the ‚Äústate of the art‚Äù of privacy engineering‚Äù;
         and by ‚Äúproviding guidance to controllers‚Äù. These are all excellent

          ‚ÄúPersonalized Privacy Assistant Project,‚Äù Privacyassistant.org (Carnegie Mellon
      232 

          University, 2018), https://www.privacyassistant.org.
          ‚ÄúThe Usable Privacy Policy Project,‚Äù Usableprivacy.org, 2019, https://www.usablepri-
      233 

          vacy.org.
          ‚ÄúPreliminary Opinion on Privacy by Design,‚Äù The European Data Protection Supervisor,
      234 

          May 31 2018, https://edps.europa.eu/sites/edp/files/publication/18-05-31_prelimi-
          nary_opinion_on_privacy_by_design_en_0.pdf.
                                                                 PROPOSITIONS




        suggestions, but they are expressed as hopes rather than
        prescriptions. The reality is that an overwhelming share of the
        research around privacy by design‚Äì whether from the field of
        social sciences or around digital innovation, originates in the
        United States ‚Äì, is often conducted through programs that mix
        federal support, private firms and scientific establishments. A truly
        remarkable research by the European Parliamentary Research
        Service (EPRS) led to a report on algorithmic accountability and
        transparency. This independent report, whose most salient
        recommendation is to encourage and protect whistleblowers inside
        organizations, includes 576 endnotes: they are overwhelmingly
        sources from American literature.235

b. It is therefore necessary to strengthen the research and the links
    between rule-makers, business and the scientific community
    beyond the level recommended by the Opinion of the EDPS.                          161
    Companies and compliance advisors unanimously praise the
    virtues of privacy by design, but they come short on how they
    actually implement it. One must surmise that there is a tension
    inside each company between business goals and privacy. By
    themselves, data protection officers may not be influential enough
    to ensure a fair balance between these goals. The GDPR
    emphasizes internal Data Protection Impact Assessments (DPIAs)
    as a formal process identifying risks, appropriate control and
    mitigation steps. Their range is narrower than privacy by design,
    and in this area, there is indeed an earlier Guideline from the
    Article 29 Working Party that preceded the EDPB.236 Insurance
    ‚ÄúA Governance Framework for Algorithmic Accountability and Transparency - Think
235 

    Tank‚Äù European Parliamentary Research Service, April 2019, http://www.europarl.
    europa.eu/thinktank/en/document.html?reference=EPRS_STU(2019)624262.
    European Commission, ‚ÄúGuidelines on Data Protection Impact Assessment,‚Äù 2017,
236 

    https://ec.europa.eu/newsroom/article29/item-detail.cfm?item_id=611236.
      D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?




              companies are perhaps the most familiar with the process, since
              they handle huge amounts of data that have strong implications
              for privacy. The need for clear policies, guidelines and work
              instructions related to data protection applies to public guidance
              as well: whether within the coming e-Privacy directive or through
              new GDPR guidelines, this is the most pragmatic way to provide
              companies ‚Äì and especially their data compliance officers ‚Äì with
              a roadmap that goes beyond abstract requirements.



      4. Providing an effective right to explanation under
          the GDPR
      a. Recital 71 provides the public a right to explanation for decisions
         reached by automated process. Algorithms are black boxes to
162      almost all users. For explanation, the largest project so far is
         Explainable Artificial Intelligence (XAI), an initiative launched in
         2016 by the Defense Advanced Research Projects Agency
         (DARPA),237 the very agency that led to ARPANET, the precursor
         of the internet. As one might already expect, the majority of stated
         goals and programs are not about users‚Äô privacy. The project is
         explicitly aimed at improving the efficiency of deep learning:
         ‚Äúmachine-learning systems will have the ability to explain their
         rationale, characterize their strengths and weaknesses, and convey
         an understanding of how they will behave in the future‚Äù. In that
         sense, public explainability to individual users and accountability
         are only a by-product. Several projects, such as Texas A&M
         University‚Äôs on detecting fake news, UC Berkeley‚Äôs on autonomous
         vehicles or on acquiring a ‚Äúreasonably accurate mental model of

          ‚ÄúExplainable Artificial Intelligence,‚Äù Defense Advanced Research Projects Agency,
      237 

          https://www.darpa.mil/program/explainable-artificial-intelligence.
                                                                   PROPOSITIONS




    robots‚Äô policies‚Äù, and the overall accent on helping users understand
    the basis for the algorithms they use, would clearly have
    implications for explanations of decisions to individuals.

        Much more mundane examples for explainability can be found
        with CreditKarma, which provides understanding of credit scores.
        Other tools have been created such as Google‚Äôs Match score on
        Maps238 or Netflix Percent Match.

b. Product liability ‚Äì warranties, the responsibility to do no harm
    ‚Äì were a consequence of the industrial revolution. Inasmuch as
    digital data is about commerce, and if we admit that the train has
    left the station where personal data could not be traded, it is a
    good model for the digital age as well. Rather than deny the
    commerciality of much personal data, we should adapt our
    enforcement process to this reality.                                                 163


c. The above-mentioned EPRS report on algorithm accountability
    has interesting policy suggestions that apply primarily to the public
    sector and to exceptions based on public interest. The first set
    applies to raising awareness through education, watchdogs and
    whistleblowers. The report recommends exceptions allowing
    reverse engineering of algorithms in cases of public interest.
    Important cases, from aspects of the Snowden affair to Propublica‚Äôs
    exposition of machine bias239 are cited ‚Äì one might add that it is
    reverse engineering that has allowed for a critical discovery of the

238 
     Mariella Moon, ‚ÄúGoogle Maps Can Predict How Much You‚Äôll like a Restaurant,‚Äù
     Engadget, July 31, 2018, https://www.engadget.com/2018/07/31/google-maps-match-
     feature/.
    Julia Angwin, et al., ‚ÄúMachine Bias,‚Äù ProPublica, May 23, 2016, https://www.propu-
239 

    blica.org/article/machine-bias-risk-assessments-in-criminal-sentencing.
      D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?




              software used by public security against Xinjiang‚Äôs population.240
              But reverse engineering is often prohibited by law or precluded
              by intellectual property rights, and whistleblower exemptions
              should therefore be granted in the public interest. To this, we
              should also add a recommendation that open-source code be
              used as much as possible for the software programs incorporating
              algorithms. Transparency has also become a source of
              cybersecurity, as opposed to millions of lines of unverifiable coding.
              The current trend towards open-source coding combines more
              cybersecurity with more accountability in the public sphere. If
              there is to be verification of the innocuity of programs, of the
              absence of malware or snippets inserted to siphon data, open-
              source algorithms and coding are necessary ‚Äì if not sufficient by
              themselves. In France, the Villani report on AI has proposed the
              creation of a group of certified public experts. These could
164           conduct audits of algorithms and databases, and carry out testing;
              they could be called in during legal proceedings.

      Inversely, for the transmission of private or sensitive personal data,
      end-to-end encryption is a frequent recommendation, even if it is
      not completely tamperproof: it is still vulnerable at both ends, and
      quantum computing is said to herald the coming day when safe
      encryption codes come to an end. The counter-argument is of course
      the need to find out about criminal activity and to prevent it from
      ‚Äúgoing dark‚Äù. These debates should not be eluded ‚Äì but neither
      should they be taken up only when emergencies occur. Privacy
      as a goal then recedes behind security.

          ‚ÄúChina‚Äôs Algorithms of Repression | Reverse Engineering a Xinjiang Police Mass
      240 

          Surveillance App,‚Äù Human Rights Watch, May 1, 2019, https://www.hrw.org/
          report/2019/05/01/chinas-algorithms-repression/reverse-engineering-xinjiang-police-
          mass-surveillance
                                                                  PROPOSITIONS




In sum, transparency is required for public algorithms that enable
decisions regarding individuals, and open-source software makes
it more difficult to hide snooping codes and other malware, provided
of course there is regular supervision. This applies to programs
collecting, interpreting and using data. On the contrary, opacity is
required for the transmission and storage of private data, whether
it belongs to individuals or to companies. In that case, end-to-end
encryption is a desirable approach. Exceptions due to considerations
for the public order should be made only with caution and with the
necessary presence of independent oversight.



5. Creating avenues for torts and litigation
The European Union used to assess anti-dumping penalties according
to the ‚Äúlesser duty rule‚Äù ‚Äì the penalties should be only enough to                     165
compensate the actual injury caused by dumped goods, rather than
be based on the overall amount of dumping.241 The huge fines
imposed in other areas by U.S. regulatory agencies on non-American
companies, and the gigantic cash resources of Silicon Valley‚Äôs IT
giants, have inspired a rethink at the European Commission on the
size of fines based on violations of competition law. Similarly, the
egregiousness of dumping practices by Chinese companies on the
EU market have also sparked a debate on the methods for assessing
anti-dumping penalties. European legal and regulatory practices
is in a welcome transition from one approach to the other. This is
in fact part of a broader trend, but it also applies to the area of

    Fran√ßois Godement, ‚ÄúChina‚Äôs Market Economy Status and the European Interest,‚Äù
241 

    European Council on Foreign Relations, June 23, 2016, p. 7, https://www.ecfr.eu/
    page/-/ECFR_180_-_CHINA_MARKET_ECONOMY_STATUS_AND_THE_EUROPEAN_
    INTEREST_%28002%29.pdf.
      D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?




      privacy violations. In the case of Facebook‚Äôs 2019 fine, the FTC
      proudly said this fine was 20 times larger than the next privacy fine
      worldwide, and that a court would have unlikely to rule in favor of
      such a change of scale. The size of the fine was meant as a signal
      to all companies.242

      The other ex-post action is litigation. The earlier mentioned Hand
      Formula, which is based on the assessment of proportionality and
      takes into account the firm‚Äôs scale, is a good start. The EPRS report
      has good proposals both on accountability of algorithms and on
      litigation. The report urges member states to make efforts for more
      public accountability on the algorithms they use for decisions.
      Examples exist, such as the publication of the tax formula in France.
      Public reviews and periodic impact assessments, given newly
      appearing techniques, are called for. The consolidation of personal
166   data from private sources and companies under public contract, and
      the subsequent number-crunching and algorithmic treatment done
      by these public agencies on privately collected data, should be a
      particular subject of attention.

      The same requirement of complete transparency does not hold true
      for the private sector and companies, in part because it is hard
      to achieve, and also because it may run against a business‚Äô
      interest: an algorithm is a vital function of a company. Here, the
      report moves in an American direction on the key issue of torts and
      litigation: ‚Äúit would be preferable to establish a legal liability
      framework that allows service providers to accept greater tort

          Lesley Fair, ‚ÄúFTC‚Äôs $5 Billion Facebook Settlement: Record-Breaking and History-
      242 

          Making,‚Äù Federal Trade Commission, July 24, 2019, https://www.ftc.gov/news-events/
          blogs/business-blog/2019/07/f tcs-5-billion-facebook-settlement-record-
          breaking-history.
                                                                      PROPOSITIONS




liability in exchange for reduced transparency and Algorithmic
Impact Assessment requirements.‚Äù243 Finally, the report extends to
global initiatives on what it calls the ‚ÄúFourth Industrial Revolution
(‚Ä¶): AI arms race‚Äù. It proposes ‚Äúa strong position in trade negotiations
to protect regulatory ability to investigate algorithmic systems and
hold parties accountable for violations of European laws and human
rights‚Äù and builds on the proposition to set up an International
Artificial Intelligence Organization on the model of the existing
International Telecommunications Organization (ITU). Their
inspiration for this: four researchers, of whom three live and work
in America and one in the United Kingdom.

We strongly endorse the EPRS proposal to establish stronger
ex-post tort while limiting new demands to private companies on
algorithmic accountability.
                                                                                            167


6. Introducing sector-specific regulations
There are clearly sectors that require specific or more fine-tuned
regulation. The health sector, financial services, police data are such
cases. It is in part achieved with the EU‚Äôs so-called Police Directive,
in fact relating to police and justice, which was adopted in 2016,
prior to the GDPR.244 As a directive, it requires transposition into
member state laws, which are open to interpretations. Nonetheless,
cases are currently being proceeded by the CJEU regarding the

    ‚ÄúA Governance Framework for Algorithmic Accountability and Transparency - Think
243 

    Tank,‚Äù European Parliamentary Research Service, April 2019, p. 73, http://www.
    europarl.europa.eu/thinktank/en/document.html?reference=EPRS_STU(2019)
    624262.
    ‚ÄúDirective (EU) 2016/680 of the European Parliament and of the Council of 27 April
244 

    2016,‚Äù EUR-Lex, https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:32016L0680.
      D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?




      legality of large-scale collection and storage of surveillance data
      by police and intelligence agencies. They have been introduced by
      the UK (following a challenge by Privacy International), and by
      French and Belgian higher administrative courts (following suits from
      NGOs) as checks on the legality of their national actions. Extensive
      hearings are underway.245

      In other sectors, there are the examples of DISHA, the extensive
      health data regulation bill now under discussion in India, and HIPAA,
      the 1996 American Health Insurance Accountability and Portability
      Act. For data protection in the financial sector, the U.S. took the lead
      as early as 1999 under the wider Gramm-Leach-Bliley Financial
      Modernization Act. Encouraging sectoral rules in some cases for
      the EU does not mean endorsing the extraordinary maze of state-
      by-state regulations that it usually ensues.
168


      7. 
         C reating simulated health data for better
         anonymization
      Our in-focus theme, health data protection, reflects a universal
      concern across systems. In the search for compromises or better
      solutions, simulated health data is emerging as a technological
      way to deal with the failures of anonymization and pseudonymization.
      One example has been developed by Simulacrum, backed by major
      pharma firms, to deal with cancer patient data turned over by Public
      Health England (PHE). This is especially significant as the UK‚Äôs

          Bill Goodwin, ‚ÄúEuropean Court to Decide on Legality of Bulk Phone and Internet
      245 

          Surveillance,‚Äù Bill Goodwin, September 13, 2019, https://www.computerweekly.com/
          news/252470666/European-court-to-decide-on-legality-of-bulk-communications-
          surveillance.
                                                    PROPOSITIONS




National Health Service has been previously criticized for turning
over data to Deepmind, and in effect to Google. With the new
technique, data is first anonymized and grouped in batches of
minimum 50 samples by PHE. Turned over to Simulacrum, the data
is then synthetized in ways that never replicate an actual patient.
This additional layer reconciles the need for big data research
with a guarantee for data privacy, and it deserves to be studied
and expanded. Unavoidably, the result is only as good as the
algorithm used, and some data details and associations are likely
to be lost in the process.




                                                                      169
                ACKNOWLEDGEMENTS

This exploration of the global digital privacy debate owes much to
the generous time granted to the author by Institut Montaigne for
research and writing. It would not have been possible without the
help of Meeta Tarani, intern and program officer, and Viviana Zhu,
policy officer, who assisted in the research and provided stimulating
remarks during writing. At one stage or another, Gilles Babinet, Eric
Chaney, Th√©odore Christakis, Mathieu Duch√¢tel, Marie-Anne
Frison-Roche, Th√©ophile Lenoir, Ang√®le Mal√¢tre-Lansac, Laure
Millet, Victor Poirier, and Stefan Soesanto have provided useful
insights and comments.

People Interviewed
‚Ä¢ Andrea Carrera Mariscal, Data Protection Legal Counsel, Orange
   S.A.
‚Ä¢ Mathieu Coulaud, Head of Legal, Microsoft France
                                                                        171
‚Ä¢ Olivier Esper, Government Affairs & Public Policy Senior Manager,
   Google France
‚Ä¢ Christophe Fessart, Data Protection Officer, Enedis
‚Ä¢ Clotilde Jolivet, Head of Government Affairs France, Sanofi
‚Ä¢ Franck Perraudin, Head of External Affairs Asia, Sanofi
‚Ä¢ Jean-Renaud Roy, Director of Corporate Affairs, Microsoft France
‚Ä¢ Ralf Sauer, Deputy Head of International Data Flows and Protection
   Unit, DG Justice and Consumers, European Commission
‚Ä¢ Fabien Venries, Head of Privacy & Marketing Stream, Orange
   Group

Finally, able proofreading was done by Paula Mart√≠nez L√≥pez and
Pierre Pinhas, and this study has been presented to you thanks to
Institut Montaigne‚Äôs Communication Team.
      D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?




             The opinions expressed in this study are not necessarily
             those of the above-mentioned persons or the institutions
172
                               that they represent.
          OUR PREVIOUS PUBLICATIONS

‚Ä¢ M√©dicaments innovants : pr√©venir pour mieux gu√©rir (September 2019)
‚Ä¢ R√©novation √©nerg√©tique : chantier accessible √† tous (July 2019)
‚Ä¢ Agir pour la parit√©: performance √† la cl√© (July 2019)
‚Ä¢ Pour r√©ussir la transition √©nerg√©tique (June 2019)
‚Ä¢ Europe-Afrique : partenaires particuliers (June 2019)
‚Ä¢ Media polarization ¬´ √† la fran√ßaise ¬ª? Comparing the French and
   American ecosystems (May 2019)
‚Ä¢ L‚ÄôEurope et la 5G : le cas Huawei (Part 2, May 2019)
‚Ä¢ L‚ÄôEurope et la 5G : passons la cinqui√®me ! (Part 1, May 2019)
‚Ä¢ Syst√®me de sant√© : soyez consult√©s ! (April 2019)
‚Ä¢ Travailleurs des plateformes : libert√© oui, protection aussi (April 2019)
‚Ä¢ Action publique : pourquoi faire compliqu√© quand on peut faire simple
   (March 2019)
‚Ä¢ La France en morceaux : barom√®tre des Territoires 2019
   (February 2019)
‚Ä¢ √ânergie solaire en Afrique : un avenir rayonnant ? (February 2019)
‚Ä¢ IA et emploi en sant√© : quoi de neuf docteur ? (January 2019)
‚Ä¢ Cybermenace : avis de temp√™te (November 2018)
‚Ä¢ Partenariat franco-britannique de d√©fense et de s√©curit√© : am√©liorer notre
   coop√©ration (November 2018)
‚Ä¢ Sauver le droit d‚Äôasile (October 2018)                                        173
‚Ä¢ Industrie du futur, pr√™ts, partez ! (September 2018)
‚Ä¢ La fabrique de l‚Äôislamisme (September 2018)
‚Ä¢ Protection sociale : une mise √† jour vitale (March 2018)
‚Ä¢ Innovation en sant√© : soignons nos talents (March 2018)
‚Ä¢ Travail en prison : pr√©parer (vraiment) l‚Äôapr√®s (February 2018)
‚Ä¢ ETI : taille interm√©diaire, gros potentiel (January 2018)
‚Ä¢ R√©forme de la formation professionnelle : allons jusqu‚Äôau bout !
   (January 2018)
‚Ä¢ Espace : l‚ÄôEurope contre-attaque ? (December 2017)
‚Ä¢ Justice : faites entrer le num√©rique (November 2017)
‚Ä¢ Apprentissage : les trois cl√©s d‚Äôune v√©ritable transformation
   (October 2017)
‚Ä¢ Pr√™ts pour l‚ÄôAfrique d‚Äôaujourd‚Äôhui ? (September 2017)
‚Ä¢ Nouveau monde arabe, nouvelle ¬´ politique arabe ¬ª pour la France
   (August 2017)
‚Ä¢ Enseignement sup√©rieur et num√©rique : connectez-vous ! (June 2017)
‚Ä¢ Syrie : en finir avec une guerre sans fin (June 2017)
‚Ä¢ √ânergie : priorit√© au climat ! (June 2017)
‚Ä¢ Quelle place pour la voiture demain ? (May 2017)
‚Ä¢ S√©curit√© nationale : quels moyens pour quelles priorit√©s ? (April 2017)
      D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?



      ‚Ä¢ Tourisme en France : cliquez ici pour rafra√Æchir (March 2017)
      ‚Ä¢ L‚ÄôEurope dont nous avons besoin (March 2017)
      ‚Ä¢ Derni√®re chance pour le paritarisme de gestion (March 2017)
      ‚Ä¢ L‚Äôimpossible √âtat actionnaire ? (January 2017)
      ‚Ä¢ Un capital emploi formation pour tous (January 2017)
      ‚Ä¢ √âconomie circulaire, r√©concilier croissance et environnement
         (November 2016)
      ‚Ä¢ Trait√© transatlantique : pourquoi pers√©v√©rer (October 2016)
      ‚Ä¢ Un islam fran√ßais est possible (September 2016)
      ‚Ä¢ Refonder la s√©curit√© nationale (September 2016)
      ‚Ä¢ Bremain ou Brexit : Europe, pr√©pare ton avenir ! (June 2016)
      ‚Ä¢ R√©animer le syst√®me de sant√© - Propositions pour 2017 (June 2016)
      ‚Ä¢ Nucl√©aire : l‚Äôheure des choix (June 2016)
      ‚Ä¢ Un autre droit du travail est possible (May 2016)
      ‚Ä¢ Les primaires pour les Nuls (April 2016)
      ‚Ä¢ Le num√©rique pour r√©ussir d√®s l‚Äô√©cole primaire (March 2016)
      ‚Ä¢ Retraites : pour une r√©forme durable (February 2016)
      ‚Ä¢ D√©centralisation : sortons de la confusion / Repenser l‚Äôaction publique
         dans les territoires (January 2016)
      ‚Ä¢ Terreur dans l‚ÄôHexagone (December 2015)
      ‚Ä¢ Climat et entreprises : de la mobilisation √† l‚Äôaction / Sept propositions
174      pour pr√©parer l‚Äôapr√®s-COP21 (November 2015)
      ‚Ä¢ Discriminations religieuses √† l‚Äôembauche : une r√©alit√© (October 2015)
      ‚Ä¢ Pour en finir avec le ch√¥mage (September 2015)
      ‚Ä¢ Sauver le dialogue social (September 2015)
      ‚Ä¢ Politique du logement : faire sauter les verrous (July 2015)
      ‚Ä¢ Faire du bien vieillir un projet de soci√©t√© (June 2015)
      ‚Ä¢ D√©pense publique : le temps de l‚Äôaction (May 2015)
      ‚Ä¢ Apprentissage : un vaccin contre le ch√¥mage des jeunes (May 2015)
      ‚Ä¢ Big Data et objets connect√©s. Faire de la France un champion de la
         r√©volution num√©rique (April 2015)
      ‚Ä¢ Universit√© : pour une nouvelle ambition (April 2015)
      ‚Ä¢ Rallumer la t√©l√©vision : 10 propositions pour faire rayonner l‚Äôaudiovisuel
         fran√ßais (February 2015)
      ‚Ä¢ March√© du travail : la grande fracture (February 2015)
      ‚Ä¢ Concilier efficacit√© √©conomique et d√©mocratie : l‚Äôexemple mutualiste
         (December 2014)
      ‚Ä¢ R√©sidences Seniors : une alternative √† d√©velopper (December 2014)
      ‚Ä¢ Business schools : rester des champions dans la comp√©tition
         internationale (November 2014)
                                         O U R P R E V I O U S P U B L I C AT I O N S



‚Ä¢ Pr√©vention des maladies psychiatriques : pour en finir avec le retard
   fran√ßais (October 2014)
‚Ä¢ Temps de travail : mettre fin aux blocages (October 2014)
‚Ä¢ R√©forme de la formation professionnelle : entre avanc√©es, occasions
   manqu√©es et pari financier (September 2014)
‚Ä¢ Dix ans de politiques de diversit√© : quel bilan ? (September 2014)
‚Ä¢ Et la confiance, bordel ? (August 2014)
‚Ä¢ Gaz de schiste : comment avancer (July 2014)
‚Ä¢ Pour une v√©ritable politique publique du renseignement (July 2014)
‚Ä¢ Rester le leader mondial du tourisme, un enjeu vital pour la France
   (June 2014)
‚Ä¢ 1 151 milliards d‚Äôeuros de d√©penses publiques : quels r√©sultats ?
   (February 2014)
‚Ä¢ Comment renforcer l‚ÄôEurope politique (January 2014)
‚Ä¢ Am√©liorer l‚Äô√©quit√© et l‚Äôefficacit√© de l‚Äôassurance-ch√¥mage
   (December 2013)
‚Ä¢ Sant√© : faire le pari de l‚Äôinnovation (December 2013)
‚Ä¢ Afrique-France : mettre en ≈ìuvre le co-d√©veloppement.
   Contribution au XXVIe sommet Afrique-France (December 2013)
‚Ä¢ Ch√¥mage : inverser la courbe (October 2013)
‚Ä¢ Mettre la fiscalit√© au service de la croissance (September 2013)
‚Ä¢ Vive le long terme ! Les entreprises familiales au service de la                     175
   croissance et de l‚Äôemploi (September 2013)
‚Ä¢ Habitat : pour une transition √©nerg√©tique ambitieuse (September 2013)
‚Ä¢ Commerce ext√©rieur : refuser le d√©clin. Propositions pour renforcer notre
   pr√©sence dans les √©changes internationaux (July 2013)
‚Ä¢ Pour des logements sobres en consommation d‚Äô√©nergie (July 2013)
‚Ä¢ 10 propositions pour refonder le patronat (June 2013)
‚Ä¢ Acc√®s aux soins : en finir avec la fracture territoriale (May 2013)
‚Ä¢ Nouvelle r√©glementation europ√©enne des agences de notation :
   quels b√©n√©fices attendre ? (April 2013)
‚Ä¢ Remettre la formation professionnelle au service de l‚Äôemploi et de la
   comp√©titivit√© (March 2013)
‚Ä¢ Faire vivre la promesse la√Øque (March 2013)
‚Ä¢ Pour un ¬´ New Deal ¬ª num√©rique (February 2013)
‚Ä¢ Int√©r√™t g√©n√©ral : que peut l‚Äôentreprise ? (January 2013)
‚Ä¢ Redonner sens et efficacit√© √† la d√©pense publique. 15 propositions pour
   60 milliards d‚Äô√©conomies (December 2012)
‚Ä¢ Les juges et l‚Äô√©conomie : une d√©fiance fran√ßaise ? (December 2012)
‚Ä¢ Restaurer la comp√©titivit√© de l‚Äô√©conomie fran√ßaise (November 2012)
      D I G I T A L P R I V A C Y: H O W C A N W E W I N T H E B A T T L E ?



      ‚Ä¢ Faire de la transition √©nerg√©tique un levier de comp√©titivit√©
         (November 2012)
      ‚Ä¢ R√©former la mise en examen Un imp√©ratif pour renforcer l‚Äô√âtat de droit
         (November 2012)
      ‚Ä¢ Transport de voyageurs : comment r√©former un mod√®le √† bout de souffle ?
         (November 2012)
      ‚Ä¢ Comment concilier r√©gulation financi√®re et croissance : 20 propositions
         (November 2012)
      ‚Ä¢ Taxe professionnelle et finances locales : premier pas vers une r√©forme
         globale ? (September 2012)
      ‚Ä¢ Remettre la notation financi√®re √† sa juste place (July 2012)
      ‚Ä¢ R√©former par temps de crise (May 2012)
      ‚Ä¢ Insatisfaction au travail : sortir de l‚Äôexception fran√ßaise (April 2012)
      ‚Ä¢ Vademecum 2007 ‚Äì 2012 : Objectif Croissance (March 2012)
      ‚Ä¢ Financement des entreprises : propositions pour la pr√©sidentielle
         (March 2012)
      ‚Ä¢ Une fiscalit√© au service de la ¬´ social comp√©titivit√© ¬ª (March 2012)
      ‚Ä¢ La France au miroir de l‚ÄôItalie (February 2012)
      ‚Ä¢ Pour des r√©seaux √©lectriques intelligents (February 2012)
      ‚Ä¢ Un CDI pour tous (November 2011)
      ‚Ä¢ Repenser la politique familiale (October 2011)
176   ‚Ä¢ Formation professionnelle : pour en finir avec les r√©formes inabouties
         (October 2011)
      ‚Ä¢ Banlieue de la R√©publique (September 2011)
      ‚Ä¢ De la naissance √† la croissance : comment d√©velopper nos PME
         (June 2011)
      ‚Ä¢ Reconstruire le dialogue social (June 2011)
      ‚Ä¢ Adapter la formation des ing√©nieurs √† la mondialisation
         (February 2011)
      ‚Ä¢ ¬´ Vous avez le droit de garder le silence‚Ä¶ ¬ª. Comment r√©former la garde
         √† vue (December 2010)
      ‚Ä¢ Gone for Good? Partis pour de bon ? Les expatri√©s de l‚Äôenseignement
         sup√©rieur fran√ßais aux √âtats-Unis (November 2010)
      ‚Ä¢ 15 propositions pour l‚Äôemploi des jeunes et des seniors
         (September 2010)
      ‚Ä¢ Afrique - France. R√©inventer le co-d√©veloppement (June 2010)
      ‚Ä¢ Vaincre l‚Äô√©chec √† l‚Äô√©cole primaire (April 2010)
      ‚Ä¢ Pour un Eurobond. Une strat√©gie coordonn√©e pour sortir de la crise
         (February 2010)
      ‚Ä¢ R√©forme des retraites : vers un big-bang ? (May 2009)
      ‚Ä¢ Mesurer la qualit√© des soins (February 2009)
                                        O U R P R E V I O U S P U B L I C AT I O N S



‚Ä¢ Ouvrir la politique √† la diversit√© (January 2009)
‚Ä¢ Engager le citoyen dans la vie associative (November 2008)
‚Ä¢ Comment rendre la prison (enfin) utile (September 2008)
‚Ä¢ Infrastructures de transport : lesquelles b√¢tir, comment les choisir ?
   (July 2008)
‚Ä¢ HLM, parc priv√©. Deux pistes pour que tous aient un toit (June 2008)
‚Ä¢ Comment communiquer la r√©forme (May 2008)
‚Ä¢ Apr√®s le Japon, la France‚Ä¶ Faire du vieillissement un moteur de
   croissance (December 2007)
‚Ä¢ Au nom de l‚ÄôIslam‚Ä¶ Quel dialogue avec les minorit√©s musulmanes en
   Europe ? (September 2007)
‚Ä¢ L‚Äôexemple inattendu des Vets. Comment ressusciter un syst√®me public
   de sant√© (June 2007)
‚Ä¢ Vademecum 2007-2012. Moderniser la France (May 2007)
‚Ä¢ Apr√®s Erasmus, Amicus. Pour un service civique universel europ√©en
   (April 2007)
‚Ä¢ Quelle politique de l‚Äô√©nergie pour l‚ÄôUnion europ√©enne ? (March 2007)
‚Ä¢ Sortir de l‚Äôimmobilit√© sociale √† la fran√ßaise (November 2006)
‚Ä¢ Avoir des leaders dans la comp√©tition universitaire mondiale
   (October 2006)
‚Ä¢ Comment sauver la presse quotidienne d‚Äôinformation (August 2006)
‚Ä¢ Pourquoi nos PME ne grandissent pas (July 2006)                                      177
‚Ä¢ Mondialisation : r√©concilier la France avec la comp√©titivit√© (June 2006)
‚Ä¢ TVA, CSG, IR, cotisations‚Ä¶ Comment financer la protection sociale
   (May 2006)
‚Ä¢ Pauvret√©, exclusion : ce que peut faire l‚Äôentreprise (February 2006)
‚Ä¢ Ouvrir les grandes √©coles √† la diversit√© (January 2006)
‚Ä¢ Immobilier de l‚Äô√âtat : quoi vendre, pourquoi, comment
   (December 2005)
‚Ä¢ 15 pistes (parmi d‚Äôautres‚Ä¶) pour moderniser la sph√®re publique
   (November 2005)
‚Ä¢ Ambition pour l‚Äôagriculture, libert√©s pour les agriculteurs (July 2005)
‚Ä¢ H√¥pital : le mod√®le invisible (June 2005)
‚Ä¢ Un Contr√¥leur g√©n√©ral pour les Finances publiques (February 2005)
‚Ä¢ Les oubli√©s de l‚Äô√©galit√© des chances (January 2004 - Re-edition
   September 2005)



                For previous publications, see our website:
                       www.institutmontaigne.org
ABB FRANCE                      CORREZE & ZAMBEZE
ABBVIE                          CR√âDIT AGRICOLE
ACCURACY                        CR√âDIT FONCIER DE FRANCE
ACTIVEO                         D‚ÄôANGELIN & CO. LTD
ADIT                            DASSAULT SYSTEMES
AIR FRANCE ‚Äì KLM                DE PARDIEU BROCAS MAFFEI
AIR LIQUIDE                     DENTSU AEGIS NETWORK
AIRBUS GROUP                    DRIVE INNOVATION INSIGHTS - DII
ALLEN & OVERY                   EDF
ALLIANZ                         EDHEC BUSINESS SCHOOL
ALVAREZ & MARSAL FRANCE         EDWARDS LIFESCIENCES FRANCE
AMAZON WEB SERVICES             ELSAN
ARCHERY STRATEGY CONSULTING     ELSEVIER SCIENCES
ARCHIMED                        ENEDIS
ARDIAN                          ENGIE
ASTORG                          EQUANCY
ASTRAZENECA                     ETHIQUE & DEVELOPPEMENT
A.T. KEARNEY                    EURAZEO
AUGUST DEBOUZY                  EUROGROUP CONSULTING
AVRIL                           EUROSTAR
AXA                             FIVES
BAKER & MCKENZIE                FONCI√àRE INEA
BANK OF AMERICA MERRILL LYNCH   GALILEO GLOBAL EDUCATION FRANCE
BEARINGPOINT                    GETLINK
BESS√â                           GIDE LOYRETTE NOUEL
BNP PARIBAS                     GOOGLE
BOLLOR√â                         GRAS SAVOYE
BOUGARTCHEV MOYNE ASSOCI√âS      GROUPAMA
BOUYGUES                        GROUPE EDMOND DE ROTHSCHILD
BRUNSWICK                       GROUPE M6
CAISSE DES D√âP√îTS               GROUPE ORANGE
CAPGEMINI                       HAMEUR ET CIE
CAPITAL GROUP                   HENNER
CAREIT                          HSBC FRANCE
CARREFOUR                       IBM FRANCE
CASINO                          IFPASS
CHA√éNE THERMALE DU SOLEIL       ING BANK FRANCE
CHUBB                           INSEEC
CIS                             INTERNATIONAL SOS
CISCO SYSTEMS FRANCE            INTERPARFUMS
CMA CGM                         IONIS EDUCATION GROUP
CNP ASSURANCES                  ISRP
COHEN AMIR-ASLANI               JEANTET & ASSOCI√âS
COMPAGNIE PLASTIC OMNIUM        KANTAR
CONSEIL SUP√âRIEUR DU NOTARIAT   KATALYSE


        Support Institut Montaigne
KPMG S.A.                              REXEL
LA BANQUE POSTALE                      RICOL LASTEYRIE CORPORATE FINANCE
LA PARISIENNE ASSURANCES               RIVOLIER
LAZARD FR√àRES                          ROCHE
LINEDATA SERVICES                      ROLAND BERGER
LIR                                    ROTHSCHILD MARTIN MAUREL
LIVANOVA                               SAFRAN
L‚ÄôOR√âAL                                SANOFI
LOXAM                                  SCHNEIDER ELECTRIC
LVMH - MO√ãT-HENNESSY - LOUIS VUITTON
                                       SERVIER
M.CHARRAIRE
                                       SGS
MACSF
                                       SIA PARTNERS
MALAKOFF M√âD√âRIC
MAREMMA                                SIACI SAINT HONOR√â
MAZARS                                 SIEMENS
MCKINSEY & COMPANY FRANCE              SIER CONSTRUCTEUR
M√âDIA-PARTICIPATIONS                   SNCF
MEDIOBANCA                             SNCF R√âSEAU
MERCER                                 SODEXO
MERIDIAM                               SOFINORD-ARMONIA
MICHELIN                               SOLVAY
MICROSOFT FRANCE                       SPRINKLR
MITSUBISHI FRANCE                      STAN
NATIXIS                                SUEZ
NEHS                                   SYSTEMIS
NESTL√â                                 TALAN
NEXITY                                 TECNET PARTICIPATIONS SARL
OBEA                                   TEREGA
ODDO BHF                               THE BOSTON CONSULTING GROUP
ONDRA PARTNERS                         TILDER
ONET
                                       TOTAL
OPTIGESTION
                                       TRANSDEV
ORANO
                                       UBER
ORTEC GROUP
                                       UBS FRANCE
PAI PARTNERS
PRICEWATERHOUSECOOPERS                 UIPATH
PRUDENTIA CAPITAL                      VEOLIA
RADIALL                                VINCI
RAISE                                  VIVENDI
RAMSAY G√âN√âRALE DE SANT√â               VOYAGEURS DU MONDE
RANDSTAD                               WAVESTONE
RATP                                   WENDEL
RELX GROUP                             WILLIS TOWERS WATSON
RENAULT                                WORDAPPEAL


        Support Institut Montaigne
         Imprim√© en France
    D√©p√¥t l√©gal : novembre 2019
         ISSN : 1771-6756
Achev√© d‚Äôimprimer en novembre 2019
                         BOARD OF DIRECTORS

                        CHAIRMAN
                        Henri de Castries

                        VICE-PRESIDENT
                        David Az√©ma Associ√©, Perella Weinberg Partners
                        Jean-Dominique Senard Pr√©sident, Renault
                        Emmanuelle Barbara Senior Partner, August Debouzy
                        Marguerite B√©rard-Andrieu Directeur du p√¥le banque de d√©tail en France,
                        BNP Paribas
                        Jean-Pierre Clamadieu Pr√©sident du Comit√© ex√©cutif, Solvay
                        Olivier Duhamel Pr√©sident, FNSP (Sciences Po)
                        Marwan Lahoud Associ√©, Tikehau Capital
                        Fleur Pellerin Fondatrice et CEO, Korelya Capital, ancienne ministre
                        Natalie Rastoin Directrice g√©n√©rale, Ogilvy France
                        Ren√© Ricol Associ√© fondateur, Ricol Lasteyrie Corporate Finance
                        Arnaud Vaissi√© Co-fondateur et Pr√©sident-directeur g√©n√©ral, International SOS
                        Florence Verzelen Directrice g√©n√©rale adjointe, Dassault Syst√®mes
                        Philippe Wahl Pr√©sident-directeur g√©n√©ral, Groupe La Poste

                        PRESIDENT D‚ÄôHONNEUR

                        Claude B√©b√©ar, Fondateur et Pr√©sident d‚Äôhonneur, AXA
Cr√©dit photo : Istock
THERE IS NO DESIRE MORE NATUR AL THAN THE DESIRE FOR K NOWLED GE




    Digital Privacy:
    How Can We Win the Battle?
    ‚ÄúGentlemen don‚Äôt read other people‚Äôs mail.‚Äù Actually, they sometimes do,
    legally or surreptitiously. How can privacy be restored to our life?
    The protection of privacy has become a pervasive concern. Legally, privacy is
    expressed in terms of personal data protection, and it is a key focus of data
    protection regulations, along with data security.
    Today, the privacy debate has two matrixes. One is clearly the United States,
    the mother of all privacy debates and an accepted reference point of our
    study. The other is Europe with its path-breaking GDPR, our primary focus.
    But two other regions are also among the world‚Äôs largest digital centers:
    India and China. Their choices regarding digital privacy will influence the
    competition with our systems and determine how much we can have a
    unified global data flow. The health sector is our theme-in-focus, where we
    argue Europeans must be ready for major changes revolutionizing health
    care, at the expense of their traditional preference for privacy.
    This study finally ends with seven specific propositions for improving or
    revising the GDPR.

                  Follow us on:                     Sign up for our weekly
                                                        newsletter on:
                                                   www.institutmontaigne.org

    Institut Montaigne
    59, rue La Bo√©tie - 75008 Paris                               November 2019

    T√©l. +33 (0)1 53 89 05 60 - www.institutmontaigne.org
                   Ministry of Government
                  and Consumer Services




Freedom of Information and
    Protection of Privacy
          Manual
                       Information, Privacy and Archives Division
                     Ministry of Government and Consumer Services

                          ¬© Queen‚Äôs Printer for Ontario, 2018




Freedom of Information and Protection of Privacy Manual             Page i
                                Acknowledgement
  The Information, Privacy and Archives Division wishes to thank all those who provided
feedback on draft versions of this manual. In particular, we wish to thank Coordinators and
staff at the Ministry of Community and Social Services, Ministry of Economic Development
and Growth; Ministry of Government and Consumer Services, Ministry of Municipal Affairs,
                   Ministry of Transportation, Cabinet Office, and Metrolinx.




 Freedom of Information and Protection of Privacy Manual                             Page ii
    Table of Contents

About this Manual .................................................................................................................. 1
  Purpose ................................................................................................................................ 1
  Audience .............................................................................................................................. 1
  Use of the Manual ................................................................................................................ 1
  Whom to Contact ................................................................................................................. 2
  Related Resources ............................................................................................................... 2
Glossary ................................................................................................................................. 3
Part I: The Legislation, Roles and Responsibilities ............................................................ 5
Chapter 1: The Legislation .................................................................................................... 6
  Introduction .......................................................................................................................... 6
  History .................................................................................................................................. 6
  Policy Goals ......................................................................................................................... 6
  Ontario Access and Privacy Laws ........................................................................................ 7
  Purpose of Legislation .......................................................................................................... 8
  Access and Privacy Principles ............................................................................................. 8
  Organization of Legislation ................................................................................................... 9
  Interactions with Other Laws ................................................................................................ 9
  Prevailing Legislation ......................................................................................................... 10
  Copyright Act ...................................................................................................................... 10
  Litigation ............................................................................................................................. 11
  Resources .......................................................................................................................... 11
Chapter 2: Government Roles and Responsibilities......................................................... 12
  Introduction ........................................................................................................................ 12
  Responsible Minister .......................................................................................................... 12
  Policy and Legal Support ................................................................................................... 13
  Information and Privacy Commissioner of Ontario ............................................................. 14
  Coverage under the Legislation ......................................................................................... 14
  Listing of Institutions and Regulation Updates ................................................................... 16
  Exceptions to Coverage ..................................................................................................... 17
  Head of an Institution ......................................................................................................... 18
  Delegation of Authority ....................................................................................................... 19
  Offences and Liability ......................................................................................................... 20
  Recordkeeping ................................................................................................................... 21
  Resources .......................................................................................................................... 21
Chapter 3: Coordinator Roles and Responsibilities ......................................................... 22
  Introduction ........................................................................................................................ 22
  Overview of Roles and Responsibilities ............................................................................. 22
  Administration .................................................................................................................... 23

   Freedom of Information and Protection of Privacy Manual                                                                        Page iii
   Policies and Procedures..................................................................................................... 23
   Processing Requests ......................................................................................................... 23
   Case File Management and Reporting ............................................................................... 24
   Research ............................................................................................................................ 24
   Mediation and Appeals ....................................................................................................... 25
   Issues Management ........................................................................................................... 25
   Publications ........................................................................................................................ 26
   Resources .......................................................................................................................... 26
Part II: Freedom of Information .......................................................................................... 27
Chapter 4: Access Fundamentals ...................................................................................... 28
  Introduction ........................................................................................................................ 28
  Applying the Legislation ..................................................................................................... 28
  Understanding Records...................................................................................................... 28
  Creating Records ............................................................................................................... 29
  Custody or Control ............................................................................................................. 30
  Records of Third Parties..................................................................................................... 31
  Notice to Affected Persons ................................................................................................. 32
  Applying Relevant Sections................................................................................................ 33
  Exclusions .......................................................................................................................... 33
  Mandatory Exemptions....................................................................................................... 36
  Discretionary Exemptions................................................................................................... 36
  Exceptions.......................................................................................................................... 38
  Exercise of Discretion......................................................................................................... 38
  Harms Test......................................................................................................................... 39
  Public Interest Override ...................................................................................................... 39
  Available for Public Review ................................................................................................ 40
  Institution Records.............................................................................................................. 41
  Directory of Institutions ....................................................................................................... 41
  Directory of Records........................................................................................................... 42
  Routine Disclosure and Open Government ........................................................................ 43
  Obligations to Disclose ....................................................................................................... 43
  Resources .......................................................................................................................... 44
Chapter 5: Exemptions and Exclusions............................................................................. 45
  Introduction ........................................................................................................................ 45
  Exemptions ........................................................................................................................ 45
  Draft By-Laws and Closed Municipal Meetings .................................................................. 46
  Cabinet Records ................................................................................................................ 47
  Advice to Government/Advice or Recommendations ......................................................... 50
  Law Enforcement ............................................................................................................... 52
  Civil Remedies Act, 2001 ................................................................................................... 58

   Freedom of Information and Protection of Privacy Manual                                                                     Page iv
 Prohibiting Profiting from Recounting Crimes Act, 2002..................................................... 59
 Relations with Other Governments .................................................................................... 59
 Relations with Aboriginal Communities .............................................................................. 60
 Defence .............................................................................................................................. 61
 Third Party Information ....................................................................................................... 62
 Economic and Other Interests ............................................................................................ 68
 Universities, Colleges and Hospitals Closed Meetings....................................................... 76
 Solicitor-Client Privilege ..................................................................................................... 77
 Danger to Safety or Health ................................................................................................. 80
 Personal Privacy ................................................................................................................ 82
 Species at Risk .................................................................................................................. 85
 Information Soon to be Published ...................................................................................... 86
 Exclusions .......................................................................................................................... 86
 Private Donations to Archives ............................................................................................ 87
 Proceedings Before a Court ............................................................................................... 87
 Performance Evaluations of Judges ................................................................................... 87
 Ontario Judicial Council Records ....................................................................................... 88
 Investigations of Associate Judge ..................................................................................... 88
 Prosecution Records .......................................................................................................... 88
 Ecclesiastical Records ....................................................................................................... 89
 Hospital Foundation ........................................................................................................... 89
 Administrative Records of Health Professionals ................................................................. 89
 Charitable Donations .......................................................................................................... 89
 Labour Relations and Employment-Related ....................................................................... 90
 Adoptions Related .............................................................................................................. 92
 Research and Teaching Material ....................................................................................... 92
 Peer Evaluations of Research and Teaching Materials ...................................................... 93
 Medical Assistance in Dying ............................................................................................... 93
 Services Relating to Abortion ............................................................................................. 94
Chapter 6: Managing the Request Process ....................................................................... 95
 Introduction ........................................................................................................................ 95
 Providing Assistance to Requesters ................................................................................... 95
 Requesters ......................................................................................................................... 96
 Maintaining Confidentiality of a Requester ......................................................................... 97
 Request Requirements....................................................................................................... 97
 Types of Requests ............................................................................................................. 98
 Time Limits ....................................................................................................................... 102
 Administrative Actions to Support Processing Requests .................................................. 105
 Abandoned or Withdrawn Requests ................................................................................. 116
 Request Processing Step by Step .................................................................................... 117

   Freedom of Information and Protection of Privacy Manual                                                                     Page v
   Step 1: Receiving a Request ............................................................................................ 117
   Step 2: Assessing a Request ........................................................................................... 118
   Step 3: Searching and Locating Records ......................................................................... 118
   Step 4: Reviewing and Analyzing Records....................................................................... 121
   Step 5: Finalizing Recommendations and a Decision ...................................................... 123
   Step 6: Preparing and Sending Records .......................................................................... 124
   Step 7: Closing the File .................................................................................................... 126
   Research Agreements...................................................................................................... 126
   Case File and Knowledge Management........................................................................... 128
   Statistical Reporting ......................................................................................................... 128
   Resources ........................................................................................................................ 129
Part III: Protection of Privacy ............................................................................................ 131
Chapter 7: Privacy Fundamentals .................................................................................... 132
  Introduction ...................................................................................................................... 132
  Understanding Privacy ..................................................................................................... 132
  Personal Information ........................................................................................................ 133
  Business Identity Information ........................................................................................... 134
  Customer Service Information .......................................................................................... 134
  Common Examples of Personal Information .................................................................... 135
  Privacy Rules ................................................................................................................... 135
  Authority to Collect ........................................................................................................... 136
  Manner of Collection ........................................................................................................ 137
  Use and Disclosure of Personal Information .................................................................... 140
  Accuracy .......................................................................................................................... 144
  Retention .......................................................................................................................... 144
  Security ............................................................................................................................ 145
  Disposal ........................................................................................................................... 145
  Public Records of Personal Information ........................................................................... 146
  Resources ........................................................................................................................ 148
Chapter 8: Personal Information and Correction Requests ........................................... 149
  Introduction ...................................................................................................................... 149
  Applying the Legislation for Personal Information Requests ............................................ 149
  Exemptions to the Right of Access to One‚Äôs Own Personal Information .......................... 150
  Personal Information of Other Individuals ........................................................................ 151
  Other Exemptions............................................................................................................. 151
  Correcting One‚Äôs Own Personal Information .................................................................... 152
  Comprehensible Form ...................................................................................................... 152
  Resources ........................................................................................................................ 153
Chapter 9: Privacy Management ...................................................................................... 154
  Introduction ...................................................................................................................... 154

   Freedom of Information and Protection of Privacy Manual                                                                     Page vi
   Define Roles and Responsibilities .................................................................................... 154
   Privacy Policy ................................................................................................................... 155
   Align Business Practices .................................................................................................. 155
   Monitor and Evaluate Privacy Program ............................................................................ 158
   Privacy Breaches ............................................................................................................. 159
   Privacy Breach Response Plan ........................................................................................ 160
   Resources ........................................................................................................................ 163
Part IV: The Office of the Information and Privacy Commissioner of Ontario ............. 165
Chapter 10: Interacting with the IPC ................................................................................ 166
  Introduction ...................................................................................................................... 166
  Guiding Principles ............................................................................................................ 166
  Obligations and Best Practices for Staff ........................................................................... 167
  Seeking Comments or Advice on New Initiatives ............................................................. 167
  IPC Initiated Contact with Institutions ............................................................................... 168
  Resources ........................................................................................................................ 168
Chapter 11: Appeals Process ........................................................................................... 169
  Introduction ...................................................................................................................... 169
  Reasons for an Appeal ..................................................................................................... 169
  IPC Powers ...................................................................................................................... 170
  Stages of Appeal .............................................................................................................. 171
  Timelines .......................................................................................................................... 171
  Initiating an Appeal ........................................................................................................... 173
  Intake Stage ..................................................................................................................... 174
  Mediation Stage ............................................................................................................... 175
  Adjudication...................................................................................................................... 175
  Providing Records to the IPC ........................................................................................... 177
  On Hold and Abandoned Appeals .................................................................................... 178
  Reconsideration of Orders ............................................................................................... 178
  Constitutional Issues ........................................................................................................ 179
  Judicial Review ................................................................................................................ 179
  Resources ........................................................................................................................ 179
Chapter 12: Privacy Complaints, Breaches and Investigations .................................... 181
  Introduction ...................................................................................................................... 181
  Privacy Complaints........................................................................................................... 181
  Institution Reported Privacy Breaches ............................................................................. 182
  Privacy Investigation Process .......................................................................................... 182
  Resources ........................................................................................................................ 183
Part V: Appendices ............................................................................................................ 184
Appendix 1: Sample Draft By-Law Designating Head Under MFIPPA........................... 185

   Freedom of Information and Protection of Privacy Manual                                                                   Page vii
Appendix 2: Sample Resolution Designating Head Under MFIPPA .............................. 186
Appendix 3: Sample Delegation of Authority .................................................................. 187
  3.1 ‚Äì Detailed Delegation of Authority .............................................................................. 187
  3.2 ‚Äì Simplified Delegation of Authority for Small Institutions .......................................... 196
Appendix 4: Template Letters for Request Processing.................................................. 197
  4.1 ‚Äì Letter to Requester Acknowledging Request - Standard......................................... 197
  4.2 ‚Äì Letter to Requester Acknowledging Request ‚Äì Application Fee Missing................. 198
  4.3 ‚Äì Letter to Requester Acknowledging Request ‚Äì Clarification Required .................... 199
  4.4 ‚Äì Letter to Requester Acknowledging Request ‚Äì Proof of Identity Required .............. 200
  4.5 ‚Äì Letter to Requester when Transferring of Forwarding a Request ............................ 201
  4.6 ‚Äì Letter to Receiving Institution when Transferring or Forwarding a Request ............ 202
  4.7 ‚Äì Letter to Requester ‚Äì Notice of Time Extension ...................................................... 203
  4.8 ‚Äì Letter to Requester ‚Äì Fee Estimate and Interim Decision - $25 to $99 Fee ............ 204
  4.9 ‚Äì Letter to Requester ‚Äì Fee Estimate and Interim Decision ‚Äì Over $100 Fee ........... 206
  4.10 ‚Äì Notice to Affected Person for Third Party Information ........................................... 208
  4.11 ‚Äì Notice to Affected Person for Personal Privacy ..................................................... 210
  4.12 ‚Äì Letter to Requester ‚Äì Notice of Delay Where a Third Party‚Äôs Interests are
  Impacted .......................................................................................................................... 211
  4.13 ‚Äì Letter to Affected Person ‚Äì Notice to Disclose Information ................................... 212
  4.14 ‚Äì Letter to Affected Person ‚Äì Notice to Withhold Information ................................... 213
  4.15 ‚Äì Letter to Requester ‚Äì Decision to Disclose All Records ........................................ 214
  4.16 ‚Äì Letter to Requester ‚Äì Decision to Deny Access in Full or in Part .......................... 216
  4.17 ‚Äì Letter to Requester ‚Äì Decision to Refuse to Confirm or Deny Existence of Record218
  4.18 ‚Äì Letter to Requester ‚Äì Decision of No Responsive Records Exist .......................... 219
  4.19 ‚Äì Letter to Requester ‚Äì Decision Approving Correction of Personal Information
  Request ............................................................................................................................ 220
  4.20 ‚Äì Letter to Requester ‚Äì Decision Denying Correction of Personal Information
  Request ............................................................................................................................ 221
  4.21 ‚Äì Letter to Requester Advising Request will be Considered Abandoned ................. 223
Appendix 5: Sample Record Search Form ...................................................................... 224
Appendix 6: Sample Fee Estimate Form.......................................................................... 226
Appendix 7: Sample Index of Records............................................................................. 228
Appendix 8: Request for Waiver of Notice to Individual of Collection of Personal
Information ......................................................................................................................... 229




   Freedom of Information and Protection of Privacy Manual                                                                  Page viii
About this Manual

Purpose
  The Freedom of Information and Protection of Privacy Manual is a general guide to the
  Freedom of Information and Protection of Privacy Act (FIPPA) and the Municipal
  Freedom of Information and Protection of Privacy Act (MFIPPA) and administration of
  these Acts.

  The manual combines policy and operational guidance to help Freedom of Information
  and Privacy Coordinators and their staff:

     ‚Ä¢   Understand the general framework of the legislation;
     ‚Ä¢   Interpret the legislation and regulations;
     ‚Ä¢   Meet administrative and operational requirements; and
     ‚Ä¢   Be aware of best practices for institutions.

  The manual also provides information to the general public on how the legislation is
  administered within institutions.

  This manual does not provide guidance on the application of the Personal Health
  Information Protection Act.


Audience
  The primary audience for the manual is provincial and municipal Freedom of Information
  and Privacy Coordinators and their staff.

  The manual is also intended to be an information resource for the general public.


Use of the Manual
  Coordinators should use the manual with an up-to-date version of the applicable
  legislation and regulations available through e-laws. There may be sections of the
  legislation and regulations that are not covered in the manual.

  The manual is not meant to provide legal advice. For legal advice, Coordinators should
  work with Legal Counsel.




  Freedom of Information and Protection of Privacy Manual                                  1
Whom to Contact
  Coordinators and the general public may contact staff in the Information, Privacy and
  Archives Division of the Ministry of Government and Consumer Services (MGCS) about
  the manual and more information regarding access to information and protection of
  privacy in Ontario.

  Coordinators and the general public may contact the Information, Privacy and Archives
  Division staff by telephone at 416-327-1600 or 1-800-668-9933 (toll-free in Ontario
  only). You may also send inquires by email to Web.Foi.MGCS@ontario.ca.


Related Resources
  Related resources from the Office of the Information and Privacy Commissioner of
  Ontario (IPC) are listed at the end of each chapter.

  The appendices to this manual include forms, templates, and letters that may be used
  and adapted by Coordinators for use in their offices.

  The manual includes hyperlinks to the relevant sections of the legislation, IPC
  documents, other resources, other parts of the manual, and its appendices.




 Freedom of Information and Protection of Privacy Manual                                  2
Glossary
 In this manual:

 ‚ÄúCase law‚Äù means decisions issued by the Courts;

 ‚ÄúCoordinator‚Äù means any provincial or municipal freedom of information and protection
 of privacy coordinator or equivalent or that person normally performing the role of the
 freedom of information and protection of privacy coordinator;

 ‚ÄúException‚Äù means types of records or information that do not qualify for an exclusion or
 exemption under FIPPA or MFIPPA;

 ‚ÄúExclusion‚Äù means a provision under FIPPA or MFIPPA which excludes records or parts
 of records from the application of the legislation;

 ‚ÄúExemption‚Äù means a provision under FIPPA or MFIPPA which exempts records or
 information from the general right of access;

 ‚ÄúHead‚Äù means the head of an institution as defined by FIPPA and MFIPPA;

 ‚ÄúInstitution‚Äù means an institution as defined by FIPPA and MFIPPA;

 ‚ÄúIPC‚Äù means the Office of the Information and Privacy Commissioner of Ontario;

 ‚ÄúLegal Counsel‚Äù means Legal Counsel assigned to support an institution;

 ‚ÄúLegislation‚Äù means both FIPPA and MFIPPA and their regulations in instances where
 the laws are similar; FIPPA and MFIPPA are referred to individually where the laws are
 different;

 ‚ÄúManual‚Äù means the Freedom of Information and Protection of Privacy Manual;

 ‚ÄúMGCS staff‚Äù means staff in the Enterprise Recordkeeping, Access and Privacy Branch;
 Information, Privacy and Archives Division of the Ministry of Government and Consumer
 Services;

 ‚ÄúMGCS Legal Counsel‚Äù means Legal Counsel in the Access and Privacy Law Group,
 Ministry of Government and Consumer Services;

 ‚ÄúPrivacy breach‚Äù means an incident where personal information is collected, retained,
 used, disclosed or disposed of in ways that do not comply with personal information
 protection requirements in statute and regulation;

 Freedom of Information and Protection of Privacy Manual                                   3
‚ÄúPrivacy Impact Assessment‚Äù means a process that reviews a new or existing
information system or program to determine whether measures are necessary to ensure
compliance with personal information protection requirements in statute and regulation
and to address the broader privacy implications of the system or program;

‚ÄúProgram area‚Äù means related activities or services within an institution for which the
institution has authority and responsibility;

‚ÄúRecord‚Äù (as defined by FIPPA and MFIPPA) means any record of information however
recorded, whether in printed form, on film, by electronic means or otherwise;

‚ÄúRequest‚Äù means a Freedom of Information (FOI) access request, including both
request for general records and requests for one‚Äôs own personal information;

‚ÄúResponsible Minister‚Äù means the Minister of Government and Consumer Services; and

‚ÄúSenior management‚Äù means all levels of managers accountable and responsible for
decision-making and approvals.




Freedom of Information and Protection of Privacy Manual                                   4
 Part I: The Legislation,
Roles and Responsibilities




Freedom of Information and Protection of Privacy Manual   5
Chapter 1: The Legislation

Introduction
  Access and privacy laws play a central role in government. These laws promote
  accountability, transparency, public participation and protect the privacy rights of
  individuals.

  This chapter provides broader context and explains how Coordinators should navigate
  through the legislation. It introduces the history and legal framework of access and
  privacy laws in Ontario; explains the legislation‚Äôs purpose and principles; how it is
  organized; and how it interacts with other laws.


History
  The Ontario Government established the Commission on Freedom of Information and
  Individual Privacy in 1977 to look at ways to improve public information policies and
  public sector access and privacy legislation. The Commission was headed by Dr. D.
  Carlton Williams and is known as the ‚ÄúWilliams Commission‚Äù.

  The framework for Ontario‚Äôs legislation is set out in the Commission‚Äôs report entitled
  ‚ÄúPublic Government for Private People, The Report of the Commission on Freedom of
  Information and Individual Privacy‚Äù published in 1980.

  FIPPA received royal assent in 1987, and came into force on January 1, 1988. The
  municipal counterpart, MFIPPA, came into force on January 1, 1991.


Policy Goals
  The William‚Äôs Commission, in making its recommendations, considered policy goals
  relating to good government such as:

  Transparency: The public‚Äôs right to know what government is doing and how decisions
  have been reached.

  Accountability: The public‚Äôs ability to hold elected representatives responsible for how
  they carry out their roles.

  Public participation: Citizen involvement in policy development and decision-making.




  Freedom of Information and Protection of Privacy Manual                                  6
  Fairness in decision-making: An individual‚Äôs ability to present their side of an issue,
  and their right to access the information on which a decision-maker will act, including
  the criteria to be applied.

  Personal privacy: The government‚Äôs records of personal information and information
  management practices, and an individual‚Äôs right to have access to government
  information concerning them.

  Administrative costs: The cost-benefit of the resources required to administer the
  legislation and the benefits to society from a more open government.


Ontario Access and Privacy Laws
  Access and privacy laws are a category of administrative law developed to ensure that:

     ‚Ä¢   The activities of government are authorized; and
     ‚Ä¢   Laws are implemented and administered in a fair and reasonable manner.

  In Ontario, there are four main laws that deal with access to information and privacy.
  Other federal and provincial legislation and municipal by-laws have specific access and
  privacy provisions that may also apply. The four main laws are listed below:

  Freedom of Information and Protection of Privacy Act (FIPPA): Applies to the
  provincial government of Ontario, universities, colleges, hospitals and designated
  agencies. FIPPA came into force on January 1, 1988.

  Municipal Freedom of Information and Protection of Privacy Act (MFIPPA): Is the
  local government equivalent of FIPPA and covers municipal institutions such as
  municipalities, cities, towns, school boards, police services and many other local
  government entities. MFIPPA came into force on January 1, 1991.

  Personal Health Information Protection Act (PHIPA): Provides rules specific to
  personal health information in the custody of health information custodians. Health
  information custodians include health care practitioners such as hospitals, long-term
  care facilities, pharmacies and more. PHIPA came into force on November 1, 2004.

  Personal Information Protection and Electronic Documents Act (PIPEDA): A
  federal legislation that governs how private companies and not-for-profit organizations
  engaging in commercial activities can handle personal information. PIPEDA came into
  force for federally regulated industries on January 1, 2001 and for all other companies
  and not-for-profit organizations engaged in commercial activities in Canada on January
  1, 2004.

  Freedom of Information and Protection of Privacy Manual                                   7
Purpose of Legislation
  FIPPA s.1 / MFIPPA s.1

  The legislation begins with stated purposes:

  a) To provide a right of access to information under the control of institutions in
  accordance with the principles that,

     ‚Ä¢   Information should be available to the public,
     ‚Ä¢   Necessary exemptions from the right of access should be limited and
         specific, and
     ‚Ä¢   Decisions on the disclosure of information should be reviewed
         independently of the institution controlling the information; and

  b) To protect the privacy of individuals with respect to personal information about
  themselves held by institutions and to provide individuals with a right of access to that
  information.

  In the context of providing advice or decision-making, Coordinators should consider the
  purpose of the legislation in their analysis.


Access and Privacy Principles
  The legislation is principle-based. The legislation balances the rights and needs of
  individuals and institutions by weighing different considerations against each other.

  From an access perspective, balance is achieved by providing individuals with the
  general right to access government records subject to limited and specific exemptions
  and exclusions.

  From a privacy perspective, balance is achieved by protecting personal information in
  terms of:

     ‚Ä¢   What and how personal information is collected, used, disclosed, and
         managed by government; and
     ‚Ä¢   Providing access to a requesters own personal information in certain
         circumstances.

  Access and privacy rights are not absolute. The facts of each situation, including the
  interests of the institution, individuals and the public, determine how the legislation
  applies and the final outcome of requests.

  Freedom of Information and Protection of Privacy Manual                                     8
Organization of Legislation
  FIPPA and MFIPPA are considered substantially similar laws and are organized the
  same way. The main parts of the legislation and content are summarized below.

  Definitions and Interpretation: Defines terms for the purpose of interpreting the
  legislation.

  Administration: This part appears only in FIPPA, but applies to MFIPPA. This part
  addresses the designation of the Responsible Minister for the legislation and the
  establishment of the Office of the Information and Privacy Commissioner.

  Freedom of Information: Addresses the right of access to records, exemptions,
  procedures for handling a request, and information to be published.

  Protection of Individual Privacy: Addresses the collection, use, disclosure and
  disposal of personal information; and an individual‚Äôs right to access and correct
  personal information about them.

  Appeal: Addresses appeal rights, and procedures for mediation and appeals.

  General: Addresses other administrative matters including fees, regulations,
  exclusions, and available information.

  Regulations: Addresses specifics of the legislation, and rules and procedures that must
  be implemented. The regulations also list institutions covered by FIPPA and MFIPPA.


Interactions with Other Laws
  The legislation should be read and interpreted together with other applicable laws.
  Interactions with other laws can be the result of:

     ‚Ä¢   Laws from other jurisdictions, including Federal laws;
     ‚Ä¢   Laws that prevail over FIPPA and MFIPPA; and
     ‚Ä¢   Laws that include confidentiality requirements.

  Coordinators should work with Legal Counsel to become familiar with the laws that may
  govern specific institution or program requirements or general government
  administrative requirements.




  Freedom of Information and Protection of Privacy Manual                               9
Prevailing Legislation
  FIPPA s. 67 / MFIPPA s. 53

  The general rule is that FIPPA or MFIPPA prevail over any other Ontario legislation.
  However, the legislation lists a number of laws that prevail over FIPPA and MFIPPA.

  Other laws must expressly state that it prevails over FIPPA or MFIPPA. If a statute has
  a confidentiality provision that prevails over FIPPA or MFIPPA, it can prevent an
  institution from providing access to a record.

  Consult the current versions of FIPPA and MFIPPA published on e-laws for a current list
  of laws that have been identified that prevail over FIPPA and MFIPPA.


Copyright Act
  The Canadian Copyright Act protects creative endeavours by ensuring that the creator
  has the sole right to authorize their publication, performance, or reproduction. Copyright
  applies to all original:

     ‚Ä¢   Literary or textual works: books, pamphlets, poems, computer programs;
     ‚Ä¢   Dramatic works; films, videos, plays, screenplays and scripts;
     ‚Ä¢   Musical works: compositions consisting of both words and music, or music
         only;
     ‚Ä¢   Artistic works: paintings, drawings, maps, photographs, and sculptures; and
     ‚Ä¢   Architectural works.

  The Copyright Act authorizes making a copy of a record for the purpose of giving
  access to a record under FIPPA or MFIPPA. The person who is given access to a
  record under the legislation is still bound by copyright.

  Copyright may apply to records of third parties submitted to an institution or records
  created by an institution.

  Where the Ontario Government creates a record, Crown copyright is indicated by
  Queen's Printer for Ontario.




  Freedom of Information and Protection of Privacy Manual                                  10
Litigation
  FIPPA s. 64 / MFIPPA s. 51

  Coordinators may be required to respond to an access request relating to matters in
  litigation. The legislation cannot be used to withhold information that is required to be
  produced by law for the purpose of litigation or a matter before an administrative
  tribunal. Coordinators should work with Legal Counsel to obtain legal advice on this type
  of request.


Resources
  Freedom of Information and Protection of Privacy Act

  FIPPA, R.R.O. 1990, Regulation 459, Disposal of Personal Information

  FIPPA, R.R.O. 1990, Regulation 460, General

  Municipal Freedom of Information and Protection of Privacy Act

  MFIPPA, R.R.O. 1990, Regulation 823, General

  MFIPPA, Ontario Regulation 372/91, Institutions

  Personal Health Information Protection Act

  Personal Information Protection and Electronic Documents Act

  Information and Privacy Commissioner of Ontario - Main Page

  IPC: Ontario‚Äôs FIPPA: A Mini Guide

  IPC: Ontario‚Äôs MFIPPA: A Mini Guide




  Freedom of Information and Protection of Privacy Manual                               11
Chapter 2: Government Roles and Responsibilities

Introduction
  Accountability and oversight is set out in the legislation and makes each institution
  responsible for complying with the provisions of the legislation and establishes that a
  Responsible Minister has specified duties. This chapter summarizes the roles and
  responsibilities of the Responsible Minister, MGCS staff, MGCS Legal Counsel, and the
  IPC.

  The concepts of an institution, a head of an institution, and how responsibilities can be
  assigned through a formal Delegation of Authority are explained.

  Responsibilities of Coordinators and their staff are discussed in Chapter 3: Coordinator
  Roles and Responsibilities.


Responsible Minister
  FIPPA s. 2, s. 3, s. 31, s. 32, s. 35, s. 39 (2), s. 45 / MFIPPA s. 2, s. 24, s. 29 (2)

  The MGCS Minister is currently the designated minister of the Crown responsible for
  both FIPPA and MFIPPA. The Responsible Minister is designated by the Lieutenant
  Governor in Council. The Responsible Minister‚Äôs main responsibilities are set out below.

  Internal oversight: Promote compliance with the legislation and regulations; and
  address matters of public interest. The Responsible Minister provides advice to Cabinet
  and institutions and responds to the IPC‚Äôs annual report.

  Amend and update the legislation and regulations: Ensure the legislation and
  regulations are reviewed for effectiveness and accuracy. The Responsible Minister also
  ensures external and internal stakeholders are consulted as necessary to proposed
  amendments and updates.

  Provide approvals where required by the legislation: Ensure that institutions comply
  with the legislation and regulations in a consistent manner by reviewing and approving
  requests from institutions on proposed program initiatives.

  Publications: Ensure that publications required to be made available under the
  legislation are publicly available. For example, the Responsible Minister is responsible
  for publishing the Directory of Institutions for all institutions covered by FIPPA and
  MFIPPA and the Directory of Records for Ontario Public Service ministries.


  Freedom of Information and Protection of Privacy Manual                                   12
Policy and Legal Support
  MGCS staff and MGCS Legal Counsel support the Responsible Minister, institutions
  and Coordinators in carrying out their responsibilities under the legislation. Legal advice
  can only be provided by Legal Counsel. The primary responsibilities of MGCS staff and
  MGCS Legal Counsel are listed below:

  Support the Responsible Minister: Ensure the Responsible Minister meets his or her
  obligations under the legislation and exercises his or her statutory decision-making
  responsibilities by preparing briefing notes, speaking notes, correspondence, and media
  responses.

  Amend and update the legislation and regulations: Review the effectiveness and
  accuracy of the legislation and regulations. They provide policy analysis and research,
  and coordinate and develop materials to support amendments to the legislation and
  regulations.

  Stakeholder engagement: Address matters of public interest raised by internal and
  external stakeholders such as institutions, the IPC, and the public. They undertake
  consultations as necessary, participate in federal-provincial-territorial access and
  privacy committees, and lead communities of practice for Coordinators, Legal Counsel,
  policy, and technology professionals.

  Advice and policy direction: Support institutions in the administration of the legislation
  and understanding of relevant policies. They respond to inquiries from institutions and
  members of the public. They develop policies, guidelines, tools and resources to
  advance access and privacy.

  Education: Share information and resources and provide training to institutions about
  the legislation and administering the legislation.

  Publications: Ensure the Responsible Minister meets his or her obligations to publish
  the Directory of Institutions and the Directory or Records. They provide direction to
  institutions and coordinate publication.

  Annual report: Review compliance statistics of ministries and undertake special
  projects to respond to annual report of the IPC.




  Freedom of Information and Protection of Privacy Manual                                  13
Information and Privacy Commissioner of Ontario
  FIPPA s. 4, s. 5, s. 6, s. 7, s. 8, s. 9

  The Commissioner is an Officer of the Legislature and is independent of the executive
  branch of government. The Commissioner is appointed by the Lieutenant Governor in
  Council with the approval of the Legislative Assembly. The Commissioner is appointed
  for a five-year term and is eligible for reappointment.

  The legislation sets out requirements for the IPC‚Äôs office and staff, but the office
  establishes its own internal processes and timelines. The IPC‚Äôs main responsibilities
  are:

  External oversight: Ensure that government organizations comply with the legislation.
  They provide advice and comments on proposed government legislation and policies.

  Education: Educate the public and institutions about Ontario's access and privacy
  laws. They publish a variety of resources for the public, organizations and professionals,
  and conducts research.

  Appeals: Hear and resolve appeals from refusals to provide access to information,
  develops appeals processes, and issues orders on the decision of the appeal.

  Privacy investigations: Investigate privacy complaints made by individuals regarding
  improper collection, use, or disclosure of their personal information, investigate self-
  reported privacy breaches, and conduct Commissioner initiated investigations into
  possible privacy breaches. The IPC can issue public investigation reports with
  recommendations and can order institutions to destroy personal information that was
  not collected in accordance with the legislation.

  Annual report: Table an annual report in the Legislature which includes information
  about institutions‚Äô compliance with FIPPA, MFIPPA and PHIPA and recommendations
  about the practices of specific institutions and proposed revisions to FIPPA, MFIPPA
  and PHIPA. The annual report is normally published in the spring of each year.


Coverage under the Legislation
  The legal term for organizations covered by the legislation is ‚Äúinstitution‚Äù. Each
  institution is a separate entity responsible for administering the legislation.

  Provincial and municipal institutions are defined differently. Institutions are either
  defined in the legislation or listed individually under the regulations.


  Freedom of Information and Protection of Privacy Manual                                  14
Provincial Institutions
FIPPA s. 2, Reg. 460

FIPPA defines institution as:

   ‚Ä¢   The Assembly;
   ‚Ä¢   A ministry of the Government of Ontario;
   ‚Ä¢   A service provider organization within the meaning of section 17.1 of
       the Ministry of Government Services Act;
   ‚Ä¢   A hospital; and
   ‚Ä¢   Any agency, board, commission, corporation or other body designated as
       an institution in the regulations.

FIPPA defines educational institutions as:

   ‚Ä¢   A college of applied arts and technology or a university.

Colleges are listed under the regulation as one entry (‚ÄúColleges of Applied Arts and
Technology‚Äù). Universities are listed individually in the regulations.

FIPPA defines hospitals as:

   ‚Ä¢   A public hospital within the meaning of the Public Hospitals Act;
   ‚Ä¢   A community health facility; within meaning of the Oversight of Health
       Facilities and Devices Act; and
   ‚Ä¢   The University of Ottawa Heart Institute.

Service Provider Organizations
FIPPA s. 65.1

The Ministry of Government Services Act established entities known as ‚ÄúService
Provider Organizations.‚Äù

Service provider organizations are not institutions under the legislation; however the
legislation establishes rules for how they must operate when providing a service on
behalf of the Government or a public body, including how they manage customer
service information and personal information.

Examples of service provider organizations and functions are ServiceOntario in the
MGCS, and government assistance programs under the Ministry of Revenue Act.


Freedom of Information and Protection of Privacy Manual                                  15
  Municipal Institutions
  MFIPPA s. 2, Reg. 372/91

  MFIPPA defines an institution as:

     ‚Ä¢   A municipality;
     ‚Ä¢   A school board, municipal service board, city board, transit commission,
         public library board, board of health, police services board, conservation
         authority, district social services administration board, local services board,
         planning board, local roads board, police village or joint committee of
         management or joint board of management established under the Municipal
         Act, 2001 or the City of Toronto Act, 2006 or a predecessor of those Acts;
         and
     ‚Ä¢   Any agency, board, commission, corporation or other body designated as
         an institution in the regulations.

  Other MFIPPA agencies, boards, commissions, corporations or other bodies not listed
  in the definition may be considered part of a municipality and fall under the authority of
  the council of the municipality.

  The relationships between organizations and whether an organization is an institution or
  covered under MFIPPA may not be clear. Issues relating to coverage should be
  discussed with Legal Counsel.


Listing of Institutions and Regulation Updates
  As noted above, institutions are either covered under the legislation by definition or by
  being listed under regulation. The process for updating the regulations is coordinated by
  MGCS staff.

  MGCS staff coordinate with provincial and municipal institutions to ensure that the
  listing of institutions is up to date. Amendments to the regulations may include:

     ‚Ä¢   The addition of new institutions;
     ‚Ä¢   The renaming of institutions or heads of institutions; and
     ‚Ä¢   The deletion of institutions.

  Coordinators should become familiar with institutions under the regulations that are
  related to their institution. Coordinators should advise MGCS staff of any changes to the
  list of institutions.


  Freedom of Information and Protection of Privacy Manual                                  16
Exceptions to Coverage
  The legislation does not apply to some public sector organizations. These organizations
  include:

     ‚Ä¢   Some records of the Legislative Assembly;
     ‚Ä¢   Constituency Offices; and
     ‚Ä¢   Courts.

     The sections below will provide more information.

  Legislative Assembly
  FIPPA has limited application to the Legislative Assembly and does not apply to
  Independent Officers of the Legislative Assembly.

  The definition of institution includes the Legislative Assembly, but only in respect to
  records of expense claims of ministers, opposition leaders and their respective staff
  under the authority of the Politicians‚Äô Expense Review Act, and in respect to the
  personal information contained in these records.

  As such, FIPPA does not apply to the following offices:

     ‚Ä¢   Auditor General of Ontario;
     ‚Ä¢   Environmental Commissioner;
     ‚Ä¢   Financial Accountability Officer;
     ‚Ä¢   French Language Services Commissioner;
     ‚Ä¢   Information and Privacy Commissioner of Ontario;
     ‚Ä¢   Integrity Commissioner.
     ‚Ä¢   Ombudsman of Ontario; and
     ‚Ä¢   Provincial Advocate for Children and Youth;

  Member of Provincial Parliament Constituency Offices
  The constituency offices of Members of Provincial Parliament (MPP), including
  ministers, are not covered by FIPPA because they are not part of an institution.

  The records of MPPs that relate to ministerial work may be subject to FIPPA.

  For example, the policy recommendations related to education reform in the office of the
  Minister of Education would be subject to FIPPA; however, records related to individual
  constituents of the same official would not be subject to FIPPA.


  Freedom of Information and Protection of Privacy Manual                                   17
  Courts
  The courts and the judiciary are not considered part of any ministry and therefore not
  included in the definition of institution. The role of the judiciary is set apart from
  government.

  Municipal Councillors‚Äô Records
  Generally, the legislation does not apply to the records of municipal councillors that
  relate to their constituents and their private political activities.

  However, records of municipal councillors may be subject to the legislation when:

      ‚Ä¢   A councillor is acting as an officer or employee of the municipality, or
          performs a duty assigned by council, such that they might be considered
          part of the institution; or
      ‚Ä¢   The records are in the custody or control of the municipality.

  Examples of when a municipal councillor may be acting as an officer of the municipality
  include when a councillor participates on a municipal committee or board or exercises
  administrative or management functions on behalf of the municipality.


Head of an Institution
  The ‚Äúhead‚Äù of an institution is the legal term that refers to the official accountable and
  responsible for:

     ‚Ä¢    Overseeing the administration of the legislation;
     ‚Ä¢    Ensuring compliance with the legislation and regulations; and
     ‚Ä¢    Making decisions regarding the legislation.

  While the legislation refers to responsibilities of the head, responsibilities can be carried
  out by other positions in an institution through a Delegation of Authority discussed
  below.

  FIPPA
  The head for Ontario ministries is the minister of the Crown who presides over a
  ministry.

  For institutions that are designated under the regulation, the regulation also indicates
  the officer designated as the head. The minister responsible for an agency is usually the


  Freedom of Information and Protection of Privacy Manual                                      18
  head. In some cases, a senior executive such as a President or Chair may be
  designated as the head of an agency, board, or commission.

  For public hospitals, the head is defined as the Chair of the Board; and for community
  health facilities the head is defined as the Superintendent.

  For colleges, the head is defined as the Chair of the Board; and for universities, the
  head is defined as the Executive Head.

  MFIPPA
  Under MFIPPA, the head is the council of a municipality or the board of a local board
  unless they choose to designate as head an individual or sub-group from among
  themselves. Examples of designations may include the mayor, a warden, a councillor, a
  special committee or a board member.

  The designation must be in writing and in the case of a municipal council; it should be
  set out in a by-law.

  See Appendix 1 for a sample draft by-law designating a head under MFIPPA. See
  Appendix 2 for a sample resolution designating a head under MFIPPA.


Delegation of Authority
  s. 62 (1) / s.49 (1)

  The legislation allows the head of an institution to delegate some or all powers and
  duties to an officer or officers of an institution or to another institution. This is known as
  a ‚ÄúDelegation of Authority‚Äù (DOA). It is through a DOA that many Coordinator
  responsibilities are formalized.

  A DOA is a legal document and should clearly identify the duties and functions being
  delegated regarding access and privacy. The responsibility that is usually delegated is
  decision-making. However, the head of an institution remains accountable for all
  decisions made and actions taken.

  The process for developing a DOA should involve the head of an institution, Legal
  Counsel, and all delegated decision-makers in order for all parties to fully understand
  their responsibilities. The DOA should be reviewed regularly and kept up-to-date.

  See Appendix 3 for a sample DOA.



  Freedom of Information and Protection of Privacy Manual                                      19
  Conflict of Interest
  The DOA should include alternate decision-makers where there is a possible conflict of
  interest. A conflict of interest may be real or can reasonably be assumed. A conflict of
  interest may exist where:

     ‚Ä¢   A public official knows they have a private interest connected to their public
         duties, or
     ‚Ä¢   A public official may be perceived to be making decisions based on a
         personal or private interest rather than the public interest.


Offences and Liability
  FIPPA s. 61, s. 62 / MFIPPA s. 48, s. 49

  The legislation includes offences or consequences for intentionally contravening some
  of its rules.

  If an offence is committed, individuals, institutions and employees can be liable for fines
  up to $5,000.

  Offences listed under the legislation include willfully and knowingly:

     ‚Ä¢   Disclosing personal information in a manner that is not authorized;
     ‚Ä¢   Maintaining a secret personal information bank that contravenes the
         legislation;
     ‚Ä¢   Making a request for access to or correction of personal information under
         false pretenses;
     ‚Ä¢   Altering, concealing or destroying a record, or the information contained in
         the record, with the intent to evade access to information requests;
     ‚Ä¢   Causing another individual to alter, conceal or destroy a record, or the
         information contained in a record, with the intent to evade access to
         information requests;
     ‚Ä¢   Obstructing the IPC‚Äôs performance of its duties; and
     ‚Ä¢   Misleading the IPC, or failing to comply with an order of the IPC.

  A prosecution for an offence under the legislation must commence within two years of
  discovery of evidence of the offence. The consent of the Attorney General must be
  sought before a prosecution can occur.

  Employees of an institution are protected against civil actions and liabilities when they
  are acting in good faith.

  Freedom of Information and Protection of Privacy Manual                                     20
Recordkeeping
 FIPPA s. 10.1 / MFIPPA s. 4.2

 The legislation requires that reasonable measures are developed, documented and put
 into place in order to preserve the organization‚Äôs records in accordance with the
 recordkeeping rules that apply to the organization. An organization‚Äôs rules can be
 established either by policy, by-law or law.

 The records of an institution serve as evidence of government activities and
 transactions. Managed properly, information provides authoritative and trustworthy
 evidence and provides proof of government decisions and accountability for those
 decisions.

 Good records management also supports compliance with the legislation.


Resources
 Appendix 1 Sample Municipal By-law for Appointing a Head under MFIPPA

 Appendix 2: Sample Resolution Appointing a Head under MFIPPA

 Appendix 3: Sample Delegation of Authority

 MGCS: Recordkeeping Amendments to FIPPA and MFIPPA ‚Äì Information Sheet

 IPC: FIPPA and MFIPPA: Bill 8 ‚Äì The Recordkeeping Amendments

 IPC: MFIPPA and Councillors‚Äô Records

 IPC: Improving Access and Privacy with Records and Information Management




 Freedom of Information and Protection of Privacy Manual                              21
Chapter 3: Coordinator Roles and Responsibilities

Introduction
  Coordinators across institutions share common roles and responsibilities. The manual is
  designed to help Coordinators develop the foundation of knowledge and skills required
  to administer the legislation.

  There are various aspects to the Coordinator‚Äôs position which may vary depending on
  institutional factors such as:

     ‚Ä¢   The institution‚Äôs mandate and size;
     ‚Ä¢   The Delegation of Authority;
     ‚Ä¢   The position of the Freedom of Information and Privacy Office within the
         institution;
     ‚Ä¢   The number and complexity of access requests;
     ‚Ä¢   The volume and sensitivity of personal information holdings; and
     ‚Ä¢   Stakeholder relations.

  Coordinators will need to build and maintain a network of internal and external contacts.
  For provincial ministries internal stakeholders could include the Minister‚Äôs Office, Deputy
  Minister‚Äôs Office, Legal Counsel, program areas, and communications staff.

  For municipalities internal stakeholders could include the City Clerk, municipal
  councillors, Legal Counsel, program areas, and communications staff.

  External stakeholders for all institutions could include individual requesters, the general
  public, the IPC, other institutions and other governments.

Overview of Roles and Responsibilities
  The Coordinator‚Äôs responsibilities cover a broad range of access and privacy activities.
  More details on specific responsibilities and activities are provided throughout this
  manual.

  In most cases, the Coordinator‚Äôs main roles include:

     ‚Ä¢   Management ‚Äì administering or supervising the operations of the freedom
         of information and privacy program.
     ‚Ä¢   Coordination - organizing the various parts of an activity to enable
         collaboration and efficient communication.


  Freedom of Information and Protection of Privacy Manual                                  22
        ‚Ä¢    Advisory - giving information or advice or a recommendation about what
             should be done.
        ‚Ä¢    Training and awareness ‚Äì teaching and raising awareness of access and
             privacy responsibilities.


Administration
  The Coordinator may be responsible for a variety of administrative activities to support
  the request process and the day-to-day operations of the office. Administrative
  considerations may include:

    ‚Ä¢       Human resources ‚Äì hiring and managing staff;
    ‚Ä¢       Office accommodation and record storage ‚Äì secure space for handling,
            reviewing and storing records, and dealing with the public;
    ‚Ä¢       Equipment (e.g., phones, computers, fax machine, copier, scanner,
            projectors);
    ‚Ä¢       Technology (e.g., email, software for severing records, tracking, website);
    ‚Ä¢       Payment processing; and
    ‚Ä¢       Mail and courier services.


Policies and Procedures
  Coordinators develop various policies and procedures to support operational efficiency.
  Examples of subjects that policies and procedures will need to address include:

        ‚Ä¢    The institution‚Äôs Delegation of Authority;
        ‚Ä¢    Routine requests;
        ‚Ä¢    Handling sensitive information;
        ‚Ä¢    Publication of records;
        ‚Ä¢    Conducting privacy impact assessments;
        ‚Ä¢    Responding to privacy breaches; and
        ‚Ä¢    Rules for collecting, using and disclosing personal information.


Processing Requests
  Coordinators need to have defined procedures for processing access requests.
  Procedures should address all aspects of responding to a request including:

        ‚Ä¢    Contacting the office (e.g., phone, mailing address, internet, email);
        ‚Ä¢    Handling inquiries;

  Freedom of Information and Protection of Privacy Manual                                 23
     ‚Ä¢   Handling incoming requests and correspondence;
     ‚Ä¢   Processing applications and fees;
     ‚Ä¢   Searching and reviewing records;
     ‚Ä¢   Providing notice to affected parties;
     ‚Ä¢   Conducting research on IPC orders and case law;
     ‚Ä¢   Obtaining legal advice;
     ‚Ä¢   Documenting decisions and recommendations;
     ‚Ä¢   Preparing copies and records for release;
     ‚Ä¢   Reviewing work for accuracy;
     ‚Ä¢   Issues management;
     ‚Ä¢   Obtaining approvals; and
     ‚Ä¢   Packaging and sending records.

     More information on processing requests can be found in Chapter 6: Managing the
     Request Process.


Case File Management and Reporting
  Coordinators must manage request case files and collect data regarding the
  administration of the legislation. Coordinators may use electronic databases or manual
  systems to manage and track requests. Effective case file management enables:

     ‚Ä¢   Management of work load and assignments relating to files, appeals, and
         projects;
     ‚Ä¢   Tracking the status of individual requests and appeal files;
     ‚Ä¢   Information and records management relating to all of the office and
         request records;
     ‚Ä¢   Tracking the stages and status of responses to privacy breaches and
         investigations.
     ‚Ä¢   Reporting to senior management; and
     ‚Ä¢   Annual reporting to the IPC.


Research
  Coordinators must conduct research to inform analysis of application of the legislation
  and to stay current on issues and trends. The following list includes some areas that
  coordinators may consult as part of the research activity:

     ‚Ä¢   IPC orders and privacy investigation reports, and case law;
     ‚Ä¢   IPC access and privacy resources;

  Freedom of Information and Protection of Privacy Manual                                   24
    ‚Ä¢   MGCS access and privacy resources;
    ‚Ä¢   Corporate directives, policies, guidelines and standards in your
        organization;
    ‚Ä¢   Media reports; and
    ‚Ä¢   Relevant resources and trends in other jurisdictions.


 Mediation and Appeals
 Coordinators are involved in some or all parts of the intake, mediation or adjudication
 stages of the IPC appeal process. This may include the following activities:

    ‚Ä¢   Preparing relevant records for an appeal to the IPC;
    ‚Ä¢   Representing the institution at all stages of the appeal process;
    ‚Ä¢   Conducting in-depth research of IPC orders and case law;
    ‚Ä¢   Obtaining legal advice and legal representation;
    ‚Ä¢   Obtaining affidavits;
    ‚Ä¢   Preparing representations; and
    ‚Ä¢   Presenting the institution‚Äôs position to the IPC.

    More information on IPC appeals can be found in Chapter 11: Appeals Process.


Issues Management
 Coordinators should ensure Senior Management and decision makers are aware of any
 contentious issues that may arise from received requests or privacy matters. This may
 include the following activities:

    ‚Ä¢   Providing ‚Äúheads up‚Äù notifications to Senior Management and other offices
        within the institution involved with communications and issues management
        when contentious requests are received;
    ‚Ä¢   Providing status updates and briefings to Senior Management as
        contentious requests are processed; and
    ‚Ä¢   Alerting Senior Management of any contentious issues that may occur in
        relation to the institution‚Äôs privacy practices.

 For more information on considerations for processing contentious requests, see
 Chapter 6: Managing the Request Process.




 Freedom of Information and Protection of Privacy Manual                                   25
Publications
  Coordinators should ensure that publication requirements under the legislation are met.
  Publication requirements include:

     ‚Ä¢   Directory of Institutions and Directory of Records; and
     ‚Ä¢   Institution documents available for public review (e.g., public records,
         manuals, directives).

     More information on the Directory of Institutions and the Directory of Records can be
     found in Chapter 4: Access Fundamentals.


Resources
  IPC: Backgrounder for Senior Managers and Information and Privacy Coordinators,
  Raising the Profile of Access and Privacy

  IPC: Basics for Freedom of Information Coordinators




  Freedom of Information and Protection of Privacy Manual                              26
             Part II: Freedom of
                Information




Freedom of Information and Protection of Privacy Manual   27
Chapter 4: Access Fundamentals

Introduction
  The legislation provides a general right of access to government information, subject to
  certain exclusions and exemptions. This chapter introduces the definition of records,
  how to understand custody and control of records, and third party records. The sections
  of the legislation that limit and support access to information are explained.

  The main topics covered in this chapter include classes of information excluded from
  the legislation (called ‚Äúexclusions‚Äù), mandatory and discretionary exemptions to the right
  of access, information available to the public, and disclosure obligations. The exercise
  of discretion, harms tests, and the public interest override are also covered.

  Chapter 5: Exemptions and Exclusions explains how exclusions and exemptions are
  interpreted and applied to records. More detail on how requests are processed and
  managed is discussed in Chapters 6: Managing the Request Process.


Applying the Legislation
  In responding to requests made under the legislation, Coordinators take the following
  basic steps to assess how the legislation applies within the context of each individual
  access request:

     1. Determine if the individual is seeking access to a record.
     2. Determine if the records are in the custody or control of the institution.
     3. Determine the relevant sections of the legislation that might apply to a
        record (in whole or in part).
     4. Determine if the criteria and tests for each relevant section of the legislation
        are met.
     5. Determine if additional legal tests apply and the criteria are met.

  The background and considerations for each step are discussed below.


Understanding Records
  FIPPA s. 2 / MFIPPA s. 2

  Individual‚Äôs access rights apply to records or parts of records. Record is defined as any
  record of information however recorded, whether in printed form, on film, by electronic
  means or otherwise, and includes:

  Freedom of Information and Protection of Privacy Manual                                   28
     ‚Ä¢   Books
     ‚Ä¢   Correspondence
     ‚Ä¢   Diagrams
     ‚Ä¢   Documentary material
     ‚Ä¢   Drawings
     ‚Ä¢   Films
     ‚Ä¢   Maps
     ‚Ä¢   Memorandums
     ‚Ä¢   Microfilms
     ‚Ä¢   Plans
     ‚Ä¢   Pictorial and graphic work
     ‚Ä¢   Photographs
     ‚Ä¢   Sound recordings
     ‚Ä¢   Videotapes

  The definition is broad and is interpreted to include records that are not completed (e.g.,
  working drafts) and recorded information using current technologies (e.g., voicemail,
  email). The definition also includes copies and any records that can be produced by
  computer hardware and software or any other equipment.

  In general, the legislation applies to an existing record regardless of whether it was
  created prior to the legislation taking effect. Hospitals are an exception to this rule
  because the legislation only applies to records that came into the custody or control of
  hospitals on or after January 1, 2007.


Creating Records
  Reg. 460 (2) / Reg. 823 (1)

  In instances where institutions receive a request for information that may reside in an
  institution, but not as a record, the Coordinator must determine the feasibility of
  producing the record. For example, if the information resides in a database.

  The legislation does not explicitly require an institution to create a record but there may
  be situations where it is effective to do so. There is a growing expectation that
  government use of information technology should facilitate, not limit, public access to
  information.

  Under the regulations a record capable of being produced from machine readable
  records is not included in the definition of record where the process of producing it
  would unreasonably interfere with the operations of an institution.
  Freedom of Information and Protection of Privacy Manual                                   29
  Factors Coordinators should consider when establishing unreasonable interference may
  include:

     ‚Ä¢   Number of hours required to produce the record;
     ‚Ä¢   Number of staff required to undertake the work and the impact on their
         regular duties and responsibilities;
     ‚Ä¢   Technical expertise required (e.g., consultant); and
     ‚Ä¢   The impact on the institution‚Äôs operations and resources (e.g., disruption,
         delay, hinder effectiveness).

  Where the institution is capable of producing a record and the production of the record
  would not interfere with the operations of the institution then the produced record would
  qualify as a record under the legislation.


Custody or Control
  The right of access only applies where the records, in whole or in part, fall within the
  custody or control of an institution.

     ‚Ä¢   Custody means the keeping, care, watch, preservation or security of the
         record for a legitimate business purpose.
     ‚Ä¢   Control means the power or authority to make a decision about the
         creation, use, disposal or disclosure of the record.

  There are a number of factors that can be considered to determine custody and control.

  Custody
  Questions Coordinators‚Äô should consider when determining whether the institution has
  custody of a record include:

     ‚Ä¢   Does the institution have physical possession of the record, either because
         it has been voluntarily provided by the creator or pursuant to a mandatory
         statutory or employment requirement?
     ‚Ä¢   If the institution does not have possession of the record, is it being held by
         an officer or employee of the institution for the purposes of his or her duties
         as an officer or employee?

  These questions are relevant for personal records of an employee that happen to be
  physically located in the office of an institution. While the record may be physically in the
  office, if the record does not relate to the employee‚Äôs duties as an employee they may
  not be considered in the custody of the institution. For example, generally speaking, an

  Freedom of Information and Protection of Privacy Manual                                    30
  employee‚Äôs personal dry cleaning receipt or personal telephone bill would not be
  considered in the custody of the institution as they do not relate to the employee‚Äôs
  duties.

  Control
  Questions Coordinators‚Äô should consider when determining whether the institution has
  control of a record include:

     ‚Ä¢   Was the record created by an officer or employee of the institution?
     ‚Ä¢   Does the record relate to the statutory or core business of the institution?
     ‚Ä¢   What use did the creator intend to make of the record?
     ‚Ä¢   Does the institution have a right to possession of the record?
     ‚Ä¢   Does the institution have the authority to regulate use and disposal of the
         record?
     ‚Ä¢   To what extent has the institution relied upon the record?
     ‚Ä¢   How closely is the record integrated with other records held by the
         institution?

  Coordinators should be familiar with the business and records of the institution, as well
  as how business is conducted and information is managed.


Records of Third Parties
  Institutions may legitimately obtain or have copies of records of third parties that are
  within their custody or control. A third party can be any one of the following:

     ‚Ä¢   Person;
     ‚Ä¢   Group;
     ‚Ä¢   Committee;
     ‚Ä¢   Organization;
     ‚Ä¢   Other government; or
     ‚Ä¢   Business.

  An employee of an institution is not a third party unless acting in a personal capacity.

  Government business and service delivery may result in institutions having custody or
  control of third party records. Some common examples where government institutions
  may have custody or control of third party records include:

     ‚Ä¢   Records that have been provided under legislated and regulatory
         requirements;
  Freedom of Information and Protection of Privacy Manual                                    31
     ‚Ä¢   Records including the personal information of individuals applying for
         benefits or services;
     ‚Ä¢   Records collected as part of a procurement of products or services;
     ‚Ä¢   Records containing expert and legal advice,
     ‚Ä¢   Records gathered during public consultations,
     ‚Ä¢   Records created through federal-provincial-municipal initiatives, and
     ‚Ä¢   Records created through public-private sector partnerships.

  Key questions Coordinators may ask to determine whether the institution has custody or
  control of records of third parties include:

     ‚Ä¢   Who owns the record?
     ‚Ä¢   Who paid for the creation of the record?
     ‚Ä¢   What are the circumstances surrounding the creation, use and retention of
         the record?
     ‚Ä¢   Is there a contract between the institution and the organization or individual
         who created the record?
     ‚Ä¢   Was the individual who created the record an agent of the institution for the
         purposes of the activity in question?
     ‚Ä¢   What is the customary practice of the individual who created the record in
         relation to possession or control of records of this nature, in similar
         circumstances?


Notice to Affected Persons
  There are additional requirements for processing requests for access to records that
  contain information about third parties such as providing notice and obtaining consent
  where applicable.

  Institutions should consider developing a policy or procedure for when third party
  information is frequently requested.

  More information on notice requirements are provided in Chapter 6: Managing the
  Request Process.




  Freedom of Information and Protection of Privacy Manual                                  32
Applying Relevant Sections
  In processing requests, Coordinators must review responsive records to determine if
  any exclusion or exemption applies to a record, either in whole or in part.

  Exclusions and exemptions of the legislation are categorized based on their purpose in
  regard to limiting or supporting access to records. Each of these categories will be
  discussed further below.

  Exclusions: These are provisions which exclude records or parts of records from the
  application of the legislation.

  Exemptions (general): These are provisions which exempt records or information from
  the general right of access. The legislation applies to a record but access can be
  denied.

      Mandatory exemption: If a mandatory exemption applies the head must refuse
      access to the record unless it has consent to disclose it. In the legislation,
      mandatory exemptions begin with the words ‚Äúshall refuse.‚Äù

      Discretionary exemption: An exemption where the head may choose to deny
      access to a record, but is not required to do so. In the legislation, discretionary
      exemptions begin with the words ‚Äúmay refuse.‚Äù

  In order to determine the relevant sections of the legislation that apply to a record, the
  content and context of a record are important factors. Coordinators may ask the
  following questions to assess the content and context of the record:

     ‚Ä¢   Who prepared the record?
     ‚Ä¢   What is the purpose of the record?
     ‚Ä¢   Who is the intended audience?
     ‚Ä¢   What is the age of the record?
     ‚Ä¢   What type of information is in the record?
     ‚Ä¢   How sensitive is the information?
     ‚Ä¢   How was the record shared (e.g., internally or publicly)?


Exclusions
  Exclusions mean that the legislation does not apply to certain types or classes of
  records. Exclusions are intended to be limited in scope. The exclusions do not apply to
  all institutions equally. Some exclusions are specific to universities, colleges or
  hospitals.

  Freedom of Information and Protection of Privacy Manual                                      33
The legislation does not prevent access to excluded records. If an institution decides to
release an excluded record, it can do so ‚Äúoutside‚Äù of the legislation. However, this
approach means that other rights (e.g., appeal rights) are not available to the requester.

The list below outlines the exclusions and to what institutions they apply:

Private donations to archives: Excludes records that have been donated to a public
archive by a non-institution such as a private individual, corporation or association. This
exclusion applies to the Archives of Ontario and archival repositories in colleges,
universities and municipal institutions. FIPPA s. 65 (1) and MFIPPA s. 52 (2).

Proceedings before a Court: Excludes records prepared for a person presiding in a
proceeding in a court of Ontario such as a judge. This exclusion applies to the Courts.
FIPPA s. 65 (3).

Performance evaluations of judges: Excludes records and information related a
judge‚Äôs performance evaluation. This exclusion applies to provincial institutions only.
FIPPA s. 65 (4).

Ontario Judicial Council records: Excludes records of the Ontario Judicial Council
that are deemed confidential, not made available to the public or relate to a proceeding
that did not occur in the public. This exclusion applies to provincial institutions only.
FIPPA s. 65 (5).

Associate judge investigations: Excludes records related to investigations into
complaints against associate judges under the Courts of Justice Act. This exclusion
applies to provincial institutions only. FIPPA s. 65 (5.1)

Prosecution records: Excludes records related to prosecutions where all matters
related to the prosecution have not been completed. This exclusion applies to all
provincial and municipal institutions. FIPPA s. 65 (5.2) and MFIPPA s. 52 (2.1).

Ecclesiastical records: Excludes the records of church or religious organizations
affiliated with an education institution or hospital. The exclusion applies only to
hospitals, universities and colleges. FIPPA s. 65 (5.3).

Hospital foundations: Excludes records of hospital foundations, even when in the
custody of hospitals. This exclusion applies only to hospitals. FIPPA s. 65 (5.4).

Administrative records of health professionals: Excludes records of health
professionals using hospital offices for personal practice. This exclusion applies only to
hospitals. FIPPA s. 65 (5.5).


Freedom of Information and Protection of Privacy Manual                                   34
Charitable donations: Excludes records related to charitable donations to hospitals.
This exclusion applies only to hospitals. FIPPA s. 65 (5.6).

Labour relations and employment-related: Excludes the majority of records related to
labour relations and employment. This exclusion applies to all provincial and municipal
institutions. FIPPA s. 65 (6) and MFIPPA s. 52 (3).

Church or religious appointments of individuals: Extends the employment records
exclusion to religious appointees in institutions. The exclusion only applies to all
provincial institutions. FIPPA s. 65 (6) 4.

Hospital appointments of persons with privileges: Extends the employment records
exclusion to doctors who have privileges at a hospital. This exclusion applies to
hospitals only. FIPPA s. 65 (6) 5.

Adoptions related: Excludes certain records related to adoptions. This exclusion
applies to all provincial institutions. FIPPA s. 65 (8).

Research and teaching materials: Excludes records related to research and teaching
materials for individuals employed or associated with a college, university or hospital.
The exclusion applies only to colleges, universities and hospitals. FIPPA s. 65 (8.1).

Peer evaluations of research and teaching materials: Excludes records related to
peer evaluation of research and teaching materials of individuals employed or
associated with a college, university or hospital. The exclusion applies only to colleges,
universities and hospitals. FIPPA s. 65 (10) s. 49 (c.1).

Medical assistance in dying: Excludes identifying information of individuals and
facilities associated with services related to medically assistance in dying. FIPPA s. 65
(11).

Abortion related services: Excludes identifying information of individuals and facilities
associated with services related to abortion services. It applies to all provincial
institutions. FIPPA s. 65 (13) (14) and (15).




Freedom of Information and Protection of Privacy Manual                                 35
Mandatory Exemptions
  In general, a record that falls under a mandatory exemption cannot be disclosed unless
  the institution obtains the consent of the affected party. In the legislation, mandatory
  exemptions start with ‚Äúshall refuse‚Äù.

  A notable difference between the municipal and provincial legislation is that MFIPPA
  has a mandatory exemption for relations with other governments and FIPPA has a
  discretionary exemption for relations with other governments.

  The four mandatory exemptions are listed below.

  Cabinet records: Protects the substance of deliberations of Executive Council or its
  committees. FIPPA s. 12.

  Personal privacy: Protects against an unjustified invasion of privacy of an individual
  other than the requester . FIPPA s. 21, MIFPPA s. 14.

  Relations with other governments: Protects confidential information received from
  other Canadian and foreign governments. MFIPPA s.9. Note: a similar exemption is
  discretionary under FIPPA.

  Third party information: Protects third parties from financial or other harms. FIPPA s.
  17, MFIPPA s. 10.


Discretionary Exemptions
  A discretionary exemption means that refusing access to a record is permitted, but not
  required. In the legislation, discretionary exemptions begin with the words ‚Äúmay refuse‚Äù.
  In general, a decision to refuse access to a record requires further analysis known as an
  exercise of discretion discussed below. It involves weighing the pros and cons of
  providing access to a record or not.

  Most of the discretionary exemptions are common to both FIPPA and MFIPPA.
  However, the discretionary exemptions specific to FIPPA are:

     ‚Ä¢   Defence;
     ‚Ä¢   Government relations; and
     ‚Ä¢   Species at risk.

  The discretionary exemption specific to MFIPPA is draft by-laws and records of closed
  meetings.


  Freedom of Information and Protection of Privacy Manual                                  36
The discretionary exemptions and their main purpose are listed below.

Draft by-laws and records of closed municipal meetings: Protects deliberations in
closed meetings or draft by-law. MFIPPA s.6.

Advice to government/Advice or recommendations: Protects records or parts of
records containing advice or recommendations used in government decision-making.
FIPPA s. 13, MFIPPA s. 7.

Law enforcement: Protects various types of records and activities relating to law
enforcement and security. FIPPA s. 14, MFIPPA s.8.

Civil Remedies Act, 2001: Protects records that if released could interfere with the
Attorney General‚Äôs ability to conduct a proceeding under the Civil Remedies Act. FIPPA
s. 14.1, MFIPPA s. 8.1.

Prohibiting Profiting from Recounting Crimes Act, 2002: Protects records that if
released could interfere with the Attorney General‚Äôs ability to conduct a proceeding
under the Prohibiting Profiting from Recounting Crimes Act, 2002. FIPPA s. 14.2,
MFIPPA s. 8.2.

Relations with other governments: Protects confidential information received from
other Canadian and foreign governments. FIPPA s.15. Note: a similar exemption is
mandatory under MFIPPA.

Relations with Aboriginal communities: Protects confidential information received
from Aboriginal communities. FIPPA s. 15.1, MFIPPA s. 9.1.

Defence: Protects records related to the national defence of Canada or a foreign state.
FIPPA s.16.

Economic and other interests of Ontario: Protects records where disclosure could
damage an institution‚Äôs economic or other interests. FIPPA s.18, MFIPPA s.11.

Closed meetings: Provides universities and hospitals with similar confidentiality
provisions in the deliberative processes of their respective governing bodies. FIPPA s.
18.1.

Solicitor-client privilege: Protects records subject to common-law solicitor-client
privilege and litigation privilege. FIPPA s.19, MFIPPA s.12.

Danger to safety or health: Protects records that if released could cause serious
threats to safety or the health. FIPPA s. 20, MFIPPA s. 13.

Freedom of Information and Protection of Privacy Manual                                37
  Species at risk: Protects information that if released could endanger species at risk or
  their habitat. FIPPA s.21.1.

  Information soon to be published: Protects records that are published, currently
  available to the public, or will be published. FIPPA s.22, MFIPPA s.15.


Exceptions
  Some exclusions and exemptions include exceptions. Where the exception applies, the
  exclusion or exemption would not be applicable to the record.

  Some exceptions relate to factual information that may be of interest or use to the public
  and specific types of records or reports produced in the course of government work. For
  example, in the advice to government/advice or recommendations exemption,
  subsection 2 lists a number of classes of records that the section does not apply to
  including: factual material, a statistical survey, a report by a valuator, and an
  environmental impact statement.

  Other exceptions set time limits for when exemptions can be used. For example, in the
  Cabinet records exemption, subsection 2 (a) states that the exemption cannot be
  claimed on records more than 20 years old.


Exercise of Discretion
  When deciding not to disclose a record the factors and reasons for the exercise of
  discretion should be well documented to support the decision, in the event of an appeal.

  The IPC has developed a list of considerations for a proper exercise of discretion. The
  factors include:

     ‚Ä¢   The purpose of the legislation; including the principles that:
            o Information should be available to the public
            o Individuals should have a right of access to their own personal
               information
            o Exemptions from the right of access should be limited and specific
            o The privacy of individuals should be protected;
     ‚Ä¢   The wording of the exemption and the interests it seeks to protect;
     ‚Ä¢   Whether the requester is seeking his or her own personal information;
     ‚Ä¢   Whether the requester is an individual or an organization;
     ‚Ä¢   The relationship between the requester and any affected persons;


  Freedom of Information and Protection of Privacy Manual                                38
     ‚Ä¢   Whether disclosure will increase public confidence in the operation of the
         institution;
     ‚Ä¢   The nature of the information and the extent to which it is significant and/or
         sensitive to the institution, the requester, and any affected person;
     ‚Ä¢   The age of the information; and
     ‚Ä¢   The historic practice of the institution with respect to similar information.

  In contrast, an improper exercise of discretion occurs when:

     ‚Ä¢   It is done in bad faith or for an improper purpose;
     ‚Ä¢   It takes into account irrelevant considerations; or
     ‚Ä¢   It fails to take into account relevant considerations.

  Where an institution makes an error in the exercise of discretion, the IPC may send the
  matter back to the institution to re-exercise discretion.


Harms Test
  Some exemptions in the legislation are harms-based. In order to apply these
  exemptions under the legislation, institutions or third parties may have to demonstrate
  the harms that could result in the disclosure of information.

  In general, meeting the criteria of a harms test requires:

     ‚Ä¢   Detailed and convincing evidence; and
     ‚Ä¢   A strong connection between the harm and disclosure of the record.

  More information on how harms tests are considered within the context of specific
  exemptions is discussed in detail in Chapter 5: Exemptions and Exclusions.


Public Interest Override
  FIPPA s. 23 / MFIPPA s. 16

  The public interest override provision provides another opportunity to consider whether
  a record should be disclosed for some exempted records.

  A two-part test determines whether public interest override applies to a record:

     ‚Ä¢   There must be a compelling public interest; and
     ‚Ä¢   The compelling public interest must clearly outweigh the purpose of the
         exemption.

  Freedom of Information and Protection of Privacy Manual                                   39
  This section of the legislation states that certain exemptions do not apply when a
  compelling public interest in the disclosure of a record clearly outweighs the purpose of
  the exemption.

  This section is commonly referred to as public interest override. It cannot be applied to
  information withheld under the following exemptions:

     ‚Ä¢   Cabinet records (FIPPA s. 12);
     ‚Ä¢   Defence (FIPPA s.16);
     ‚Ä¢   Draft by-laws, etc. (MFIPPA s. 6);
     ‚Ä¢   Law enforcement (FIPPA s. 14, MFIPPA s. 8);
     ‚Ä¢   Solicitor-client privilege (FIPPA s. 19, MFIPPA s. 12);
     ‚Ä¢   Information soon to be published (FIPPA s. 22, MFIPPA s. 15).

  Factors that Coordinators should consider when determining whether the public interest
  override applies include:

     ‚Ä¢   Where there is a relationship between the record and the legislation‚Äôs
         central purpose of shedding light on the operations of government?
     ‚Ä¢   Where the record serves the purpose of informing the public to make
         political choices and express public opinion? or
     ‚Ä¢   Whether the interest in the record is public or private.

  Generally, a public interest does not exist where the requester‚Äôs interests in a record are
  essentially private in nature.


Available for Public Review
  FIPPA s. 32, s. 33, s. 35, s. 45 / MFIPPA s. 25, s. 34

  The legislation requires that the Responsible Minister and institutions make certain
  information available to the public for review to support:

     ‚Ä¢   Public awareness of government information holdings;
     ‚Ä¢   Public access to information outside of the formal information request
         process, and
     ‚Ä¢   An individual‚Äôs ability to contact institutions and request information.




  Freedom of Information and Protection of Privacy Manual                                 40
Institution Records
  FIPPA s. 33

  The legislation requires that certain records of institutions be available to the public for
  review. The requirement applies where records about the interpretation of laws and
  programs of the institution are necessary for the purpose of:

     ‚Ä¢   Determining applications by individuals for rights, privileges or benefits;
     ‚Ä¢   Changes to the provision or the new conditions of rights, privileges, or
         benefits already granted; and
     ‚Ä¢   General administration or enforcement.

  Other such records of institutions include manuals, directives, guidelines, instructions,
  procedures, objectives prepared for the officers of an institution. The publishing
  requirement does not apply to internal operations and administration of the institution
  (e.g., equipment manual).

  The documents required for publishing are subject to the same exemptions under the
  legislation. Portions can be severed if they are exempt from disclosure under the
  legislation. Any severing or deletion must include a statement that a deletion has been
  made, the nature of the information deleted and the exemption applied.

  These documents must be available for inspection and copying by the public, in these
  locations:

     ‚Ä¢   On the Internet; or
     ‚Ä¢   In a reading room, library, or designated office.

  Available for public review does not imply the record is provided to the public at no cost.
  Fees may be associated with accessing certain institutional records.


Directory of Institutions
  FIPPA s. 31, s. 35, s. 36 / MFIPPA s. 24

  The Directory of Institutions (DOI) is a compilation of all institutions covered by the
  legislation. The DOI sets out where a request for information should be made by
  providing the title, address and other contact information for the individual responsible
  for administering the legislation in the institution.

  Publication of the DOI is a responsibility of the Minister and is coordinated by MGCS
  staff. The DOI is published every three years.
  Freedom of Information and Protection of Privacy Manual                                     41
Directory of Records
  FIPPA s. 32, s. 35, s. 45 / MFIPPA s. 25, s. 34

  The Directory of Records (DOR) is a publication that provides:

     ‚Ä¢   A description of the programs, functions or responsibilities of offices within
         an institution;
     ‚Ä¢   Information on the general classes or types of records of each institution;
         and
     ‚Ä¢   An index of the personal information banks maintained by each institution.

  Publication of the DOR for Ontario Public Service ministries is a responsibility of the
  Minister and is coordinated by MGCS staff. The head of each provincial institution is
  responsible for providing this information to the Minister for publication.

  Hospitals, universities, some provincial agencies and MFIPPA institutions are also
  required to make similar information available about their institution but do so
  independently.

  Personal Information Banks
  FIPPA s. 44, s. 45 / MFIPPA s. 34

  The index of Personal Information Banks (PIB) forms part of the DOR. A PIB is any
  collection or set of personal information where personal information is organized by:

     ‚Ä¢   The individual‚Äôs name;
     ‚Ä¢   An identifying number or symbol; or
     ‚Ä¢   Other particular identifier assigned to the individual.

  A PIB could be an electronic database or a paper filing system.

  The PIB index sets out the following for each PIB:

     ‚Ä¢   Its name and location;
     ‚Ä¢   Legal authority for its establishment;
     ‚Ä¢   Types of personal information maintained in it;
     ‚Ä¢   How the personal information is used on a regular basis;
     ‚Ä¢   To whom the personal information is disclosed on a regular basis;
     ‚Ä¢   Categories of individuals about whom personal information is maintained;
         and


  Freedom of Information and Protection of Privacy Manual                                   42
     ‚Ä¢   Policies and practices applicable to the retention and disposal of the
         personal information; and
     ‚Ä¢   Exceptions to the uses and disclosures noted above.


Routine Disclosure and Open Government
  FIPPA s. 63 / MFIPPA s. 50

  The legislation does not prevent institutions from providing information to the public
  through alternative means outside of the formal request process. Institutions may
  proactively make records available through routine disclosure or other initiatives.

  ‚ÄúOpen Government‚Äù refers to government programs aimed at improving public access
  to government information and data; and increased public participation in policy
  development and dialogue between individuals and the government.

  The IPC is a strong supporter of Open Government as it enhances transparency of
  government actions and decisions, improves accessibility of government services and
  information, and promotes public participation.

  Coordinators and Legal Counsel should participate in routine disclosure or Open
  Government initiatives to ensure that the information and raw data being published does
  not reveal personal information of identifiable individuals or other sensitive information.


Obligations to Disclose
  FIPPA s. 11 / MFIPPA s. 5

  The legislation requires disclosure of a record that reveals a grave environmental,
  health or safety hazard to affected persons or the public, and where it is in the public
  interest.

  This section overrides all other provisions of the legislation. There is no requirement that
  a request must be made before action is taken. While this section includes a notice
  provision to any person to whom the information relates, it must be practicable to do so.
  The record must be disclosed as soon as possible. Disclosure is to be made by
  announcement to the public generally or to those individuals that are particularly
  affected by the information in the record.

  The following conditions must be met:

     ‚Ä¢   The information must be in record form; and

  Freedom of Information and Protection of Privacy Manual                                    43
    ‚Ä¢   The situation must be grave meaning serious and likely to produce great
        harm or danger.


Resources
 MGCS: Directory of Institutions

 IPC: Accessing Information

 IPC: Open Government




 Freedom of Information and Protection of Privacy Manual                          44
Chapter 5: Exemptions and Exclusions

Introduction
  As discussed in Chapter 4: Access Fundamentals, the legislation provides a general
  right of access to records in the custody or control of institutions, subject to limited and
  specific exemptions.

  This chapter will review the exemptions to the right of access. Some exemptions exist
  only in FIPPA or MFIPPA. This guide will indicate the applicable legislation for each
  exemption.

  The legislation also outlines a number of classes of records that are excluded from the
  legislation in their entirety. This chapter will review in detail these exclusions and
  indicate the applicable legislation for each exclusion.

  This chapter does not identify all the interpretations or issues related to these
  exemptions and exclusions that have been considered by the IPC and the Courts. As
  such, this chapter is not meant to be exhaustive, but only to summarize how the
  exemptions and exclusions generally operate.

  The law relating to the interpretation of these exemptions and exclusions is constantly
  evolving and it is up to individual reader to ensure that their understanding of a provision
  is up to date. Coordinators should always refer to the legislation for the exact language
  of a provision they are considering and not rely solely on the paraphrasing or
  descriptions of the provisions contained in this chapter.


Exemptions
  Exemptions to the general right of access are either mandatory or discretionary. The
  sections below will review each exemption and outline:

     ‚Ä¢   The nature of the exemption;
     ‚Ä¢   Any legal tests that may apply;
     ‚Ä¢   Factors to consider in determining whether the exemption applies to a
         record;
     ‚Ä¢   Any exceptions to the exemption that exist;
     ‚Ä¢   Whether the public interest override applies to the exemption.

  Note that a record may contain information subject to more than one exemption.


  Freedom of Information and Protection of Privacy Manual                                    45
Draft By-Laws and Closed Municipal Meetings
  MFIPPA s. 6

  Draft By-Laws
  The discretionary exemption protects draft by-laws or draft private bills that have not
  been considered in an open meeting. The term "considered" involves examination or
  deliberation. Only the draft by-law itself would be exempt as this provision does not
  exempt from disclosure records that would reveal the contents of drafts.

  For example, disclosing background records used in preparing the draft by-law may
  allow an accurate inference to be drawn about the nature of the draft by-law but this
  exemption cannot be applied to prevent their release.

  Closed Meetings
  This discretionary exemption also protects from disclosure the deliberations at meetings
  of a council, board, commission, or other body, or a committee of any of those bodies
  made in camera (meaning in private or in absence of the public). For this exemption to
  apply, the in camera nature of the meeting must be authorized by a statute.

  This exemption also permits the institution to prevent disclosure of a record which
  reveals the substance of deliberations of a closed meeting of a council, board,
  commission or other body or a committee of one of them. In order to qualify for this
  exemption, the institution must establish that:

     ‚Ä¢   A meeting was held in the absence of the public;
     ‚Ä¢   A statute authorizes the holding of the meeting in the absence of the public;
         and
     ‚Ä¢   Disclosing the record would reveal the actual substance of deliberations of
         the meeting.

  The term "substance of deliberations" has been interpreted to mean more than the
  subject of the deliberations but instead the actual substance of the deliberations. The
  exemption has been found to not protect records that merely refer to matters discussed
  at the in camera meeting. For example, the exemption would not apply to names of
  attendees or dates, times, and locations of meetings.

  A distinction, however, must be made between the results of the deliberations and the
  subject matter. A mere disclosure or reporting of a decision made at an in camera
  meeting cannot be characterized as a "consideration" of the subject matter of the in



  Freedom of Information and Protection of Privacy Manual                                   46
  camera deliberations. As well, a discussion of the product or results of deliberations
  does not necessarily reveal details about subject matter discussed in camera.

  For example, municipal councillors deliberated on the line items of a budget during an in
  camera meeting and later the consolidated budget was formally adopted at a public
  meeting. The consolidated budget was found by the IPC to be only the product of the
  subject matter of the deliberations in camera (regarding the line items), rather that the
  subject matter of the deliberations itself. Therefore, the deliberations about the line
  items in the budget were not found to have been ‚Äúconsidered‚Äù at the public meeting, and
  were still subject to this exemption.

  Exceptions
  For draft by-laws and private bills, this exemption does not apply where the draft has
  been later considered in open meetings.

  With respect to in camera deliberations, if the subject matter of the deliberations is later
  considered in an open meeting, this exemption no longer applies to the record.

   For both the draft by-law and the closed meetings, the exemption does not apply to
  records more than 20 years old.

  Public Interest Override
  The public interest override does not apply to this exemption.


Cabinet Records
  FIPPA s. 12

  This mandatory exemption protects deliberations of the Executive Council and its
  committees from disclosure. Deliberations have been interpreted as discussions
  conducted with a view towards making a decision.

  The FIPPA exemption for Cabinet records applies to the Executive Council (Cabinet) or
  its committees including:

     ‚Ä¢   Treasury Board/Management Board of Cabinet,
     ‚Ä¢   Legislation and Regulations Committee, and
     ‚Ä¢   Cabinet policy committees.

  A current list of Cabinet policy committees can be found on the Ontario government‚Äôs
  website.

  Freedom of Information and Protection of Privacy Manual                                   47
Evidence that a record was either sent to Cabinet or its committees for its deliberation
or was prepared with the specific intention of presenting it to Cabinet or its committees,
is critical in determining if this exemption applies to a record.

In rare cases, records do not have to be directly sent to Cabinet or its committees to be
exempt. Where the disclosure of records would permit a reader to draw an accurate
inference concerning the substance of deliberations, the exemption would likely apply.

An institution has the right to claim the exemption even if the record was disclosed
without the knowledge of the institution. However, where an issue or matter never gets
to Cabinet or its committees and where there is no prospect that it ever will, the
exemption cannot be claimed.

This exemption is broadly defined. Any record that would reveal the substance of
deliberations at Cabinet or its committees is subject to the exemption. This section also
provides a non-exhaustive list of types of records that are included in this exemption:

   ‚Ä¢   An agenda, minute or other record of deliberations or decisions;
   ‚Ä¢   Policy options or recommendations submitted or prepared for submission;
   ‚Ä¢   Background explanations or analyses of problems submitted or prepared
       for submission for consideration before decisions are made and
       implemented;
   ‚Ä¢   The subject of consultations among ministers on matters relating to
       government decision-making or the formulation of policy;
   ‚Ä¢   Briefing materials for a minister in relation to matters before or proposed; or
       are the subject of consultations among ministers relating to government
       decision-making or the formulation of policy; and
   ‚Ä¢   Draft legislation or regulations.

A discussion of policy options and recommendations (e.g., Cabinet submission) that is
exempt under this section will continue to be exempt from disclosure even after the
related decisions is made. Records about the implementation of a policy or
recommendation that was previously approved may still include policies or
recommendations and therefore may still be exempt.

Background explanations and analyses (e.g., briefing notes) must be submitted or
prepared for submission to Cabinet in order to be exempt under this section. This type
of background information is time limited and is only exempt until steps are taken to give
effect to a decision.

Consultations among ministers may involve records such as memoranda to and from
ministers, and minutes of meetings. Consultations among deputy ministers and public
Freedom of Information and Protection of Privacy Manual                                  48
servants are not exempt unless those consultations would reveal the substance of
deliberation of Cabinet or its committees.

Briefing a Minister usually involves records prepared by an institution‚Äôs staff or a
minister‚Äôs political staff. The reason why the record was prepared should be clear, and
linked to matters currently before or proposed to be brought before Cabinet in order to
claim the exemption.

Draft legislation and regulations are exempt from disclosure until the draft has been
considered by Cabinet and Cabinet has consented to the public distribution of the draft
for comment. A Minister can approve sharing draft statute and regulations with
interested parties in the development process of the legislation.

Exceptions
An institution must, on request, disclose a Cabinet record that is more than 20 years
old.

Records that are 20 years old or less can be disclosed where the Cabinet for which the
record was prepared gives consent. One Cabinet cannot consent to the release of
another's records. A Cabinet is considered to have changed where there has been an
election or a change of government. Consent of a previous government cannot be
practically sought.

While an institution is not required to seek consent of a current Cabinet, it should
consider the merits of seeking Cabinet consent in every case because it can be an
issue raised on appeal. Whether to seek consent should take into consideration:

   ‚Ä¢   The subject matter;
   ‚Ä¢   If the government policy has been announced or implemented;
   ‚Ä¢   If disclosure would reveal the nature of Cabinet discussion; and
   ‚Ä¢   If the record has, in fact been considered by Cabinet.

Public Interest Override
The public interest override does not apply to this exemption.




Freedom of Information and Protection of Privacy Manual                                 49
Advice to Government/Advice or Recommendations
  FIPPA s. 13 / MFIPPA s. 7

  This discretionary exemption is called Advice to Government under FIPPA and Advice
  or Recommendations under MFIPPA. There are minor differences in wording between
  the two legislations.

  Under FIPPA, the advice and recommendations must be given by a public servant, any
  person employed in the service of the institution, or a consultant retained by the
  institution. A consultant provides professional services under a formal agreement.

  Under MFIPPA, the advice and recommendations must be given by an officer or
  employee of an institution, or a consultant retained by an institution. An officer is
  considered a high ranking individual in municipal government who has management
  and administrative functions. Municipal or city councillors are not officers.

  A record continues to be exempt under this exemption, even if the institution has
  completed its decision-making, or acted on the recommendation at issue.

  ‚ÄúAdvice‚Äù and ‚Äúrecommendations‚Äù have two distinct meanings.

  Recommendations refer to a suggested course of action that will ultimately be accepted
  or rejected by the person being advised. Recommendations can be expressed or
  inferred.

  Advice has a broader meaning and can include ‚Äúpolicy options.‚Äù This can include:

      ‚Ä¢   Lists of alternative courses of action to be accepted or rejected in relation
          to a decision that is to be made;
      ‚Ä¢   An employee‚Äôs identification and consideration of alternative decisions that
          could be made; and
      ‚Ä¢   Views and opinions of an employee as to the range of policy options to be
          considered by a decision-maker, even if they do not include a
          recommendation.

  If an accurate inference concerning advice and recommendations may be drawn from a
  record it would be exempt. The exemption applies to both draft documents that have not
  yet been given to decision-makers as well as finalized documents that have been
  delivered to decision-makers for consideration.




 Freedom of Information and Protection of Privacy Manual                                  50
Exceptions
This exemption sets out a number of exceptions that are not considered advice or
recommendations. These exceptions ensure that factual information often found in
reports is available to the public, if other exemptions do not apply to the records.

A report has been interpreted as a formal statement or account of the results of the
collation and consideration of information. A report is not just observations or recordings
of fact.

The following lists the types of information and reports that are exceptions to the
exemption and may be disclosed (provided that they are not subject to other
exemptions):

   ‚Ä¢   Factual material;
   ‚Ä¢   A statistical survey;
   ‚Ä¢   A report by a valuator;
   ‚Ä¢   An environmental impact statement or similar record;
   ‚Ä¢   A report of a test carried out on a product for the purpose of government
       equipment testing or a consumer test report (FIPPA only);
   ‚Ä¢   A report or study on the performance or efficiency of an institution;
   ‚Ä¢   A feasibility study or other technical study;
   ‚Ä¢   A report containing the results of field research;
   ‚Ä¢   A final plan or proposal to change a program of an institution, or for the
       establishment of a new program;
   ‚Ä¢   A report of an interdepartmental committee task force or similar body, or of
       a committee or task force within an institution, which has been established
       for the purpose of preparing a report on a particular topic;
   ‚Ä¢   A report of a committee, council or other body which is attached to an
       institution and which has been established for the purpose of undertaking
       inquiries and making reports or recommendations to the institution;
   ‚Ä¢   The reasons for a final decision, order or ruling of an officer of the institution
       made during or at the conclusion of the exercise of discretionary power
       conferred by or under an enactment or scheme administered by the
       institution.

Under both FIPPA and MFIPPA, the exemption does not apply to records that are more
than twenty years old.




Freedom of Information and Protection of Privacy Manual                                     51
 Further, under FIPPA only, this exemption does not apply to a record that has been
 publicly cited by the head of an institution as a basis for making a decision or
 formulating policy.

 Public Interest Override
 The public interest override applies to this exemption.


Law Enforcement
 FIPPA s. 14 / MFIPPA s. 8

 The discretionary exemption for law enforcement protects various types of records and
 activities relating to justice issues such as:

    ‚Ä¢   Policing;
    ‚Ä¢   Investigations;
    ‚Ä¢   Prosecutions;
    ‚Ä¢   Court proceedings;
    ‚Ä¢   Intelligence information;
    ‚Ä¢   Crime prevention;
    ‚Ä¢   Corrections; and
    ‚Ä¢   Safety and security.

 The legislation defines law enforcement as:

    a) Policing,
    b) Investigations or inspections that lead or could lead to proceedings in a
       court or tribunal if a penalty or sanction could be imposed in those
       proceedings, or
    c) The conduct of proceedings referred to in (b).

 The context for law enforcement records has been found to be broader than criminal
 law and policing. The exemption applies to all federal and provincial laws and municipal
 by-laws that provide authority for law enforcement.

 Some institutions also have broad regulatory and law enforcement powers established
 by a statute. Examples include the Ministry of Labour‚Äôs ability to investigate under the
 Occupational Health and Safety Act and the Office of the Fire Marshal under the Fire
 Prevention and Protection Act.



 Freedom of Information and Protection of Privacy Manual                                52
An institution‚Äôs records may fall under the law enforcement exemption because the
institution:

   ‚Ä¢   Has law enforcement responsibilities for other organizations; or
   ‚Ä¢   Is the subject of an investigation.

An institution is not required to carry out law enforcement activities for the exemption to
apply to a record under its custody or control.

The exemption allows an institution to refuse to confirm or deny the existence of a
record under this exemption where:

   ‚Ä¢   The record, if it exists, would quality for exemption under this section; and
   ‚Ä¢   Disclosure of whether the record does or does not exist would, in and of
       itself, disclose enough information that could reasonably be expected to
       harm an interest protected by the exemption.

A harms test applies to where the phrase ‚Äúcould reasonably be expected to‚Äù is used.
Detailed and convincing evidence has been found to be necessary is order to show that
the risk of harm is well beyond the merely possible or speculative although it need not
prove that that disclosure will in fact result in such harm.

The law enforcement exemption has several subsections that address a range of
scenarios.

Each of the scenarios listed in the subsections of the exemption will be discussed
below. A head may refuse to disclose a record where the disclosure could reasonably
be expected to:

Interfered with a Law Enforcement Matter
This exemption applies if disclosure could reasonably be expected to interfere with a
law enforcement matter.

‚ÄúInterfere‚Äù has been interpreted to mean that the disclosure would have the effect of
hindering or impeding the carrying out of a law enforcement activity. Interfere does not
mean that disclosure would altogether prevent a law enforcement investigation from
taking place, but rather that disclosure would frustrate or impede the carrying out of an
investigation.

A ‚Äúmatter‚Äù may go beyond a specific investigation, such as a firearm registry database
created and used by the police or a prosecution before a court.


Freedom of Information and Protection of Privacy Manual                                  53
For this exemption to apply, the law enforcement matter must be ongoing.

Interfere with a Law Enforcement Investigation
An institution may refuse to disclose a record where the disclosure could reasonably be
expected to interfere with an investigation undertaken with a view to a law enforcement
proceeding or from which a law enforcement proceeding is likely to result.

An "investigation" is the methodical determination of facts and gathering of evidence. In
some cases, the evidence gathered in an investigation will be insufficient to support the
commencement of a proceeding in a court or tribunal. A record of the investigation
could still be exempt, however, since it is undertaken with a view to a law enforcement
proceeding.

An ‚Äúinternal or employment-related investigation‚Äù may also be a ‚Äúlaw enforcement
investigation‚Äù if it:

   ‚Ä¢   Involves the police, or result in subsequent police investigation; or
   ‚Ä¢   Leads to a proceeding that could result in internal discipline, termination of
       an employee or other actions.

Reveal Investigative Techniques
This subsection applies where disclosure could reasonably be expected to reveal
investigative techniques and procedures in use or likely to be used in law enforcement.
Institutions should be able to demonstrate that disclosure of the technique or procedure
to the public would hinder or compromise its effective utilization. If the technique or
procedure is generally known to the public, reliance on this exemption would not be
successful.

Reveal a Confidential Source
An institution may refuse disclosure where it would reveal the identity of a confidential
source of information in respect of a law enforcement matter, or disclose information
furnished only by the confidential source.

A ‚Äúconfidential source of information‚Äù must have a reasonable expectation that the
information provided would be kept confidential, based on the sensitivity of the matter
and seriousness of consequences.




Freedom of Information and Protection of Privacy Manual                                     54
Endanger the Safety of a Law Enforcement Officer or Any Other Person
An institution may refuse disclosure of a record where it would endanger the safety of a
law enforcement officer or any other person. This provision is similar to the danger to
safety and health exemption in the legislation.

Compromise a Fair Trial or Impartial Adjudication
This exemption prevents premature disclosure of information that could deprive a
person of a fair trial or impartial adjudication. Once the proceeding has been completely
disposed of (including appeals), the exemption no longer applies. In order to
demonstrate unfairness under this subsection, the institution must produce more
evidence than the mere commencement of a legal action. The institution must present
specific arguments as to how or why disclosure of specific parts of the record could
reasonably be expected to deprive a person of a fair trial or impartial adjudication.

This subsection does not contain a reference to law enforcement and, accordingly, the
exemption could apply to proceedings that do not fall within the definition of law
enforcement such as tribunals established by law to adjudicate individual or collective
rights. There must be evidence that the disclosure of the records would result in
unfairness.

The term ‚Äúperson‚Äù is not limited to a specific person and may include some unknown
person or persons in the future.

Reveal Intelligence Information
This subsection exempts from disclosure records where the disclosure could reasonably
be expected to interfere with the gathering of or reveal law enforcement intelligence
information respecting organizations or persons.

‚ÄúIntelligence information‚Äù is defined as information gathered by a law enforcement
agency in a covert manner with respect to ongoing efforts devoted to the detection and
prosecution of crime or the prevention of possible violation of law.

Reveal Confiscated Records
This exemption applies where disclosure would actually release confiscated records or
could reasonably be expected to reveal records confiscated by a peace officer in
accordance with an act or regulation.




Freedom of Information and Protection of Privacy Manual                                55
Endanger the Security of Property
Disclosure may be refused where it could reasonably be expected to endanger the
security of a building or the security of a vehicle carrying items (e.g., things or articles),
or of a system or procedure established for the protection of items, for which protection
is reasonably required.

Facilitate Escape
Records are exempt where the disclosure could reasonably be expected to facilitate the
escape from custody of a person who is under lawful detention. Custody indicates that
an individual is not free to leave a place of confinement without restriction. In general,
any person held in custody pursuant to a valid warrant or other authorized order is
under lawful detention.

The term "facilitate" means make easier or less difficult. The exemption has been found
to apply, for example, to construction plans and specifications regarding a maximum
security facility. It is not necessary that the plans be extremely detailed.

The fact that the plans for the secured facility were available to the public in the past
does not mean that this section requires that they continue to be available. There must
still be a determination of whether the current plans under the current circumstances
would reasonably be expected to facilitate an escape.

Jeopardize the Security of a Centre for Lawful Detention
This provision exempts records where disclosure could reasonably be expected to
jeopardize the security of a centre for lawful detention. This includes records containing
details of previous investigations of escape attempts and details of security measures in
place.

Facilitate an Unlawful Act
Records are exempt where the disclosure could reasonably be expected to facilitate the
planning or committing of an unlawful act or hamper the control of a crime. ‚ÄúUnlawful
conduct‚Äù means a violation of a statute or regulation or of a municipal by-law.

The exemption also lists a number of records that may be exempted based on their type
or nature. For these records, the institution does not need to prove that the disclosure of
the record would result in a specific harm unless the wording of the subsection indicates
that requirement. A head may refuse a record that is:




Freedom of Information and Protection of Privacy Manual                                     56
A Law Enforcement Report
This subsection exempts from disclosure a report prepared in the course of law
enforcement inspections or investigations by an agency responsible for enforcing and
regulating compliance with a law.

A "report" must consist of a formal statement or account of the results of the collation
and consideration of information. Generally speaking, reports would not include mere
observations or recordings of fact.

"Agency" includes organizations acting on behalf of or as agents for law enforcement
agencies.

Protected by an Act of Parliament
This subsection exempts a law enforcement record where disclosure would be an
offence under an Act of Parliament.

For example, the Youth Criminal Justice Act makes it an offence to knowingly disclose
certain court, police and government records relating to young offenders, except as
authorized by that Act.

Could Expose Someone to Civil Liability
This subsection exempts a law enforcement record where disclosure could reasonably
be expected to expose the author of the record, or any person who had been quoted or
paraphrased in the record, to civil liability.

Civil liability could include lawsuits against law enforcement officials, witnesses or
informants for defamation.

Related to a Person Under the Control or Supervision of a Correctional Authority
This subsection exempts records that contain information relating to an individual's
correctional history while the individual is under the control or supervision of a
correctional authority.

This exemption applies to individuals on parole, probation, a temporary absence permit,
under bail supervision or performing community service work.




Freedom of Information and Protection of Privacy Manual                                    57
  Exceptions
  There are two categories of exceptions to the exemption:

     ‚Ä¢   Routine inspection reports; and
     ‚Ä¢   Law enforcement program success (e.g., statistical analysis).

  As discussed above, reports prepared in the course of law enforcement, inspections or
  investigations are a type of report which is exempt from disclosure, but routine
  inspection reports are not included in this exemption. ‚ÄúRoutine inspections‚Äù are
  inspections that are carried out by an agency with statutory authority to enforce and
  regulate compliance with standards (e.g. the enforcement or compliance branch of an
  institution); and where there are no specific allegations that standards have been
  breached. However, other exemptions, such as personal privacy, may apply to portions
  of these records.

  The exception for law enforcement program success means the exemption cannot be
  applied to statistical analyses of law enforcement programs unless the disclosure of
  such a record may prejudice, interfere with or adversely affect any of the matters
  referred to in those sections.

  Public Interest Override
  The public interest override does not apply to this exemption.


Civil Remedies Act, 2001
  FIPPA s. 14.1 / MFIPPA s. 8.1

  This discretionary exemption allows institutions to withhold information that could
  reasonably be expected to interfere with the ability of the Attorney General to determine
  whether a proceeding should be commenced under the Civil Remedies Act, 2001,
  conduct a proceeding under that Act, or enforce an order under that Act.

  Institutions may also refuse to confirm or deny the existence of a record if it would
  likewise impact the ability of the Attorney General to undertake the same processes
  related to the Civil Remedies Act, 2001.

  Public Interest Override
  The public interest override does not apply to this exemption.



  Freedom of Information and Protection of Privacy Manual                                 58
Prohibiting Profiting from Recounting Crimes Act, 2002
  FIPPA s. 14.2 / MFIPPA s. 8.2

  This discretionary exemption allows institutions to withhold information that could
  reasonably be expected to interfere with the ability of the Attorney General to determine
  whether a proceeding should be commenced under the Prohibiting Profiting from
  Recounting Crimes Act, 2002, conduct a proceeding under that Act or enforce an order
  under that Act.

  Institutions may also refuse to confirm or deny the existence of a record if it would
  likewise impact the ability of the Attorney General to undertake the same processes
  related to the Prohibiting Profiting from Recounting Crimes Act, 2002.

  Public Interest Override
  The public interest override does not apply to this exemption.


Relations with Other Governments
  FIPPA s. 15 / MFIPPA s. 9

  The FIPPA exemption for relations with other governments is discretionary; however, it
  is a mandatory exemption under MFIPPA. While this exemption is discretionary under
  FIPPA, if an institution wishes to disclose records for which the head has reason to
  believe this exemption applies, the head must obtain prior approval from the Executive
  Council.

  Under both statutes, this exemption protects records which could reasonably be
  expected to reveal information received in confidence by an institution from another
  government or its agencies, or an international organization of states or its bodies.

  Under FIPPA, this exemption also applies where the disclosure of the record could
  reasonably be expected to prejudice the conduct of intergovernmental relations.

  "Other governments" include the:

     ‚Ä¢   Government of Canada;
     ‚Ä¢   Government of Ontario (for MFIPPA institutions);
     ‚Ä¢   Another provincial or territorial government;
     ‚Ä¢   A government of a foreign country or state;
     ‚Ä¢   An agency of government referred to above; or
     ‚Ä¢   An international organization of states (e.g. United Nations).

  Freedom of Information and Protection of Privacy Manual                                 59
  Ontario municipalities are not considered ‚Äúother governments‚Äù under either legislation.

  There are further differences in the wording of this exemption between FIPPA and
  MFIPPA. For example, within MFIPPA, the exemption explicitly states that institutions
  shall disclose a record to which the exemption applies if the government, agency or
  organization from which the record was received consents to the disclosure. There is no
  similar expressed provision in FIPPA.

  The types of records that may come under this exemption include letters, meeting notes
  or minutes, transcripts of confidential meetings, draft agreements, briefing materials,
  presentations, and reports.

  An institution must provide evidence that the record was received implicitly in
  confidence. For example, a record may be clearly marked ‚Äúin confidence‚Äù or the parties
  may have an agreement that supports confidentiality.

  Public Interest Override
  The public interest override applies to this exemption.


Relations with Aboriginal Communities
  FIPPA s. 15.1 / MFIPPA s. 9.1

  This discretionary exemption would allow institutions to withhold records where
  disclosure could reasonably be expected to prejudice the conduct of relations between
  an Aboriginal community and the institution; or reveal information received in confidence
  from an Aboriginal community by an institution.

  The exemption is similar to the exemptions for relations with other governments.

  In the legislation, an ‚ÄúAboriginal community‚Äù means:

     a) A band within the meaning of the Indian Act (Canada),
     b) An Aboriginal organization or community that is negotiating or has
        negotiated with the Government of Canada or the Government of Ontario
        on matters relating to,
           i.  Aboriginal or treaty rights under section 35 of the Constitution Act,
               1982, or
          ii.  A treaty, land claim or self-government agreement, and
     c) Any other Aboriginal organization or community prescribed by the
        regulations.


  Freedom of Information and Protection of Privacy Manual                               60
 The regulations under FIPPA and MFIPPA do not, at this time, identify any other
 ‚ÄúAboriginal organizations.‚Äù

 In Ontario, Aboriginal people belong to a rich and diverse range of communities,
 cultures, membership and affiliations. Section 35 of the Constitution Act, 1982 uses the
 term ‚Äúaboriginal peoples of Canada‚Äù to include the ‚ÄúIndian, Inuit and M√©tis peoples of
 Canada‚Äù.

 If it is unclear if a specific community or organization is captured in the definition,
 Coordinators may wish to seek advice from Legal Counsel.

 Public Interest Override
 The public interest override applies to this exemption.


Defence
 FIPPA s. 16

 This discretionary exemption protects the national defence of Canada and international
 relations. Defence includes the prevention of attacks or other acts of aggression. The
 exemption also addresses espionage, sabotage and terrorism. The focus is on
 prejudice and injury that may result from disclosure.

 The exemption extends to protect foreign states allied or associated with Canada from
 prejudice or injury resulting from disclosure of a record. An allied state is one with which
 Canada has concluded formal alliances or treaties. An associated state is a state with
 which Canada may be linked for trade or other purposes outside the scope of a formal
 alliance.

 Defence is primarily a federal concern. However, the Ontario government may have
 records that relate to national defence as a result of working in areas of broader
 international concern or negotiations such as:

    ‚Ä¢   The environment;
    ‚Ä¢   Energy;
    ‚Ä¢   Emergency planning;
    ‚Ä¢   Immigration;
    ‚Ä¢   Economic development,
    ‚Ä¢   Trade;
    ‚Ä¢   Education; and
    ‚Ä¢   Cultural and social matters.

 Freedom of Information and Protection of Privacy Manual                                   61
  The Ontario government is consulted by the federal government and may act on its own
  in contacting representatives of other governments. As such, there is a range of
  provincial diplomatic activity which generates records relating to international relations.

  Factual information relating to defence or international relations can be sensitive and
  require protection from disclosure. Factual information may include technical and non-
  technical information.

  It is difficult to predict future events and what information may be of interest to a foreign
  government or a hostile party. Whether portions or types of information are available
  from other public sources has been found to be not, on its own, a determinative factor.
  The inclusion of factual or other information in a record exempt under defence may
  result in all of the information in the record being exempt.

  An institution must seek Cabinet consent to disclose a record under the defence
  exemption.

  Public Interest Override
  The public interest override does not apply to this exemption.


Third Party Information
  FIPPA s. 17 / MFIPPA s. 10

  This mandatory exemption protects confidential information supplied to institutions by a
  third party. A third party can be any supplier of information to an institution that meets
  the section requirements including a:

     ‚Ä¢   Person;
     ‚Ä¢   Group;
     ‚Ä¢   Committee;
     ‚Ä¢   Organization,
     ‚Ä¢   Institution; or
     ‚Ä¢   Business, including contracted vendors.

  Generally, an employee of an institution and other institutions under the legislation are
  not considered a third party.

  The exemption does not protect all third party information but informational assets that
  have ‚Äúvalue‚Äù such as:


  Freedom of Information and Protection of Privacy Manual                                    62
   ‚Ä¢   Trade secrets;
   ‚Ä¢   Scientific information;
   ‚Ä¢   Technical information;
   ‚Ä¢   Commercial information;
   ‚Ä¢   Financial information; and
   ‚Ä¢   Labour relations information.

Institutions typically have third party information because of:

   ‚Ä¢   Legal or regulatory requirements such as assessments or reporting; and
   ‚Ä¢   For the purchase of goods and services including information received in a
       competitive bidding process.

The exemption is intended to protect the position and interests of a third party rather
than the institution. The areas of possible harm are:

   ‚Ä¢   Significant prejudice to a competitive position or significant interference with
       contractual or other negotiations;
   ‚Ä¢   Similar information no longer being supplied to an institution where it is in
       the public interest that similar information continue to be so supplied;
   ‚Ä¢   Undue loss or gain;
   ‚Ä¢   Revealing information relating to a labour relations dispute; and
   ‚Ä¢   Revealing tax information relating to tax liabilities and collection (FIPPA
       only).

There must be a reasonable expectation of harm to the third party not merely
speculation of harm.

A three-part test must be met for the exemption to apply. Each part must apply in order
for the exemption to apply. If information fails one part of the three-part test, further
analysis would not be required and the exemption could not be claimed. The three-part
test requires that:

   1. The records must include information that is a trade secret, or scientific,
      technical, commercial, financial or labour relations information.
   2. a) The information must be supplied by the third party to the institution.
      b) The information must be supplied in confidence, implicitly or explicitly.
   3. The disclosure of the information could reasonably be expected to cause
      one or more of the specified harms above.



Freedom of Information and Protection of Privacy Manual                                   63
An institution may disclose records if the third party or third parties to whom the
information relates consent to the disclosure.

Part 1: Type of Information
The legislation identifies six types of information eligible for the exemption. The list
below provides a definition and examples of each. The terms have specific meanings
and are distinct from each other.

Trade secret: A formula, pattern, compilation, program, method, technique, or process
or information contained or embodied in a product, device or mechanism which (i) is, or
may be used in a trade or business, (ii) is not generally known in that trade or business,
(iii) has economic value from not being generally known, and (iv) is the subject of efforts
that are reasonable under the circumstances to maintain its secrecy. Examples include
software or hardware system, a vaccine formula, and an algorithm.

Scientific: An organized field of knowledge in either the natural, biological or social
sciences or mathematics that must also relate to observing and testing specific
hypotheses or conclusions and be undertaken by an expert in the field. Examples
include research, results of raw data analysis, and chemical substance testing.

Technical: An organized field of knowledge that would fall under the general categories
of applied sciences or mechanical arts, and will usually involve information prepared by
a professional in the field. Examples include records relating to architecture,
engineering, electronics, or construction including drawings, site plans and
specifications for building.

Commercial: The buying, selling or exchange of products or services, and can apply to
both profit-making and non-profit organizations. Examples include product information,
tenders, marketing strategies, cost quotations, price, supplier, and customer lists, client
information, and business proposals.

Financial: Specific data and information that relates to finance or money matters,
including the use and distribution of money. Examples include accounting methods,
financial statements, pricing practices, bid information, property tax information, sales
revenues, and employment costs.

Labour relations: The collective bargaining relationship between an employer and its
employees. Examples include records relating to the impact of human resources
policies, labour dispute plans and pay equity plans.




Freedom of Information and Protection of Privacy Manual                                     64
Part 2a: Supplied by a Third Party
Information is supplied when a third party gives or submits it to an institution and the
information is not subject to change. The manner in which information is supplied is not
relevant.

If a record includes information that may reveal or enable an inference to be made
about information supplied by a third party, it may still qualify as being supplied.

Below is a list of examples where information could be considered to be supplied by a
third party:

   ‚Ä¢   The third party is required by statute to supply the information;
   ‚Ä¢   The third party supplied the information in response to a request for
       proposal (RFP);
   ‚Ä¢   The third party supplies specific product or technical details as a schedule
       to an agreement or contract with an institution; and
   ‚Ä¢   The third party submits test results.

Information is not considered supplied when:

   ‚Ä¢   The institution produces or calculates the information independently;
   ‚Ä¢   The information is generated together by a third party and an institution;
   ‚Ä¢   The information is a product of negotiations; and
   ‚Ä¢   The information is merely about a third party.

Below is a list of examples where information would not be considered to be supplied by
a third party:

   ‚Ä¢   An employee of an institution performs an inspection of a third party;
   ‚Ä¢   A project status report;
   ‚Ä¢   A negotiated agreement between an institution and a third party; and
   ‚Ä¢   Procurement evaluation information (that does not include informational
       assets taken directly from a RFP, methodology, and results.

A negotiated agreement between an institution and a third party normally does not
qualify as having been supplied because terms of contracts are generally mutually
generated by both the institution and the third party, rather than ‚Äúsupplied‚Äù by the third
party. This applies even when the contract is preceded by little or no negotiation or
where the final agreement reflects information that originated form a single party.




Freedom of Information and Protection of Privacy Manual                                      65
There are two exceptions to this general rule which are described as the ‚Äúinferred
disclosure‚Äù and ‚Äúimmutability‚Äù exceptions. The ‚Äúinferred disclosure‚Äù exception applies
where disclosure of the information in a contract would permit accurate inferences to be
made with respect to underlying non-negotiated confidential information supplied by the
third party to the institution. The immutability exception arises where the contract
contains information supplied by the third party, but the information is not susceptible to
negotiation. An example would be schedules to a contract that include detailed price
lists for services or products supplied by the vendor.

Part 2b: Supplied in Confidence
A third party must have an expectation of confidentiality at the time the information was
supplied. The expectation can be:

   ‚Ä¢   Implicit ‚Äì meaning it is understood without being stated directly; or
   ‚Ä¢   Explicit ‚Äì meaning stated directly or explained so that you cannot doubt
       what is meant.

The main factors that have been used to determine reasonableness are set out below.

   ‚Ä¢   If the document marked as ‚ÄúConfidential‚Äù;
   ‚Ä¢   If the third party communicated to the institution that the record is
       confidential and is to be kept confidential; or
   ‚Ä¢   If a confidentiality clause in a contract.

Factors to consider when determining if the information was protected as confidential
include:

   ‚Ä¢   Was access to the record or information limited, such as being available on
       a ‚Äúneed to know‚Äù basis?
   ‚Ä¢   Was a policy for security or recordkeeping followed?
   ‚Ä¢   Hs the record been consistently treated as confidential by the third party
       through demonstrated concern for its protection from disclosure prior to
       disclosure to the institution?
   ‚Ä¢   Was the information available to the public through other sources?
   ‚Ä¢   Did the institution or third party publicly disclose the information?
   ‚Ä¢   Was the information guarded from competitors?
   ‚Ä¢   Was the information prepared for a purpose that would not include
       disclosure?
   ‚Ä¢   Does the record reveal the substance negotiations?



Freedom of Information and Protection of Privacy Manual                                  66
Part 3: Expectation of Harm
The discussion of harm is concerned with the third party whose information may be
disclosed, not the institution. In some cases a harm may also occur to another person,
group, or to an organization, other than the third party.

Both an institution and the third party should provide representations about harms. More
than one of the harms can apply to the information but at least one must apply.

The third party is in the best position to make a strong and informed argument regarding
the likelihood of harms. The particular facts of each case must be evaluated. The
evidence must be detailed and convincing.

The Courts and the IPC have confirmed that the ‚Äúcould reasonably be expected‚Äù
threshold does not require proof that harm is probable or that on a balance of
probabilities the harm will occur. There must, however, be a reasonable basis to support
the identified risk. The party relying on the exemption must show that the risk of harm is
well beyond the merely possible or speculative using detailed and convincing evidence.

Below is a discussion of some types of harms and circumstances where these harms
could be found to ‚Äúreasonably be expected to‚Äù result after the disclosure of the third
party information.

Competitive position: For this harm to meet the test, the impact must be significant.
The factors that have been found to be relevant include:

   ‚Ä¢   A competitive industry;
   ‚Ä¢   A critical success factor to a business;
   ‚Ä¢   Time and expense invested in development;
   ‚Ä¢   Usefulness to a competitor;
   ‚Ä¢   Detailed pricing or breakdown of pricing; and
   ‚Ä¢   A measureable harm or injury.

Interference with contractual obligations or negotiations: For this harm to meet the
test, the impact must be significant. This section is often applied in the context of
commercial or union negotiations that have not been finalized.

Similar information no longer supplied: For this harm to meet the test, the supply of
information must be voluntary and there must be a public interest in the continued
supply of the information. This harm is concerned with situations where a third party,
faced with the prospect that their information will be disclosed under the legislation, may
no longer voluntarily supply similar information in the future to the institution.

Freedom of Information and Protection of Privacy Manual                                  67
  Undue loss or gain: For this harm to meet the test, the loss or gain must be undue
  which means:

     ‚Ä¢   Excessive,
     ‚Ä¢   Disproportionate,
     ‚Ä¢   Not suitable, and
     ‚Ä¢   Not owed.

  Losses and gains are generally argued together because a loss to one party usually
  means a gain to another party. Losses or gains are generally related to investments of
  money, time, and effort; and impact revenues. The possibility of a law suit is not
  sufficient grounds for the exemption to apply

  The loss or gain can be to any person, group, committee, financial institution or agency,
  and not necessarily to the third party submitting the information.

  Labour relations disputes: For this harm to meet the test, individuals or third parties
  must be appointed under law to resolve a labour relations dispute. Examples include
  mediators, conciliation, review, and labour relations officers. Harm would be found to
  result if disclosure of the record could be reasonably expected to reveal the information
  produced for or prepared by these third parties.

  Tax Information
  Under FIPPA only, there is an additional exemption for tax information. Disclosure must
  reveal information obtained on a tax return, or information gathered to collect or
  determine a tax liability of a specific taxpayer.

  The tax information section only applies to FIPPA and the harms test of ‚Äúcould be
  reasonably expected to‚Äù does not apply.

  Public Interest Override
  The public interest override applies to this exemption.


Economic and Other Interests
  FIPPA s. 18 / MFIPPA s. 11

  This discretionary exemption allows institutions to protect certain proprietary information
  and prevent the premature disclosure of certain plans or negotiating strategies. This
  gives the Ontario government and institutions similar protection to that given to third
  parties under the third party information exemption.
  Freedom of Information and Protection of Privacy Manual                                 68
The protections are focused on issues such as:

   ‚Ä¢   Generating revenue,
   ‚Ä¢   Competitive position,
   ‚Ä¢   Monetary gain or loss, and
   ‚Ä¢   Strategic positioning.

The exemption covers various business roles and activities of the Ontario government
and institutions such as:

   ‚Ä¢   Engaging in commercial activities;
   ‚Ä¢   Conducting research;
   ‚Ä¢   Managing finances and the economy;
   ‚Ä¢   Carrying out negotiations;
   ‚Ä¢   Managing the administration of institutions and personnel;
   ‚Ä¢   Proposing plans, policies, and projects;
   ‚Ä¢   Conducting examinations and testing; and
   ‚Ä¢   Deciding on municipal boundary submissions.

The exemption protects the parties involved from gaining any advantages or
disadvantages from the disclosure of information. The exemption takes into account the
impact of premature disclosure of information in activities that are current or not yet
completed.

The protections are against prejudice or injury. A harms test applies to some but not all
of the subsections.

The exemption has nine subsections. Some of these refer to the Government of Ontario
broadly, some refer to an institution, and some refer to both.

In some subsections ‚Äúinformation‚Äù is referenced generally and in others specific types of
information or records are identified and the definitions of these terms must be applied
in the analysis.

Commercial Information
This exemption is commonly referred to as commercial information. The definitions are
the same as the third party information exemption, with the exception of labour relations
which is omitted. It has a three part test.

The institution must establish that the information contained in the record:



Freedom of Information and Protection of Privacy Manual                                69
    1) Is a trade secret, or financial, commercial, scientific or technical
       information; and
    2) Belongs to the Government of Ontario or an institution; and
    3) Has monetary value or potential monetary value.

Part 1: Type of information

This exemption identifies five types of information:

   ‚Ä¢   Trade secret;
   ‚Ä¢   Scientific information;
   ‚Ä¢   Technical information;
   ‚Ä¢   Commercial information; and
   ‚Ä¢   Financial information.

Definitions based on interpretations of these types of information can be found in the
above section on the third party exemption.

Part 2: Belongs to an institution

The information must belong to the Government of Ontario or an institution.

‚ÄúBelongs to‚Äù refers to more than just bare or simple ownership or possession. There
must also be some proprietary interest. A proprietary interest can be an intellectual
property interest. Examples include copyright or trademark.

A proprietary interest can also exist where the law would recognize a substantial
interest in protecting the information from misappropriation by another party. Examples
include trade secrets, customer or supplier lists, or price lists.

The information may belong to the institution with custody of the record or another
institution. If the information is in the public domain, the exemption may not apply.

Part 3: Monetary value

The information must have monetary value or potential monetary value which means
that the information can yield a price in the market or is potentially marketable.

Monetary value can be established by demonstrating:

   ‚Ä¢   A market or demand for information;
   ‚Ä¢   Similar records are available for a fee (more than just an administrative
       fee);

Freedom of Information and Protection of Privacy Manual                                  70
   ‚Ä¢   Willing buyers and sellers for the information; and
   ‚Ä¢   An intention to provide the information for monetary gain.

Employee Research
This exemption protects information obtained through employee research where
disclosure could reasonably be expected to deprive the employee‚Äôs priority of
publication.

The definition of research is the same as in other sections. Research has been defined
by the IPC as ‚Äúa systematic investigation designed to develop or establish principles,
facts or generalizable knowledge, or any combination of them, and includes the
development, testing and evaluation of research.‚Äù

The research must be linked to a specific researcher. There must be strong evidence of
an intention to publish such as:

   ‚Ä¢   A sworn affidavit;
   ‚Ä¢   A document prepared for internal peer review;
   ‚Ä¢   An announcement at a conference; or
   ‚Ä¢   Having data already available for publication.

The exemption does not apply to raw data.

Economic and Competitive Interests of an Institution
This exemption protects an institution‚Äôs economic interests and competitive position
against prejudice. This has been interpreted to not require that the information belong to
the institution that is claiming the exemption. To claim this exemption, the institution
must provide detailed and convincing evidence that disclosure could reasonably be
expected to prejudice the economic interests or competitive position of an institution.

Economic interests concern the production, distribution and consumption of goods and
services, related costs and prices.

Competitive position applies where institutions are engaged in the supply of goods and
services for profit and compete for business on a competitive basis.

The exemption does not apply where an institution has a monopoly or there are no
competitors. Also, certain information may lose value as it gets dated.

Questions to consider in determining if this exemption applies include: Could the
institution pay a higher price for goods and services if the information is disclosed?
Could the institution lose revenue if the information is disclosed?
Freedom of Information and Protection of Privacy Manual                                  71
An institution should have the ability to negotiate the best possible deal regardless of
the type of contract. Contractual negotiations are implicit in some of the examples
above but the exemption may also apply to financial settlements.

Relevant factors relevant in assessing whether records relating to contractual
negotiations are exempt could include:

   ‚Ä¢   Are the negotiations current?
   ‚Ä¢   Will disclosure affect the willingness of parties to negotiate in the future?
   ‚Ä¢   Will disclosure affect the institution‚Äôs ability maximize profits?
   ‚Ä¢   Will there be a chilling effect on future business ventures?
   ‚Ä¢   Will disclosure affect Ontario‚Äôs ability to prepare and submit competitive
       bids for industry (compared to other provinces)? or
   ‚Ä¢   Is a significant donation involved?

The exemption is found not to apply to contract information that may help a competitor
in other ways such as responsibility for delays, non-performance, revenue sharing, term
of the agreement, or termination provisions.

The exemption does not apply to disclosure of a final agreement unless it can be
established that other parties may use past information or reveal information about the
institution‚Äôs position, such as whether the institution is willing to absorb costs. Also, a
confidentiality clause in settlement agreements may not be enough for the exemption to
apply.

Government‚Äôs Financial Interests and Managing the Economy
 This exemption protects the broader economic interests of Ontario against injury.
Where economic interests are negatively affected, financial interests are usually also
affected. To claim this exemption the institution must provide detailed and convincing
evidence that disclosure could reasonably be expected to be injurious to the financial
interests of the government or to manage the economy of Ontario.

Examples of how financial interests of the Government of Ontario may be affected
include:

   ‚Ä¢   Where the government is the sole shareholder;
   ‚Ä¢   Where the government faces serious threats to economic security;
   ‚Ä¢   Where the government faces security threats to infrastructure or buildings;
   ‚Ä¢   Where the government may be required to make a significant financial
       payment as a remedy;
   ‚Ä¢   Where the integrity of a government system could be jeopardized.

Freedom of Information and Protection of Privacy Manual                                    72
There must be a link between disclosure of a record and the injury. The exemption does
not apply to an injury that may result from the government‚Äôs own conduct or practices.

Proposed Plans or Criteria for Negotiations
This exemption protects the ability of the Government of Ontario or an institution to
negotiate effectively with third parties.

A negotiation means discussions and communications where the government or an
institution and a third party are seeking to arrive at a legally binding agreement,
settlement or contract.

Negotiations must be in the context of financial, commercial, labour, international or
similar situations such as:

   ‚Ä¢   Aboriginal land claims;
   ‚Ä¢   Commercial fishing agreements;
   ‚Ä¢   Allocation of forest resources; and
   ‚Ä¢   Settlement of litigation.

This exemption does not apply where the government is consulting stakeholders with a
view to developing policy or legislation, or regarding possible litigation.

The exemption applies to positions, plans, procedures, criteria, or instructions to be
applied to any negotiations.

A plan is defined as a formulated and detailed method by which a thing is to be done, or
a design or scheme.

Positions, procedures, criteria, instructions are pre-determined courses of action or
ways of proceeding, and includes comments on the strength of positions, bottom line or
fall-back positions, options, or tactics developed as part of the negotiation process.

The exemption has a four-part test:

Part 1: The records must contain positions, plans, procedures, criteria or instructions.

Part 2: The records must be applied to or intended to be applied to the negotiations.

Part 3: The negotiations must be current and ongoing which means carried on or to be
carried on.

Part4: The negotiations must be conducted by or on behalf of an institution or the
Government of Ontario.

Freedom of Information and Protection of Privacy Manual                                    73
The exemption does not apply to:

   ‚Ä¢   Factual information used to develop positions, plans etc.;
   ‚Ä¢   Information that could apply to future negotiations not yet contemplated,
       planned or started; or
   ‚Ä¢   Minutes of settlement, release, and resignation with an employee.

Plans for Management of Personnel or Administration
This exemption protects plans relating to managing personnel or the administration of
an institution that are not yet in operation or public. The purpose is to protect an
institution's internal management plans such as a reorganization, relocation, or creation
of an agency prior to implementation.

The timing and the nature of the plan are important factors. A plan must have sufficient
detail to qualify such as methods, schemes or designs, recommendations, and plans for
action. A plan that has been put into effect or been announced does not qualify.

The exemption has a three part test:

Part 1: A record must contain a plan or plans.

Part 2: The plan or plans must relate to the management or personnel of the
administration of the institution.

Part 3: The plan or plans have not yet been put into operation or made public.

Proposed Plans, Policies and Projects
The exemption protects proposed plans, policies or projects that may result in:

   ‚Ä¢   Premature disclosure of a pending policy decision; or
   ‚Ä¢   Undue financial benefit or loss to a person.

A harms test must be met, meaning that the institution must be able to demonstrate a
harm would likely result from the disclosure of these records. The exemption only
applies when one of the two specified results can reasonably be expected to occur due
to the disclosure. Records must be prescriptive rather than descriptive.

The term proposed means a planned undertaking that has not already been completed.
There must be a decision which the institution has already made.




Freedom of Information and Protection of Privacy Manual                                74
A pending policy decision refers to a situation where a decision has been reached but
not yet implemented. The exemption may apply to operational decisions depending on
the circumstances of each case.

Undue means excessive, disproportionate, not suitable, and not owed, similar to how it
has been interpreted in other sections.

The exemption does not refer to a situation where a policy paper or consultations
undertaken for a policy review, or a policy decision is before an institution for
consideration or deliberation.

The undue financial benefit or loss must be to the third party and not the institution.

Examinations and Testing
This exemption protects questions used in examinations and tests by institutions with an
educational purpose which is generally informed by the mandate of the organization,
such as schools, colleges, universities.

The exemption covers questions that:

   ‚Ä¢   Are to be used; or
   ‚Ä¢   Have been used that can lead to an accurate inference of future questions.

Relevant factors include:

   ‚Ä¢   The difficulty of generating new questions;
   ‚Ä¢   If a re-use protocol is in place; and
   ‚Ä¢   If feedback is given without returning tests.

The fact that an institution may choose to re-use the same questions is not sufficient to
satisfy the requirement. Other factors must exist for the exemption to apply to reused
questions.

The wording of the exemption is different under FIPPA and MFIPPA. The FIPPA
exemption broadens the wording to include testing procedures and techniques.

Another difference is that FIPPA includes a harms test while MFIPPA does not. FIPPA
requires that the disclosure could reasonably be expected to prejudice the use or results
of the tests or testing procedures and techniques.




Freedom of Information and Protection of Privacy Manual                                   75
  Municipal Boundary Submissions
  This exemption protects submissions made by a municipality or other body in respect of
  a matter under the Municipal Boundary Negotiations Act before this statute was
  repealed by the Municipal Act, 2001. The matter must have been commenced before
  the repeal and it must not be resolved yet.

  Quality of Care Information
  This exemption protects information provided in confidence to a hospital, or records
  prepared with the expectation of confidentiality by a hospital committee, to assess or
  evaluate the quality of health care and directly related programs and services provided
  by a hospital, if the assessment or evaluation is for the purpose of improving that care
  and the programs and services.

  Exceptions
  Only FIPPA provides for exceptions to the economic and other interests exemption. The
  exemption does not apply to a record that contains the results of product or
  environmental testing carried out by or for an institution, unless:

     ‚Ä¢   Testing is done as a service for a fee to a person, group of persons or an
         organization other than an institution; or
     ‚Ä¢   Testing was conducted as preliminary or experimental tests for the purpose
         of developing methods of testing.

  Public Interest Override

  The public interest override provision applies to this exemption.


Universities, Colleges and Hospitals Closed Meetings
  FIPPA s. 18.1

  The discretionary exemption for closed meetings applies to universities, colleges and
  hospitals. It is similar to the draft by-laws and closed meetings exemption under
  MFIPPA and provides universities, colleges and hospitals with similar confidentiality
  provisions in the deliberative processes of their respective governing bodies.

  The exemption is intended to be limited and specific and to protect deliberations where
  the substance of those deliberations deals with the:

     ‚Ä¢   A draft of a by-law, resolution, or legislation; and


  Freedom of Information and Protection of Privacy Manual                                 76
     ‚Ä¢   Litigation or possible litigation.

  The exemption requires that a meeting and deliberations take place. A meeting is not
  any gathering. A meeting occurs where attendees come together for the purpose of
  exercising their power or authority or in preparation of doing so. Deliberations refer to
  discussions conducted with a view towards making a decision.

  The exemption requires that meetings be held in the absence of the public which are
  generally known as in camera meetings. The requirements for an in camera meeting
  include:

     ‚Ä¢   Legal authority under a statute to hold a closed meeting;
     ‚Ä¢   That the body holding the closed meeting has proper legal authority;
     ‚Ä¢   That the closed meeting was properly convened in the absence of the
         public; and
     ‚Ä¢   That meeting records are kept confidential.

  Exceptions
  The exemption does not apply when the information is not held confidentially or the
  subject matter of the deliberations was considered in a meeting open to the public.

  The exemption does not apply to records that are more than twenty years old.

  Public Interest Override
  The public interest override applies to this exemption.


Solicitor-Client Privilege
  FIPPA s. 19 / MFIPPA s. 12

  The discretionary exemption of solicitor-client privilege covers records subject to the
  common law solicitor-client privilege (referred to as ‚ÄúBranch 1‚Äù). The exemption also
  covers records prepared by or for Crown counsel or counsel employed or retained by an
  institution, for use in giving legal advice or in contemplation of litigation or for use in
  litigation (referred to as ‚ÄúBranch 2‚Äù).

  "Legal advice" includes a legal opinion about a legal issue and a recommended course
  of action based on legal considerations. It does not include information which was
  provided about a matter having legal implications where no legal opinion was expressed
  or where no course of action based on legal considerations was recommended. The


  Freedom of Information and Protection of Privacy Manual                                     77
fact that a lawyer reviewed a record does not of itself mean that the record falls within
the exemption.

The opinion of an institution's legal advisors should always be sought before this
exemption is used. Institutions must take care to ensure that legal opinions are not
released to another party as the solicitor-client privilege might be jeopardized.

Branch 1 ‚ÄìThe common law privilege applies to:

   ‚Ä¢   All communications, verbal or written, of a confidential character, between a
       client, or their agent, and Legal Counsel employed by the institution, directly
       related to seeking, formulating or giving of legal advice or legal assistance
       (including the Legal Counsel's working papers directly related thereto); and
   ‚Ä¢   Papers and materials created or obtained especially for a Legal Counsel‚Äôs
       brief for litigation, whether existing or contemplated.

For solicitor-client privilege to apply, four criteria must be met:

   ‚Ä¢   There must be a written or oral communication;
   ‚Ä¢   The communication must be of a confidential nature;
   ‚Ä¢   The communication must be between an institution and a Legal Counsel;
       and
   ‚Ä¢   The communication must be directly related to seeking, formulating or
       giving legal advice.

Solicitor-client privilege protects the confidentiality of communications between Legal
Counsel and a client to ensure full, free and frank communications. This privilege is
permanent, subject to waiver. A confidential relationship is essential condition of the
effective administration of justice.

Branch 2 ‚Äì Records Prepared by or for an Institution‚Äôs Legal Counsel for Use in
Giving Advice or Contemplation of Litigation: This branch of the exemption applies
to all materials prepared for use in actual or contemplated litigation. The records do not
have to contain confidential communications between the institution and counsel or be
communications at all.

For a document to be "prepared... in contemplation of litigation", two criteria must be
met:

   ‚Ä¢   Contemplated litigation must be the dominant purpose for preparing the
       record, and


Freedom of Information and Protection of Privacy Manual                                     78
   ‚Ä¢   There must be a reasonable prospect of such litigation at the time the
       document was prepared; the litigation must be more than just a vague or
       theoretical possibility.

Litigation privilege applies to settlement and mediation records that are considered
confidential communications between parties trying to settle a dispute, including oral
and written communications made with a view to reconciliation and settlement.

Litigation privilege has been found to extend to alternate dispute resolution records.

Common law litigation privilege applies only until the end of the litigation; however, the
statutory litigation privilege has been found to be permanent.

This is similar to the common-law privilege but specifically identifies the lawyer as being
Crown counsel or counsel employed or retained by an educational institution or hospital.
The privilege applies to both advice given and records made in contemplation of for use
in litigation.

Counsel employed by the Ontario government, including outside counsel retained by
the Ontario government are ‚ÄúCrown‚Äù counsel.

In government, the solicitor-client relationship is typically between the institution‚Äôs
counsel and the institution. When Legal Counsel advises on non-legal issues, it is not
considered legal advice. The exemption does not apply just because Legal Counsel
reviews a document. Advice must be related to legal issues.

Waiver of Solicitor-Client Privilege
Solicitor-client privilege is a client‚Äôs privilege, and a client may decide to disclose
privileged information obtained from their Legal Counsel and in this way ‚Äúwaive‚Äù the
privilege.

Waiver does not occur where the disclosure of information is required by law. It is also
not considered to be waived if privileged information is shared with other employees
within an institution or department.

Waiver is established where the client as the holder of the privilege:

   ‚Ä¢   Knows of the existence of the privilege;
   ‚Ä¢   Voluntarily demonstrates an intention to waive the privilege;
   ‚Ä¢   The record was disclosed to an outside party; or
   ‚Ä¢   The communication was made in open court.


Freedom of Information and Protection of Privacy Manual                                   79
  Disclosure of privileged information to outsiders generally constitutes waiver. For
  example, waiver would be applied to a letter sent between opposing counsel.

  Solicitor-client privilege is not considered to be waived when records are provided to the
  IPC for the purposes of an appeal.

  A waiver does not necessarily occur when a small amount of information from the
  conclusion of a legal opinion or a summary statement of a legal opinion is disclosed.
  The substance of the whole legal opinion may remain privileged.

  A waiver does not occur when solicitor-client privileged information is shared among
  parties who are found to have a common interest. Examples of where common interest
  may exist include:

     ‚Ä¢   The sender and receiver anticipate litigation against a common adversary;
     ‚Ä¢   A legal opinion was distributed to a group of entities in connection with
         shared advice; and
     ‚Ä¢   Multiple parties shared a legal opinion in confidence in an effort to put them
         on equal footing in negotiations.

  This exemption is considered to be class-based and therefore records subject to this
  exemption cannot be severed, but rather are withheld in full.

  Public Interest Override
  This exemption is not subject to the public interest override.


Danger to Safety or Health
  FIPPA s. 20 / MFIPPA s. 13

  This discretionary exemption is focused on serious threats to the safety or the health of
  an individual if a record is disclosed. The term individual is meant to include any
  individual, regardless of whether the individual is acting in a personal or professional
  capacity.

  Generally an individual is identifiable or named but it is not necessary for the exemption
  to apply.

  The exemption may also apply in situations where an individual is:

     ‚Ä¢   Acting on behalf of a group;
     ‚Ä¢   A member of a group at risk; or

  Freedom of Information and Protection of Privacy Manual                                 80
   ‚Ä¢   Employed to do dangerous or controversial work.

This exemption is related to the law enforcement exemption which protects against
danger to the life or physical safety, but not the health, of law enforcement officers and
any other person.

A harms test has been developed for this exemption. The harms test does not require
the institution to prove that harm resulting from the disclosure is probable, but that there
is a reasonable basis for believing that disclosure could ‚Äúseriously threaten the safety or
health of an individual.‚Äù

The reasonable expectation of harm must be objective rather than subjective. There
must be clear and direct evidence of a connection between the disclosure of the
contents of the record and the expectation of harm.

An individual‚Äôs fear alone may not be enough to satisfy the requirement. Factors to
consider when determining if the harms test is met include:

   ‚Ä¢   An actual threat;
   ‚Ä¢   Persistent and harassing behaviour;
   ‚Ä¢   Pattern of abusive and intimidating correspondence;
   ‚Ä¢   Past violent behaviour;
   ‚Ä¢   Likelihood of retaliation;
   ‚Ä¢   History of frivolous or vexatious complaints;
   ‚Ä¢   Time between alleged behaviour and request.

The exemption may be found to apply where a record reveals a physical location that
may be linked to an individual.

The exemption does not apply to general or statistical information, for example in a case
concerning suicide statistics. The statistics did not reveal the locations or methods of
the suicides and the institution was found not to have provided enough evidence of a
reasonable expectation of harm resulting from the disclosure of the statistics.

Public Interest Override
The public interest override applies to this exemption.




Freedom of Information and Protection of Privacy Manual                                  81
Personal Privacy
  FIPPA s. 21 / MFIPPA s. 14

  This mandatory exemption protects the personal information of individuals other than
  the requester, except in the circumstances specified in this section. See Chapter 7:
  Privacy Fundamentals, for a detailed definition of personal information.

  This section is one of the keystone provisions in the legislation. It balances the public‚Äôs
  right of access to records and the individual‚Äôs right of privacy respecting personal
  information.

  This exemption requires institutions to refuse disclosure of personal information to
  individuals other than to which the information relates except in the circumstances
  specified in the legislation.

  The exemption also allows institutions to disclose personal information in certain
  circumstances, as set out below.

  Written consent: When an individual provides written consent to the disclosure of their
  personal information to another person.

  Health and safety of an individual: When compelling circumstances affecting the
  health or safety of an individual require disclosure. Institutions must provide notification
  of the disclosure to the affected individual at the individual‚Äôs the last known address.

  Public record: When the personal information was collected and is maintained
  specifically for the purpose of creating a record available to the general public. For more
  information on personal information maintained for the purposes of creating a public
  record, see Chapter 7: Privacy Fundamentals.

  Authorized by statute: When an Act of Ontario or Canada expressly authorizes the
  disclosure of personal information.

  Research agreement: Personal information can be disclosed for research purposes if
  certain conditions are met. For more information on research agreements, see Chapter
  6: Managing the Request Process.

  No unjustified invasion of privacy: Disclosure of personal information is also
  permitted if the disclosure would not constitute an unjustified invasion of privacy.

  The exemption sets out a non-exhaustive list of criteria for determining whether the
  disclosure of personal information would constitute an unjustified invasion of privacy.

  Freedom of Information and Protection of Privacy Manual                                   82
Some of the criteria favour disclosure, while others favour non-disclosure of personal
information. Criteria that favour disclosure include:

   ‚Ä¢   Is the disclosure is desirable for the purpose of subjecting the activities of
       the Government of Ontario and its agencies to public scrutiny?
   ‚Ä¢   May access to the personal information promote public health and safety?
   ‚Ä¢   Would access to the personal information promote informed choice in the
       purchase of goods and services? and
   ‚Ä¢   Is the personal information relevant to a fair determination of rights affecting
       the person who made the request?

Criteria that favour non-disclosure include:

   ‚Ä¢   Would the individual to whom the information relates be exposed unfairly to
       pecuniary or other harm?
   ‚Ä¢   Is the personal information highly sensitive?
   ‚Ä¢   Is the personal information unlikely to be accurate or reliable?
   ‚Ä¢   Was the personal information supplied by the individual to whom the
       information relates in confidence? and
   ‚Ä¢   Would the disclosure unfairly damage the reputation of any person referred
       to in the record?

The exemption also sets out the types of personal information that, if released, are
presumed to result in an unjustified invasion of personal privacy. The list includes
personal information that:

   ‚Ä¢   Relates to medical, psychiatric or psychological history, diagnosis,
       condition, treatment or evaluation;
   ‚Ä¢   Was compiled and is identifiable as part of an investigation into a possible
       violation of law, except to the extent that disclosure is necessary to
       prosecute the violation or to continue the investigation;
   ‚Ä¢   Relates to eligibility for social service or welfare benefits or to the
       determination of benefit levels;
   ‚Ä¢   Relates to employment or educational history;
   ‚Ä¢   Was obtained on a tax return or gathered for the purpose of collecting a tax;
   ‚Ä¢   Describes an individual‚Äôs finances, income, assets, liabilities, net worth,
       bank balances, financial history or activities, or creditworthiness;
   ‚Ä¢   Consists of personal recommendation or evaluations, character references
       or personnel evaluations; and
   ‚Ä¢   Indicates the individual‚Äôs racial or ethnic origin, sexual orientation or
       religious or political beliefs or associations.
Freedom of Information and Protection of Privacy Manual                                   83
The exemption further sets out instances where the disclosure of personal information
would not constitute an unjustified invasion of personal privacy. These instances
include:

Certain employee information: The classification, salary range and benefits, or
employment responsibilities of an individual who is or was an officer or employee of an
institution or a member of the staff of a minister. Disclosure of exact salary information
and other details related to employees of an institution, in some cases, may constitute
an unjustified invasion of personal privacy.

Details of contracts for personal services: Financial or other details of a contract for
personal services between an individual and an institution.

Discretionary financial benefits: In FIPPA only, details of a licence or permit or a
similar discretionary financial benefit conferred on an individual by an institution or a
head under circumstances where the individual represents one per cent or more of all
persons and organizations in Ontario receiving a similar benefit, and the value of the
benefit to the individual represents one per cent or more of the total value of similar
benefits provided to other persons and organizations in Ontario.

Personal information about a deceased individual: Personal information about a
deceased individual may be disclosed to the spouse or a close relative of the deceased
individual, if the head is satisfied that, in the circumstances, the disclosure is desirable
for compassionate reasons.

The legislation defines ‚Äúclose relative‚Äù a parent, child, grandparent, grandchild, brother,
sister, uncle, aunt, nephew or niece, including by adoption.

Compassionate circumstances are not defined in the legislation; however, the IPC has
found that compassionate circumstances includes when providing the information will
allow close family members to have more information about the circumstances
surrounding the death of a loved one, or to help provide closure.

The legislation allows an institution to not confirm or deny the existence of a record if
disclosure of the existence of a record would constitute an unjustified invasion of
privacy. If the decision to not confirm or deny the existence of a record is appealed to
the IPC, an institution must provide detailed and convincing evidence that:

   ‚Ä¢   The disclosure of the mere existence of the requested records would
       convey information to the requester, and
   ‚Ä¢   This nature of this information alone would constitute an unjustified invasion
       of privacy.

Freedom of Information and Protection of Privacy Manual                                     84
  For example, an institution may refuse to confirm or deny the existence of a record
  where acknowledging the existence of a record would confirm that an individual was the
  subject of a law enforcement investigation.

  Though not expressed in the legislation, the IPC has considered the absurd result
  principle in adjudicating appeals related to this exemption. The absurd result principle
  states that to prevent disclosure of information which the requester had provided to a
  government body would be a manifestly absurd result, and the exemptions related to
  privacy protection would not apply in these circumstances. The absurd principle applies
  where the requester originally supplied the personal information of others or the
  requester is otherwise aware of it. For example, when the requester is seeking access
  to his or her own witness statement given to the police.

  Public Interest Override
  The public interest override applies to this exemption.


Species at Risk
  FIPPA s. 21.1

  This discretionary exemption under FIPPA has no MFIPPA equivalent. It makes
  reference to the provisions in the Endangered Species Act, 2007.

  This exemption permits an institution to withhold information that could reasonably be
  expected to lead to the killing, harming, harassing, capturing or taking a living member
  of a species that is at risk of extermination.

  Further, information that could reasonably be expected to lead to the possessing,
  transporting, collecting, buying or selling of a living or dead member of a species would
  also be exempt from disclosure.

  Finally, information that if released could reasonably be expected to lead to the
  damaging or destroying of the habitat of a species at risk would also be exempt.

  For the list of species at risk, see Ontario Regulation 230/08.

  Public Interest Override
  The public interest override applies to this exemption.




  Freedom of Information and Protection of Privacy Manual                                85
Information Soon to be Published
  FIPPA s. 22 / MFIPPA s. 15

  This discretionary exemption provides institutions with the ability to exempt information
  that is already publicly available or published or will be published within 90 days of
  receipt of the request, or within such time as is necessary for printing or translating
  material.

  This exemption also allows institutions to refer requesters to existing publications and
  information that is, or soon will be, publicly available.

  According to decisions of the IPC, in order for the institution to claim this exemption, the
  requested record must be either published or publicly available through a regularized
  system of access. Examples of a regularized system of access include a public library
  or a government publication centre.

  Where there is a fee to obtain the information, the information may still be considered
  publicly available as long as the fee is applied to anyone who wishes to obtain the
  information and the fee is not prohibitively expensive. The pricing structure of the
  supplier does not have to align with the fees set out in the legislation.

  Public Interest Override
  The public interest override does not apply to this exemption.


Exclusions
  In addition to the exemptions that are listed above, the legislation also establishes
  classes of information that are excluded from coverage of the legislation.

  Unlike exemptions, exclusions are classes of information where the public has no
  general right of access to the information. Though information may be excluded from
  the legislation, institutions may still choose to provide public access to the information.
  However, such access would be at the sole discretion of the institution based on the
  institutions own policies.

  The sections below describe each exclusion and identify any exceptions to the
  exclusions. Where an exclusion only applies to a specific institution or type of institution
  (such as hospital or educational institution) this will be noted.




  Freedom of Information and Protection of Privacy Manual                                    86
Private Donations to Archives
  FIPPA s. 65 (1) / MFIPPA s. 52 (2)

  The legislation does not apply to records placed in the Archives of Ontario, the archives
  of a college or university, or the archives of a municipal institution when the donor of the
  records is not an institution under the either FIPPA or MFIPPA, or a health information
  custodian as defined in Personal Health Information Protection Act.

  As such, records donated to archives by outside individuals, families, corporations,
  associations or groups do not become subject to the legislation when donated.

  This exclusion does not apply to records deposited in the above mentioned archives by
  institutions. Government records that were previously subject to the legislation remain
  subject to the legislation after transfer to an archive.


Proceedings Before a Court
  FIPPA s. 65 (3)

  FIPPA does not apply to notes prepared by or for a person presiding in a proceeding in
  a court of Ontario if those notes are prepared for that person‚Äôs personal use in
  connection with the proceeding. A person presiding in a court of Ontario could be a
  judge or other judicial official.

  The exclusion has not been found to extend to the notes of board members of tribunals.
  The IPC looks at whether a tribunal, as an institution, has custody or control over the
  notes of members in making its determination.


Performance Evaluations of Judges
  FIPPA s. 65 (4)

  FIPPA does not apply to ‚Äúanything‚Äù contained in a judge‚Äôs performance evaluation or to
  ‚Äúany information‚Äù collected in connection with a judge‚Äôs performance evaluation under
  the Courts of Justice Act.




  Freedom of Information and Protection of Privacy Manual                                  87
Ontario Judicial Council Records
  FIPPA s. 65 (5)

  FIPPA does not apply to records of the Ontario Judicial Council where:

     ‚Ä¢   The Council or its subcommittee has ordered that the record or information
         in the record not be disclosed or made public;
     ‚Ä¢   The Council has determined that the record is confidential; or
     ‚Ä¢   The record was prepared the record in connection with a meeting or
         hearing of the Council that was not open to the public.

  The exclusion applies even if the record of the Council is in the possession of the
  Attorney General.


Investigations of Associate Judge
  FIPPA s. 65 (5.1)

  The legislation does not apply to records of a committee investigating a complaint
  against an associate judge under the Courts of Justice Act. This exclusion applies to
  records that are in the custody of the committee, the Chief Justice of the Superior
  Court of Justice, the Attorney General or any person if the following conditions apply:


     1. The committee has ordered that the record or information in the record not
        be disclosed or made public; or
     2. The record was prepared in connection with the committee‚Äôs investigation
        of the complaint and the complaint was not dealt with in a manner that was
        open to the public.


Prosecution Records
  FIPPA s. 65 (5.2) MFIPPA s. 52 (2.1)

  The legislation does not apply to a record relating to a prosecution if all proceedings
  with respect of the prosecution have not been completed. A prosecution is only
  considered completed when all appeal periods have expired.

  The prosecution must be in regards to a criminal or quasi-criminal offence that will be
  brought before an Ontario court or other tribunal.



  Freedom of Information and Protection of Privacy Manual                                   88
  To qualify for the exclusion the information only requires ‚Äúsome connection‚Äù to the
  prosecution. The exclusion is not limited to just a Crown prosecutor‚Äôs brief.

  Once the prosecution is over, the exclusion does not apply. The request for records
  must then be processed under the legislation, including consideration of any applicable
  exemptions.


Ecclesiastical Records
  s. 65 (5.3)

  FIPPA does not apply to the ecclesiastical records of a church or religious organization
  that is affiliated with a college, university or hospital.

  ‚ÄúEcclesiastical records‚Äù are defined in FIPPA as the operational, administrative and
  theological records, including records relating to the practice of faith, of a church or
  other religious organization.


Hospital Foundation
  s. 65 (5.4)

  FIPPA does not apply to records regarding the operations of a hospital foundation. It
  would seem likely that records of hospital foundations in the custody of hospitals remain
  excluded under the legislation.


Administrative Records of Health Professionals
  s. 65 (5.5)

  FIPPA does not apply to the administrative records of health professionals that relate to
  the health professional‚Äôs personal practice.


Charitable Donations
  s. 65 (5.6)

  FIPPA does not apply to records of charitable donations made to a hospital.




  Freedom of Information and Protection of Privacy Manual                                    89
Labour Relations and Employment-Related
  s. 65 (6), 65 (7) / s. 52 (3), 52 (4)

  The legislation does not apply ‚Äúto records collected, prepared, maintained or used by or
  on behalf of an institution in relation to any of the following‚Äù:

     ‚Ä¢   Proceedings or anticipated proceedings before a court, tribunal or other
         entity relating to labour relations or to the employment of a person by the
         institution;
     ‚Ä¢   Negotiations or anticipated negotiations relating to labour relations or to the
         employment of a person by the institution between the institution and a
         person, bargaining agent or party to a proceeding or an anticipated
         proceeding; and
     ‚Ä¢   Meetings, consultations, discussions or communications about labour
         relations or employment-related matters in which the institution has an
         interest.

  The exclusion has been interpreted broadly and most records that relate to employee
  management or labour relations are generally considered excluded. The exclusion is
  not time sensitive and once records are excluded, they remain excluded.

  Labour relations generally refer to the relationship between a union and an employer in
  a unionized workplace.

  Employment generally refers to the relationship between an employer and an employee

  This exclusion would apply to records such as those regarding internal complaints
  against employees, investigations of employee misconduct, grievances under a
  collective agreement or arbitration proceedings. The exclusion also applies to records
  regarding former employees and individuals considered for employment through a hiring
  or recruitment process.

  The exclusion would also apply to records regarding the planning of labour relations or
  employee related issues including legal opinion.

  Exceptions
  There are exceptions to this exclusion. The following types of records are considered to
  be subject to the legislation:

     ‚Ä¢   An agreement between an institution and a trade union;


  Freedom of Information and Protection of Privacy Manual                                  90
   ‚Ä¢   An agreement between an institution and one or more employees which
       ends a proceeding before a court, tribunal or other entity relating to labour
       relations or to employment-related matters;
   ‚Ä¢   An agreement between an institution and one or more employees resulting
       from negotiations about employment-related matters between the institution
       and the employee or employees; and
   ‚Ä¢   An expense account submitted by an employee of an institution to that
       institution for the purpose of seeking reimbursement for expenses incurred
       by the employee in his or her employment.

The records listed above are subject to the legislation and therefore may be disclosed
under a request, subject to a review for the application of any other exemptions that
may apply to the records.

Church or Religious Appointments of Individuals
s. 65 (6), s. 65 (7)

Hospitals engage church and religious individuals in various positions but these
individuals may not be found to be ‚Äúemployees‚Äù. This exclusion provides similar
protection to the employment records exclusion.

FIPPA does not apply to the appointment or placement of individuals by a church or
religious organization within an institution, or within the church or religious organization.
The exclusion applies to meetings, consultations, discussions or communications in the
appointment process.

Hospital Appointments of Persons with Privileges
s. 65 (6), s. 65 (7)

Many doctors and other professionals are granted privileges in hospitals but may be
found to not be "employees". This exclusion provides similar protection to the
employment records exclusion.

FIPPA does not apply to hospital appointments, the appointments or privileges of
persons who have hospital privileges, or anything that forms part of their personnel file.




Freedom of Information and Protection of Privacy Manual                                    91
Adoptions Related
  s. 65 (8)

  This exclusion applies to all FIPPA institutions and applies to the following information:

     ‚Ä¢   Notices registered under section 48.3 of the Vital Statistics Act and notices
         and information registered under section 48.4 of that Act;
     ‚Ä¢   Notices, certified copies of orders and other information given to the
         Registrar General under section 48.5-48.10 of that Act
     ‚Ä¢   Disclosure vetoes registered under section 48.5 of the Vital Statistics Act;
         and
     ‚Ä¢   Information and records in files that are unsealed under section 48.6 of the
         Vital Statistics Act.


Research and Teaching Material
  s. 65 (8.1), s. 65 (9)

  FIPPA does not apply to the research and teaching work of employees or persons
  associated with an educational institution or hospital. This allows the exclusion to apply
  to students and other research partners who are not formally employed by the
  institution.

  FIPPA does not apply to:

     ‚Ä¢   Records respecting or associated with conducted or proposed research;
         and
     ‚Ä¢   Records of teaching materials collected, prepared, or maintained for the
         institution.

  The only difference between educational institutions and hospitals is that clinical trials
  are also excluded for hospitals.

  The IPC has interpreted what is research and what is considered a research project
  through adjudicative decisions.

  Research has been found to refer to ‚Äúa systematic investigation designed to develop or
  establish principles, facts or generalizable knowledge, or any combination of them, and
  includes the development, testing and evaluation of research.‚Äù




  Freedom of Information and Protection of Privacy Manual                                      92
  Research must refer to a specific, identifiable project that has been conceived by a
  specific faculty member, employee or associate of an educational institution or hospital
  that can include a non-employee at an off-site location.

  The phrase ‚Äúconducted and proposed‚Äù requires consideration of the facts and context to
  determine what stage research is in.

  Exceptions
  An educational institution or hospital must disclose the subject matter and amount of
  funding being received for research projects undertaken by the institution or a person
  associated with the institution.


Peer Evaluations of Research and Teaching Materials
  FIPPA s. 65 (8.1), s. 65 (10)

  The term ‚Äúpeer evaluations‚Äù is not used in the wording of this exclusion. The legislation
  refers to evaluative or opinion material compiled in respect of teaching materials or
  research. This information is normally excluded from FIPPA.

  Exception
  Despite the exclusion, evaluative and opinion material compiled in respect of teaching
  materials or research is subject to FIPPA only in the context of an individual‚Äôs request
  for their own personal information. In the context of a personal information request,
  individuals maintain a right of access to this material, subject to limited and defined
  exemptions.

  See Chapter 8: Personal Information and Correction Requests for more information on
  exemptions to the right of access to one‚Äôs own personal information.


Medical Assistance in Dying
  s. 65 (11), s. 65 (12)

  FIPPA does not apply to ‚Äúidentifying information‚Äù in a record relating to medical
  assistance in dying.

  In this section, ‚Äúidentifying information‚Äù means information that:

     ‚Ä¢   Relates to medical assistance in dying, and


  Freedom of Information and Protection of Privacy Manual                                  93
     ‚Ä¢   Identifies an individual or facility, or for which it is reasonably foreseeable in
         the circumstances that it could be utilized, either alone or with other
         information, to identify an individual or facility.

  ‚ÄúMedical assistance in dying‚Äù means medical assistance in dying within the meaning of
  section 241.1 of the Criminal Code of Canada.


Services Relating to Abortion
  s. 65 (13), s. 65 (14), s.65 (15)

  FIPPA does not apply to information relating to the provision of abortion services if:

     ‚Ä¢   The information identifies an individual or facility, or it is reasonably
         foreseeable in the circumstances that it could be utilized, either alone or
         with other information, to identify an individual or facility; or
     ‚Ä¢   Disclosure of the information could reasonably be expected to threaten the
         health or safety of an individual, or the security of a facility or other building.

  In this subsection a ‚Äúfacility‚Äù includes reference to a pharmacy, hospital pharmacy or
  institutional pharmacy, as those terms are defined in subsection 1 (1) of the Drug and
  Pharmacies Regulation Act.

  The legislation specifies that FIPPA applies to statistical or other information relating to
  the provision of abortion services that does not identify individuals or facilities or could
  not reasonably be expected to threaten the health or safety of an individual or security
  of a facility.




  Freedom of Information and Protection of Privacy Manual                                      94
Chapter 6: Managing the Request Process

Introduction
  The legislation establishes administrative procedures for responding to requests for
  information received under the legislation.

  This chapter focuses on how institutions manage the request process. This includes
  information about requesters, types of requests, timelines for response, and step by
  step guidance on processing requests.

  Within the step by step instruction is information related to various requirements under
  the legislation including calculation of fees, providing notice to affected parties and
  more.


Providing Assistance to Requesters
  In addition to striving to achieve legal compliance with the legislation, institutions should
  provide quality customer service. Coordinators should assist requesters by:

     ‚Ä¢   Providing information about the request process;
     ‚Ä¢   Communicating in a professional and timely manner; and
     ‚Ä¢   Accommodating special needs of the requester.

  Coordinators also assist requesters by providing guidance on reformulating unclear
  requests through clarification. Clarifying requests is discussed later in this chapter.

  All written communications with a requester and third parties should:

     ‚Ä¢   Use plain language;
     ‚Ä¢   Be responsive to all issues in the request;
     ‚Ä¢   Explain decisions and reasons fully; and
     ‚Ä¢   Specify timelines for response and relevant dates.

  Template letters are available Appendix 4. These templates include details that are
  required under the legislation to include within certain communications and can be
  modified by the institution based on the context of the request. Consult the table of
  contents for a list of the available template letters.




  Freedom of Information and Protection of Privacy Manual                                    95
Requesters
 The legislation allows any person to make a request for access to records. A requester
 is not required to identify or justify the purpose for making a request. The right of access
 is not limited by citizenship or place of residence.

 A person making a request can be an individual or organization. There may be
 situations where one person represents another individual or an organization.

 The legislation allows an individual‚Äôs rights or powers to be exercised by:

    ‚Ä¢   A person with the written consent of the individual that has been verified
        (e.g., agent, lawyer).
    ‚Ä¢   A person having lawful custody of a child under the age of sixteen. A
        person with lawful custody of a child does not have absolute access rights.
        The exercise of any rights should be in the best interests of child and not for
        the personal objectives of the custodian.
    ‚Ä¢   A guardian for an individual appointed by a court, or the individual‚Äôs
        attorney under power of attorney, or the Public Guardian and Trustee;
        under the Mental Health Act or Substitute Decisions Act.
    ‚Ä¢   A personal representative of a deceased individual (e.g., executor named in
        a will, administrator or trustee appointed by a court) only if the exercise of
        the power relates to the administration of the individual‚Äôs estate.

 In instances where individuals or agents request information on behalf of another
 individual, institutions will require official documentation to prove an individual or agent
 has the authority to act on behalf of another individual or organization. Examples of
 official documentation an institution could accept include:

    ‚Ä¢   Signed consent form accompanied with a photocopy of government issued
        photo identification;
    ‚Ä¢   A notarized will identifying the name of the executor to an estate;
    ‚Ä¢   A signed affidavit or court order identifying an individual as the guardian to
        an individual under the age of 16; or
    ‚Ä¢   A notarized Power of Attorney.




 Freedom of Information and Protection of Privacy Manual                                    96
Maintaining Confidentiality of a Requester
  When responding to a request for general records, Coordinators should safeguard all
  information about a requester‚Äôs identity and the request should remain confidential.

  When an individual makes a personal information request, the Coordinator may identify
  the requester on a need to know basis to other employees in order to locate the records
  or make decisions on access.

  Anyone should be entitled to make a request without being unnecessarily identified and
  without fear of negative repercussions.

  A disclosure of the identity of a requester could be an invasion of an individual‚Äôs privacy.
  These confidentiality requirements also apply to privacy complaints and investigations.


Request Requirements
  FIPPA s. 24, s. 48, s. 63 / MFIPPA s. 17, s. 37, s. 50

  A request can be made in two ways: formally under the legislation and informally. A
  request can be made for a general record or for a personal information record.

  To be considered a complete request, a formal request for a general or a personal
  information record must be:

     ‚Ä¢   In writing;
     ‚Ä¢   Include sufficient detail for an experienced employee to identify the record;
     ‚Ä¢   Indicate the request is being made under the legislation; and
     ‚Ä¢   Include a $5.00 application fee.

  An individual requesting their personal information must also provide valid identification
  to the institution prior to receipt of a record. Valid identification may include government
  issued photo identification such as drivers licence or passport. In instances where no
  official identification is available, the institution should work with the requester to verify
  their identity. The application fee cannot be waived.

  Informal Requests
  The legislation does not prevent giving access to information in the absence of a formal
  request. This is referred to as an informal request. Informal requests are not processed
  under the legislation and allow an institution to handle a request verbally.



  Freedom of Information and Protection of Privacy Manual                                     97
  To handle a request informally, the individual and institution must be in agreement to do
  so because the individual loses the opportunity to appeal an institution‚Äôs decision on
  access. Any agreement to manage the request informally should be confirmed in writing
  with the requester.


Types of Requests
  The legislation distinguishes between two primary types of requests:

     ‚Ä¢   General record requests where the requester is asking for general
         information or information that includes personal information about
         someone else; and
     ‚Ä¢   Personal information requests where the requester, or authorized
         representative, is asking for information about himself or herself.

  This chapter will primarily address general record requests. Many of the provisions
  outlined in this chapter apply to both general record requests and personal information
  requests. For example, time limits for response, clarifying requests, and fee estimates.
  Where differences exist, such as allowable fees, it will be noted. For additional
  information and considerations for requests for personal information, see Chapter 8:
  Personal Information and Correction Requests.

  Formal requests can vary in size, scope and complexity. The sections below will
  examine:

     ‚Ä¢   Routine requests;
     ‚Ä¢   Contentious requests;
     ‚Ä¢   Voluminous requests;
     ‚Ä¢   Continuing access requests; and
     ‚Ä¢   Frivolous and Vexatious requests.

  These request types are not specified within the legislation; however, categorizing
  requests in this manner may be helpful to institutions depending on the volume and
  complexity of requests received by an institution.

  Routine
  A routine request is straightforward in terms of search, content, disclosure, and
  decision-making. Routine request are requests where responsive records are:

     ‚Ä¢   On a related topic; and
     ‚Ä¢   Frequently requested.

  Freedom of Information and Protection of Privacy Manual                                98
Contentious
A contentious request is a request where the records contain sensitive information that
is likely to be widely disseminated.

There are two criteria that define contentious requests:

   ‚Ä¢   The request is submitted by an individual or organization that may
       disseminate requested information publicly.
   ‚Ä¢   The records contain sensitive information. In this case, the source of the
       request is not a determining factor.

The IPC does not oppose the institutions establishing a contentious issues
management process as long as the process:

   ‚Ä¢   Does not impact decision making on access;
   ‚Ä¢   Does not lengthen timelines on response; and
   ‚Ä¢   Does not reveal the identity of the requester.

For more information on issues management activities, see Chapter 3: Coordinator
Roles and Responsibilities.

Voluminous
A voluminous request generally involves one or more of the following:

   ‚Ä¢   A search through a large number of records;
   ‚Ä¢   The review of a large number of responsive records;
   ‚Ä¢   The coordination of searches through multiple program areas in the
       institution;
   ‚Ä¢   Potential interference with the institution‚Äôs operations; or
   ‚Ä¢   The requirement of additional staff or resources to complete the request.

A time extension may be required when responding to a voluminous request if the
request cannot be responded to within the 30 day limit.

Some strategies for working with the requester and managing resources may include:

   ‚Ä¢   Contacting the requester and discussing options to narrow the scope of the
       request to reduce the time required to respond and fees potentially;
   ‚Ä¢   Working with the requester to agree on a compromise such as offering the
       requester an opportunity to review the original records and only select those
       they wish to have copies of;

Freedom of Information and Protection of Privacy Manual                                99
   ‚Ä¢   Establishing an agreement to a staged release of records based on the
       requester‚Äôs priorities; or
   ‚Ä¢   Allocating additional staff resources to work on the request on an ad-hoc or
       emergency basis.

Requests for Continued Access
s. 24 / s. 17

A requester can make a request and seek continuing access to the records for a period
of up to two years. This is called continued access and applies to general records only
(i.e., not personal information). The record must exist at the time of the request.

Continuing access is not intended for records that are only produced once; but rather is
intended for records that are produced on an ongoing basis. Continued access is not
appropriate where:

   ‚Ä¢   Access is denied in full to the initial request; and
   ‚Ä¢   There is no possibility that future records will come into existence during the
       two-year continuing access period.

For example: A requester may be granted access to a report and request any updates
to the report over the next year. If the report is quarterly, a quarterly schedule for
disclosure would be proposed to the requester.

An institution provides the requester with a proposed schedule of dates for disclosure
based on a reasonableness test. For example, if a requested record is produced on a
quarterly basis, it would be reasonable to establish a quarterly schedule. The institution
should provide the requester with a rationale for the schedule. The proposed schedule
can be appealed to the IPC.

In practical terms, the original request is brought forward on each of the dates listed in
the schedule and processed as if it were made on that day. The $5.00 application fee is
required for each of the schedule dates. For convenience, it is advisable for institutions
to request payment for application fees in a lump sum. For example, the institution may
request $25.00 for five requests. Requesters may also pay individual $5.00 application
fees according to the schedule if that is their preferred option.

Each time the request is scheduled to begin, the request needs to be reviewed and an
access decision needs to be made. An institution cannot rely on its first decision for
subsequent requests in the schedule. As a result, different requests under a single
continuing access request may have different decisions on access, depending on the

Freedom of Information and Protection of Privacy Manual                                  100
responsive records in each request and what exemptions or exclusions may apply to
the records.

Frivolous and Vexatious
FIPPA s. 10 (1) (b), s. 24 (1.1), s. 27.1, Reg. 460 (5.1) / MFIPPA s. 4 (1) (b), s. 17 (1.1),
s. 20.1, Reg. 823 (5.1)

An institution may issue a decision letter to a requester indicating that no records will be
provided if the institution views the request as frivolous or vexatious. Under the
regulations, a frivolous or vexatious request occurs where:

   ‚Ä¢   The request is part of a pattern of conduct that amounts to an abuse of the
       right of access;
   ‚Ä¢   The requester is acting in bad faith or for a purpose other than access; or
   ‚Ä¢   Responding to the request would interfere with the operations of the
       institution.

According to the IPC, examples of abuse of process include using the request process
to:

   ‚Ä¢   Make repeated requests for the same or similar information;
   ‚Ä¢   Make an excessive number of requests;
   ‚Ä¢   Resubmit a request previously abandoned;
   ‚Ä¢   Make requests that are excessively broad in scope or unusually detailed;
   ‚Ä¢   Coincide with the timing of other events (e.g., court proceedings); or
   ‚Ä¢   Accomplish an objective unrelated to the process (e.g., harass, cause a
       nuisance, break or burden the system).

A requester‚Äôs pattern of conduct could unreasonably interfere with the operations of an
institution which means that responding to the requests would obstruct or hinder the
effectiveness of the institution‚Äôs activities. The concept of interference is relative to the
circumstances and size of the institution.

A request is also frivolous and vexatious when it is made in bad faith. This implies the
conscious doing of wrong for a dishonest purpose (e.g., creating a nuisance). It also
suggests a state of mind which views the request process with contempt. Bad faith is
not simply bad judgment or negligence.

According to the IPC, factors to consider in establishing bad faith include:




Freedom of Information and Protection of Privacy Manual                                   101
     ‚Ä¢   Nature and quality of interaction between the requester and the institution‚Äôs
         staff; and
     ‚Ä¢   There is an escalation of a requester‚Äôs uncooperative and harassing
         behaviour.

  The institution is required to inform the requester why the request is denied and the
  reasons it is considered frivolous and vexatious.

  The requester may appeal a determination that the request is frivolous or vexatious. In
  the case of such an appeal the institution is required to present evidence that the
  request is frivolous or vexatious and the IPC will determine if the institution's decision is
  reasonable.


Time Limits
  FIPPA s. 25 (1) (2), s. 26, s. 27, s. 28 / MFIPPA s. 18 (2) (3), s. 19, s. 20, s. 21

  In the context of processing a request, there are time frames for responding to
  requesters. This is commonly referred to as the ‚Äúclock.‚Äù

  In general, access requests must be completed within 30 calendar day (i.e., counting
  Monday to Sunday). The 30 day time limit starts the day the institution receives a
  complete request and finishes the day the final decision letter is sent.

  If the time limit expires on a Saturday, Sunday or statutory holiday, the timeframe for
  responding to the requester is extended to the next business day of the institution.

  The day the request is received is considered ‚Äúday zero.‚Äù When calculating the due date
  for a request, Coordinators should count 30 days starting from the next calendar day
  from when a request was received.

  If a request is received after business hours, the request is generally considered to have
  been received the next business day. For example, if a request is submitted at 9:00 pm
  to an office that closes at 5:00 pm, the request will be considered received by the
  institution the following day.

  If an institution fails to issue a decision letter to a requester within the 30 day time limit
  (if no time extension or notice to affected person is issued), it may be considered a
  deemed refusal. A requester can appeal a deemed refusal to the IPC.

  The table below gives more information on how the 30 day clock is impacted by
  different administrative actions:

  Freedom of Information and Protection of Privacy Manual                                     102
Issue             Clock Status        Time Limit to       Considerations
                                      Issue Decision

Forwarding a      Clock does not      30 days             Must be done within 15
request           stop.                                   days. It is advisable to
                                                          transfer a request as soon
                                                          as possible.
                                                          Clock does not stop while
                                                          the request is in transit.
Transferring a    Clock does not      30 days             Must be done within 15
request           stop.                                   days of receipt. It is
                                                          advisable to transfer a
                                                          request as soon as
                                                          possible.
                                                          Clock does not stop while
                                                          the request is in transit.
Clarifying a      Clock does not      30 days from        Request must be
request           start until         date of             considered complete in
                  request is          clarification       order for the clock to
                  clarified with                          begin.
                  requester.                              If clarification is required,
                                                          this indicates the
                                                          requester has not provided
                                                          enough information for an
                                                          experienced employee to
                                                          locate a record.
Fee Estimate      Clock does not      30 days in total    Requesters are not
/Interim          stop.               unless a time       required to pay a deposit
Decision ‚Äì                            extension is        on requests where fees
Fee Estimate                          applied             are estimated to be below
Under $100                                                $100.
Fee Estimate      Clock stops on      30 days in total    Requesters are required to
/Interim          the date the        unless a time       pay a 50% deposit when
Decision ‚Äì        fee estimate is     extension is        fees for processing the
Fee Estimate      issued.             taken               request are over $100.
Over $100         Clock starts                            When deposit is paid, the
                  again on the                            clock starts again. For
                  date the fee                            example, if the institution
                  deposit is                              issues a letter on day 10,
                  received. The                           the institution has 20 days
                  institution has                         left to issue a final

Freedom of Information and Protection of Privacy Manual                            103
Issue             Clock Status        Time Limit to          Considerations
                                      Issue Decision

                  the remaining                              decision after receiving the
                  days to                                    deposit.
                  complete.
                                      30 days from
Notice to         Clock                                      The institution must issue
                                      issuing the notice
Affected          continues, but                             the notice to affected
                                      to affected parties
Person /Third     request due                                person within 30 days of
                                      decision must be
                  date changes                               receipt of the request or
Party Notice                          issued to
                  to be 30 days                              within extended time limits.
                                      requester.
                  after the notice
                  is issued.
                                      Within these 30
                                      days, the affected
                                      parties have 20
                                      days to respond to
                                      the institution. The
                                      institution then has
                                      10 days to issue a
                                      decision.
Time              Clock               The timeline for       Only one time extension
Extensions        continues.          the extension is       allowed and must be taken
                                      not defined in         within 30 days of receipt of
                                      the legislation        the request.
                                      but is based on        Time extensions can only
                                      a reasonable           be granted for volume of
                                      assessment of          search or responsive
                                      time required.         records; or where
                                                             consultation with an
                                                             outside individual is
                                                             required to complete the
                                                             request.
                                                             If a Notice to Affected
                                                             Person is issued during a
                                                             time extension, this may
                                                             further extend the
                                                             timelines for response.




Freedom of Information and Protection of Privacy Manual                              104
Administrative Actions to Support Processing Requests
  In order to process requests under the legislation, there are various administrative tasks
  and processes that may need to be completed depending on the circumstances.

  The legislation provides rules around these administrative actions. Many of these
  processes involve communicating with the requester who has the right to appeal to the
  IPC on the validity of applying some of these provisions.

  The sub-sections below provide details on when these actions are required or allowed
  and what information institutions must communicate to the requester.

  Clarifying Requests
  FIPPA 24 (2) / MFIPPA 17 (2)

  When a request is not clear and experienced employees of an institution do not have
  sufficient information to begin a search for responsive records, Coordinators are
  obligated to clarify the request. The legislation requires requesters to provide sufficient
  detail to enable an experienced employee to identify the requested records.

  It is necessary to clarify a request when:

     ‚Ä¢   The request is open-ended, vague or unclear;
     ‚Ä¢   The record is not described sufficiently to allow an experienced employee to
         undertake a search; or
     ‚Ä¢   The request comes in the form of a question where no record exists to
         satisfy the answer.

  Clarifying a request can be done by phone or in writing. The institution and requester
  can work together to reformulate the request. See Appendix 4.3 for a template letter
  acknowledging a request that requires clarification.

  Coordinators can also help the requester better understand what types of records may
  be responsive to a request and what is, and is not, available in response to the request.
  In addition, it may be helpful for Coordinators to explain processing fees and provide an
  estimate of costs, if possible. Potential fees are often an important consideration for a
  requester. With this knowledge, the requester can then decide how to proceed.

  After a request has been clarified it should be clear to both the institution and the
  requester what records are being requested. For an institution this means that an
  experienced employee will be able to identify the requested records. This agreement
  should be confirmed in writing and essentially makes the request complete.

  Freedom of Information and Protection of Privacy Manual                                  105
Narrowing a Request
Sometimes a request will capture a significant number of records, because of the way it
has been worded (e.g., ‚Äúaccess to any and all records‚Äù) or due to an extensive
timeframe for the record search. A broad request can still provide sufficient detail to
identify records. Narrowing a request refers to reducing the scope of the request (e.g.,
reducing a request for three years of records to one year of records).

Narrowing the scope of a broad request is not considered to be a clarification if the
original request provided the institution with sufficient detail about the requested record,
and narrowing simply reduces the scope.

Coordinators should work collaboratively with requesters to narrow the scope of a
request. Alternatively, the Coordinator can choose to interpret the request literally,
which may involve an institution-wide search for records and potential time extensions.

Forwarding a Request
FIPPA s. 25 (1) / MFIPPA s. 18 (2)

Sometimes a requester may send a request to the wrong institution. When a request is
received and the institution does not have custody or control of the records, the
legislation requires that:

   ‚Ä¢   Inquiries be made to determine if another institution has responsive
       records; and
   ‚Ä¢   The first institution should forward the request to the institution, as required.

Requests can only be forwarded between any provincial or municipal institutions.

Coordinators should take action to forward the request within 15 days or as soon as
possible, because the clock will continue to run while the request is being forwarded.

Coordinators should take the following steps:

   ‚Ä¢   Make reasonable inquiries to determine if another institution has the record
       if unknown;
   ‚Ä¢   Notify or telephone the Coordinator at the correct institution;
   ‚Ä¢   Forward the request and determine whether to:
           o Return the application fee payment (e.g.: cheque or money order) to
              the requester and advise them to submit a new payment to the
              correct payee;


Freedom of Information and Protection of Privacy Manual                                    106
          o Forward the application fee to the new institution (if the institutions
               are the same payee); or
          o Deposit the application fee on behalf of the receiving institution (if the
               institutions are the same payee); and
   ‚Ä¢   Notify the requester about the new contact in writing.

See Appendix 4.5 for a sample template letter to use when notifying a requester a
request has been forwarded or transferred to another institution. See Appendix 4.6 for a
sample template letter to use notifying another institution when a request has been
transferred or forwarded.

Transferring a Request
FIPPA s. 25 (2) (3) / MFIPPA s. 18 (2) (4)

An institution may receive a request that applies to records that another institution may
have a greater interest in, and it would be more appropriate that the other institution
make a decision on access. Often this occurs when institutions share records for the
purpose of seeking advice or when partnering or collaborating on a project.

The legislation allows a request and a record to be transferred to any provincial or
municipal institution. The purpose of transferring a request is to ensure the institution
best positioned to make an access decision may do so.

Transferring a request and record is discretionary which means an institution is not
obligated to do so. Institutions may also transfer a request in part, where each institution
issues decision letters on the records that the institutions have a greater interest in
disclosure.

An institution has a greater interest in a record than another institution if:

   ‚Ä¢   The record was originally produced in or for that institution; or
   ‚Ä¢   The other institution was the first institution to receive the record or a copy
       of it.

Coordinators should take action to transfer the request within 15 days or as soon as
possible, because the clock continues to run while the request is being transferred.

Coordinators should take the following steps:

   ‚Ä¢   Make reasonable inquiries to determine if another institution has a greater
       interest in the record if unknown;
   ‚Ä¢   Notify or telephone the Coordinator at the institution;

Freedom of Information and Protection of Privacy Manual                                  107
   ‚Ä¢   Transfer the request and determine whether to:
          o Return the application fee payment (e.g.: cheque or money order) to
               the requester and advise them to submit a new payment to the
               correct payee;
          o Transfer the application fee to the new institution (if the institutions
               are the same payee); or
          o Deposit the application fee on behalf of the receiving institution (if the
               institutions are the same payee); and
   ‚Ä¢   Notify the requester about the new contact in writing.

See Appendix 4.5 for a sample template letter to use when notifying a requester a
request has been forwarded or transferred to another institution. See Appendix 4.6 for a
sample template letter to use notifying another institution when a request has been
transferred or forwarded.

Records of Other Governments
Institutions can only forward or transfer requests to provincial or municipal institutions.
Therefore institutions cannot forward or transfer a request to the federal government,
another provincial government, a municipality outside of Ontario or the government of
another country.

When a requester sends a request to the wrong government, Coordinators should
return the request and application fee to the requester and advise them to re-submit the
request to the appropriate government.

Time Extensions
FIPPA s. 27/ MFIPPA s. 20

The legislation allows a time extension where it is unreasonable to complete the request
within the 30 day time limit. The legislation is not explicit about the amount of allowable
time so any time extension must be reasonable and justifiable.

A Coordinator should consider all of the potential factors that may contribute to the need
for a specific length of a time extension. A decision to extend the time limit must be
made within the 30 day limit and the requester must be given notice. Only one time
extension can be taken for each request. It is generally best practice to inform the
requester of the time extension concurrently with informing the requester of estimated
fees (if any). More information on fee estimates is provided later in this chapter.

Time extensions are permitted for two reasons:


Freedom of Information and Protection of Privacy Manual                                  108
   ‚Ä¢   When requests that have a high volume of records to search or review and
       the extensive search and review would unreasonably interfere with
       operations; and
   ‚Ä¢   When requests require consultations with a person or organization outside
       the institution in order to complete the request.

The IPC encourages institutions to work with requesters to find practical solutions when
several requests are submitted at one time. Time extensions for a request are
determined on a case by case basis.

Factors generally found to support a time extension include:

   ‚Ä¢   A large number of records requiring careful review; and
   ‚Ä¢   A substantial amount of time required to prepare records at a critical
       operational time requiring staff to attend to their duties.

Factors generally found not to support a time extension include:

   ‚Ä¢   The number of requests being processed at any given time;
   ‚Ä¢   Staff vacation;
   ‚Ä¢   The expense of producing a record where the expense is caused by the
       size, number or physical location of records.

Written notice must be provided to the requester when a time extension is being
applied. The institution must communicate to the requester:

   ‚Ä¢   The reason for the time extension;
   ‚Ä¢   The length of the time extension and the new due date for the request; and
   ‚Ä¢   That the requester has the right to request the IPC review the decision by
       the institution to apply the time extension.

See Appendix 4.7 for a template letter notifying a requester of a time extension.

Consultations
FIPPA s. 27/ MFIPPA s. 20

The purpose of consultations is generally to obtain information or advice that can inform
decision-making. Consultations are different from providing notices to affected persons
as discussed below.

Consultations may be required with a person (e.g., past employee) or organization (e.g.,
other governments) outside the institution who may have knowledge of the records at

Freedom of Information and Protection of Privacy Manual                              109
issue. For example, a consultation may be required when responsive records to a
request include records prepared by another government body and subject matter
expertise is required in order to make a decision on access.

The legislation allows a time extension for consultations that is reasonable in the
circumstances.

Notice to Affected Person (Third Party Notice)
FIPPA s. 28 / MFIPPA s. 21

Some records responsive to a request may contain information concerning an affected
party such as a person other than the requester. In these instances, institutions are
required to provide notice to the affected persons. Further, the institution must provide a
notice of delay to the requester regarding the notice process.

A notice to an affected party must be given if the institution has reason to believe that
the records might contain third party information or contain personal information of an
identifiable individual, and the institution is contemplating disclosing the records.

The notice process gives the affected party an opportunity to make representations
about the proposed disclosure of records that affect them. The threshold for determining
if the third party information exemption applies to a record is very low. This means that
institutions should carry out the notice to affected persons process whenever the
responsive records ‚Äúmight‚Äù be subject to this exemption. Observing a low threshold
ensures procedural fairness and reduces the risk that exempted information is disclosed
in error.

A notice to the affected person must include the following information:

   ‚Ä¢   A statement that the institution intends to release a record or part of a
       record that may affect the interests of the person or organization;
   ‚Ä¢   The contents of the record or the part that relates to the affected person;
   ‚Ä¢   That the affected person must make representations in writing as to why the
       record in whole or in part should not be released; and
   ‚Ä¢   That the affected person has 20 days after the notice is given to reply.

There is discretion to hear representations verbally if necessary.

Affected persons may not be familiar with the legislation. It is advisable that
Coordinator‚Äôs contact affected to explain the notice process and answer any questions
they may have. It is also helpful to provide:


Freedom of Information and Protection of Privacy Manual                                 110
   ‚Ä¢   Copies of the relevant sections of the legislation; and
   ‚Ä¢   An actual copy of the record where practical.

After the 20 day time limit for response from the affected party has elapsed, institutions
have 10 days to issue a decision on access to the requester.

Institution‚Äôs may agree or disagree with the response from an affected person. The
affected person has a right to appeal the institution‚Äôs decision on the access request
regardless of whether the institution agrees or disagrees with their response. As a
result, the institution must hold the records until the appeal period of 30 days has past.
Once the appeal period has passed, the Coordinator should confirm with the IPC that
no appeal has been received before releasing the records to the requester (subject to
payment of fees).

If records capture information that is not subject to notice, institutions should disclose
the records which are not subject to the notice at the time the decision letter is issued
(subject to the application of other exemptions and exclusions and payment of fees).

The timelines stated above may be extended if the 20 day response time presents a
‚Äúbarrier‚Äù as defined by the Accessibility for Ontarians with Disabilities Act. Under that
legislation, a barrier means ‚Äúanything that prevents a person with a disability from fully
participating in all aspects of society because of his or her disability, including a physical
barrier, an architectural barrier, an information or communications barrier, an attitudinal
barrier, a technological barrier, a policy or a practice.‚Äù

When timelines are extended for the notice to affected due to accessibility reasons, the
requester should be informed of the new expected due date for response.

See Appendix 4.10 for a template letter providing notice to affected person for third
party information. See Appendix 4.11 for a template letter providing notice to affected
person for personal privacy. Finally, see Appendix 4.12 for a template letter to provide
notice to requester of delay where a third party‚Äôs interests are impacted.

Fees
FIPPA s. 57, Reg. 460 (5.2), (6), (6.1) / MFIPPA s. 45, Reg. 823 (5.2), (6), (6.1)

The legislation adopts a user pay principle. This means an individual making a request
must pay some of the costs the institution incurs to process the request.

For this reason, a requester must have sufficient information to review the costs and
decide how to proceed.


Freedom of Information and Protection of Privacy Manual                                   111
Fees must be calculated for every request starting from the time a request is received.
The fees apply to time, materials and services. The fees are set out in the regulations.

Fees must be charged unless they are waived by the institution, or unless another
statute has an overriding provision for charging fees.

Fees and fee estimates can be appealed to the IPC. Fees and fee estimates should be
detailed and reasonable. The IPC can order institutions to lessen or change fees if they
find the institution has erred in their calculation of fees.

Types of Fees
Reg. 460 (5.2), (6), (6.1) / Reg. 823 (5.2), (6), (6.1)

The fees that are chargeable under the legislation are described below. Note that not all
fees can be charged for personal information requests.

Federal and provincial sales tax is not chargeable for application and processing fees.

Application fee: A $5.00 application fee is required for all requests. Failure to collect
the application fee may not prevent the IPC from hearing an appeal about a request that
has been completed.

Search time: Institutions may charge $7.50 per 15 minutes for each staff member‚Äôs
time to manually search for responsive records. Searching includes examining file
plans, listings of records and paper or electronic searches.

Search time cannot be applied to other administrative tasks such as photocopying and
travel to offices.

Search time can only be charged for general record requests and not personal
information requests.

Preparation time: Institutions may charge $7.50 per 15 minutes for each staff
member‚Äôs time to prepare records for disclosure. Preparation includes severing a
record, activities required to generate computer reports and running computer reports,
and scanning records into digital format.

Preparation time cannot be applied to other tasks such as reviewing records for relevant
exemptions and exclusions, photocopying records, preparing an index of records,
preparing records for third party notice, or transporting records.




Freedom of Information and Protection of Privacy Manual                               112
Preparation time can only be charged for general record requests and not personal
information requests.

Computer time: $15.00 per 15 minutes for any person developing a computer program
or other method of producing a record from a machine readable record.

Computer time cannot be applied to existing search functions to generate reports, SQL
queries or data manipulation.

Photocopies and computer printouts: Institutions may charge $.20 per page to
photocopy or print material for disclosure. Double-sided copies count for 2 copies.

Compact Disks: Institutions may charge $10.00 per disk for disclosing records in
electronic format.

External Services: Institutions may charge for work that is required to be completed
that cannot be done by internal staff due to the specialization required. An example
would be reformatting or copying rare media types. In order to charge this fee,
institutions must receive an invoice for the cost of the outside service.

This fee cannot be applied to work that could be completed by an institution‚Äôs staff, even
if an invoice exists.

Shipping: Institutions may charge for postage and courier delivery for disclosed
records.

See Appendix 6 for a Sample Fee Estimate Form program areas can use to calculate
and document fee estimates.

Estimating Fees and Interim Decisions
FIIPPA s. 57 (3), Reg. 460 (7) / MFIPPA s. 45 (3), Reg. 823 (7)

Institutions have different notice requirements depending on the estimated amount of
fees required for processing the request.

Three thresholds for fees and steps Coordinators should follow are outlined below.

Fees less than $25: The institution is not required to send a fee estimate to the
requester for fees less than $25.00. The institution completes all necessary work and
notifies the requester of the fee amount to be paid in the final decision letter. Fee
payment is required before records are released.



Freedom of Information and Protection of Privacy Manual                               113
Fees between $25 and $100: The institution must notify the requester of the
approximate fee. No deposit is required for fees in this range and the institution
completes all necessary work and issues a final decision letter. Fee payment is required
before records are released.

Fees in excess of $100: Institutions must prepare a detailed fee estimate with an
interim decision letter. The institution may require a 50% deposit before taking further
action on processing the request. This means the ‚Äúclock‚Äù is stopped and the request is
on hold until the institution receives the deposit. Upon receipt of the deposit payment,
the institution completes all necessary work and issues a final decision letter with the
final fee calculation. Fee payment of the remaining balance is required before records
are released.

An interim decision is not final or binding and cannot be appealed. An interim decision
also lets the requester know what exclusions and exemptions may apply to the records
and that a fee waiver can be claimed by the requester.

While the initial determination that some exemptions are likely to apply cannot be
appealed, requesters may appeal fee estimates to the IPC.

When a search is required through a large number of records or where a large number
of records are responsive to a request, a fee estimate for searching records can be
calculated using a representative sample. A representative sample should be complete
and include:

   ‚Ä¢   A search of all types of records (e.g., paper, electronic, special media);
   ‚Ä¢   A reasonable sample size of a computer file folder, drawer, storage box;
       and
   ‚Ä¢   A useful measurement is a paper stack where one-inch of paper holds
       about 150-200 single-sided pages.

The estimated total search time can be calculated using:

   ‚Ä¢   Number of hours to search the sample;
   ‚Ä¢   Number of responsive pages in the sample;
   ‚Ä¢   Number of pages requiring severances;
   ‚Ä¢   Number of severances per page; and
   ‚Ä¢   Any additional work estimated to complete the request.

It is best practice to inform the requester of any time extension when providing a fee
estimate and interim letter as this information will impact the requester‚Äôs decision to pay
a fee deposit.

Freedom of Information and Protection of Privacy Manual                                114
See Appendix 4.8 for a sample template letter to use for fee estimates and interim
decisions where the fee estimate is between $25.00 and $99.00. See Appendix 4.9 for a
sample template letter to use for fee estimates and interim decisions where the fee
estimate is $100 or more.

Payment of Fees
Institutions generally receive payment for application and other fees by cheque, though
other forms of payment are acceptable (e.g., electronic, money order, cash). Institutions
cannot stipulate how payment is made. Also, institutions are not required to accept
methods of payment they are not set up to do so (e.g., credit card, electronic transfer).

For provincial ministries and some agencies, boards and commissions cheques are
made payable to the ‚ÄúMinister of Finance‚Äù. For other provincial and municipal
institutions, cheques are payable directly to the institution.

Waiving Fees
FIPPA s. 57 (4), Reg. 460 (8) / MFIPPA s. 45 (3), Reg. 823 (8)

Processing fees are mandatory under the legislation; however, a requester may seek a
fee waiver. Institutions must notify requesters of their right to request a fee waiver when
issuing fees or fee estimates. A requester is responsible for submitting a request for a
fee waiver and providing a rationale as to why granting a fee waiver would be fair and
equitable.

The legislation and regulations lists factors for institutions to take into account when
determining whether granting a fee estimate would be fair and equitable. These factors
include:

   ‚Ä¢   The extent to which the actual cost of processing, collecting and copying
       the record varies from the amount of payment required;
   ‚Ä¢   Whether the payment will cause a financial hardship to the requester;
   ‚Ä¢   Whether the access to the record will benefit public health or safety;
   ‚Ä¢   Whether the requester gets access to the record; and
   ‚Ä¢   Whether the amount of the payment is too small to justify requiring payment
       (e.g., $5.00 or less).

While the institution must consider these factors, these factors do not necessarily need
to be present in order for the institution to grant a full or partial fee waiver.

Other relevant factors should also be considered when deciding whether or not a fee
waiver is fair and equitable. These include:
Freedom of Information and Protection of Privacy Manual                                115
    ‚Ä¢   The manner in which the institution attempted to respond to the appellant's
        request;
    ‚Ä¢   Whether the institution worked with the appellant to narrow and/or clarify
        the request;
    ‚Ä¢   Whether the institution provided any documentation to the appellant free of
        charge;
    ‚Ä¢   Whether the appellant worked constructively with the institution to narrow
        the scope of the request;
    ‚Ä¢   Whether the request involves a large number of records;
    ‚Ä¢   Whether or not the appellant has advanced a compromise solution which
        would reduce costs; and
    ‚Ä¢   Whether a fee waiver would shift an unreasonable burden of the cost from
        the appellant to the institution, and cause significant interference with the
        operations of the institution.

 If a requester seeks a fee waiver, they must submit the request for the waiver in writing.
 Institutions are required to review and decide whether or not to grant a full or partial fee
 waiver. The institution‚Äôs decisions to grant or not grant fee waivers may be appealed by
 the requester to the IPC.


Abandoned or Withdrawn Requests
 When a requester communicates to an institution that they are no longer seeking
 access to the requested information, the request can be considered ‚Äúwithdrawn.‚Äù
 Coordinators should retain documentation, such as email correspondence with the
 requester that shows the requester‚Äôs intent to withdraw the request. When a requester
 withdraws a request verbally, it is best practice to confirm this intention in writing.

 When a Coordinator has attempted to contact a requester in order to proceed with
 processing the request and has not had a response from the requester, the request can
 be considered ‚Äúabandoned.‚Äù The IPC advises institutions to allow 30 days to pass
 before marking a request as abandoned for general record requests. For personal
 information requests, the IPC advises allowing one year for response before closing the
 request.

 Requesters should be notified in writing that their request may be abandoned. The letter
 should state the exact date at which the institution will close the file if no response is
 received. Institutions may choose to include this in a fee estimate letter or clarification
 letter; or write a separate abandonment letter.



 Freedom of Information and Protection of Privacy Manual                                 116
  See Appendix 4.20 for a sample template letter to use to advice requesters their request
  will be considered abandoned if no response is received by the institution.


Request Processing Step by Step
  Processing requests is an administrative function that requires knowledge about the
  legal requirements of the legislation and an institution‚Äôs programs and records.

  Responding to requests is a collaborative process that includes a number of steps. The
  sections below outline these steps at a high level. Not all steps may be required for
  processing all requests depending on the institution and the context of the request.


Step 1: Receiving a Request
  The legislation requires that requests be received by the institution in writing and
  accompanied with a $5.00 application fee. For practical reasons, most institutions can
  only receive requests by mail or in-person delivery. Some institutions may have the
  ability to receive requests online or via fax.

  Once a request is received, Coordinators and their staff should:

     ‚Ä¢   Review the request to ensure the request is complete, which means the
         request is in writing, includes the $5.00 application fee, and provides
         sufficient detail.
     ‚Ä¢   Open a file, assign a file number, and calculate the 30 day time limit for a
         response. Note that if the due date falls on a Saturday, Sunday or holiday,
         the due date is moved to the next business day.
     ‚Ä¢   If the institution is using an electronic case file management system, update
         the system.
     ‚Ä¢   Make copies of the original request to work with.
     ‚Ä¢   Make copies of any administrative forms to put in the file.
     ‚Ä¢   Notify the program area or a program contact if known, of a request.

  Coordinators should communicate to the office or offices that are likely to have
  responsive records (program area) and provide them with the following information:

     ‚Ä¢   Wording of the request;
     ‚Ä¢   Instructions for conducting the search and recording actions and time taken;
     ‚Ä¢   Timelines for completing the record search; and
     ‚Ä¢   Instructions on how to deliver copies of responsive records to the FOI
         office.

  Freedom of Information and Protection of Privacy Manual                                117
  Coordinators should contact requesters immediately if the request is missing the
  application fee or requires clarification in order to proceed. See Appendix 4.2 for a
  template letter acknowledging receipt of a request that requires a $5.00 application fee
  and Appendix 4.3 for a template letter acknowledging receipt of a request that requires
  clarification.


Step 2: Assessing a Request
  For routine requests, Coordinators can proceed directly to searching for records or
  finalizing recommendations and a decision. However, for many requests, additional
  steps may be required to assess how to proceed with processing a request.

  In all instances, it is best practice for the institution to send an acknowledgement letter
  to the requester confirming the receipt of the request. This acknowledgment letter
  should include the request number assigned to the request, indicate the date of receipt
  of the request and provide contact information of the staff member responsible for
  processing the request. See Appendix 4.1 for a template letter for a standard
  acknowledgement letter.

  Coordinators or their staff should discuss the request with the program area that is likely
  to have responsive records in order to understand their business, any concerns,
  possible impact of the search on operations, and alternate ways to respond.

  If the request has been sent to your institution in error, or if another institution has a
  greater interest in the disclosure of the record, Coordinators should take reasonable
  steps to determine which institution should receive the request and forward or transfer
  the request.

  Coordinators should determine if the request provides sufficient detail for an
  experienced employee to locate responsive records. If the request does not have
  sufficient detail, Coordinators should clarify the request with the requester.

  If it is determined that a request is considered contentious, Coordinators should notify
  their issues management team of the request and timelines for response.


Step 3: Searching and Locating Records
  The legislation requires that institutions complete a ‚Äúreasonable search‚Äù for responsive
  records to a request. A search is considered reasonable when an experienced
  employee expending reasonable effort conducts a search to identify any records that
  are reasonably related to the request in locations where records in question might

  Freedom of Information and Protection of Privacy Manual                                 118
reasonably be located. An institution does not have to prove absolutely that no records
exist but, only that it conducted a reasonable search.

IPC appeals respecting the adequacy or reasonableness of a search often require an
institution to demonstrate that steps were taken to work cooperatively with a requester
in scoping their request.

The following are the essential steps that should be taken into consideration in order to
conduct a reasonable search.

Clearly understand the search parameters: Requests must be reviewed in detail by
the Coordinator and affected program areas responding to ensure there is a clear
understanding of what is being requested. Clarification is an important first step and
should be undertaken as soon as possible after receipt of the request where a request
is unclear or ambiguous. Refer to the section on clarification for more information.

Initiate the record search and ensure all relevant documents are retained: The
Coordinator should immediately notify all program areas that may have responsive
records to alert them about the request and to ensure that potentially responsive
records are secured.

Identify staff to conduct searches: Experienced staff with knowledge of the subject
matter of the request and the records management system should oversee and/or
conduct searches for responsive records.

Institutions should also assign program area contacts to be responsible for overseeing
search efforts in their program. These individuals should work in consultation with the
Coordinator to ensure they are familiar with the requirements to fulfil a reasonable
search.

Provide clear search instructions: Ensure all employees participating in the search
are provided with clear written instructions about what to search for and how to conduct
the search. Consider including a step-by-step guide instructing staff how to conduct
email, electronic and paper record searches. Coordinators should work with program
areas to develop step-by-step guides relevant to the records management practices in
those offices.

Specify a date to complete the search, keeping in mind the legislated timeline for
response and all of the tasks associated with completing the file. Time extensions may
be required in appropriate circumstances and should be discussed with the Coordinator
as soon as possible.


Freedom of Information and Protection of Privacy Manual                               119
Identify all databanks and places to be searched and develop a search plan:
Experienced staff with knowledge of the subject matter of the request and/or with
special knowledge of the institution‚Äôs record holdings should be the ones to identify the
databanks and places to be searched.

Searches must include all record repositories that may reasonably be expected to
contain responsive records including: on-site file storage and off-site storage facilities. In
this regard, records retention schedules and file plans for each office should be
consulted.

In general, a search of electronic records should be undertaken where such records
may reasonably exist in the electronic recordkeeping environment established by an
institution, including email accounts, shared drives, electronic archives, and other
electronic storage systems.

Exceptionally and in extraordinary circumstances, a search of a system maintained for
disaster recovery purposes (e.g. back-up tapes) may be considered, for example, where
evidence exists that responsive records may have been deleted or lost out of the normal
recordkeeping environment and the lost records are likely to be located on the back-up
tapes. In these cases, consultations should be undertaken with the institution‚Äôs
Coordinator, records management leads, Legal Counsel and information technology
staff prior to commencing a search.

Document search steps: All staff who participated in the search for responsive records
should document their search steps including their name, the date they conducted a
search, the databanks, the types of files, and other record holdings searched, and finally
their search results (even when a search does not locate records).

In case of an appeal, an institution should be prepared to verify in an affidavit:

   ‚Ä¢   Staff who conducted the search;
   ‚Ä¢   Staff qualifications, position, and responsibilities;
   ‚Ä¢   The dates staff conducted the search;
   ‚Ä¢   Information about the type of files searched;
   ‚Ä¢   The nature and location of the search; and
   ‚Ä¢   Any further steps undertaken.

The affidavit should be signed by the person who conducted the search or was
responsible for overseeing it.




Freedom of Information and Protection of Privacy Manual                                  120
  See Appendix 5 for a sample template form program areas can use to document search
  activities. Also see Appendix 6 for a sample template fee estimate form program areas
  can use for extensive searches that may require a fee deposit before work is completed.


Step 4: Reviewing and Analyzing Records
  When the search has been completed and all responsive records have been identified,
  the next step is for Coordinators or their staff to review and analyze the records.

  The program area that conducted the search may conduct a preliminary review of the
  records and identify any concerns regarding access. As the program area is the
  custodian of the record, they are often in a better position to understand the context of
  the records. For example, the program area would know whether records received by a
  third party were supplied in confidence.

  A review of records requires careful examination of the content of records, in
  consideration of how the legislation applies to make an access decision. The process of
  reviewing records is iterative and may take place numerous times until a final decision is
  made.

  Reviews and consultation may be necessary with program areas, Legal Counsel and
  decision-makers. In the review process, it may become evident that the following
  additional steps may be required:

     ‚Ä¢   Issuing a time extension for volume of records or consultations;
     ‚Ä¢   Notifying an affected person; and/or
     ‚Ä¢   Issuing a fee estimate and interim decision.

  Institutions should include the responses from affected parties in their analysis when
  determining whether an exemption applies to a record.

  If the request is for access to personal information for research purposes, Coordinators
  should consider whether a research agreement would be appropriate. More details on
  research agreements are provide later in this chapter.

  Coordinators and their staff should research IPC orders and judicial review case law for
  interpretation of the relevant sections of the legislation. Guidance documents and
  internal policies and procedures may also be helpful in determining whether exemptions
  or exclusions apply to a record.

  Coordinators should review previous decisions they have made regarding access to
  similar records for consistency. However, Coordinators should not rely on past

  Freedom of Information and Protection of Privacy Manual                                  121
decisions alone as the legislation may have been amended, or new case law may exist
that has changed interpretations of the legislation.

The legislation requires that institutions only sever the information that is subject to
exemptions or exclusion. Institutions should take steps to sever only exempted
information and disclose as much of the records as can be reasonably disclosed without
revealing the information that falls under one of the exemptions or exclusions.

A requester must be notified of the section or sections that apply to the severed
information. This should be noted on the record beside the severed information. In
some instances where few exemptions are cited, this information can be summarized in
the decision letter to the requester.

Severing should not be applied where the legislation exempts an entire class of records
or where severing would leave only disconnected pieces of information within a record.
For example, an entire Cabinet agenda would be exempt under the Cabinet records
exemption. In this case, the record should be withheld in full. Institutions should inform
the requester of the number of pages of records that have been withheld in full and the
exemption or exclusion that applies.

Coordinators may consider creating an index of records to help organize information to
respond to the request. The index of records typically includes sufficient detail to
support decision-making. In the event of an appeal, the IPC will require the institution to
create an index of records for any records that have been severed or withheld in full.

An index of records should include:

   ‚Ä¢   The assigned document number to each record;
   ‚Ä¢   Date of the record;
   ‚Ä¢   Page number/paragraph;
   ‚Ä¢   A general description of each record;
   ‚Ä¢   The exemption or exclusion claimed (if any); and
   ‚Ä¢   Indicate whether access has been granted or refused for all or part of the
       record.

The index of records should not include personal information or other information that
would reveal the substance of an exemption. For instance, the index of records should
not reveal the substance of a Cabinet meeting or the subject of solicitor-client privilege
communication.

See Appendix 7 for a sample template index of records.


Freedom of Information and Protection of Privacy Manual                                 122
Step 5: Finalizing Recommendations and a Decision
  Finalizing recommendations and a decision on disclosure involves preparing records
  and a draft final decision letter to the requester, as well as getting approvals and sign-off
  from decision-makers. Approvals processes will vary depending on the request and an
  institution‚Äôs delegation of authority.

  It is best practice to document the recommended decision in writing by preparing a
  briefing note. The requirements for recommendations on access will vary amongst
  institutions. Briefings may not be required for all requests. Routine requests may be
  processed without detailed documentation.

  A briefing note can form the basis for future appeal submissions to the IPC, if access
  decisions are appealed. A briefing note is especially useful in an institution where
  decision-making has been delegated to senior officials who need relevant and concise
  information to make an informed decision.

  A briefing note will typically contain six sections:

  Background: This section of the note should describe the request either as it was
  received, or in its clarified form. Other relevant matters that may help put records in
  context should also be described, for example, the current status of a program or
  initiative that records relate to. It may be important for a decision-maker to know
  whether a program is in the planning, pre-implementation or operational stage of
  development, as there are different considerations regarding the release of records,
  depending on the maturity of the project.

  Additional information, such as the outcome of any past requests for the same or similar
  information, should also be described in this section.

  Description of records: This section should describe the different types of responsive
  records. Listing the broad categories of documents is sufficient along with the general
  content of the records. This description should identify which program areas conducted
  the search for responsive records.

  When a request is voluminous, an index of records may be prepared for the decision-
  maker to review.

  Analysis: This section of the note provides decision-makers with an understanding of
  how any recommended exemption or exclusion applies to the requested records, and,
  more importantly, why there is a need under the circumstances that exist at the time of
  the request to withhold information.


  Freedom of Information and Protection of Privacy Manual                                   123
  Highlighting relevant case law and/or describing similar decisions previously made by
  the institution will help assure the decision-maker there is a sound legal basis for the
  decision. Where there are complex legal points, it may be sufficient to provide an
  overview and confirm that the Coordinator and Legal Counsel were consulted and have
  endorsed a particular recommendation.

  Exercise of discretion: This section should list the relevant factors considered by the
  Coordinator in conducting an exercise of discretion. This will ensure that the decision-
  maker has undertaken the required exercise of discretion, and will serve as evidence of
  that fact if the institution‚Äôs decision is appealed to the IPC.

  Fees: This section should provide the decision-maker with background information on
  how fees were calculated, especially for requests where a large fee estimate was
  issued and/or there is a possibility that fees may become an issue on appeal.

  A decision-maker may be required to decide whether or not to waive fees associated
  with processing a request. Where a request for a fee waiver has been requested this
  can be noted in this section along with the reason for the fee waiver request.

  Recommendation: This section provides a clear statement of the recommended
  access decision. Access recommendation statements can include:

     ‚Ä¢   Access is provided to the records in full;
     ‚Ä¢   Access is provided to the records in part, noting the exemptions or
         exclusions that are claimed,
     ‚Ä¢   Access is withheld in full, noting the exemptions or exclusions that are
         claimed; or
     ‚Ä¢   Access cannot be provided as no responsive records exist.

  Other possible decisions, such as the application of fees or the granting of a fee waiver,
  should also be stated.

  Reasonable steps should be taken to ensure approvals are finalized within legislated
  timelines.


Step 6: Preparing and Sending Records
  Upon approval of the access recommendation, Coordinators and their staff should take
  steps to issue the final decision letter to the requester and prepare records for
  disclosure.



  Freedom of Information and Protection of Privacy Manual                               124
A decision letter outlines to the requester the final decision on granting or denying
access to a record. The legislation outlines requirements for decision letters. The list
below includes legislated requirements and best practices based on the circumstances
of the request:

   ‚Ä¢   The volume of responsive records located;
   ‚Ä¢   What records are being released in full;
   ‚Ä¢   The exemptions and exclusions applied (if any);
   ‚Ä¢   The number of pages severed or withheld for each exemption or exclusion;
   ‚Ä¢   Copies of the relevant sections of the legislation;
   ‚Ä¢   If any fees apply, the final calculation of fees and required payment;
   ‚Ä¢   If any fees apply, information on requests for fee waivers;
   ‚Ä¢   The name and position of the decision-maker for the request;
   ‚Ä¢   For high volume requests, consider including a copy of the index of records;
   ‚Ä¢   Notice that the requester can appeal the decision to the IPC within 30 days
       and appeal requirements; and
   ‚Ä¢   Contact information for an individual who can answer questions regarding
       the processing of the request.

Disclosure of responsive records may be dependent upon receipt of final fee payment
and/or allowing 30 days for affected parties to appeal decisions on access.

The legislation allows a requester the option to:

   ‚Ä¢   Receive a copy of all or part of a record;
   ‚Ä¢   Examine an original record; and
   ‚Ä¢   Request a copy of all or part of a record after examining a record.

The legislation qualifies the above by using the term where it is reasonably practicable
to do so. The particular facts of each case must be considered before making a
decision.

A requester may seek records in a preferred format when it is reasonably practical for
the institution to do so. An example is providing electronic copies of records that only
exist in paper form.

A requester must be given a copy of what has been requested unless there is good
reason not to provide copies. Copies of records must be clean and legible when
possible. A copy of a record should be clearly marked where copyright protection
applies.


Freedom of Information and Protection of Privacy Manual                               125
  It may not be reasonably practicable to examine original records because of:

     ‚Ä¢   Age and condition of records;
     ‚Ä¢   Physical location of records;
     ‚Ä¢   Size or volume of records;
     ‚Ä¢   Cost of transporting records to a convenient site;
     ‚Ä¢   Security of the records cannot be ensured and original records could be
         damaged, altered, or stolen;
     ‚Ä¢   Undue inconvenience or disruption of operations of the institution; and
     ‚Ä¢   Legal requirements for maintaining records on site.

  It is recommended to have staff present when a requester is viewing original records
  especially if some of the responsive information is being withheld under an exclusion or
  exemption.

  See Appendix 4 for template decision letters.


Step 7: Closing the File
  Upon issuing the final decision letter, the request case file should be closed and
  information related to the request should be recorded for statistical compliance
  purposes.

  If an institution is using a case file management system, staff should update the system
  to indicate the date the final decision was communicated to the requester, the outcome
  of the request including whether or not any exemptions or exclusions were applied to
  the records.

  The request case file should be kept in an accessible location for the 30 day appeal
  period and in accordance with records retention schedules or policies.


Research Agreements
  FIPPA s. 21 (1) (e), Reg. 460 (10) / MFIPPA s. 14 (1) (e), Reg. 823 (10)

  In the context of a request, personal information may be disclosed for research
  purposes under a research agreement when certain conditions are met. "Research
  purposes" are distinct from administrative, operational or regulatory uses of personal
  information in that research uses do not directly affect the individual to whom the
  information relates and do not relate to the usual administration of a program.



  Freedom of Information and Protection of Privacy Manual                                126
Program audits, evaluations and operational reviews are not research for the purposes
of the legislation. "Research" means a systematic investigation into and study of
materials and sources in order to establish facts and reach new conclusions and an
endeavour to discover new or to collate old facts by the scientific study or by a course of
critical investigation.

The conditions that need to be met in order to provide access to personal information
for research purposes include:

   ‚Ä¢   The disclosure is consistent with the conditions or reasonable expectations
       of disclosure under which the personal information was provided, collected
       or obtained;
   ‚Ä¢   The research purpose for which the disclosure is to be made cannot be
       reasonably accomplished unless the information is provided in individually
       identifiable form; and
   ‚Ä¢   The person who is to receive the record has agreed to comply with the
       conditions relating to security and confidentiality prescribed by the
       regulations.

A research agreement that the institution and researcher enter into governs the
conditions listed above. The regulations under the legislation reference the standard
agreement available on the Central Forms Repository of the Government of Ontario.
The following links direct users to downloadable versions of these template agreements
for FIPPA and MFIPPA.

The following considerations should be included in a research agreement:

   ‚Ä¢   The researcher must agree to use the information only for a research
       purpose set out in the agreement or for which the person has written
       authorization from the institution;
   ‚Ä¢   The agreement must name any other persons who will be given access to
       personal information in a form in which the individual to whom it relates can
       be identified;
   ‚Ä¢   The researcher must keep the information in a physically secure location to
       which access is given only to the person and to the persons authorized;
   ‚Ä¢   The researcher must destroy all individual identifiers in the information by
       the date specified in the agreement;
   ‚Ä¢   The researcher must not contact any individual to whom personal
       information relates, directly or indirectly, without the prior written authority of
       the institution; and


Freedom of Information and Protection of Privacy Manual                                      127
     ‚Ä¢   The researcher must notify the institution in writing immediately if the
         person becomes aware that any of the conditions set out in this section
         have been breached.


Case File and Knowledge Management
  In order to manage the request process, institutions must implement a case file
  management system, whereby information associated with requests is easily
  accessible.

  Case file information may be managed either with an electronic case file management
  system, a manual register such as excel spreadsheets, or in paper format.

  In addition to case file management Coordinators should maintain documentation to
  support decision-making. To ensure effective knowledge management, resources that
  should be maintained include:

     ‚Ä¢   Guidance and resources from MGCS and the IPC;
     ‚Ä¢   Case law established by IPC orders and court decisions;
     ‚Ä¢   An institution‚Äôs corporate policies, guidelines and standards; and
     ‚Ä¢   Relevant resources and trends in other jurisdictions as required.

  Coordinators should routinely review active request files to ensure the request is on
  track to completion within the legislated timelines. Part of this review should include
  ensuring proper documentation exists for administrative actions taken on the request.

  Coordinators should contact their institution‚Äôs records management office for more
  information on their institution‚Äôs records retention policies and procedures.


Statistical Reporting
  FIPPA 34 / MFIPPA 26

  All institutions need to meet the reporting requirements of the IPC for annual reporting.
  The IPC publishes guidelines and procedures to be followed on their internet site.

  The following information should be tracked for statistical reporting purposes:

     ‚Ä¢   Number of requests received and completed within reporting year;
     ‚Ä¢   Type of record requested (e.g., general or personal information);
     ‚Ä¢   Number of requests for correction received and completed within a
         reporting year;

  Freedom of Information and Protection of Privacy Manual                               128
    ‚Ä¢   Request source (e.g., individual, agent, business, media, academic,
        association, government);
    ‚Ä¢   Number of requests transferred to or from another institution
    ‚Ä¢   Number of requests responded to within 30 days, 31-60 days, 61-90 days
        and more than 90 days;
    ‚Ä¢   Number of requests where timelines were extended under allowable time
        extension;
    ‚Ä¢   Number of requests where notices to affected parties were issued;
    ‚Ä¢   Number of requests completed within legislated timelines, including
        extended timelines;
    ‚Ä¢   Disposition of request:
           o All information disclosed
           o Partial information disclosed
           o No information disclosed
           o Request withdrawn or abandoned
           o No records exist
    ‚Ä¢   Frequency of application of exemptions or exclusions to a request; and
    ‚Ä¢   Fees collected and fees waived

 Institutions may find it useful to track other information that may help plan, assign
 resources or improve performance.


Resources
 Appendix 4: Template Letters Request Processing

 Appendix 5: Sample Records Search Form

 Appendix 6: Sample Fee Estimate Form

 Appendix 7: Sample Index of Records

 IPC: Fact Sheet ‚Äì Frivolous and Vexatious Requests

 IPC: Fact Sheet ‚Äì Reasonable Search

 IPC: Fees, Fee Estimates and Fee Waivers

 IPC: Practice 15 ‚Äì Clarifying Requests

 IPC: The Year-End Statistical Report for the IPC - Workbook and Completion Guide,
 FIPPA

 Freedom of Information and Protection of Privacy Manual                                 129
IPC: The Year-End Statistical Report for the IPC: Workbook and Completion Guide,
MFIPPA

Ontario Central Forms Repository: Access or Corrections Request Form

Ontario Central Forms Repository: Security and Confidentiality Agreement of Personal
Information for Research Purposes - FIPPA

Ontario Central Forms Repository: Security and Confidentiality Agreement of Personal
Information for Research Purposes - MFIPPA




Freedom of Information and Protection of Privacy Manual                            130
          Part III: Protection of
                  Privacy




Freedom of Information and Protection of Privacy Manual   131
Chapter 7: Privacy Fundamentals

Introduction
  One of the primary purposes of the legislation is to protect the privacy of individuals with
  respect to their personal information in the custody or control of institutions. This
  chapter introduces the concepts of privacy protection and personal information.

  The legislation protects privacy by providing rules for institutions to follow for the
  collection, use, disclosure, accurate maintenance, retention, security and disposal of
  personal information. This chapter reviews these privacy rules in detail and outlines how
  institutions can be compliant with the legislation.

  Chapter 8: Personal Information and Correction Requests explains special
  considerations for requests for an individual‚Äôs own personal information and requests to
  correct personal information in the custody or control of an institution. Chapter 9:
  Privacy Management provides best practices for how institutions can create a privacy
  management program to ensure compliance with the legislation.

  Some institutions may also be subject to the Personal Health Information Protection Act.
  These institutions will have additional considerations for rules regarding the collection,
  use and disclosure of personal health information within their custody. This chapter
  does not provide guidance on this topic, and institutions should refer to the IPC for more
  information.


Understanding Privacy
  The legislation does not define privacy explicitly. The legislation defines personal
  information and sets out privacy rules regarding the collection, use, disclosure,
  retention, security, disposal and destruction of personal information that institutions
  must follow.

  The legislation protects privacy by:

     ‚Ä¢   Providing rules as to what and how personal information can be collected
         by institutions;
     ‚Ä¢   Providing rules on how institutions handle, manage, and share personal
         information between institutions and other government organizations; and
     ‚Ä¢   Establishing procedures for individuals to access their own personal
         information, subject to some necessary and defined exemptions.


  Freedom of Information and Protection of Privacy Manual                                   132
Personal Information
  FIPPA s. 2 / MFIPPA s. 2

  The legislation defines personal information as recorded information about an
  identifiable individual. Personal information does not include information about an
  individual that has been deceased for more than 30 years.

  Information will likely qualify as personal information if an individual can reasonably be
  identified from either the information alone, or from the information in combination with
  other information.

  An important exception to the definition of a ‚Äúrecord‚Äù is that personal information may
  also include information that is not recorded (e.g., a verbal disclosure).

  Personal information includes, but is not limited to:

     ‚Ä¢   Name
     ‚Ä¢   Personal address
     ‚Ä¢   Personal email address
     ‚Ä¢   Personal telephone number
     ‚Ä¢   Race
     ‚Ä¢   National origin
     ‚Ä¢   Ethnic origin
     ‚Ä¢   Skin colour
     ‚Ä¢   Religion
     ‚Ä¢   Age
     ‚Ä¢   Date of birth
     ‚Ä¢   Sex
     ‚Ä¢   Sexual orientation
     ‚Ä¢   Marital status
     ‚Ä¢   Family status
     ‚Ä¢   Education
     ‚Ä¢   Medical history
     ‚Ä¢   Employment history
     ‚Ä¢   Financial transactions involving the individual
     ‚Ä¢   Identifying number
     ‚Ä¢   Identifying symbol
     ‚Ä¢   Photograph of the individual
     ‚Ä¢   Other identifying particular
     ‚Ä¢   Finger prints
  Freedom of Information and Protection of Privacy Manual                                 133
     ‚Ä¢   Blood type
     ‚Ä¢   Correspondence sent to an institution by the individual that is implicitly or
         explicitly of a private or confidential nature, or replies to the correspondence
         that would reveal the contents of the original correspondence
     ‚Ä¢   The personal opinions or view of the individual except where they relate to
         another individual
     ‚Ä¢   The views or personal opinions of another individual about the individual


Business Identity Information
  FIPPA s. 2 (3), s. 2 (4) / MFIPPA s. 2 (2.1), s. 2 (2.2).

  The legislation clarifies what is not personal information in a business context.

  Business identity information includes the name, title, contact information or designation
  of an individual that identifies the individual in a business, professional, or official
  capacity.

  Business identify information applies even if an individual carries out business,
  professional or official responsibilities from their home or dwelling, and the contact
  information relates to the dwelling.


Customer Service Information
  FIPPA s. 65.1 (2)

  FIPPA authorizes a service provider organization to collect the customer service
  information of an individual with their consent. In the Ontario Government
  ServiceOntario is an example of a service provider organization.

  Customer service information is a separate category of personal information that
  includes:

     ‚Ä¢   The name, address, and telephone number or other contact information;
     ‚Ä¢   The transaction or receipt number provided;
     ‚Ä¢   Information relating to the payment of any fee; and
     ‚Ä¢   Other prescribed information.




  Freedom of Information and Protection of Privacy Manual                                   134
Common Examples of Personal Information
  A name by itself is not personal information by definition. A name is personal
  information, where it appears with other personal information relating to an individual or
  where the disclosure of the name would reveal other personal information about the
  individual.

  Information that relates to an individual‚Äôs characteristics, background and history are
  common examples of personal information. Examples include race, ethnicity, country of
  origin, gender, gender identity, employment history, educational history and more.

  An identifying number is typically a unique number connected to an individual in a
  particular context. Examples include Health Card number, medical record numbers
  assigned by hospitals, Social Insurance Number (SIN), driver‚Äôs licence number, student
  numbers, and address information. It may also include personal fax numbers or Internet
  Protocol addresses.

  An identifying symbol is something that stands for, or suggests, something else by
  reason of relationship, association, convention, or accidental resemblance. Examples
  include a signature, a degree or professional designation, a tattoo, an emblem, or a
  scar.

  Other identifying particulars may include biometrics such as a handprint, footprint, iris
  scan or DNA. Behavioural biometrics may include keystrokes and voiceprints.


Privacy Rules
  In order to deliver services and programs to the public, institutions need to collect,
  manage, disclose and dispose of personal information. Institutions should ensure that
  the manner of collection, use, disclosure and disposition of personal information is in
  compliance with the privacy rules outlined in the legislation.

  As the privacy rules in the legislation are general, most institutions have other privacy
  policies, standards or procedures that assist in operationalizing these rules within the
  specific context of the institution. For instance, internal policies may outline roles and
  responsibilities within the institution for various privacy-related activities.

  The privacy rules apply to all personal information held by institutions with the exception
  of public records of personal information discussed at the end of the chapter and certain
  employment-related and labour relations records.




  Freedom of Information and Protection of Privacy Manual                                  135
  The privacy rules and their main purpose are summarized below, followed by a more
  detailed discussion.

  Authority to collect: Limits the collection of personal information by institutions to
  authorized activities.

  Manner of collection: Ensures the collection of personal information is directly from the
  individual, except in limited circumstances.

  Notice requirements: Informs the individual of the collection of personal information.

  Proper use and disclosure: Limits use and controls sharing or distribution of personal
  information for authorized activities.

  Accuracy: Ensures processes are in place to keep personal information accurate.

  Retention: Ensures that an individual can obtain access to their own personal
  information for a certain period.

  Security: Ensures the security and confidentiality of personal information.

  Disposal and destruction: Ensures disposal and destruction of personal information is
  authorized and secure.


Authority to Collect
  FIPPA s. 38 / MFIPPA s. 28

  An institution can only collect personal information under one of these conditions:

     ‚Ä¢   The collection of personal information is expressly authorized by a statute;
     ‚Ä¢   The information collected is used for the purposes of law enforcement; or
     ‚Ä¢   The collection is necessary for the proper administration of a lawfully
         authorized activity.

  The phrase ‚Äúexpressly authorized by statute‚Äù requires that the specific types of personal
  information be described in a statute (i.e., law) or a general reference to the activity be
  set out in the statute.

  ‚ÄúPurposes of law enforcement‚Äù refers to the definition of law enforcement that is outlined
  in section 2 of the legislation. Refer to the section on Law Enforcement in Chapter 5:
  Exemptions and Exclusions for more information on the definition of law enforcement.


  Freedom of Information and Protection of Privacy Manual                                  136
  ‚ÄúNecessary to administer a lawfully authorized activity‚Äù refers to instances where
  institutions need to collect personal information in order to deliver a service or program
  that is authorized by the government. For provincial ministries, authorization may
  include legislation, regulations or orders-in-council. For municipal institutions,
  authorization may include statute, by-law or regulation.

  A key word in this provision is ‚Äúnecessary.‚Äù Institutions should be able to show that each
  element of personal information that is collected for the administration of a program is
  necessary in order to properly and effectively administer the program. Personal
  information that is merely helpful to the institution would not qualify for this collection
  authorization.

  A collection occurs when an institution actively acquires the information or invites an
  individual or others to send personal information to the institution. When an institution
  collects personal information in a non-written form (i.e., verbally), this activity would also
  be considered a collection of personal information.

  Where an individual submits personal information without being requested by an
  institution, a collection is deemed to occur only if the institution keeps or uses the
  information.


Manner of Collection
  FIPPA s. 39 (1) / MFIPPA s. 29 (1)

  Direct Collection
  The legislation requires that personal information be collected directly from the
  individual to whom the information relates.

  The legislation provides limited circumstances where personal information can be
  collected indirectly, which means from a source other than the individual to whom the
  personal information relates.

  Indirect Collection
  Institutions may indirectly collect personal information when an individual consents to
  this manner of collection. Institutions should retain a record with the date and details of
  the authorization including the:

     ‚Ä¢   Personal information to be collected;
     ‚Ä¢   Source of the personal information; and


  Freedom of Information and Protection of Privacy Manual                                   137
   ‚Ä¢   Name of the collecting institution.

Other circumstances that permit indirect collection of personal information include:

   ‚Ä¢   Where other statutes provide authority to collect in another manner; or
   ‚Ä¢   Where institutions have legal authority to disclose personal information.
   ‚Ä¢   For conducting a proceeding or a possible proceeding before a court or
       tribunal;
   ‚Ä¢   For law enforcement purposes; and
   ‚Ä¢   For determining suitability of an honour or award.

Examples of statutes that provide the authority for an institution to indirectly collect
personal information include:

   ‚Ä¢   The Assessment Act;
   ‚Ä¢   The Family Responsibility and Support Arrears Enforcement Act,;
   ‚Ä¢   The Municipal Health Services Act; and
   ‚Ä¢   The Consumer Reporting Act.

Some examples of quasi-judicial proceedings or tribunals include the Ontario Municipal
Board, Property Standards Committee, Social Assistance Review Board, and
Committees of Adjustment.

In order to justify collecting personal information from another institution, an institution
should be able to demonstrate that there is a common or shared purpose for the
personal information.

Authority of the Information and Privacy Commissioner
The legislation gives the IPC the authority to permit an indirect collection where:

   ‚Ä¢   The collection is not specifically allowed under this section; or
   ‚Ä¢   It is not possible or practical to collect the personal information directly or to
       obtain authorization directly from the individual concerned.

An institution must make an application for an exemption to the IPC.




Freedom of Information and Protection of Privacy Manual                                     138
Notice Requirements
FIPPA s. 39 (2), s. 39 (3) / MFIPPA s. 29 (2), s. 29 (3)

An institution must inform the individual to whom the information relates that a personal
information collection has occurred. Whenever possible, the notice should be provided
to an individual at the time of collection, or included on program forms and
communications.

The notice to the individual must state:

   ‚Ä¢   The legal authority for the collection;
   ‚Ä¢   A reference to the specific law, section or by-law;
   ‚Ä¢   The principal and any secondary uses of the personal information; and
   ‚Ä¢   The title and business contact information of an official of the institution.

Notice must be provided each time there is a collection. The notice should address
separate legal authorities or collections if a form is used for multiple purposes.

Notice should be stated or written clearly, and provide enough detail to inform the
individual but not limit the institution unnecessarily. The needs of affected individuals
and of the business should inform the manner in which notice is provided. There are
many options available, such as providing the notice verbally, in writing, via mail outs, or
through public advertisements.

Notice should be reviewed periodically to ensure the information is accurate and up to
date. The designated official should be available and able to answer questions about
privacy and how the personal information will be used and disclosed.

Exception to Notice Requirements
FIPPA s. 39 (3) / MFIPPA s. 29 (3)

The legislation allows the Responsible Minister to grant a waiver for a notice of
collection based on the merits of the case. A waiver request should apply to a class or
group of individuals rather than an individual.

A waiver may be warranted in circumstances where:

   ‚Ä¢   There is legal authority for an indirect collection;
   ‚Ä¢   Notice would interfere with an indirect collection for unique programs or
       investigations;
   ‚Ä¢   It is impossible or very difficult to provide notice;

Freedom of Information and Protection of Privacy Manual                                 139
     ‚Ä¢   The administrative burden and cost of providing notice is excessive
         compared to the need for notice; and
     ‚Ä¢   Subsequent disclosures from an institution are inconsistent with the first
         notice.

  Appendix 8 is a form institutions can use to apply for a waiver from the Responsible
  Minister and provides an outline of considerations for institutions contemplating
  requesting a waiver of notice from the Responsible Minister. Institutions should consult
  Legal Counsel when considering a waiver of notice application.


Use and Disclosure of Personal Information
  FIPPA s. 41, s. 42 / MFIPPA s. 31, s. 32

  The legislation puts a number of conditions on the use and disclosure of personal
  information. In general, personal information can only be used or disclosed for the
  purpose for which it was collected.

  There are circumstances where the use and disclosure of personal information is
  permitted for other purposes. These purposes are discussed in the sections below.

  Consistent Purpose
  FIPPA s. 41 (1) (b), s. 42 (1) (c), s. 43 / MFIPPA s. 31 (b), s. 32 (c), s. 33

  The legislation allows institutions to use and disclose personal information where it is
  consistent with the purpose indicated in the notice of collection. A purpose is consistent
  and compatible where an individual might reasonably have expected the use or
  disclosure of the personal information at the time of collection.

  For example, disclosing the name and address of an individual to a courier company for
  the purpose of delivering a package would be considered a consistent purpose where
  the individual had requested new vehicle licence plates from the Ministry of
  Transportation.

  In the context of an indirect collection from another institution, an institution must show
  compatibility with the original collection.




  Freedom of Information and Protection of Privacy Manual                                  140
Consent
FIPPA s. 41 (1) (a), s. 42 (1) (b) / MFIPPA s. 31 (a), s. 32 (b)

An individual can provide consent for an indirect collection or for a secondary use of
personal information. An institution can also use and disclose personal information
where the individual provides consent.

Consent should be in writing and the specific information for which consent is given
must be identified. Where consent is not obtained in writing, institutions should
document:

   ‚Ä¢   The specific personal information to be disclosed;
   ‚Ä¢   To whom the information may be disclosed and for what purpose it is to be
       used;
   ‚Ä¢   The date of the consent; and
   ‚Ä¢   The institution to which consent is given.

Compliance with Other Laws
FIPPA s. 41 (1) (c), s. 42 (1) (e) / MFIPPA s. 31 (c), s. 32 (e)

An institution can use and disclose personal information for the purpose of complying
with an Act of the Legislature or an Act of Parliament or a treaty, agreement or
arrangement. The agreement or arrangement must be authorized by a federal or
provincial law.

Some examples include:

   ‚Ä¢   The Child, Youth and Family Services Act;
   ‚Ä¢   The Highway Traffic Act; and
   ‚Ä¢   The Ombudsman Act.

Performance of Duties
FIPPA s. 41 (1) (c), s. 42 (1) (d) / MFIPPA s. 31 (c), s. 32 (d)

An institution may use or disclose personal information within an institution for purposes
other than the purpose stated at collection where:

   ‚Ä¢   The record is necessary for the proper discharge of an institution‚Äôs
       functions; and



Freedom of Information and Protection of Privacy Manual                                  141
   ‚Ä¢   Needed by an officer, employee, consultant or agent of an institution for the
       performance of their duties.

There must be sufficient need and necessity. Disclosures that are merely based on
concern or convenience are not permitted under this section.

Between Organizations
FIPPA s. 42 (1) (j) to (n) / MFIPPA s. 32 (j) to (l)

The legislation allows institutions to disclose personal information for reasons other than
the reasons for which the information was collected in limited and defined
circumstances. Institutions can disclose personal information to other organizations or
representatives such as:

   ‚Ä¢   The Responsible Minister;
   ‚Ä¢   The IPC;
   ‚Ä¢   A member of the Legislative Assembly who has been authorized by a
       constituent under FIPPA;
   ‚Ä¢   A member of a bargaining agent who has been authorized by an employee
       under FIPPA; or
   ‚Ä¢   The Government of Canada in order to facilitate the auditing of shared cost
       programs (e.g., General Welfare Assistance Act).

Compassionate Circumstances
FIPPA s. 42 (1) (i) / MFIPPA s. 32 (i)

An institution may disclose personal information in compassionate circumstances to
facilitate contact with a relative or a friend of an injured, ill or deceased individual. The
personal information to be disclosed may relate to either party. Only the information
necessary to facilitate contact should be disclosed.

This section is not relevant in deciding whether personal information may be disclosed
under an access request. Compassionate circumstances considerations for processing
requests are discussed in Chapter 5: Exemptions and Exclusions in the section
regarding the personal privacy exemption.




Freedom of Information and Protection of Privacy Manual                                    142
Fundraising
FIPPA s. 41 (1) (d), s. 42 (1) (o), s. 42 (2), s. 42 (3)

Educational institutions can use and disclose personal information from their alumni
records and hospitals can use and disclose personal information from their hospital
records for fundraising purposes provided that:

   ‚Ä¢   Notice is given to an individual at first contact;
   ‚Ä¢   Notice is given periodically to an individual; and
   ‚Ä¢   A public notice is published periodically.

The purpose of the each type of notice is to inform and allow an individual to refuse or
stop the use of their personal information for fundraising.

FIPPA requires that a fundraising agreement be in place between an educational
institution or hospital with any person or associated organization that carries out
fundraising activities.

Consult PHIPA for its fundraising provision for health information custodians and
personal health information.

Health and Safety
FIPPA s. 42 (1) (h) / MFIPPA s. 32 (h)

An institution can disclose personal information in compelling circumstances affecting
the health and safety of an individual. The disclosure must be followed by notification
mailed to the last known address of the individual to whom the information relates.

Law Enforcement
FIPPA s. 42 (1) (f) to (g) / MFIPPA s. 32 (f) to (g)

An institution can disclose personal information for law enforcement purposes where
disclosure is:

   ‚Ä¢   By a law enforcement institution;
   ‚Ä¢   To a law enforcement agency in a foreign country under an arrangement, a
       written agreement or treaty or legislative authority;
   ‚Ä¢   To another law enforcement agency in Canada; and
   ‚Ä¢   To an institution or law enforcement agency in Canada to aid an
       investigation that will likely result in a law enforcement proceeding.

Freedom of Information and Protection of Privacy Manual                                143
  Appropriate internal authorizations must be obtained prior to disclosure. Personal
  information should not be disclosed for general speculation or fact-finding and should
  only be disclosed for a specific law enforcement matter.

  In some instances, the institution should insist on seeing an order or warrant before
  disclosing personal information to law enforcement.


Accuracy
  FIPPA s. 40 (2), s. 40 (3) / MFIPPA s. 30 (2), s. 30 (3)

  An institution must take reasonable steps to ensure that personal information records
  used by the institution are accurate and up to date.

  However, this standard of accuracy does not apply to personal information if collected
  for law enforcement purposes in the course of an investigation. For instance, witness
  statements collected by law enforcement officers do not need to be changed if others
  disagree with the accuracy of the contents of the witness statements.


Retention
  FIPPA s. 40 (1), Reg. 460 (5) / MFIPPA s. 30 (1), Reg. 823 (5)

  Records retention schedules may be impacted by various legal requirements, business
  needs, or information management policies. The legislation requires that personal
  information must be retained for a minimum of one year after its use to ensure that an
  individual has a reasonable opportunity to obtain access.

  The retention requirements are set out in the regulations and allow for four exceptions
  that permit a destruction to occur earlier than the one year retention rule. These
  exceptions are:

     ‚Ä¢   An individual may consent to an earlier destruction;
     ‚Ä¢   If information is credit or debit card payment data;
     ‚Ä¢   When a different retention period is set out by a municipal by-law (MFIPPA
         institutions only); and
     ‚Ä¢   Personal information stored on telecommunication logger tapes may be
         disposed after 45 days (FIPPA institutions only).

  The use of the personal information is important in determining retention requirements.
  For instance, personal information collected on surveillance video cameras would not
  be considered used if the tapes are not reviewed for security incident investigations.
  Freedom of Information and Protection of Privacy Manual                                 144
  Therefore, institutions can develop shorter retention periods for surveillance tapes that
  are not used by the institution.

  Further if institutions receive personal information in error that is not used by the
  institution, the institution is not required to retain the personal information for one year.


Security
  Reg. 460 (3), (4) / Reg. 823 (2), (3)

  An institution must ensure the security and confidentiality of personal information
  records. Institutions must implement reasonable security measures such as policies,
  procedures, and standards to address various security requirements.

  The security requirements set out in the regulations are summarized below:

     ‚Ä¢   Access to an original record must ensure security;
     ‚Ä¢   The identity of an individual seeking access to his or her personal
         information must be verified; and
     ‚Ä¢   Unauthorized access must be prevented taking into account the nature of
         the records to be protected.

  Additional security requirements are found in the disposal requirements set out in the
  next section.


Disposal
  Reg. 459 (4), (5), (6)

  The regulations require that institutions only dispose of personal information with
  authorization of the head. Institutions must maintain a disposal record setting out what
  information has been transferred or destroyed, and the date of transfer or destruction.

  The transfer and destruction of personal information must meet security requirements,
  and the reconstruction or retrieval of destroyed personal information must not be
  possible.

  Under FIPPA, institutions may dispose of personal information by:

     ‚Ä¢   Transferring it to the Archives of Ontario; or
     ‚Ä¢   Destroying it.


  Freedom of Information and Protection of Privacy Manual                                    145
  Further to this, an educational institution may only dispose of personal information by:

     ‚Ä¢   Transferring it to the archives of an educational institution with an
         agreement authorizing the transfer; or
     ‚Ä¢   Transferring it to the Archives of Ontario with an agreement authorizing the
         transfer; or
     ‚Ä¢   Destroying it.

  There is no similar regulation for MFIPPA institutions. However, MFIPPA institutions
  should follow the principles of Regulation 459 and dispose of personal information in a
  similar manner. MFIPPA institutions should seek approval before disposing of personal
  information and maintain a similar record of disposal. In accordance with the principles
  in Regulation 459, MFIPPA institutions should dispose of personal information by:

     ‚Ä¢   Transferring it to a municipal or local government archive; or
     ‚Ä¢   Destroying it.


Public Records of Personal Information
  FIPPA s. 37 / MFIPPA s. 27

  The legislation provides an exception to the privacy rules where personal information is
  maintained for the purpose of creating a record that is available to the public. However,
  the legislation does not include requirements for governing public records.

  Public records of personal information are records to which access is given to all
  members of the public. Personal information that is only accessible to some members of
  the public and not others is not a public record.

  Public records are maintained for some or all of the following reasons:

     ‚Ä¢   Allow for the proper administration of programs, activities and services;
     ‚Ä¢   Promote government accountability by providing information relating to the
         issuance of licenses, permits, government contracts, etc.;
     ‚Ä¢   Promote informed choice and consumer protection; and
     ‚Ä¢   Allow for the fair determination of rights.

  A public record does not mean that there are no terms or conditions on public access.
  For example, access to public records may include fees.

  Similar personal information may exist in multiple contexts. For instance, personal
  information may exist in one context for maintaining a public record, and another

  Freedom of Information and Protection of Privacy Manual                               146
context for the administration of a program. Despite the availability of the personal
information in the public record context, the personal information maintained in the
alternative context remains confidential.

The public records exception applies only if the institution maintains the information
expressly for the purpose of creating a public record. Other institutions cannot claim the
benefit of the public records exception unless they too have the same authority.

The following list includes examples of records containing personal information that are
maintained for the purposes of making the information publicly available:

   ‚Ä¢   Assessment rolls created under the Assessment Act.
   ‚Ä¢   Some conviction related information subject to the Regulatory
       Modernization Act.
   ‚Ä¢   Lists of electors created under the Municipal Elections Act.

How Public Records Are Created
In Ontario, public records can be created either by statute or by a policy decision of the
institution.

When a public record is created by statutes, regulations or by-laws they generally
contain terms and conditions regarding the administration of the information such as the
authority to charge fees, and the times and location of access.

When a public record is created by policy without statutory authority, the institution must
establish a strong policy rationale that the public‚Äôs right to access the information
outweighs the privacy rights of individuals to whom the information relates.

The following are some of the factors to consider in the creation and maintenance of
public records:

   ‚Ä¢   Does the public‚Äôs "need to know" outweigh the privacy rights of the
       individuals concerned?
   ‚Ä¢   Will the release of the information advance informed choice?
   ‚Ä¢   Will the information be accessible to everyone?
   ‚Ä¢   Does the public need the information to assist in the conduct of business?
   ‚Ä¢   If the information is made publicly available, would the disclosure constitute
       an unjustified invasion of personal privacy?
   ‚Ä¢   Is the personal information particularly sensitive?
   ‚Ä¢   Is the information relevant to the fair determination of a requester's rights?


Freedom of Information and Protection of Privacy Manual                                 147
Resources
 IPC: Fact Sheet ‚Äì What is Personal Information?

 IPC: Collection, Use and Disclosure of Personal Information




 Freedom of Information and Protection of Privacy Manual       148
Chapter 8: Personal Information and Correction
Requests

Introduction
  As discussed earlier in this manual, the legislation provides individuals with a right to
  request access to their own personal information in the custody and control of
  institutions. Like requests for general records, this right of access is subject to limited
  and defined exemptions. Individuals may also request institutions to correct their own
  personal information in the custody or control of the institution.

  Requests for one‚Äôs own personal information are referred to as a ‚Äúpersonal information
  requests.‚Äù Requests to correct one‚Äôs own personal information are referred to as
  ‚Äúcorrection requests.‚Äù

  This chapter outlines considerations specific to personal information requests and
  correction requests. For additional information on access request processing
  requirements, refer to Chapter 6: Managing the Request Process.


Applying the Legislation for Personal Information Requests
  As discussed in Chapter 6: Managing the Request Process, Coordinators must ensure
  requests contain enough information for an experienced employee to locate a record;
  coordinate a search for responsive records; and review responsive records to determine
  if relevant exemptions and exclusions apply to the records.

  In personal information requests, there are some additional considerations. To assess
  how the legislation applies to responsive records within the context of a personal
  information request, the basic steps include:

     1. Determine if any exclusions apply. If yes, do not go to the steps that follow.
     2. Determine if the definition of personal information is met.
     3. Determine if exemptions to an individual‚Äôs right of access to one‚Äôs own
        personal information may apply to the record.
           ‚Ä¢ Exemptions to the right of access are outlined in the legislation and
              discussed in greater detail in the sections below.
           ‚Ä¢ If yes, determine if the criteria and tests for each relevant exemption
              are met.
     4. Determine if additional legal tests apply and the criteria are met.



  Freedom of Information and Protection of Privacy Manual                                   149
     5. Determine if the record has personal information of an individual other than
        the requester.
           ‚Ä¢ If yes, determine if disclosure would result in an unjustified invasion
               of privacy.
     6. Complete the exercise of discretion if any discretionary exemptions have
        been considered to apply to a record.

  The relevant factors and issues of each step are discussed further below.


Exemptions to the Right of Access to One‚Äôs Own Personal Information
  FIPPA s. 49 (a) / MFIPPA s. 38 (a)

  The legislation outlines that while individuals have a general right of access to their own
  personal information, there are limited and defined exemptions to this general right of
  access. The legislation sets out which specific exemptions apply to personal information
  requests. The majority of the exemptions apply to personal information requests. The
  exemptions that apply are:

     ‚Ä¢   Draft by-laws and closed municipal meetings;
     ‚Ä¢   Cabinet records;
     ‚Ä¢   Advice to government/Advice or recommendations;
     ‚Ä¢   Law enforcement;
     ‚Ä¢   Civil Remedies Act;
     ‚Ä¢   Prohibiting Profiting from Recounting Crimes Act;
     ‚Ä¢   Relations with other governments;
     ‚Ä¢   Relations with Aboriginal communities;
     ‚Ä¢   Defence;
     ‚Ä¢   Third party information;
     ‚Ä¢   Economic and other interests
     ‚Ä¢   Solicitor-client privilege;
     ‚Ä¢   Danger to safety or health; and
     ‚Ä¢   Information soon to be published.

  See Chapter 5: Exemptions and Exclusions, for more information on how these
  exemptions are defined. Further discussions on any exceptions or legal tests that may
  apply to an exemption are also discussed in that chapter.




  Freedom of Information and Protection of Privacy Manual                                150
Personal Information of Other Individuals
  FIPPA s. 49 (b) / MFIPPA s. 38 (b)

  In addition to the identified exemptions, the legislation specifies that an institution may
  withhold a record where disclosure might constitute an unjustified invasion of privacy of
  an individual other than the requester.

  See Chapter 5: Exemptions and Exclusions for a detailed discussion on the personal
  privacy exemption and determining when the disclosure of information would constitute
  an unjustified invasion of privacy.


Other Exemptions
  FIPPA s. 49 (c) to (f) / MFIPPA s. 38 (c) to (e)

  The legislation also identifies specific types of records or personal information that are
  exempt.

  Correctional records: This exemption applies if the record is from a correctional facility
  and could reasonably be expected to reveal information supplied in confidence.

  Confidential evaluative or opinion material relating to awarding government
  contracts or other benefits: This exemption applies to confidential evaluative or
  opinion material compiled solely for the purpose of awarding Government contracts or
  other benefits. This exemption applies where releasing the record would identify a
  source that provided information to the institution and where it was reasonably assumed
  that their identity was expected to be held in confidence.

  Other confidential and evaluative opinion material: This exemption protects
  information that is evaluative or opinion material that is supplied explicitly or implicitly in
  confidence solely for the purpose of:

      ‚Ä¢   Assessing teaching materials or research of an employee or associated
          person of an educational institution or hospital (FIPPA only);
      ‚Ä¢   Determining admission to an academic program of an educational
          institution or hospital (FIPPA only); or
      ‚Ä¢   Determining suitability for an honour or award (both FIPPA and MFIPPA).

  Medical information: This exemption applies to personal information that could
  reasonably be expected to prejudice the mental or physical health of the requester.



  Freedom of Information and Protection of Privacy Manual                                     151
Correcting One‚Äôs Own Personal Information
  FIPPA s. 47 (2) / MFIPPA s. 36 (2)

  The legislation provides an individual who is given access to their personal information
  through the access request process, the right to request correction of the information.
  The IPC has found that there are three factors to consider in a correction request:

     ‚Ä¢   The information must be personal information;
     ‚Ä¢   The information must be inexact, incomplete or ambiguous; and
     ‚Ä¢   The information is not opinion material provided by another individual.

  Opinion material that is provided by someone other than the individual requesting the
  correction is not usually subject to correction. Opinion material can be changed where it
  can be demonstrated that it was inaccurately recorded.

  A head must determine whether the information submitted for correction can be verified
  and if so approve the correction. If the correction is made, the requester should be
  notified with a copy of the corrected record.

  If the correction is not made, the individual can require the institution to attach a
  ‚Äústatement of disagreement‚Äù to the information. The statement of disagreement
  identifies the correction was requested but was not made.

  Further, the requester should be informed of the reasons the institution refused to make
  the correction and informed of the right to:

     ‚Ä¢   Appeal the decision to the IPC;
     ‚Ä¢   Require that a statement of disagreement be attached to the information; or
     ‚Ä¢   Have the statement of disagreement provided to any person or body to
         whom the personal information was disclosed in the last 12 months.


Comprehensible Form
  FIPPA s. 48 (4) / MFIPPA s. 37 (4)

  In a response to a request from an individual for their own personal information,
  institutions should ensure that the personal information is provided in a comprehensible
  form. Sometimes personal information may be stored or used in a way that is not easily
  understood by the requester.




  Freedom of Information and Protection of Privacy Manual                                 152
 For example, institutions may be required to translate the language used in personal
 information records; increase font sizes to assist individuals with visual impairments; or
 take other actions to make the record comprehensible to the requester.


Resources
 Ontario Central Forms Repository: Access or Corrections Request Form

 Appendix 4.19 - Letter to Requester ‚Äì Decision Approving Correction of Personal
 Information

 Appendix 4.20 - Letter to Requester ‚Äì Decision Denying Correction of Personal
 Information




 Freedom of Information and Protection of Privacy Manual                                153
Chapter 9: Privacy Management

Introduction
  Chapter 7: Privacy Fundamentals outlined the rules that govern collection, use,
  disclosure, retention, accuracy, security and disposal of personal information.
  Institutions need to build a privacy management program that enables the institution to
  be compliant with these rules.

  Each institution will have slightly different needs regarding the management of privacy
  depending on the volume of personal information within their custody, the sensitivity of
  the personal information they manage, and relationships with third parties, including
  vendors. The guidance in this chapter consists of best practices that may be adapted to
  each specific institution based on their needs.

  This chapter outlines the importance of defining roles and responsibilities for privacy,
  building privacy into business practices, education and awareness; monitoring the
  effectiveness of a privacy program, and preventing and managing privacy breaches.


Define Roles and Responsibilities
  As discussed in Chapter 2: Government Roles and Responsibilities, the head of an
  institution is accountable for compliance with the legislation. In most institutions, some
  or all of the powers of a head are delegated to an officer or officers through a formal
  Delegation of Authority.

  Senior level accountability for privacy protection must be established within an
  institution. This senior official should understand the personal information holdings of
  the institution, the safeguards that are in place to protect personal information, and act
  as a champion for privacy protection at the senior level.

  Further, the management of privacy needs to be an institution-wide initiative, engaging
  employees at all levels. All employees who work with personal information are
  accountable for protecting the personal information in the custody and control of the
  institution.

  Obligations to safeguard personal information should be outlined in job descriptions,
  codes of conduct, and in performance development plans for all institutional employees
  who collect, use, or disclose personal information as part of their official duties.




  Freedom of Information and Protection of Privacy Manual                                 154
  Education about privacy, as well as the legislation‚Äôs requirements, will help employees
  understand why privacy is important, how to protect it, as well as employees‚Äô
  responsibilities with regards to safeguarding personal information. Coordinators should
  develop a privacy awareness training program that ensures employees can identify
  personal information and understand appropriate uses for the personal information.

  Institutions should also make available to the public contact information where inquires
  can be made regarding the privacy practices of the institution. Providing this contact
  information supports transparency and accountability.

  As discussed in Chapter 7: Privacy Fundamentals, this contact information should be
  included in any notice of collection. Additionally, this information should be generally
  available to the public on the institution‚Äôs public website or other publicly available
  source.


Privacy Policy
  Institutions should develop a comprehensive privacy policy that outlines the institution‚Äôs
  commitment to the protection of privacy and how the institution will be compliant with
  the privacy rules established in the legislation.

  While the legislation has general requirements, an institution‚Äôs policy can be more
  instructive to employees and outline how the institution will specifically address each
  privacy rule. The privacy policy translates abstract rules into concrete commitments that
  are relevant to the institution.

  The privacy policy should outline mandatory requirements for managing privacy, steps
  to take in the event of a privacy breach, and outline when privacy impact assessments
  are required.


Align Business Practices
  Institutions should align business practices by integrating the protection of personal
  information into existing programs, systems, and policies.

  It is easier and less expensive to build privacy protective measures into technology,
  contracts, programs, practices and business continuity plans from the beginning, than to
  retrofit them after privacy breaches occur. Therefore, institutions should consider
  privacy when identifying your strategic priorities, deliverables and performance
  measures. Privacy should not be an after-thought.

  Areas that institutions should consider adding privacy considerations include:
  Freedom of Information and Protection of Privacy Manual                                  155
   ‚Ä¢   Assessment of potential vendors and partners;
   ‚Ä¢   Contracts with vendors and partners;
   ‚Ä¢   Information sharing agreements;
   ‚Ä¢   Information technology planning;
   ‚Ä¢   Policy development; and
   ‚Ä¢   Program development.

Privacy Impact Assessment
A privacy impact assessment (PIA) is an analytical process involving several activities
and deliverables. It is not a single document or end-product.

A PIA process will support institutions in identifying and addressing privacy risks when
planning, designing, acquiring and implementing any program, system, process,
practice, service, technology, application or other deliverable that involves personal
information. It is relevant to new initiatives, as well as changes to existing information
management processes or systems.

The PIA is often described as an ‚Äúearly warning system‚Äù because it enables institutions
to identify and understand potential privacy risks, to prevent or mitigate negative privacy
consequences, and to enhance privacy protection. The PIA should be started as early in
a project‚Äôs lifecycle as possible.

The following examples are the types of projects that may involve a substantial change
to the collection, use or disclosure of personal information and, therefore, would be
benefit from a PIA:

   ‚Ä¢   New programs that will involve significant collection, use, or disclosure of
       personal information, particularly enterprise-wide initiatives or those
       involving multiple programs or partners;
   ‚Ä¢   Major changes to existing programs that will involve a significant change in
       the collection, use and disclosure of personal information including those
       resulting from: an integration of programs; broadening of target population;
       change in service delivery channels; expansion of amount or type of data
       collection; constraining or eliminating opportunities for anonymity or
       pseudonymity; or major shift toward indirect collection of personal
       information;
   ‚Ä¢   Use of new technology or one known to impact privacy that could raise
       significant privacy risks (e.g., biometrics, smart cards, drug testing, or
       technology with surveillance capabilities);



Freedom of Information and Protection of Privacy Manual                                 156
   ‚Ä¢   Major changes to technology that will alter the functionality of information
       management, access to personal information (by program/system
       administrators, customers or third parties), or security features;
   ‚Ä¢   Creation or modification of databases that will contain personal information,
       particularly where the data is sensitive or relates to a significant number of
       people, or that will link separate databases or create files that index or point
       to personal information on such databases; or
   ‚Ä¢   Creation or modification of identification and authentication schemes that
       will involve multi-purpose identifiers, biometrics or identity cards.

Privacy and Contracting Services
Institutions may contract with private sector organizations or enter into relationships with
other types of organizations to provide services on behalf of the institution. These
services may include:

   ‚Ä¢   Delivering a program on behalf of government;
   ‚Ä¢   Establishing and/or managing a database;
   ‚Ä¢   Providing system support such as troubleshooting;
   ‚Ä¢   Providing disaster recovery services;
   ‚Ä¢   Conducting research;
   ‚Ä¢   Administering a call centre;
   ‚Ä¢   Providing records storage; or
   ‚Ä¢   Supplying other services such as off-site shredding or recycling of
       information storage media.

Under these contracts it may be necessary for private sector organizations to handle
personal information or other sensitive government information. However, the institution
remains accountable for ensuring that the private sector organization manages the
personal information in accordance with the legislation.

Institutions should take steps to assess the risk and develop mitigation strategies when
contracting services that involve the collection, use, storage, retention, disclosure or
disposal of personal information. The following steps provide guidance on how
institutions can proactively protect personal information when contracting services:

Assess risk: Assess the sensitivity of the data and conduct a PIA and threat risk
analysis when considering contracting services involving personal information.

Develop an information protection plan: Develop a plan to address control,
accountability, security and mitigation strategies for any identified risks.

Freedom of Information and Protection of Privacy Manual                                   157
  Procurement and contractual requirements: Work with procurement specialists and
  Legal Counsel to build privacy and security requirements into procurement and
  contracting process. Contractual requirements can address identified risks.

  Audit and monitor contract: Conduct ongoing audit and consistently review
  contractor‚Äôs performance in managing personal and sensitive information as
  documented in the contract.


Monitor and Evaluate Privacy Program
  Institutions should periodically review privacy policies and practices, and commit to
  ongoing improvements to ensure compliance with the legislation.

  A privacy audit is a tool to support monitoring and evaluating a privacy program. A
  privacy audit is a self-assessment of the institution‚Äôs practices to identify:

     ‚Ä¢   The institution‚Äôs personal information holdings;
     ‚Ä¢   The information needs of a program areas or corporate functions; and
     ‚Ä¢   Existing privacy and information management policies, practices, and
         procedures.

  A privacy audit allows institutions to determine the extent to which personal information
  in the institution‚Äôs custody and control is maintained in accordance with the legislation. A
  privacy audit will also help identify gaps in compliance and can help focus efforts to
  improve practices within the institution.

  The basic steps to follow for a privacy audit include:

     ‚Ä¢   Take inventory of the types of personal information that are collected, used,
         disclosed, retained or disposed of by the institution;
     ‚Ä¢   Confirm the legal authority for collecting the personal information; and
     ‚Ä¢   Describe the end-to-end business processes or activities that support the
         program in delivering those services.

  Following the completion of the privacy audit, institutions should identify
  recommendations and next steps to fill in any gaps in compliance with the legislation.
  Some examples of next steps that could be identified from a privacy audit include:

     ‚Ä¢   Updating notices of collection to include all necessary requirements under
         the legislation;
     ‚Ä¢   Increased security on personal information stored within systems;


  Freedom of Information and Protection of Privacy Manual                                 158
     ‚Ä¢   Updates to an institution‚Äôs personal information bank index in the Directory
         of Records; or
     ‚Ä¢   Updated training for employees within the institution.


Privacy Breaches
  A privacy breach is an incident where personal information is collected, retained, used,
  disclosed or disposed of in ways that do not comply with the provisions of the
  legislation.

  Common examples of a privacy breach include personal information being stolen, lost,
  or accessed by unauthorized persons. Circumstances that could lead to a privacy
  breach include:

     ‚Ä¢   Personal information being mailed, faxed or emailed to a wrong address,
         email address or fax number;
     ‚Ä¢   Loss or theft of equipment containing personal information, such as external
         hard drives, laptops, are memory sticks;
     ‚Ä¢   Disposal of equipment or paper records without secure destruction of the
         personal information; or
     ‚Ä¢   A malicious cyber-attack on an information system.

  Addressing privacy breaches is an important part of an institution‚Äôs privacy management
  program. When a privacy breach occurs, both the individuals affected by the breach and
  the institutions involved are potentially vulnerable to adverse consequences:

  Consequences for individuals: Unauthorized disclosure of personal information
  violates an individual‚Äôs privacy. It creates the potential for harm, including identity theft
  and other forms of fraud, physical safety issues such as stalking or harassment,
  financial loss, adverse impact on employment or business opportunities, and damage to
  reputation.

  Consequences for institutions: In addition to not meeting the legal requirements of
  the legislation there are other consequences, including:

     ‚Ä¢   Reduced productivity as staff respond to a breach or deal with a complaint;
     ‚Ä¢   Lost public trust and confidence due to public disclosure of a major privacy
         breach;
     ‚Ä¢   Cost of emergency measures necessary to control a breach; and
     ‚Ä¢   Replacement costs of hardware, software and data affected by the breach.


  Freedom of Information and Protection of Privacy Manual                                  159
Privacy Breach Response Plan
  Despite an institution‚Äôs best efforts, privacy breaches will occur and the development of
  a privacy breach response plan will enable an institution to respond to a breach in a
  timely and effective manner.

  Having such a plan enables institutions to respond to privacy breaches in a coordinated
  manner. As part of a privacy management program, institutions should evaluate the
  effectiveness of the institution‚Äôs response plan annually and implement changes, as
  necessary. The creation of a response plan may involve documenting existing practices
  for dealing with privacy breaches.

  Given the diversity of institutions and the varied nature of privacy breaches, no ‚Äúone
  size fits all‚Äù response protocol is possible or practical. However, as a best practice,
  institutions should first assess whether a privacy breach has occurred and in the event
  of a breach, institutions may take the following actions:

     ‚Ä¢   Respond and contain;
     ‚Ä¢   Notify;
     ‚Ä¢   Investigate; and
     ‚Ä¢   Implement change.

  These steps can take place simultaneously, or in rapid succession, depending upon the
  circumstances. Each step does not have to be completed before beginning the next
  step.

  Each step of the protocol is described below and includes suggested roles and
  responsibilities for the key players.

  Once an incident or suspected incident has occurred, it should be reported by the
  employee who discovered it immediately the employee‚Äôs direct supervisor and the
  Coordinator. The Coordinator will work with the program area to determine if a privacy
  breach has occurred.

  Assessing a Suspected Breach
  When an incident has been reported to the manager or the Coordinator within an
  institution, they must immediately determine if a privacy breach has occurred. In making
  this assessment, two important questions need to be answered:

     ‚Ä¢   Is personal information involved?
     ‚Ä¢   Has the personal information been collected, used, accessed or disclosed
         in an unauthorized manner?
  Freedom of Information and Protection of Privacy Manual                               160
Not all data in the custody or control of an institution is personal information. Therefore,
the first part of your assessment is to identify the type of information affected by the
incident. See Chapter 7: Privacy Fundamentals for a definition of personal information
and examples.

If the answer to both questions is ‚Äúyes‚Äù, a privacy breach has occurred.

Respond and Contain
Coordinators or other employees should contain the privacy breach by taking corrective
action. Corrective action may include retrieving personal information, or isolating or
suspending activity on a system or website.

The privacy breach should be reported to key players within the institution including
senior leadership and impacted program areas.

The institution should document the details of the privacy breach. Documentation
should be as detailed as possible and address the ‚Äúwho, what, where, when and how‚Äù
of the incident.

Finally, Coordinators should brief senior management on the privacy breach and how it
is being managed and resolved, as appropriate.

Notify
Coordinators should work with the program area and Legal Counsel to plan notification
of the breach. Notifying the individuals impacted by the privacy breach should be the
default course of action. The purpose of providing notice of a privacy breach to the
individuals whose personal information was involved in the incident is to provide them
with sufficient information about:

   ‚Ä¢     What happened;
   ‚Ä¢     The nature of potential or actual risks of harm;
   ‚Ä¢     Appropriate action to take to protect themselves against harm; and
   ‚Ä¢     A brief explanation of the individual‚Äôs right to complain to the IPC about your
         institution‚Äôs handling of their personal information.

Such notice supports the purposes of the legislation and the institution‚Äôs responsibility to
protect the privacy of individuals with respect to personal information. It is also
consistent with the fair information practices of openness and accountability.

Notice should take place at the earliest opportunity. However, institutions should not
compound the potential harm caused by a privacy breach by providing premature notice

Freedom of Information and Protection of Privacy Manual                                    161
based on incomplete facts or taking any action that might make identity theft or other
harm more likely to occur as a result.

Notice should be delayed if law enforcement determines immediate notice would
impede a criminal investigation; or the breach resulted from a security or information
system failure, restore and test the integrity of the system before disclosing details of
the breach.

Notifying the individuals affected by a privacy breach may not be appropriate,
reasonably possible, or necessary in the following limited circumstances:

   ‚Ä¢   Law enforcement determines notice would impede a criminal investigation;
   ‚Ä¢   Notice is not in the individual‚Äôs interest (e.g., notice could potentially
       endanger an individual or result in greater harm to the individual); or
   ‚Ä¢   Notice would serve no useful purpose (e.g., if all the personal information
       involved in the privacy breach is: already publicly available; recovered
       before an unauthorized party could possibly access it; or protected by
       technology, such as encryption, that would mean unauthorized access and
       use of the data is not reasonably possible).

Coordinators should consider consulting with the IPC when planning to provide notice to
individuals impacted by privacy breaches.

See Chapter 12: Privacy Complaints, Breaches and Investigations for more information
on institutions self-reporting privacy breaches to the IPC.

Investigate
Institutions should investigate to:

   ‚Ä¢   Identify and analyze the events that led to the privacy breach;
   ‚Ä¢   Evaluate the institution‚Äôs response and containment of the breach; and
   ‚Ä¢   Recommend remedial action to help prevent future breaches.

Document the results of the internal investigation including:

   ‚Ä¢   Background and scope of the investigation;
   ‚Ä¢   Legislative implications;
   ‚Ä¢   How the investigation was conducted (who did it, who was interviewed,
       what questions asked, what policies and practices considered, etc.);
   ‚Ä¢   The source and cause of the privacy breach;
   ‚Ä¢   An inventory of systems and programs affected by the breach;

Freedom of Information and Protection of Privacy Manual                                 162
   ‚Ä¢   Determination of the adequacy of existing security and privacy policies,
       procedures and practices;
   ‚Ä¢   Assessment of the effectiveness of the institution‚Äôs response to the breach;
       and
   ‚Ä¢   Findings including a chronology of events and recommendations for
       remedial actions.

Senior management should be informed of the results of the investigation to ensure
recommendations are enacted.

Implement Change
The final step of the response plan is to implement changes within the institution to
prevent future privacy breaches. When determining what changes and remedial action
needs to be implemented, Coordinators should consider if it is necessary to:

   ‚Ä¢   Review relevant information management systems to enhance compliance
       with the legislation;
   ‚Ä¢   Amend or reinforce your existing policies and practices for managing and
       safeguarding personal information;
   ‚Ä¢   Develop and implement new security or privacy measures;
   ‚Ä¢   Train staff on legislative requirements, security and privacy policies,
       practices and procedures to reduce the potential of future breaches; or
   ‚Ä¢   Test and evaluate remedial actions to determine if they have been
       implemented correctly, and if your policies and practices need to be
       modified.

In addition, Coordinators should evaluate whether the notice individuals impacted by the
privacy breach was done in a reasonably timely manner, whether the tone and content
of the notice was appropriate, and whether there was sufficient support provided to
individuals impacted by the breach.


Resources
IPC: Planning for Success: Privacy Impact Assessment Guide

IPC: Privacy Breach Protocol Guidelines for Government Organizations

IPC: Thinking About Clouds? Privacy, Security, and Compliance Considerations for
Ontario Public Sector institutions

IPC: Open Government and Protecting Privacy

Freedom of Information and Protection of Privacy Manual                               163
IPC: Fact Sheet, Video Surveillance




Freedom of Information and Protection of Privacy Manual   164
   Part IV: The Office of the
   Information and Privacy
   Commissioner of Ontario




Freedom of Information and Protection of Privacy Manual   165
Chapter 10: Interacting with the IPC

Introduction
  Chapter 2: Government Roles and Responsibilities introduced the IPC and the purpose
  and responsibilities of that office. This part of the manual will provide detailed
  information about the important role and powers of this independent Officer of the
  Legislature.

  This chapter outlines expectations for institutional staff when interacting with the IPC
  and outlines when and how to seek advice or comments from the IPC on new initiatives.

  Chapter 11: Appeals Process outlines in detail the IPC appeal process. Chapter 12:
  Privacy Complaints, Breaches and Investigations provides more information on privacy
  complaints and investigations.


Guiding Principles
  Below are guiding principles that employees of institutions should follow when
  interacting with the IPC:

  Independence: The IPC is independent of the government. This independence enables
  the IPC to fulfil its role of independently reviewing the decisions and practices of
  government institutions concerning access to information and the protection of privacy
  when collecting, using and disclosing personal information under the legislation.

  Authority: The IPC has specific powers under the legislation to ensure institutions
  comply with the provisions of the legislation. Employees dealing with the IPC should
  recognize the IPC‚Äôs legal authority.

  Responsibility: Employees involved with an IPC matter should ensure a positive and
  constructive approach to working with the IPC. In particular, Coordinators, managers
  and supervisors should ensure that this constructive relationship is maintained
  throughout the course of an appeal, privacy investigation or any matter related to the
  IPC‚Äôs mandate.

  Cooperation: It is the responsibility of employees to provide the IPC with timely access
  to information and records necessary for the IPC to perform its statutory responsibilities.




  Freedom of Information and Protection of Privacy Manual                                166
Obligations and Best Practices for Staff
  When involved with the IPC, employees of institutions subject to the legislation must:

     ‚Ä¢   Comply with the law, with policies and any orders made by the IPC
         pursuant to the exercise of the IPC‚Äôs powers under the legislation;
     ‚Ä¢   Provide clear and full disclosure of requested information to the IPC;
     ‚Ä¢   Subject to legal advice, permit access to the institution‚Äôs employees for the
         purposes of interviews in the context of a privacy investigation or
         adjudication where possible;
     ‚Ä¢   Respect that the IPC has the authority to decide the pertinence of
         information requested from institutions subject to the legislation;
     ‚Ä¢   Not interfere with the IPC‚Äôs exercise of powers under the legislation; and
     ‚Ä¢   Act honestly, ethically, and with integrity and remember employee actions
         and comments always reflect on the institution.

  In addition, employees of institutions are expected to:

     ‚Ä¢   Provide timely and accurate responses and assistance and not limit or
         unreasonably delay the time required to provide information;
     ‚Ä¢   Treat IPC staff with respect, courtesy and fairness;
     ‚Ä¢   Exercise general diligence, care and attention in responding to the issues
         raised by the IPC; and
     ‚Ä¢   Help foster and support a positive working relationship with the IPC.

  In adopting the above best practices, employees of institutions must understand that
  access to information is fundamental to the IPC‚Äôs role. Coordinators, managers and
  supervisors have a particular responsibility to ensure a positive working relationship with
  the IPC.


Seeking Comments or Advice on New Initiatives
  In addition to its important role as the oversight body for access and privacy legislation,
  the IPC is also a key government stakeholder for transparency and accountability. The
  legislation gives the IPC the authority to offer comment on the privacy protection
  implications of proposed legislative schemes or government programs.

  The IPC provides feedback to government institutions that have consulted with the IPC
  on matters that have access and privacy implications. The IPC‚Äôs Director of Policy is the
  point of contact for institutions who wish to consult with the IPC on access and privacy



  Freedom of Information and Protection of Privacy Manual                                 167
  matters. It is important to obtain IPC feedback early and address any concerns in the
  early stages of an initiative such as a new project, strategy or program.

  Examples where consultation with the IPC may be appropriate include:

     ‚Ä¢   When an initiative would involve new collections of personal information;
     ‚Ä¢   On early drafts of legislation that impact access to government records and
         privacy; or
     ‚Ä¢   When the institution is contemplating use of new technologies where
         impacts on privacy are not well known.

  Providing background policy documents that support an initiative is important in
  enabling informed and meaningful IPC comment. For example, if a program area has
  completed a privacy impact assessment on a new initiative, it may be beneficial to
  provide this documentation to the IPC prior to meeting. Institutions may want to involve
  the IPC especially when the matter has significant impact on the public. The IPC will
  manage highly sensitive and confidential matters appropriately by restricting staff
  involvement and securing information appropriately.

  It is best practice for institutions to include their Coordinator and Legal Counsel when
  contemplating consulting with the IPC.


IPC Initiated Contact with Institutions
  There may be situations where the IPC initiates contact with institutions on issues that
  are of public interest. The IPC may contact an institution directly, or the IPC may first
  contact MGCS, as the Minister of MGCS is the Responsible Minister for the legislation.
  In these cases, the IPC may request information through email or through letters to
  ministers‚Äô offices or senior employees, in order to fully understand the issue. It is
  important that institutions provide timely responses to these types of IPC inquiries. An
  institution‚Äôs Legal Counsel and Coordinator may be able to assist with these matters.

  The IPC may also appear before, or make submissions to, various Standing
  Committees. The purpose of these submissions is for the IPC to provide its views or
  recommendations to the Standing Committee on bills that have access and privacy
  implications. Employees should provide assistance and information to the IPC relating
  to these proceedings, when requested.


Resources
  Information and Privacy Commissioner of Ontario - Main Page

  Freedom of Information and Protection of Privacy Manual                                168
Chapter 11: Appeals Process

Introduction
  Individuals may request a review of access decisions under the legislation to the IPC.
  This review is referred to as an appeal.

  This chapter provides an overview of the appeals process, general requirements, and
  timelines. The IPC is responsible for developing and managing the appeals process and
  procedures. Institutions are required to follow the IPC‚Äôs Code of Procedures and related
  practice directions.

  It is important for Coordinators to work with Legal Counsel to establish roles and
  responsibilities in the appeals process.


Reasons for an Appeal
  FIPPA s. 50 / MFIPPA s. 39

  The legislation provides a right of appeal to requesters and affected parties impacted by
  a request. The legal term for an individual who makes an appeal is an appellant. The
  IPC keeps the identity of the appellant confidential in public orders regarding the appeal.
  The IPC does disclose the identity of the appellant to the institution involved in the
  appeal.

  An appeal can be triggered by different events and can occur at different stages of the
  request process. Fees, time extensions, and deemed refusals may all be appealed
  during the request process.

  An individual does not have appeal rights when a request for information is handled
  informally or outside of the formal access request process.

  The main reasons for an appeal include:

  Access denied: Requester disagrees with an institution‚Äôs decision to deny access to
  some or all of the information requested. This includes information that was determined
  to be subject to an exemption and information that was determined to be subject to an
  exclusion.

  Affected party: Affected party objects to an institution‚Äôs decision to provide access to
  personal or proprietary information of the affected party.


  Freedom of Information and Protection of Privacy Manual                                169
 Correction denied: Requester disagrees with an institution‚Äôs decision to deny
 correction of personal information.

 Deemed refusal: Requester did not receive any response from the institution within 30
 days of the institution‚Äôs receipt of the request.

 Failure to disclose: Requester received decision letter from the institution; however,
 the institution failed to disclose records after fee payment.

 Fees and fee waiver: Requester disagrees with the amount of fees being charged or
 disagrees with the institution‚Äôs decision not to grant a fee waiver.

 Reasonable search: Requester disagrees with an institution‚Äôs decision regarding the
 existence of records and believes efforts to locate records were inadequate.

 Time extension: Requester disagrees with an institution‚Äôs decision to extend the time
 limit to respond to a request.


IPC Powers
 The IPC manages the appeal process and coordinates the exchange of information,
 relevant to each stage of the appeal. Coordinators may deal with a number of
 individuals at the IPC on an appeal.

 The scope and complexity of an appeal and the number of parties involved in an appeal
 are some of the factors the IPC takes into account in determining an approach to
 resolution. The IPC can depart from its Code of Procedure where it is appropriate, and
 as long as it does not prejudice the parties.

 The IPC has authority to obtain and examine records. The IPC never releases records
 at issue to a requester or an affected party and is prohibited from disclosing information
 provided to the IPC as part of the appeal process.

 Institutions do not waive solicitor-client privilege if legal advice is contained in a
 particular record. The same reasoning applies to confidentiality provisions contained in
 other statutes.

 The IPC decides matters under appeal by issuing an order in the form of a report. An
 IPC order is binding on all parties to the appeal. An order is not subject to appeal;
 however it may be subject to a request for reconsideration or a judicial review. These
 processes will be discussed later in the chapter.

 The IPC has broad order-making powers to:
 Freedom of Information and Protection of Privacy Manual                                170
     ‚Ä¢   Uphold or overturn an institution‚Äôs decisions related to the application of
         exclusions, exemptions, and fees;
     ‚Ä¢   Order an institution exercise or re-exercise its discretion;
     ‚Ä¢   Order an institution to conduct further searches for records;
     ‚Ä¢   Order an institution to produce a record where certain machine readable
         records are requested;
     ‚Ä¢   Issue an interim order on specific issues or records and defer a final
         decision; and
     ‚Ä¢   Order an institution to produce the records to the IPC.

  The IPC notifies all parties to the appeal that an order has been issued and sends a
  copy of the order to the parties.


Stages of Appeal
  The appeal process has four stages:

     1. Initiating an Appeal: A requester files an application for an appeal.
     2. Intake: The IPC screens appeals received and decides whether it can be
        resolved informally, dismissed or moved toward mediation or adjudication.
        Orders can be issued at this stage for ‚Äúdeemed refusals‚Äù and ‚Äúfailure to
        disclose‚Äù appeals.
     3. Mediation: A mediator contacts the parties, investigates, attempts to effect
        settlement, and issues a report.
     4. Adjudication: An adjudicator contacts the parties, begins the inquiry,
        receives representations, and issues an order.


Timelines
  The appeal process does not have an official start and end date. The length of time
  required for an appeal depends on the complexity of the issues within the appeal,
  number of parties involved, and how many appeals the IPC is managing.

  However, within the appeal process there are specific time limits by which institutions
  must respond to the IPC. The table below summarizes the time limits and further
  discussion of the steps follows in the chapter.

  Appeal Stage                      Action                               Timeline
  Initiating an appeal    Requester or affected          Requester or affected party must file
                          party files an appeal          an appeal within 30 days of date of
                                                         institution‚Äôs decision

  Freedom of Information and Protection of Privacy Manual                                171
Appeal Stage                    Action                            Timeline
Intake stage          Institution receives        The institution must identify affected
                      notice of appeal            parties for IPC as soon as possible
                                                  upon receipt of notice of appeal
Intake stage          Institution receives        The institution has eight days to
                      Confirmation of Appeal      respond to the IPC and provide
                      from IPC                    requested documentation
Intake stage          IPC may grant a time        When a time extension is granted,
                      extension for providing     the institution has up to fourteen
                      records to IPC              additional days to provide requested
                                                  documentation
Mediation stage       IPC sends Notice of         The Notice of Mediation sets out a
                      Mediation                   35 day deadline for an institution to
                                                  claim any additional discretionary
                                                  exemptions
Mediation stage       IPC initiates mediation     The mediation process is generally
                      stage of appeal             four months; however, timelines can
                                                  be flexible depending on the
                                                  circumstances
Adjudication stage    IPC sends Notice of         Institutions must submit
                      Inquiry                     representations within 21 days of
                                                  receipt of Notice of Inquiry
Adjudication stage    IPC may grant               Any request for a time extension
                      extension of time in        exceeding one week must be made
                      providing                   in writing, including an explanation
                      representations             for the delay, to the adjudicator
Adjudication stage    Institution may issue a     The new decision letter must be sent
                      new decision letter to      to the appellant within 35 days of the
                      appellant                   Notice of Inquiry unless IPC
                                                  specifies a date. The adjudicator
                                                  should be copied on the letter
Adjudication stage    Institution may raise       Institutions may raise constitutional
                      constitutional questions    questions within 35 days after
                                                  receiving the Notice of Inquiry
Adjudication stage    Appeal abandoned if         The IPC will determine an appeal is
                      IPC does not get a          abandoned after 21 days have
                      response from the           passed where the IPC has been
                      appellant                   unable to contact or receive a
                                                  response from the appellant


Freedom of Information and Protection of Privacy Manual                           172
  Appeal Stage                      Action                               Timeline
  Adjudication stage       Comply with order ‚Äì no         Institutions must comply with an
                           affected parties               order by the date that is specified in
                                                          the order
  Adjudication stage       Comply with Order ‚Äì            Institutions must comply with an
                           affected parties               order within 35 days, but no earlier
                                                          than 30 days of date of order, unless
                                                          another date is specified in the order
  Adjudication stage       Retain appeal records          60 days from date of order
  Review of IPC            Application for a              Within 21 days of date of order or
  Order                    reconsideration order          before the first specified date or time
                                                          period in the order
  Review of IPC            Application for Judicial       Within 30 days of date of order
  Order                    Review
  Review of IPC            Institution notifies IPC to    As soon as possible, by the end of
  Order                    return records                 the 3-month or 1-year period


Initiating an Appeal
  Institutions are required to provide requesters information about their appeal rights and
  the appeal process. This information should be provided to the requester in fee estimate
  letters, time extension letters and decision letters.

  An appellant must file a written notice with the IPC Registrar within 30 days of receiving
  a decision. The legislation allows counsel or an agent to appeal on behalf of an
  appellant with appropriate authorization.

  The IPC asks the appellant to complete an appeal form including the following
  information:

     ‚Ä¢   The appellant‚Äôs contact information (name, address, telephone);
     ‚Ä¢   The institution‚Äôs name and file number assigned; and
     ‚Ä¢   A brief explanation of the reason for the appeal.

  In addition, the IPC requires a copy of the institution‚Äôs decision letter and a copy of the
  original request. Where the appellant does not have a copy of the original request, the
  institution can provide this to the IPC.

  The appellant is responsible for paying appeal fees to the IPC. However, there are no
  appeal fees for an affected party appealing an institution‚Äôs decision, or for subsequent
  appeals arising from a decision.

  Freedom of Information and Protection of Privacy Manual                                  173
  Appeal fees cannot be waived.

  The appeal fee for decisions (exemptions and exclusions applied), deemed refusals,
  fees, fee waivers and reasonableness of search for access requests for general records
  is $25.00.

  The appeal fee for decisions (exemptions and exclusions applied), deemed refusals,
  fees, fee waivers and reasonableness of search for access requests for one‚Äôs own
  personal information is $10.00.


Intake Stage
  During the intake stage, the IPC screens an appeal and either dismisses, resolves, or
  streams the appeal to a later stage of the process. Intake analysts have delegated
  authority to screen out files where:

     ‚Ä¢   The matter, on its face, is not within the IPC‚Äôs jurisdiction; or
     ‚Ä¢   The matter falls within the IPC‚Äôs jurisdiction, but the matter, on its face, is
         one that the IPC believes should not proceed through the appeal process.

  Institutions may receive the following notices from the IPC during the intake stage:

  Notice of Appeal: This notice informs the institution that an appeal has been filed and
  may also notify any affected parties that an appeal has been filed.

  Confirmation of Appeal: This notice informs institutions that an appeal has been
  accepted. The letter will include the name of the requester, the appeal file number, the
  IPC contact, and IPC requirements.

  Notice of Inquiry: The IPC may send a Notice of Inquiry during the intake stage for
  appeals regarding deemed refusals or where institutions have issued decisions, but
  have failed to disclose the records to the requester. This notice informs all parties about
  the nature of the appeal and requests representations from parties in the appeal.

  During the intake stage, institutions may be contacted by the IPC to provide additional
  information including information about affected persons.




  Freedom of Information and Protection of Privacy Manual                                  174
Mediation Stage
  A mediator will contact the parties to investigate the circumstances of an appeal and
  attempts to effect a settlement of the issues.

  The mediator can review the records and provide an opinion to the parties on the likely
  outcome of the appeal at adjudication. The mediator can convene conference calls with
  the parties to canvass the issues, or engage in shuttle mediation by speaking separately
  with the appellant and the institution.

  Mediators may request additional information from institutions, including information for
  the purpose of notifying affected persons. Institutions can issue revised decisions during
  mediation to disclose additional records to an appellant.

  Most appeals are resolved through mediation. Even if an appeal is not settled in its
  entirety, mediation can result in streamlining the issues that need to be adjudicated.

  The mediator issues a report to the parties. If the mediation is not successful in settling
  the dispute, the appeal moves on to the adjudication stage.


Adjudication
  The first step in the adjudication stage usually begins when an adjudicator sends a
  Notice of Inquiry to the parties bearing the initial onus. This notice informs parties to an
  appeal of the issues to be decided in the appeal and requests representations on those
  issues. The adjudicator also summarizes the background of the appeal and describes
  the records at issue. Once representations are received from the first party, the
  adjudicator usually sends a Notice of Inquiry to the other party or parties inviting
  representations.

  Making Representations
  Representations are arguments or evidence presented to the adjudicator to persuade
  them to resolve the appeal in a particular way. The legislation places the burden of
  proof on different parties depending on the issue:

     ‚Ä¢   The appellant when claiming that a record should to exist; and when
         claiming a public interest override in favour of disclosure;
     ‚Ä¢   The institution when claiming an exemption or exclusion applies; and
     ‚Ä¢   The affected party when claiming harms and arguing against disclosure.

  When preparing representation an institution should consider:


  Freedom of Information and Protection of Privacy Manual                                  175
   ‚Ä¢   The type of appeal;
   ‚Ä¢   Address each of the issues raised by the adjudicator;
   ‚Ä¢   Provide detailed evidence and argument;
   ‚Ä¢   Organize representations logically;
   ‚Ä¢   Demonstrate the applicability of exemptions;
   ‚Ä¢   Reference the most recent orders and decisions where possible;
   ‚Ä¢   Indicate which information in the representations the institution regards as
       confidential and should be withheld from other parties and the reasons why;
   ‚Ä¢   Provide copies of reference materials;
   ‚Ä¢   Provide an index of appeal documents; and
   ‚Ä¢   Use plain language.

Written or Oral Submissions
The IPC‚Äôs general practice is to conduct inquiries through written submissions. It may
require an institution to make certain submissions by affidavit. Any party to the appeal
may request an opportunity to make oral submissions, and an oral hearing may be held
if the IPC believes that it would aid in an exploration of the issues.

In an inquiry, the IPC has the power to summon and examine witnesses under oath.
Anything said or any document or thing produced during an inquiry, whether oral or
written is privileged to the same extent as it would be before a court.

An inquiry must be conducted in a manner that protects the confidentiality of the records
pending the IPC‚Äôs decision. It is not a public hearing, witnesses are generally not cross-
examined, and testimony may not be used in other proceedings.

Sharing Representations
An adjudicator may share representations with other parties to the appeal, unless there
are overriding confidentiality concerns. The adjudicator will consider if:

   ‚Ä¢   Disclosure would reveal the substance of the record;
   ‚Ä¢   The information would be exempt if it was in a record;
   ‚Ä¢   The information was communicated to the IPC in confidence with the
       understanding that it would not be disclosed; and
          o Confidentiality is essential to maintaining relations with the IPC;
          o The relation is one which in the opinion of the community should be
              maintained; and
          o The injury to the relation would outweigh the benefit correctly
              determining the appeal.


Freedom of Information and Protection of Privacy Manual                               176
  Although a party is not entitled to access an opposing party‚Äôs submissions, the IPC may
  decide to share submissions, in part or in whole, with other parties. The IPC gives
  parties the opportunity to object to the sharing of their submissions before deciding
  whether or not to share them. A party may also request a copy of the submission
  directly from the institution.

  Affidavit and Other Evidence
  An affidavit is a statement of facts which is sworn to (or affirmed) before an officer
  who has authority to administer an oath. Affidavits are a common method of
  providing evidence. The IPC may request an affidavit be provided in mediation or
  adjudication. An affidavit may be shared with consent of the parties.

  Institutions may decide that an affidavit is an effective way of providing evidence
  regarding searches, existence of records and contentious issues.

  Affidavit evidence should be:

     ‚Ä¢   Detailed enough for the parties to understand the contents; and
     ‚Ä¢   Confined to facts within the personal knowledge of the person swearing.

  It is a criminal offence under the Criminal Code to swear a false affidavit.

  Claiming Additional Exemptions
  The IPC may allow an institution to claim additional discretionary exemptions at the
  inquiry stage. This requires that an institution send a new decision letter to the
  appellant. In these circumstances, further representations may need to be submitted by
  the parties.

  The adjudicator may at any stage request supplementary representations by the parties.


Providing Records to the IPC
  Once an appeal is received by the IPC, institutions will be required to:

     ‚Ä¢   Notify the IPC if the records are voluminous (e.g., 500 pages plus and more
         than three exemptions);
     ‚Ä¢   Provide relevant records;
     ‚Ä¢   Advise the IPC if the institution plans to claim additional discretionary
         exemptions; and
     ‚Ä¢   Notify the IPC if records are to be returned to the institution.

  Freedom of Information and Protection of Privacy Manual                               177
  Only the documents in the appeal should be sent to the IPC. The relevant documents
  include:

     ‚Ä¢   A copy of the original request and its file number;
     ‚Ä¢   A copy of the head's decision letter;
     ‚Ä¢   Any correspondence related to the request or the decision making process;
     ‚Ä¢   An index of the records under appeal and the exemptions applied to the
         records;
     ‚Ä¢   A severed copy of the records under appeal where severances have been
         made, and
     ‚Ä¢   Unsevered copy of the records.

  The severed and unsevered copies of records do not need to be provided to the IPC in
  appeals related to fee estimates, time extensions or reasonableness of search.

  Institutions should provide the IPC with a well-organized, legible, and complete package
  of records. The IPC may agree to on-site inspection of records in special circumstances
  such as highly voluminous requests or fragile records.

  Records should be sent securely to the IPC. Records should be sent by bonded courier
  or may be delivered by an employee of the institution.


On Hold and Abandoned Appeals
  The IPC may place an appeal ‚Äúon hold‚Äù for later resolution. The IPC may treat an
  appeal as abandoned if an appellant has not responded to the IPC within a time period
  specified by the IPC.


Reconsideration of Orders
  A reconsideration of an order is not an appeal of an order on its merits. An adjudicator
  may reconsider a decision where there is:

     ‚Ä¢   A fundamental defect in the adjudication process;
     ‚Ä¢   Some other jurisdictional defect in the decision; or
     ‚Ä¢   A clerical error, accidental error or omission or other similar error in the
         decision.

  A request for reconsideration must comply with the criteria above and be made within
  21 days or before the first specified date or time period in the order. An institution must
  still comply with the terms of an order unless otherwise directed by the IPC. A request

  Freedom of Information and Protection of Privacy Manual                                 178
  for reconsideration does not preclude a party from seeking other legal remedies for
  review that may be available.


Constitutional Issues
  A constitutional question may be raised by one of the parties in an appeal or by the IPC
  for one of the parties to address. A constitutional question raises issues relating to:

     ‚Ä¢   The constitutional validity or applicability of the legislation, a regulation or a
         by-law, or a rule of common law; or
     ‚Ä¢   A claim for a remedy under the Canadian Charter of Rights and Freedoms.

  The IPC must be served notice of a constitutional question using the IPC‚Äôs form along
  with written submissions.


Judicial Review
  Judicial review proceedings are governed by the Judicial Review Procedure Act. The
  criteria for a judicial review by a party to an appeal include where it is alleged that the
  IPC‚Äôs decision was patently unreasonable or otherwise outside the IPC‚Äôs jurisdiction.

  Applications for judicial review are made to the Divisional Court and must be made
  within 30 days of the order or reconsideration order. Appellants and institutions may file
  judicial review applications. If an appellant or institution does not file an application for
  judicial review within 30 days, the institution must comply with the IPC‚Äôs order.


Resources
  IPC: Appeals Process

  IPC: Filing an Appeal

  IPC: Code of Procedures

  IPC: Mediation Tips for Institutions

  IPC: Best Practices for Institutions in Mediating Appeals

  IPC: Practice Direction #1 ‚Äì Providing Records to the IPC During an Appeal

  IPC: Practice Direction # 2 ‚Äì Participating in a Written FIPPA or MFIPPA Inquiry



  Freedom of Information and Protection of Privacy Manual                                     179
IPC: Practice Direction #3 ‚Äì Guidelines for Individuals Whose Personal Information is at
Issue in an Appeal

IPC: Practice Direction #4 ‚Äì Guidelines for Parties Whose Commercial or Business
Information is at Issue in an Appeal

IPC: Practice Direction #5 ‚Äì Guidelines for Institutions in Making Representations

IPC: Practice Direction #6 ‚Äì Affidavit and Other Evidence

IPC: Practice Direction #7 ‚Äì Sharing of Representations

IPC: Practice Direction #8 ‚Äì Reasonable Search Appeals and Fee Appeals

IPC: Practice Direction #9 ‚Äì Constitutional Questions

IPC: Practice Direction #10 ‚Äì Appeal fees

IPC: Practice Direction #11 ‚Äì Appeal Form




Freedom of Information and Protection of Privacy Manual                              180
Chapter 12: Privacy Complaints, Breaches and
Investigations

Introduction
  The IPC has authority under the legislation to investigate matters related to an
  institution‚Äôs collection, use, disclosure, retention, security, disposal and destruction of
  personal information.

  When an individual believes an institution has collected, used or disclosed personal
  information in a manner not consistent with the legislation, they may file a privacy
  complaint with the IPC.

  Institutions may also self-report privacy breaches where the institution has discovered
  personal information has been accessed by an unauthorized individual or was disclosed
  in a manner not consistent with the legislation, either intentionally or in error.

  In rare cases, an IPC initiated privacy complaint may be opened. This could involve a
  matter that the IPC considers worthy of investigation but where there is no complainant.

  This chapter outlines the IPC‚Äôs approach to investigating privacy complaints and
  breaches.


Privacy Complaints
  Individuals have the right to complain to the IPC when they believe that an institution
  has not complied with the privacy rules on the collection, use, disclosure, retention,
  security, disposal and destruction of their personal information.

  Privacy complaints are usually the result of a privacy breach, which is an incident where
  personal information is collected, retained, used, disclosed or disposed of in ways that
  do not comply with the provisions of the legislation.

  Individuals are encouraged to attempt to resolve privacy complaints directly with
  institutions. Institutions have an obligation to work with individuals in addressing privacy
  concerns.

  If an individual believes an institution has not adequately addressed their concerns, the
  individual may file a privacy complaint with the IPC. The IPC requires the individual to
  complete a form outlining the following information:


  Freedom of Information and Protection of Privacy Manual                                   181
     ‚Ä¢   The individual‚Äôs own contact information (name, address, telephone);
     ‚Ä¢   The institution‚Äôs name;
     ‚Ä¢   Details of the nature of the complaint; and
     ‚Ä¢   Details of how the complaint should be resolved.

  The privacy complaint form should be filed with the IPC‚Äôs Registrar.


Institution Reported Privacy Breaches
  Institutions may also self-report privacy breaches and incidents to the IPC. While the
  legislation does not require institutions to report privacy breaches to the IPC, it is best
  practice for institutions to self-report substantial breaches to the IPC.

  Reporting substantial privacy breaches allows the IPC to understand the nature of the
  breach and steps being taken to contain and respond to the breach. Proactively sharing
  information gathered from the institution‚Äôs own internal investigation and details about
  how the institution is responding to the breach assists the IPC in determining if further
  investigation or remedial action is required.


Privacy Investigation Process
  The IPC has broader authority in the context of privacy investigations. A privacy breach
  is likely the most common reason for an investigation. In addition, the IPC may also
  comment on the privacy protection implications of proposed legislative schemes or
  government programs.

  As a result of an investigation, the IPC can order an institution to cease a collection of
  personal information practice, and destroy a collection of personal information that
  contravenes the legislation.

  The IPC can handle privacy investigations informally and formally. In either case, the
  institution needs to provide information and participate in discussions and meetings with
  the IPC.

  A privacy complaint can be handled informally when the complainant and institution can
  agree on an approach to resolve the issue. This usually involves information sharing to
  achieve an understanding of what happened or why the information was used in a
  certain way.

  In these cases, the IPC may confirm the resolution by writing a letter to the institution
  rather than publishing a formal investigation report. If the complaint is not resolved in a

  Freedom of Information and Protection of Privacy Manual                                  182
 mutually satisfactory way, a formal privacy investigation will follow. There are also times
 when the individual who submitted the complaint is not satisfied, but the IPC dismisses
 the complaint at an early stage based on the information presented to it. If the complaint
 is not resolved or dismissed at an early stage, a formal investigation may proceed.

 In a formal privacy investigation, the IPC follows the main steps outlined below:

 Notice and request for information: The IPC notifies the institution that a complaint
 has been received and requests information relating to the institution's position on the
 matter.

 Investigation: The investigation may require a personal visit to the institution by the
 investigator and/or meetings with key program staff. Copies of relevant documents must
 be provided.

 Draft report: The IPC may conclude with a letter in straightforward matters. In other
 cases, the matter will proceed with a draft report. Where a privacy breach has occurred,
 the draft report may include recommendations to prevent future breaches.

 Both the institution and the individual who submitted the complaint are asked to
 comment on errors or omissions in the draft report.

 Final report: Formal investigations may result in a formal public report, usually if the
 matter is of interest to the public. Where recommendations have been made, the IPC
 will request evidence of implementation of the recommendations within six months of
 the date of the final report.

 Evidence can be in the form of a letter and supporting documentation, such as a copy of
 a new policy or notice form.

 Follow-up: Within six months, the IPC will contact the institution to find out the status of
 recommendations, and if nothing has been done, the reason why.


Resources
 IPC: Filing a Privacy Complaint

 IPC: The Privacy Complaint Process

 IPC: Privacy Complaint Form




 Freedom of Information and Protection of Privacy Manual                                 183
            Part V: Appendices




Freedom of Information and Protection of Privacy Manual   184
Appendix 1: Sample Draft By-Law Designating Head
Under MFIPPA
 THE CORPORATION OF THE [insert name of Municipal Corporation]

 BY-LAW NO. [Insert by-law number]

 Being a By-law to designate a head of the Municipal Corporation for the purposes of the
 Municipal Freedom of Information and Protection of Privacy Act.

 Whereas, under Section 3, subsection 1 of the Municipal Freedom of Information and
 Protection of Privacy Act, R.S.O. 1990, c.M.56 (here after the ‚ÄòAct‚Äô), the council of a
 municipal corporation may by by-law designate from among its members an individual
 or a committee of the council to act as head of the municipal corporation for the
 purposes of the Act:

 And, whereas the council deems it necessary and expedient to designate a head for the
 purposes of the Act:

 NOW THEREFORE THE COUNCIL OF THE CORPORATION OF THE [insert name]
 ENACTS AS FOLLOWS:

 1. That [insert name/position of member of council or committee of council] be
 designated as head for the purposes of the Act.

 2. That this by-law come into force and effect on [insert date].

 Read a first and second time this [insert day] day of [insert month], [insert year].

 [Insert signature of Clerk and head of Council]

 Read a third time and passed this [insert day] day of [insert month], [insert year].

 [Insert signature of Clerk and head of Council]

 [Insert seal of the municipal corporation]




 Freedom of Information and Protection of Privacy Manual                                185
Appendix 2: Sample Resolution Designating Head
Under MFIPPA
 RESOLUTION

 FOR [insert board commission or other body name]

 MOVED BY: [insert name of individual]

 SECONDED BY: [insert name of individual]

 Whereas, under Section 3, subsection (2) of the Municipal Freedom of Information and
 Protection of Privacy Act, R.S.O. 1990 c.M.56 (here after the ‚ÄòAct‚Äô) the members elected
 or appointed to a board, commission or other body that is an institution under the Act
 may designate in writing from among its members an individual or committee of the
 body to act as head of the institution for the purposes of the Act:

 And whereas the [insert name of board, commission or other body] deems it
 necessary and expedient to designate a head for the purposes of the Act:

 Now, therefore, the [insert name of board, commission or other body] resolves as
 follows:

    1. That the [insert name of board, commission or other body] hereby
       designates [insert the name or position of individual or committee] as
       head for the purposes of the Act.
    2. That this resolution come into force and effect on [insert date here].

 [Insert signature of Secretary or Chairperson]




 Freedom of Information and Protection of Privacy Manual                             186
Appendix 3: Sample Delegation of Authority

3.1 ‚Äì Detailed Delegation of Authority
  [Insert institution name]

  DELEGATION OF POWERS AND DUTIES OF THE HEAD UNDER THE [MUNICIPAL
  FREEDOM OF INFORMATION AND PROTECTION OF PRIVACY ACT ([M]FIPPA)

  1.0 Delegation

  1.1 Pursuant to subsection [62 (1) of the Freedom of Information and Protection of
  Privacy Act, R.S.O. 1990, c. F.31 OR 49 (1) of the Municipal Freedom of
  Information and Protection of Privacy Act, R.S.O. 1990, c. M.56] and subject to
  section 2.0 below, I hereby delegate the powers and duties of the [insert title of head]
  as head of the [insert institution name] to the following officers of the institution:

  (a) [Insert title of delegated position]; and

  (b) [Insert additional title of delegated positions as needed];

  2.0 Limitations, restrictions, conditions and requirements

  2.1 The officers of the institution listed in section 1.1 above shall exercise the delegated
  powers and perform the delegated duties as outlined in Schedules A, B, and C.

  2.2 The powers and duties delegated to each of the delegates named herein may also
  be exercised by such persons who hold the position in an acting capacity to which he or
  she has been duly appointed, or by such persons who are duly authorized to act for the
  delegate in his or her absence.

  3.0 Effect on previous delegations

  3.1 The previous delegations under the Act by the [insert title of head of institution]
  are hereby revoked.

  4.0 Term of delegation

  4.1 This delegation is effective from the date set out below and shall remain in effect
  until such date as it is revoked by the [insert title of head of institution].

  [Insert signature of head] [Insert date]


  Freedom of Information and Protection of Privacy Manual                                 187
  Schedule A to Sample Delegation of Authority ‚Äì Access to Information
  Decisions

                                                                   Alternate
                                              Delegated
  Section               Decision                                   Delegated
                                            Decision-Maker
                                                                Decision-Maker
FIPPA: 10       Grant access in whole       [insert position   [insert position
                to general information.     of delegated       of alternate
MFIPPA: 4
                                            decision-maker]    delegated
                                                               decision maker if
                                                               applicable]
FIPPA: 10       Determine whether the       [insert position   [insert position
(1), 25         institution has custody     of delegated       of alternate
                or control of a record.     decision-maker]    delegated
MFIPPA: 4
(1), 18                                                        decision maker if
                                                               applicable]
FIPPA: 10       Determine whether           [insert position   [insert position
(1)(a), 12 to   exemptions apply in         of delegated       of alternate
22              whole or in part to a       decision-maker]    delegated
MFIPPA: 4       record.                                        decision maker if
(1)(a), 6 to                                                   applicable]
15
FIPPA: 10       Determine whether a         [insert position   [insert position
(1)(b), 24      request is frivolous or     of delegated       of alternate
(1.1), Reg.     vexatious.                  decision-maker]    delegated
460 (5.1)                                                      decision maker if
                                                               applicable]
MFIPPA: 4
(1)(b), 20.1,
Reg. 823
(5.1)
FIPPA: 10       Refuse access in whole      [insert position   [insert position
(2)             to general information or   of delegated       of alternate
MFIPPA: 4       grant access in part.       decision-maker]    delegated
                                                               decision maker if
(2)
                                                               applicable]
FIPPA: 11       Disclose a record in the    [insert position   [insert position
                public interest where the   of delegated       of alternate
MFIPPA: 5
                record reveals a grave      decision-maker]    delegated
                environmental, health or                       decision maker if

  Freedom of Information and Protection of Privacy Manual                     188
                                                                  Alternate
                                              Delegated
 Section               Decision                                   Delegated
                                            Decision-Maker
                                                                Decision-Maker
               safety hazard and if                            applicable]
               practicable, give notice
               to any person to whom
               the information relates.
FIPPA: 23      Determine whether a          [insert position   [insert position
               compelling public            of delegated       of alternate
MFIPPA:
               interest outweighs the       decision-maker]    delegated
16
               exemptions under                                decision maker if
               sections [13, 15, 15.1,                         applicable]
               17, 18, 20, 21 or 21.1
               OR 7, 9, 9.1, 10, 11, 13
               or 14].
FIPPA: 28      Decide whether to grant      [insert position   [insert position
(7)            or refuse access to          of delegated       of alternate
               whole record or part of      decision-maker]    delegated
MFIPPA:
21 (7)         record; give notice of                          decision maker if
               decision to affected                            applicable]
               person and requester.
FIPPA: 47      Grant access in whole        [insert position   [insert position
(1)            to personal information      of delegated       of alternate
               to the individual to         decision-maker]    delegated
MFIPPA:
36 (1)         whom it relates.                                decision maker if
                                                               applicable]
FIPPA: 49      Refuse an individual         [insert position   [insert position
               access in whole or in        of delegated       of alternate
MFIPPA:
               part to their own            decision-maker]    delegated
38
               personal information.                           decision maker if
                                                               applicable]
FIPPA: 47      Grant or refuse an           [insert position   [insert position
(2)            individual‚Äôs request for     of delegated       of alternate
               correction of their          decision-maker]    delegated
MFIPPA:
               personal information.                           decision maker if
36 (2)
                                                               applicable]




  Freedom of Information and Protection of Privacy Manual                     189
   Schedule B to Sample Delegation of Authority ‚Äì Administering the FOI
   Process

                                                                    Alternate
                                               Delegated
  Section               Decision                                   Delegated-
                                             Decision-Maker
                                                                 Decision Maker

FIPPA: 24       Offer assistance to          [insert position   [insert position of
(2)             requester in                 of delegated       alternate
                reformulating request        decision-maker]    delegated
MFIPPA: 17
                when it is unclear                              decision maker if
(2)
                (clarifying a request).                         applicable]

FIPPA: 24       Provide requester with       [insert position   [insert position of
(4)             schedule of dates for        of delegated       alternate
                continuing access            decision-maker]    delegated
MFIPPA: 17
                requests.                                       decision maker if
(4)
                                                                applicable]

FIPPA: 25       Determine which              [insert position   [insert position of
(1)             institution has custody      of delegated       alternate
                or control of the            decision-maker]    delegated
MFIPPA: 18      requested records,                              decision maker if
(2)             forward request and                             applicable]
                notify requester.
FIPPA: 25       Determine if another         [insert position   [insert position of
(2)             institution has a greater    of delegated       alternate
                interest in the record       decision-maker]    delegated
MFIPPA: 18      and transfer the request                        decision maker if
(3)             and, if necessary, the                          applicable]
                record.
FIPPA: 26,      Give notice of access to     [insert position   [insert position of
27.1, 29, 30    a record or notice of        of delegated       alternate
                refusal to give access to    decision-maker]    delegated
MFIPPA:         a record. If access is                          decision maker if
19, 20.1, 22,   given, provide access to                        applicable]
23              record or cause record
                to be produced; allow
                examination of original
                record. If access is

   Freedom of Information and Protection of Privacy Manual                       190
                                                                    Alternate
                                               Delegated
  Section               Decision                                   Delegated-
                                             Decision-Maker
                                                                 Decision Maker

                denied, provide
                explanation.
FIPPA: 27       Extend time limit and        [insert position   [insert position of
                give notice of time          of delegated       alternate
MFIPPA: 20      extension to requester.      decision-maker]    delegated
                                                                decision maker if
                                                                applicable]
FIPPA: 28       Give notice to affected      [insert position   [insert position of
                persons; give notice of      of delegated       alternate
MFIPPA: 21      delay to requester.          decision-maker]    delegated
                                                                decision maker if
                                                                applicable]
FIPPA: 33       Make manuals,                [insert position   [insert position of
                directives and               of delegated       alternate
MFIPPA:         guidelines available on      decision-maker]    delegated
N/A             the internet or in a                            decision maker if
                reading room.                                   applicable]
FIPPA: 34       Produce Annual Report.       [insert position   [insert position of
                                             of delegated       alternate
MFIPPA: 26
                                             decision-maker]    delegated
                                                                decision maker if
                                                                applicable]
FIPPA: 36       FIPPA: Information to        [insert position   [insert position of
                be made available to         of delegated       alternate
MFIPPA: 25      responsible Minister for     decision-maker]    delegated
                Directory of Records.                           decision maker if
                                                                applicable]
                MFIPPA: Information to
                be made available to the
                public for Directory of
                Records.
FIPPA: 24       Fee administration           [insert position   [insert position of
(1)(c), 57,     (including application       of delegated       alternate
Reg. 460        fee), calculation of fees                       delegated

   Freedom of Information and Protection of Privacy Manual                       191
                                                                    Alternate
                                               Delegated
  Section                Decision                                  Delegated-
                                             Decision-Maker
                                                                 Decision Maker

(5.2), (6),      and deposits.               decision-maker]    decision maker if
(6.1) (7), (9)                                                  applicable]

MFIPPA: 14
(1)(c), 57,
Reg. 823
(5.2), (6),
(6.1) (7), (9)
FIPPA: 57        If it is determined to be   [insert position   [insert position of
(4), Reg.        fair and equitable, grant   of delegated       alternate
460 (8)          a fee waiver                decision-maker]    delegated
                                                                decision maker if
MFIPPA: 45                                                      applicable]
(4), Reg.
823 (8)
FIPPA: 63        Grant access in the         [insert position   [insert position of
(1)              absence of a written        of delegated       alternate
                 request.                    decision-maker]    delegated
MFIPPA: 50                                                      decision maker if
(1)                                                             applicable]




   Freedom of Information and Protection of Privacy Manual                       192
  Schedule C to Sample Delegation of Authority ‚Äì Privacy and Security
  Responsibilities

                                                   Delegated           Alternate
    Section                 Decision               Decision-          Delegated-
                                                    Maker           Decision Maker

FIPPA: 38 (2)       Ensure personal              [insert position   [insert position
                    information is collected     of delegated       of alternate
MFIPPA: 28 (2)      with lawful authority        decision-          delegated
                                                 maker]             decision maker
                                                                    if applicable]
FIPPA: 39 (2)       Provide a proper notice      [insert position   [insert position
                    when personal                of delegated       of alternate
MFIPPA: 29 (2)      information is collected.    decision-          delegated
                                                 maker]             decision maker
                                                                    if applicable]
FIPPA: 40 (1),      Ensure personal              [insert position   [insert position
Reg. 460(5)         information is retained      of delegated       of alternate
                    for a period of one year     decision-          delegated
MFIPPA: 30 (1),     after use (or a lesser       maker]             decision maker
Reg. 823 (5)        time frame with                                 if applicable]
                    consent).
FIPPA: 40 (2)       Ensure personal              [insert position   [insert position
                    information used is          of delegated       of alternate
MFIPPA: 30 (2)      accurate and up-to-          decision-          delegated
                    date.                        maker]             decision maker
                                                                    if applicable]
FIPPA: 41, 43       Ensure personal              [insert position   [insert position
                    information is used with     of delegated       of alternate
MFIPPA: 31, 33      lawful authority.            decision-          delegated
                                                 maker]             decision maker
                                                                    if applicable]
FIPPA: 42, 43       Ensure personal              [insert position   [insert position
                    information is disclosed     of delegated       of alternate
MFIPPA: 32, 33      with lawful authority.       decision-          delegated
                                                 maker]             decision maker
                                                                    if applicable]

  Freedom of Information and Protection of Privacy Manual                        193
                                                   Delegated           Alternate
    Section                 Decision               Decision-          Delegated-
                                                    Maker           Decision Maker

FIPPA: 21 (1)(e),   Approve the disclosure       [insert position   [insert position
Reg. 460 (10)       of personal information      of delegated       of alternate
                    for a research purpose.      decision-          delegated
MFIPPA: 14                                       maker]             decision maker
(1)(e), Reg. 823                                                    if applicable]
(10)
FIPPA: 40 (4),      Authorization to destroy     [insert position   [insert position
Reg. 459 (3)        personal information.        of delegated       of alternate
                                                 decision-          delegated
MFIPPA: 40(4)                                    maker]             decision maker
                                                                    if applicable]
FIPPA: 40 (4),      Take steps to ensure the     [insert position   [insert position
Reg. 459 (4)        security and                 of delegated       of alternate
                    confidentiality of           decision-          delegated
MFIPPA: 40 (4)      personal information         maker]             decision maker
                    transferred to the                              if applicable]
                    Archives or destroyed.
FIPPA: 10.1         Ensure recordkeeping         [insert position   [insert position
                    polices are put in place     of delegated       of alternate
MFIPPA: 4.1         and records are              decision-          delegated
                    managed in accordance        maker]             decision maker
                    with polices and                                if applicable]
                    requirements.

FIPPA: Reg. 460     Prevent unauthorized         [insert position   [insert position
(4)                 access to records.           of delegated       of alternate
                                                 decision-          delegated
MFIPPA: Reg.                                     maker]             decision maker
823 (3)                                                             if applicable]

FIPPA: Reg. 460     Protect records from         [insert position   [insert position
(4)                 inadvertent destruction      of delegated       of alternate
                    and damage.                  decision-          delegated
MFIPPA: Reg.                                     maker]             decision maker


  Freedom of Information and Protection of Privacy Manual                        194
                                                   Delegated           Alternate
    Section                 Decision               Decision-          Delegated-
                                                    Maker           Decision Maker

823 (3)                                                             if applicable]

FIPPA: 44           Developing personal          [insert position   [insert position
                    information banks.           of delegated       of alternate
MFIPPA: 34                                       decision-          delegated
                                                 maker]             decision maker
                                                                    if applicable]
FIPPA: 46           Record and notification      [insert position   [insert position
                    of inconsistent uses or      of delegated       of alternate
MFIPPA: 35          disclosures of personal      decision-          delegated
                    information.                 maker]             decision maker
                                                                    if applicable]




  Freedom of Information and Protection of Privacy Manual                            195
3.2 ‚Äì Simplified Delegation of Authority for Small Institutions
  [Insert institution name]

  DELEGATION OF POWERS AND DUTIES OF THE HEAD UNDER THE [MUNICIPAL
  FREEDOM OF INFORMATION AND PROTECTION OF PRIVACY ACT ([M]FIPPA)

  [I OR We] [Insert name of head of the institution for the purposes of the Act],
  delegate all powers and duties under the [Municipal] Freedom of Information and
  Protection of Privacy Act to [insert position title of delegated decision maker]
  effective on [insert date].

  [Insert signature and title of head]

  [Insert date]




  Freedom of Information and Protection of Privacy Manual                            196
Appendix 4: Template Letters for Request Processing

4.1 ‚Äì Letter to Requester Acknowledging Request - Standard
  [Insert date]

  [Insert name and address of requester]

  Access Request: [insert access request number]

  Dear [insert name of requester]:

  I am writing to inform you that your access request under the [Municipal] Freedom of
  Information and Protection of Privacy Act, along with your $5.00 application fee, was
  received in our office on [insert date] and is being processed.

  Your request is for the following information:

  [Insert details of records requested]

  Should you have any questions, please contact [insert name, title and phone number
  of person responsible]. We would appreciate you using the above listed access
  request number in any further correspondence.

  Sincerely,

  [Insert signature]




  Freedom of Information and Protection of Privacy Manual                                 197
4.2 ‚Äì Letter to Requester Acknowledging Request ‚Äì Application Fee Missing
  [Insert date]

  [Insert name and address of requester]

  Access Request: [insert access request number]

  Dear [insert name of requester]:

  I am writing to inform you that your access request under the [Municipal] Freedom of
  Information and Protection of Privacy Act was received in our office on [insert date].

  Your request is for the following information:

  [Insert details of records requested]

  In order to proceed with your request, we require a $5.00 application fee. If paying by
  cheque or money order, please make the application fee payable to [insert payee
  information]. Once we have received payment, we will proceed with processing your
  request.
  If we do not hear from you within 30 days of this letter's date, we will close your file.

  Should you have any questions, please contact [insert name, title and phone number of
  person responsible]. We would appreciate you using the above listed access request
  number in any further correspondence.
  Sincerely,

  [Insert signature]




  Freedom of Information and Protection of Privacy Manual                                     198
4.3 ‚Äì Letter to Requester Acknowledging Request ‚Äì Clarification Required
  [Insert date]

  [Insert name and address of requester]

  Access Request: [insert access request number]

  Dear [insert name of requester]:

  I am writing to inform you that your access request under the [Municipal] Freedom of
  Information and Protection of Privacy Act, along with your $5.00 application fee, was
  received in our office on [insert date].

  Your request is for the following information:

  [Insert details of records requested]

  Unfortunately, the request does not provide sufficient detail to identify the record(s).
  Please supply the following information so that we may begin to process your request:

  [Insert details of information needed]

  We would be happy to answer any questions or assist you in clarifying or reformulating
  your request.

  If we do not hear from you within 30 days of this letter's date, we will close your file.

  Should you have any questions, please contact [insert name, title and phone number of
  person responsible]. We would appreciate you using the above listed access request
  number in any further correspondence.
  Sincerely,

  [Insert signature]




  Freedom of Information and Protection of Privacy Manual                                     199
4.4 ‚Äì Letter to Requester Acknowledging Request ‚Äì Proof of Identity Required
  [Insert date]

  [Insert name and address of requester]

  Access Request: [insert access request number]

  Dear [insert name of requester]:

  I am writing to inform you that your access request under the [Municipal] Freedom of
  Information and Protection of Privacy Act was received in our office on [insert date].

  Your request is for the following information:

  [Insert details of records requested]

  Pursuant to paragraph 3 (3) of Regulation 460, we are required to verify the identity of
  individuals seeking access to their own personal information. Please provide our office
  with a photocopy of one piece of valid government issued photograph identification.

  Once we have received verification of your identity, we will proceed with processing
  your request.

  Should you have any questions or require an alternate method to verify your identity,
  please contact [insert name, title and phone number of person responsible]. We
  would appreciate you using the above listed access request number in any further
  correspondence.
  Sincerely,

  [Insert signature]




  Freedom of Information and Protection of Privacy Manual                                  200
4.5 ‚Äì Letter to Requester when Transferring of Forwarding a Request
  [Insert date]

  [Insert name and address of requester]

  Access Request: [insert access request number]

  Dear [insert name of requester]:

  I am writing to inform you that your access request under the [Municipal] Freedom of
  Information and Protection of Privacy Act (hereafter ‚Äòthe ‚ÄòAct‚Äô), along with your $5.00
  application fee, was received in our office on [insert date].

  [Insert name and address of other institution] has [custody and control of OR a
  greater interest in] the records you seek. Under section [25 OR 18] of the Act, we
  [forwarded OR transferred] your access request to them. We have enclosed a copy of
  section [25 OR 18] for your review.

  [If receiving institution is a different payee, add: Since we are not processing your
  request we are returning your $5.00 application fee with this letter. Please send a
  new application fee to the institution listed above.]

  [If the receiving institution is the same payee (ie: the Minister of Finance), add: We have
  forwarded your $5.00 application fee along with your request. OR We have
  processed your $5.00 application fee on behalf of the receiving institution.]

  Should you have any questions, please contact [insert name, title and phone number of
  person responsible]. We would appreciate you using the above listed access request
  number in any further correspondence.
  Sincerely,

  [Insert signature]




  Freedom of Information and Protection of Privacy Manual                                   201
4.6 ‚Äì Letter to Receiving Institution when Transferring or Forwarding a Request
  [Insert date]

  [Insert name and address of receiving institution]

  Dear [insert name of requester]:

  The enclosed request for access was received by our office on [insert date request
  was received].

  This request is [transferred OR forwarded] to you under section [25 of the Freedom
  of Information and Protection of Privacy Act OR 18 of the Municipal Freedom of
  Information and Protection of Privacy Act] as we believe your institution has
  [custody or control of OR a greater interest in] the record.

  [If receiving institution is a different payee, add: Since we are not processing your
  request we are returning your $5.00 application fee with this letter. Please send a
  new application fee to the institution listed above.]

  [If the receiving institution is the same payee (ie: the Minister of Finance), add: We have
  forwarded your $5.00 application fee along with your request. OR We have
  processed your $5.00 application fee on behalf of the receiving institution.]

  Should you have any questions, please contact [insert name, title and phone number of
  person responsible].

  Sincerely,
  [Insert signature]




  Freedom of Information and Protection of Privacy Manual                                202
4.7 ‚Äì Letter to Requester ‚Äì Notice of Time Extension
  [Insert date]

  [Insert name and address of requester]

  Access Request: [insert access request number]

  Dear [insert name of requester]:

  I am writing regarding your access request under the [Municipal] Freedom of
  Information and Protection of Privacy Act (hereafter, ‚Äòthe Act‚Äô) received by our office on
  [insert date request was received].

  A request under the Act usually must be answered within 30 calendar days; however,
  section [27 OR 20] allows for time extensions under certain circumstances. The time
  limit for answering your request has been extended for an additional [insert number of
  days] days to [insert new due date].

  The reason for the time extension is [due to a large volume of records that must be
  searched in order to respond to your request OR due to a large volume of records
  that must be reviewed in order to respond to your request OR to conduct
  consultations with external parties].

  A copy of section [27 OR 20] of the Act is enclosed for your information.

  You may request the Information and Privacy Commissioner to review this decision to
  extend the timeline to response within thirty days from the date of this letter. The
  Commissioner‚Äôs address is Suite 1400, 2 Bloor Street East, Toronto, Ontario, M4W 1A8.
  The appeal fee is [$25.00 (for general record requests) OR $10.00 (for personal
  information requests)], payable by cheque or money order to the Minister of Finance and
  must be included with your correspondence.

  Should you have any questions, please contact [insert name, title and phone number of
  person responsible]. We would appreciate you using the above listed access request
  number in any further correspondence.
  Sincerely,

  [Insert signature]




  Freedom of Information and Protection of Privacy Manual                                203
4.8 ‚Äì Letter to Requester ‚Äì Fee Estimate and Interim Decision - $25 to $99 Fee
  [Insert date]

  [Insert name and address of requester]

  Access Request: [insert access request number]

  Dear [insert name of requester]:

  I am writing regarding your access request under the [Municipal] Freedom of
  Information and Protection of Privacy Act (hereafter, ‚Äòthe Act‚Äô) received by our office on
  [insert date request was received].

  As we have not yet reviewed the records in detail, no final decision has been made
  regarding access but the following exemptions will likely apply. [Generally describe
  what exemptions might apply to the records].

  Section [57 or 45] of the Act requires fees to be charged for processing a request. The
  fee estimate for processing this request is [insert total fee estimate]. The breakdown
  for your fee estimate is as follows:

     ‚Ä¢   [$XX] for search time based on [insert time] hours of time @ $7.50 per
         quarter hour;
     ‚Ä¢   [$XX] for records preparation based on [insert time] hours of time @ $7.50
         per quarter hour; and
     ‚Ä¢   [$XX] for photocopying based on [insert page numbers] pages @ 20 cents
         per page.

  Please note: this represents an estimate of fees based on preliminary work. The final
  fee calculation may vary. Please do not provide any fee payment at this time.

  The Act provides that all or part of the fee can be waived if in our opinion it is fair and
  equitable to do so. You may be required to provide proof to support any waiver claims.
  Please notify [insert name, title and phone number] as soon as possible of your wish
  to proceed with a request for a fee waiver.

  You may request the Information and Privacy Commissioner to review this fee estimate
  within thirty days from the date of this letter. The Commissioner‚Äôs address is Suite 1400,
  2 Bloor Street East, Toronto, Ontario, M4W 1A8. The appeal fee is [$25.00 (for general
  record requests) OR $10.00 (for personal information requests)], payable by cheque or
  money order to the Minister of Finance and must be included with your correspondence.


  Freedom of Information and Protection of Privacy Manual                                 204
Copies of section [insert sections of exemptions that may be claimed in interim
decision] and [57 OR 45] of the Act are enclosed for your information.

Should you have any questions, please contact [insert name, title and phone number of
person responsible]. We would appreciate you using the above listed access request
number in any further correspondence.
Sincerely,

[Insert signature]




Freedom of Information and Protection of Privacy Manual                           205
4.9 ‚Äì Letter to Requester ‚Äì Fee Estimate and Interim Decision ‚Äì Over $100 Fee
  [Insert date]

  [Insert name and address of requester]

  Access Request: [insert access request number]

  Dear [insert name of requester]:

  I am writing regarding your access request under the [Municipal] Freedom of
  Information and Protection of Privacy Act (hereafter, ‚Äòthe Act‚Äô) received by our office on
  [insert date request was received].

  As we have not yet reviewed the records in detail, no final decision has been made
  regarding access but the following exemptions will likely apply. [Generally describe
  what exemptions might apply to the records].

  Section [57 or 45] of the Act requires fees to be charged for processing a request. The
  fee estimate for processing this request is [insert total fee estimate]. The breakdown
  for your fee estimate is as follows:

     ‚Ä¢   [$XX] for search time based on [insert time] hours of time @ $7.50 per
         quarter hour;
     ‚Ä¢   [$XX] for records preparation based on [insert time] hours of time @ $7.50
         per quarter hour; and
     ‚Ä¢   [$XX] for photocopying based on [insert page numbers] pages @ 20 cents
         per page.

  Subsection 7(1) of Regulation [460 OR 823] under the Act provides that when an
  estimate is over $100.00 the institution may collect 50% of the estimated fee prior to the
  completion of the request. Please make your cheque or money order for [insert 50% of
  total fee estimate] payable to the [insert payee information] and forward it to my
  attention. Receipt of the fee deposit is requested prior to completing your request.
  Please do not send the full estimate at this time. We will calculate the actual fee and
  include the balance owing when you are notified of the decision regarding your access
  request.

  The Act provides that all or part of the fee can be waived if in our opinion it is fair and
  equitable to do so. You may be required to provide proof to support any waiver claims.
  Please notify [insert name, title and phone number] as soon as possible of your wish
  to proceed with a request for a fee waiver.


  Freedom of Information and Protection of Privacy Manual                                 206
You may request the Information and Privacy Commissioner to review this fee estimate
within thirty days from the date of this letter. The Commissioner‚Äôs address is Suite 1400,
2 Bloor Street East, Toronto, Ontario, M4W 1A8. The appeal fee is [$25.00 (for general
record requests) OR $10.00 (for personal information requests)], payable by cheque or
money order to the Minister of Finance and must be included with your correspondence.

Copies of section [insert sections of exemptions that may be claimed in interim
decision] and [57 OR 45] of the Act are enclosed for your information.

Should you have any questions, please contact [insert name, title and phone number of
person responsible]. We would appreciate you using the above listed access request
number in any further correspondence.
Sincerely,

[Insert signature]




Freedom of Information and Protection of Privacy Manual                               207
4.10 ‚Äì Notice to Affected Person for Third Party Information
  [Insert date]

  [Insert name and address of third party]

  Access Request: [insert access request number]

  Dear [insert name of third party]:

  [Insert name of institution] has received a request for access to records under the
  [Municipal] Freedom of Information and Protection of Privacy Act (hereafter ‚Äòthe Act‚Äô) to
  disclose [describe in detail the records as they relate to the affected third party].

  According to section [28 OR 21] of the Act, a third party whose interests may be
  affected must be given the opportunity to make representations to the head of an
  institution concerning disclosure of the records.

  To successfully qualify for a third party exemption, all of the following three tests must
  be met:

     ‚Ä¢   The information must fit within one of the specified categories of third party
         information: trade secret or scientific, technical, commercial, financial or
         labour relations information;
     ‚Ä¢   The information must have been supplied by the third party in confidence,
         implicitly or explicitly; and
     ‚Ä¢   The disclosure of the information could reasonably be expected to cause
         one of the harms indicated below:
             o Prejudice your competitive position or interfere with any contractual
                 rights you possess, or
             o Result in you no longer supplying this or similar information to [name
                 of institution], or
             o Result in undue loss or gain to any person, business, or organization
                 of which you are aware.

  Under section [17 or 10] of the Act, we must release these records unless the above
  conditions are met. Copies of sections [17 and 28 OR 10 and 21] of the Act are
  enclosed along with the impacted records. Please review the attached records.

  If you have concerns about the release of the records please contact us, in writing, no
  later than [insert date] outlining your concerns. In order to support your claims against
  the release of the records or portions of the records, you must show how those records
  meet the third party criteria listed above.

  Freedom of Information and Protection of Privacy Manual                                 208
We will notify you in writing by [insert date] about our decision regarding the release of
the records.

Should you have any questions, please contact [insert name, title and phone number of
person responsible]. We would appreciate you using the above listed access request
number in any further correspondence.
Sincerely,

[Insert signature]




Freedom of Information and Protection of Privacy Manual                                209
4.11 ‚Äì Notice to Affected Person for Personal Privacy
  [Insert date]

  [Insert name and address of affected party]

  Access Request: [insert access request number]

  Dear [insert name of affected party]:

  [Insert name of institution] has received a request for access to records under the
  [Municipal] Freedom of Information and Protection of Privacy Act (hereafter ‚Äòthe Act‚Äô) to
  disclose [describe in detail the records as they relate to the s affected individual].

  Section [28 OR 21] of the Act says individuals have the opportunity to make
  representations about the release of their personal information to a third party.

  Your views regarding disclosure of these records would be appreciated. Please indicate
  in writing whether or not you consider that the disclosure of the enclosed records would
  be an invasion of your personal privacy. Section [21 OR 14] of the Act outlines
  circumstances where the disclosure of personal information may be an unjustified
  invasion of personal privacy.

  Copies of sections [21 and 28 OR 14 and 21] are enclosed for your review.

  Your response must be received no later than [insert date]. You will be notified in
  writing by [insert date] about our decision regarding the release of the records.

  Should you have any questions, please contact [insert name, title and phone number of
  person responsible]. We would appreciate you using the above listed access request
  number in any further correspondence.
  Sincerely,

  [Insert signature]




  Freedom of Information and Protection of Privacy Manual                               210
4.12 ‚Äì Letter to Requester ‚Äì Notice of Delay Where a Third Party‚Äôs Interests are
Impacted
  [Insert date]

  [Insert name and address of requester]

  Access Request: [insert access request number]

  Dear [insert name of requester]:

  I am writing regarding your access request under the [Municipal] Freedom of
  Information and Protection of Privacy Act (hereafter, ‚Äòthe Act‚Äô) received by our office on
  [insert date request was received].

  The disclosure of the records may affect the interests of a third party.

  Under section [28 OR 21], we are required to notify third parties whose interest may be
  affected by the disclosure of records. Third parties then have an opportunity to make
  representations about the release of the record(s).

  This process requires the timelines for response to be adjusted. A decision on whether
  or not the record(s) will be disclosed will be made by [insert date].

  A copy of section [28 OR 21] of the Act is enclosed for your information.

  Should you have any questions, please contact [insert name, title and phone number of
  person responsible]. We would appreciate you using the above listed access request
  number in any further correspondence.
  Sincerely,

  [Insert signature]




  Freedom of Information and Protection of Privacy Manual                                211
4.13 ‚Äì Letter to Affected Person ‚Äì Notice to Disclose Information
  [Insert date]

  [Insert name and address of affected party]

  Access Request: [insert access request number]

  Dear [insert name of affected party]:

  Thank you for your representations dated [insert date on representations] concerning
  disclosure to [insert description or details of records]. A decision has been made to
  grant access [OR partial access] to the requester.

  The official responsible for making the access decision on your request is [insert name
  and title of delegated decision maker].

  You may request the Information and Privacy Commissioner to review this decision
  within thirty days from the date of this letter. The Commissioner‚Äôs address is Suite 1400,
  2 Bloor Street East, Toronto, Ontario, M4W 1A8.

  If no appeal is filed with the Information and Privacy Commissioner, full access to these
  records will be provided to the requester after [insert date].

  Should you have any questions, please contact [insert name, title and phone number of
  person responsible]. We would appreciate you using the above listed access request
  number in any further correspondence.
  Sincerely,

  [Insert signature]




  Freedom of Information and Protection of Privacy Manual                               212
4.14 ‚Äì Letter to Affected Person ‚Äì Notice to Withhold Information
  [Insert date]

  [Insert name and address of affected party]

  Access Request: [insert access request number]

  Dear [insert name of affected party]:

  Thank you for your representations dated [insert date on representations] concerning
  disclosure to [insert description or details of records].

  After consideration of these representations, [insert name of institution] agrees with
  your submissions. Pursuant to section [17 OR 21 OR 10 OR 14] of the [Municipal
  Freedom of Information and Protection of Privacy Act, a decision has been made to
  deny the requester access to [insert description or details of records] in their
  entirety.

  The official responsible for making the access decision on your request is [insert name
  and title of delegated decision maker].

  Please be advised that the requester may appeal this decision to the Information and
  Privacy Commissioner of Ontario.

  Should you have any questions, please contact [insert name, title and phone number of
  person responsible]. We would appreciate you using the above listed access request
  number in any further correspondence.
  Sincerely,

  [Insert signature]




  Freedom of Information and Protection of Privacy Manual                             213
4.15 ‚Äì Letter to Requester ‚Äì Decision to Disclose All Records
  [Insert date]

  [Insert name and address of requester]

  Access Request: [insert access request number]

  Dear [insert name of requester]:

  I am writing regarding your access request under the [Municipal] Freedom of
  Information and Protection of Privacy Act (hereafter, ‚Äòthe Act‚Äô) received by our office on
  [insert date request was received].

  A search has been conducted and the responsive records have been reviewed. A
  decision has been made to grant access to the records in full. [If no fee required add:
  The responsive records are enclosed.]

  [If some records were subject to a notice under section 28/21 add: Some of the
  responsive records impacted the interests of other parties. As a result, we cannot
  disclose these records for an additional 30 days, to allow the affected parties an
  opportunity to appeal this decision. If no notice of appeal is received by our office
  within 30 days, we can proceed with disclosing these records.]

  The official responsible for making the access decision on your request is [insert name
  and title of delegated decision maker].

  The estimated fee for processing your request was [insert estimated total]. The actual
  fee for processing your request is [insert actual total]. The breakdown for your fee is
  as follows:

     ‚Ä¢   [$XX] for search time based on [insert time] hours of time @ $7.50 per
         quarter hour;
     ‚Ä¢   [$XX] for records preparation based on [insert time] hours of time @ $7.50
         per quarter hour; and
     ‚Ä¢   [$XX] for photocopying based on [insert page numbers] pages @ 20 cents
         per page.

  [If fee deposit was paid, add: Your deposit of $XX will be deducted from this total
  fee.] The records will be prepared and made available to you upon receipt of the
  outstanding balance of [insert fee total OR fee total minus fee deposit]. Please note,
  if we do not receive your fee payment within 30 days of the date on this letter, we will
  consider your request abandoned and close the file.

  Freedom of Information and Protection of Privacy Manual                                214
The Act provides that all or part of the fee can be waived if in our opinion it is fair and
equitable to do so. You may be required to provide proof to support any waiver claims.
Please notify [insert name, title and phone number] as soon as possible of your wish
to proceed with a request for a fee waiver.

You may request the Information and Privacy Commissioner to review this decision and
fee within thirty days from the date of this letter. The Commissioner‚Äôs address is Suite
1400, 2 Bloor Street East, Toronto, Ontario, M4W 1A8. The appeal fee is [$25.00 (for
general record requests) OR $10.00 (for personal information requests)], payable by
cheque or money order to the Minister of Finance and must be included with your
correspondence.

A copy of section [57 OR 45] of the Act is enclosed for your information.

Should you have any questions, please contact [insert name, title and phone number
of person responsible]. We would appreciate you using the above listed access
request number in any further correspondence.

Sincerely,

[Insert signature]




Freedom of Information and Protection of Privacy Manual                                 215
4.16 ‚Äì Letter to Requester ‚Äì Decision to Deny Access in Full or in Part
  [Insert date]

  [Insert name and address of requester]

  Access Request: [insert access request number]

  Dear [insert name of requester]:

  I am writing regarding your access request under the [Municipal] Freedom of
  Information and Protection of Privacy Act (hereafter, ‚Äòthe Act‚Äô) received by our office on
  [insert date request was received].

  A search has been conducted and the responsive records have been reviewed. A
  decision has been made to [grant access in part OR deny access in full]. Information
  on [XX] of records will be severed and [XX] of pages will be withheld in full pursuant to
  sections [insert relevant exceptions] of the Act.

  [If some records were subject to a notice under section 28/21 add: Some of the
  responsive records impacted the interests of other parties. As a result, we cannot
  disclose these records for an additional 30 days, to allow the affected parties an
  opportunity to appeal this decision. If no notice of appeal is received by our office
  within 30 days, we can proceed with disclosing these records.]

  The official responsible for making the access decision on your request is [insert name
  and title of delegated decision maker].

  The estimated fee for processing your request was [insert estimated total]. The actual
  fee for processing your request is [insert actual total]. The breakdown for your fee is
  as follows:

     ‚Ä¢   [$XX] for search time based on [insert time] hours of time @ $7.50 per
         quarter hour;
     ‚Ä¢   [$XX] for records preparation based on [insert time] hours of time @ $7.50
         per quarter hour; and
     ‚Ä¢   [$XX] for photocopying based on [insert page numbers] pages @ 20 cents
         per page.

  [If fee deposit was paid, add: Your deposit of $XX will be deducted from this total
  fee.] The records will be prepared and made available to you upon receipt of the
  outstanding balance of [insert fee total OR fee total minus fee deposit]. Please note,


  Freedom of Information and Protection of Privacy Manual                                216
if we do not receive your fee payment within 30 days of the date on this letter, we will
consider your request abandoned and close the file.

The Act provides that all or part of the fee can be waived if in our opinion it is fair and
equitable to do so. You may be required to provide proof to support any waiver claims.
Please notify [insert name, title and phone number] as soon as possible of your wish
to proceed with a request for a fee waiver.

You may request the Information and Privacy Commissioner to review this decision and
fee within thirty days from the date of this letter. The Commissioner‚Äôs address is Suite
1400, 2 Bloor Street East, Toronto, Ontario, M4W 1A8. The appeal fee is [$25.00 (for
general record requests) OR $10.00 (for personal information requests)], payable by
cheque or money order to the Minister of Finance and must be included with your
correspondence.

Copies sections [insert relevant sections for exemptions claimed] and [57 OR 45] of
the Act are enclosed for your information.

Should you have any questions, please contact [insert name, title and phone number
of person responsible]. We would appreciate you using the above listed access
request number in any further correspondence.

Sincerely,

[Insert signature]




Freedom of Information and Protection of Privacy Manual                                 217
4.17 ‚Äì Letter to Requester ‚Äì Decision to Refuse to Confirm or Deny Existence
of Record
  [Insert date]

  [Insert name and address of requester]

  Access Request: [insert access request number]

  Dear [insert name of requester]:

  I am writing regarding your access request under the [Municipal] Freedom of
  Information and Protection of Privacy Act (hereafter, ‚Äòthe Act‚Äô) received by our office on
  [insert date request was received].

  Pursuant to section [21(5)/14(5) OR 14(3)/8(3)], we cannot confirm or deny the
  existence of the record, as the disclosure of the existence of a record would [constitute
  an unjustified invasion of privacy OR compromise a law enforcement matter].

  The official responsible for making the access decision on your request is [insert name
  and title of delegated decision maker].

  You may request the Information and Privacy Commissioner to review this decision
  within thirty days from the date of this letter. The Commissioner‚Äôs address is Suite 1400,
  2 Bloor Street East, Toronto, Ontario, M4W 1A8. The appeal fee is [$25.00 (for general
  record requests) OR $10.00 (for personal information requests)], payable by cheque or
  money order to the Minister of Finance and must be included with your correspondence.

  Copies sections [insert relevant sections for exemptions claimed] of the Act are
  enclosed for your information.

  Should you have any questions, please contact [insert name, title and phone number
  of person responsible]. We would appreciate you using the above listed access
  request number in any further correspondence.

  Sincerely,

  [Insert signature]




  Freedom of Information and Protection of Privacy Manual                                218
4.18 ‚Äì Letter to Requester ‚Äì Decision of No Responsive Records Exist
  [Insert date]

  [Insert name and address of requester]

  Access Request: [insert access request number]

  Dear [insert name of requester]:

  I am writing regarding your access request under the [Municipal] Freedom of
  Information and Protection of Privacy Act (hereafter, ‚Äòthe Act‚Äô) received by our office on
  [insert date request was received].

  A search has been conducted and the responsive records and no responsive records
  were located.

  The official responsible for making the access decision on your request is [insert name
  and title of delegated decision maker].

  You may request the Information and Privacy Commissioner to review the sufficiency of
  our institution‚Äôs search for records within thirty days from the date of this letter. The
  Commissioner‚Äôs address is Suite 1400, 2 Bloor Street East, Toronto, Ontario, M4W
  1A8. The appeal fee is [$25.00 (for general record requests) OR $10.00 (for personal
  information requests)], payable by cheque or money order to the Minister of Finance
  and must be included with your correspondence.

  Should you have any questions, please contact [insert name, title and phone number
  of person responsible]. We would appreciate you using the above listed access
  request number in any further correspondence.

  Sincerely,

  [Insert signature]




  Freedom of Information and Protection of Privacy Manual                                219
4.19 ‚Äì Letter to Requester ‚Äì Decision Approving Correction of Personal
Information Request
  [Insert date]

  [Insert name and address of requester]

  Access Request: [insert access request number]

  Dear [insert name of requester]:

  Your request under the [Municipal] Freedom of Information and Protection of Privacy
  Act for a correction of personal information was received on [insert date].

  The correction was made and a copy of the corrected record is attached. On request,
  you are entitled to have the correction sent to those persons to whom the information
  was disclosed over the past 12 months.

  [NOTE: With this notice, the institution may wish to include a listing of the persons to
  whom the personal information was disclosed over the past 12 months. The personal
  information of individuals acting in a personal capacity should not be included on the
  list.]

  The official responsible for making the access decision on your request is [insert name
  and title of delegated decision maker].

  Should you have any questions, please contact [insert name, title and phone number of
  person responsible]. We would appreciate you using the above listed access request
  number in any further correspondence.
  Sincerely,

  [Insert signature]




  Freedom of Information and Protection of Privacy Manual                                220
4.20 ‚Äì Letter to Requester ‚Äì Decision Denying Correction of Personal
Information Request
  [Insert date]

  [Insert name and address of requester]

  Access Request: [insert access request number]

  Dear [insert name of requester]:

  Your request under the [Municipal] Freedom of Information and Protection of Privacy
  Act for a correction of personal information was received on [insert date].

  The correction was not made to the personal information. [Insert reason why request
  was refused consider including discussion of three part test: 1) whether the information
  is personal and private; 2) whether the information is inexact, incomplete or ambiguous
  and 3) whether the correction would be a substitution of opinion OR whether it is a law
  enforcement record.]

  You are entitled to require that a statement of disagreement be attached to the record
  and that the statement of disagreement be sent to any person to whom the record was
  disclosed over the past 12 months.

  [NOTE: With this notice, the institution may wish to include a listing of the persons to
  whom the personal information was disclosed over the past 12 months. The personal
  information of individuals acting in a personal capacity should not be included on the
  list.]

  The official responsible for making the access decision on your request is [insert name
  and title of delegated decision maker].

  You may request the Information and Privacy Commissioner to review this decision
  within thirty days from the date of this letter. The Commissioner‚Äôs address is Suite 1400,
  2 Bloor Street East, Toronto, Ontario, M4W 1A8. The appeal fee is $10.00, payable by
  cheque or money order to the Minister of Finance, and must be included with your
  correspondence.

  Should you have any questions, please contact [insert name, title and phone number of
  person responsible]. We would appreciate you using the above listed access request
  number in any further correspondence.
  Sincerely,


  Freedom of Information and Protection of Privacy Manual                                221
[Insert signature]




Freedom of Information and Protection of Privacy Manual   222
4.21 ‚Äì Letter to Requester Advising Request will be Considered Abandoned
  [Insert date]

  [Insert name and address of requester]

  Access Request: [insert access request number]

  Dear [insert name of requester]:

  I am writing regarding your access request under the [Municipal] Freedom of
  Information and Protection of Privacy Act (hereafter, ‚Äòthe Act‚Äô) received by our office on
  [insert date request was received].

  On [insert date] our office contacted you regarding [a fee estimate OR clarifying your
  request]. We have not yet received your reply.

  In absence of a response, we will consider your request abandoned and close the file
  on [insert date].

  Should you have any questions, please contact [insert name, title and phone number of
  person responsible]. We would appreciate you using the above listed access request
  number in any further correspondence.
  Sincerely,

  [Insert signature]




  Freedom of Information and Protection of Privacy Manual                                223
Appendix 5: Sample Record Search Form
Instructions to Program Area:

Complete this form if the total search time amounts to less than 3 hours or if you have been
expressly asked to complete a full retrieval.

This form documents the search activities of the program area and is used to calculate fees
for searching and retrieving records. Note: additional fees may be applied for records
preparation or other administrative actions.

Please complete this form and return it electronically. If you are mailing your retrieved
records, please include a copy of this form.

Before sending digital records or digitally scanning any records, please contact the FOI
Coordinator for instructions.

If your search time will be more than 3 hours, use Fee Estimate Form.

Reference #: [insert access request number]

Program Area Contact: [insert program area contact including responsible for conducting or
coordinating search - one form per program area is requested ‚Äì include name, position and
office telephone number]

Program Area: [insert name of program area or office conducting search]

Date(s) of search: [insert dates of search]

1. Indicate the information banks that were searched [whose computer, which files (hard
   copy and shared drives)], which offices or file rooms.



2. Name(s) and position title(s) of staff contacted during the search.



3. Methods/processes used to conduct the search and types of files searched (searching
   emails, other electronic files, paper files, file lists, off-site file lists, microfiche, etc.)




  Freedom of Information and Protection of Privacy Manual                                     224
4. Were responsive records located? If no, is there another location where they may be? If
   responsive records once existed but were destroyed, or have gone missing, please
   explain.



5. Do any responsive records contain personal information of the requester? If yes, search
   time and severing should not be included in the search fee.



6. Are there any issues/sensitivities around these records or this request? If yes, please
   explain. Please keep in mind that our staff may have no familiarity with your records.



7. Number of hours required to complete search (to the nearest ¬º hour, do not
   include photocopy time):        hrs.

8. Was a computer programmer required to write code to retrieve any of the
   records for this request? Yes/No




  Freedom of Information and Protection of Privacy Manual                               225
Appendix 6: Sample Fee Estimate Form
Instructions to Program Area:

Complete this form if the total search is estimated to take more than 3 hours (do not retrieve
records).

This information will be used to create a fee estimate for the requester, and to inform them of
the general nature of the responsive records. Other staff‚Äôs search time should be considered
when determining whether more than 3 hours will be required (if multiple program areas are
impacted).

Please complete this form and return it electronically.

If your search time will be less than 3 hours, use Record Search Form.

Reference #: [insert access request number]

Program Area Contact: [insert program area contact including responsible for conducting or
coordinating search - one form per program area is requested ‚Äì include name, position and
office telephone number]

Program Area: [insert name of program area or office conducting search]

Date(s) of search: [insert dates of search]

1. Indicate which locations will require searching (whose computer, which files, which
   offices or file rooms).



2. Name(s) of all staff contacted.



3. Methods/processes used to arrive at the estimate and the types of files searched (emails,
   paper files, etc.)



4. Was a representative sample utilized? If so, describe locations searched, sample size,
   the number of hours used to search the sample, the number of pages of responsive
   records found in the sample and any other costs incurred in searching the sample etc.



  Freedom of Information and Protection of Privacy Manual                                226
5. Do any responsive records contain personal information of the requester? If yes, search
   time and severing should not be included in the search fee.



6. Estimate hours required to complete search (to the nearest ¬º hour, do not include
   photocopy time):      hrs.

7. Estimated number of pages of responsive records:               pgs.

8. Estimated number of pages which may require partial or full MFIPPA severances
   (third party or personal information, legal advice, etc.)           pgs.

9. Will a computer programmer be required to write code to retrieve any of the
   records for this request? Yes/No

10. Is there likely to be any third party information in the responsive records, if so, please
    explain.


11. Types of records likely to be retrieved (emails, correspondence, spreadsheets, maps,
    briefing notes etc.).




  Freedom of Information and Protection of Privacy Manual                                   227
Appendix 7: Sample Index of Records
 Document       Document                   Number          Decision to            Exemptions     Comments
 Number         Description                of Pages        Release                Applied

 [number        [briefly describe each     [calculate      [enter decision:       [enter         [enter relevant
 each           document, include          number of       release in full,       exemptions     comments including
 document]      date]                      pages in        withhold in full, or   applied to     IPC orders or case
                                           document]       withhold in part]      withheld       law supporting
                                                                                  information]   application of
                                                                                                 exemptions]

 [add more
 rows as
 needed]




 Freedom of Information and Protection of Privacy Manual                          Page 228
Appendix 8: Request for Waiver of Notice to Individual
of Collection of Personal Information
 Instructions to Institution:

 Pursuant to section 39 (2) of the Freedom of Information and Protection of Privacy Act
 (FIPPA) and section 29 (3) (b) of the Municipal Freedom of Information and Privacy Act
 institutions may request a waiver of notice to individual of collection of personal
 information.

 Complete the following form and attach any relevant background material and submit
 your request for waiver to the Information, Privacy and Archives Division of the Ministry
 of Government and Consumer Services by email to Web.Foi.MGCS@ontario.ca.

 1. Institution: [Insert Name of Institution]

 2. Description of information to be collected: [Describe personal information to be
 collected.]

 3. Authority for collection: [Describe legal authority for collection of personal
 information.]

 4. Manner of collection: [e.g.: directly from the individual to whom the information
 relates for indirectly. If the collection is indirect, indicate the authority to do so.]

 5. Anticipated number of individuals in respect of whom waiver is sought: [Insert
 anticipated number of individuals.]

 6: Use of personal information collected: [Describe the purpose for collection and
 include any FIPPA/MFIPPA section authorizing additional uses or disclosure.]

 7. Personal Information Bank: [Is personal information maintained in a personal
 information bank and is this described in the Directory of Records.]

 8. Reason for Waiver: [Identify reason for waiver from list below.]

    ‚Ä¢   Notification Frustrates Purpose of Indirect Collection
    ‚Ä¢   Statutory Authority for Indirect Collection
    ‚Ä¢   Administrative Burden/Cost of Notification
    ‚Ä¢   Authorization of Commissioner
    ‚Ä¢   Implied Consent
    ‚Ä¢   Collection is from another Institution which has notified Individual
    ‚Ä¢   Other (explain)

 Freedom of Information and Protection of Privacy Manual                             Page 229
9. Explain why notification cannot be given: [Give detailed explanation.]

10. Other material attached: [List additional material provided with this application.]

Date: [Insert date]

Head of Institution: [Insert name and signature of the Head of the Institution for the
purposes of FIPPA or MFIPPA.]




Freedom of Information and Protection of Privacy Manual                            230
Guidance on Preparing Information Sharing Agreements Involving Personal Information - Canada.ca



                                                                                                                                                                                          Fran√ßais

                                                                                                                                               Search Canada.ca                                  ÓÄÉ


            MENU           ÓÑî



           Canada.ca ÓÇÄ Treasury Board of Canada Secretariat ÓÇÄ Access to information and privacy




           Guidance on Preparing Information Sharing Agreements
           Involving Personal Information

           On this page
                   Introduction and background
                   Defnitions: Personal information and Information Sharing Arrangement
                   Deciding whether an ISA is the appropriate tool
                   Alternatives to sharing personal information with other insitutions
                   Sharing information available within your insitution
                   Defning the scope of the ISA
                   Assessing the risks
                   Drafting the ISA and completing the ISA template
                   ISA components
                   Defnitions
                   Reference documents and useful links



                                            Template: Information sharing                                                                    Template: Annexes
                                            arrangement between federal                                                                      (DOC, 79 KB)
                                            insitutions
                                            (DOC, 82 KB)




           Introduction and background
           This guidance is designed to assis parties in completing the Information Sharing Agreement (ISA)
           template as well as its supporting annexes when sharing personal information among federal insitutions.
           All ISAs are unique, and the template and content should be adapted to suit your needs.

           This guidance should be considered in conjunction with all applicable federal laws, regulations, policies
           and guidelines. A detailed assessment may be required to identify privacy risks before being able to

https://www.canada.ca/en/treasury-board-secretariat/services/access-information-privacy/privacy/guidance-preparing-information-sharing-agreements-involving-personal-information.html[11/6/2024 12:58:26 PM]
Guidance on Preparing Information Sharing Agreements Involving Personal Information - Canada.ca



           implement an ISA. Parties are srongly encouraged to consult their insitution‚Äôs ATIP Coordinator, privacy
           advisors, legal advisors, information management advisors and security experts to identify, review and
           consider all applicable laws and policies that may have an impact on privacy and security issues prior to
           entering into an ISA.

           For this guidance, unless specifcally mentioned otherwise, ISAs should be undersood as non-legally
           binding arrangements between two or more federal insitutions.


           Defnitions: Personal information and Information Sharing
           Arrangement
           What is personal information?
           Personal information is defned by section 3 of the Privacy Act as information about an identifable
           individual that is recorded in any form. Examples include:

                   contact information; information relating to the race, national or ethnic origin, colour, religion, age or
                   marital satus of the individual
                   information relating to the education or the medical, criminal or employment hisory of the individual
                   any information that can be linked to the identifcation of an individual, such as an account number,
                   licence number, an Internet Protocol (IP) address, a biometric identifer, or a photographic image

           Certain information is excluded from the defnition of personal information. For example, information that
           relates to the position or functions of an employee of a government insitution is not considered personal
           information.

           When considering whether information is considered personal, keep in mind that a person‚Äôs name is not
           always the determining factor. The defnition of personal information includes any recorded information
           that permits or leads to the possible identifcation of an individual whether alone or when combined with
           information from sources that are otherwise available, including public sources. ‚ÄúPossible identifcation‚Äù
           means a serious possibility. It should be something more than a frivolous chance, but less than a balance
           of probabilities.

           For the purposes of this guidance, the term personal information is to be interpreted broadly and includes
           ‚Äúdata‚Äù when it is relates to an identifable individual.


           What is an Information Sharing Arrangement (ISA)?
           The Privacy Act does not contain any reference to ISAs. That being said, the basic requirements relating
           to ISAs can be found in the Directive on Privacy Practices under section 4.2.23 through section 4.2.27.

           Information sharing may mean that one party is disclosing information while the other party is collecting
           information, or that information is exchanged, where both parties are disclosing and collecting information.



https://www.canada.ca/en/treasury-board-secretariat/services/access-information-privacy/privacy/guidance-preparing-information-sharing-agreements-involving-personal-information.html[11/6/2024 12:58:26 PM]
Guidance on Preparing Information Sharing Agreements Involving Personal Information - Canada.ca


           The benefts of using an ISA to share personal information among federal insitutions include:

                   clarifying the obligations and accountabilities of the parties involved
                   ensuring compliance with the Privacy Act and its related policies
                   esablishing protocols for addressing security incidents including privacy breaches
                   providing awareness and insructions for saf
                   ensuring transparency for afected individuals


           Deciding whether an ISA is the appropriate tool
           Is the information to be shared personal?                                                                                                                            ÓÖó Yes
           Have you reviewed the information to be shared to determine whether it involves                                                                                      ÓÖó No
           personal information as defned under section 3 of the Privacy Act?

           For more details on this subject, see
           Defnition of ‚Äúpersonal information‚Äù

           If the information is not personal information, please see Alternatives to Sharing
           Personal Information with Other Insitutions below

           Have you verifed your legal authority?                                                                                                                               ÓÖó Yes
           If your insitution seeks to receive personal information, have you verifed that your                                                                                 ÓÖó No
           insitution has the legal authority (legislation, regulation, or order-in-council) to collect
           personal information?

           It is recommended that you consult with your legal services to ensure legal authority
           prior to collecting and sharing any personal information.

           If you do not have legal authority to collect or share personal information, you may not
           proceed with any personal information sharing activity.

           Do you need an ISA?                                                                                                                                                  ÓÖó Yes
           Developing an ISA is recommended when sharing any personal information. If sharing                                                                                   ÓÖó No
           the personal information is required under federal law, then the insitution mus share in
           accordance with the requirements of that law. In this case, an ISA is recommended but
           not srictly required.



           Alternatives to sharing personal information with other
           insitutions
           Given the privacy implications of sharing personal information, parties should consider whether alternative
           approaches would be more appropriate.

           Where possible, personal information mus be collected directly from the individuals concerned insead of


https://www.canada.ca/en/treasury-board-secretariat/services/access-information-privacy/privacy/guidance-preparing-information-sharing-agreements-involving-personal-information.html[11/6/2024 12:58:26 PM]
Guidance on Preparing Information Sharing Agreements Involving Personal Information - Canada.ca


           by way of an ISA.

           Alternatively, using and sharing de-identifed personal information should be considered. De-identifed
           information is personal information that has been modifed so that the risk of re-identifying the individual is
           greatly diminished and cannot be readily done.

           Where this is used, there should remain little-to-no possibility of re-linking any of the information to
           identifable individuals or making undue inferences about groups of individuals.

           It is recommended to seek advice from satisical experts when de-identifying information. Such
           consultations would provide assurance that the information to be disclosed would not result in the re-
           identifcation of individuals. You may also contact your insitution‚Äôs privacy experts to assis with
           determining the level of de-identifcation required given your proposed information sharing. Further
           guidance on de-identifcation, especially when working with small populations, can be found in the Privacy
           Implementation Notice 2023-01: De-Identifcation and in the Privacy Implementation Notice 2020-03:
           Protecting privacy when releasing information about a small number of individuals.

           When drafting the ISA, make sure that:

                   consideration was given to sharing de-identifed information insead of personal information
                   when de-identifed is to be used, clauses prohibiting the receiving party from re-identifying individuals
                   and prohibiting the onward sharing of that de-identifed information are included in the ISA

           If personal information is not required, you may consider other types of agreements, such as a
           memorandum of undersanding, a maser agreement, or a service level agreement, as outlined in the
           Guideline on Service Agreements: Essential Elements.


           Sharing information available within your insitution
           In some cases, legislation other than the Privacy Act explicitly permits uses or disclosures, within or
           outside the insitution. For example, the Income Tax Act , the Statisics Act, the Security of Canada
           Information Disclosure Act, and the Department of Employment and Social Development Act contain
           specifc authorities for the use or disclosure of personal information.

           The program area that has control of the personal information to be shared can determine whether
           sharing is appropriate with the advice of the insitution‚Äôs privacy and legal advisors. There may be other
           alternatives, but the benefts and privacy risks associated with a particular option should be assessed
           before making a decision.


           Defning the scope of the ISA
           All parties to an ISA mus consider what is directly related to the activity to ensure that only the minimal
           amount of personal information is disclosed and collected. The ISA mus include a lis of all the personal
           information elements that will be exchanged along with the purpose for each and specifc resrictions that


https://www.canada.ca/en/treasury-board-secretariat/services/access-information-privacy/privacy/guidance-preparing-information-sharing-agreements-involving-personal-information.html[11/6/2024 12:58:26 PM]
Guidance on Preparing Information Sharing Agreements Involving Personal Information - Canada.ca


           may apply to any of the elements (such as, the parties may wish to specify that more sensitive personal
           information elements require sronger protections or impose more sringent conditions on subsequent use
           or disclosure, for example, if the social insurance number is shared).

           Disclosing too much personal information or collecting more personal information than is necessary to
           fulfll a particular purpose is contrary to the Privacy Act and may lead to a privacy breach.


           Assessing the risks
           In addition to ensuring that they have the legal authority to carry out their information-sharing initiatives,
           parties mus ensure that any related privacy and security risks are addressed before entering into any
           arrangements to share personal information.

           Assessing privacy and security risks can be done in a consisent manner through the use the
           recommended tools such as a privacy impact assessment (PIA), privacy protocol, or a security
           assessment and authorization (SA&A), previously known as a threat and risk assessment (TRA), detailed
           below.


           Privacy impact assessment (PIA)
           The Directive on PIA outlines a process that provides decision makers with:

                   an assessment of the initiative‚Äôs compliance with the Privacy Act and its related policies
                   probable impacts associated with issues or non-compliance
                   mitigation srategies and timelines to reduce or eliminate privacy risks


           Privacy protocols
           Under the Policy on Privacy Protection, privacy protocols are a set of documented procedures that are
           consisent with the principles of the Privacy Act for using personal information for non-adminisrative
           purposes such as research, satisics, audit, evaluation and policy analysis.


           Security assessment and authorization (SA&A)
           When sharing personal information, the parties mus maintain efective adminisrative, technical and
           physical safeguards to protect the information. An SA&A is a process by which federal insitutions ensure
           that appropriate safeguards are implemented in their physical and technological environments. When
           warranted, all parties involved in the information sharing initiative could be required to conduct this
           assessment process to evaluate the potential threats and risks to the information as a pre-condition of
           sharing or require confrmation that such safeguards are implemented and periodically reviewed to ensure
           protection of the information.


           Documenting the decision:
           Under section 4.3.2.10 of the Policy on Service and Digital, deputy heads are required to document

https://www.canada.ca/en/treasury-board-secretariat/services/access-information-privacy/privacy/guidance-preparing-information-sharing-agreements-involving-personal-information.html[11/6/2024 12:58:26 PM]
Guidance on Preparing Information Sharing Agreements Involving Personal Information - Canada.ca



           decisions and decision-making processes. When entering into an ISA, it is recommended that parties
           summarize the due diligence that led the authorized ofcials of the insitutions to conclude that an ISA is
           necessary. Appropriate documentation may include a memo to the program head, supporting legal
           analysis, a PIA or a SA&A.


           Drafting the ISA and completing the ISA template
           An ISA should set out the terms and conditions that will govern the sharing of personal information
           between the parties. An ISA template and annexes for federal insitutions is provided as part of this
           guidance. The ISA should be specifc and precise, written in plain language to ensure that all terms are
           fully undersood, provide some fexibility to allow for limited amendments, and be drafted with guidance
           from program and project ofcials, privacy policy and information management experts, legal advisors and
           functional specialiss, such as information technology sysem specialiss and security experts.

           While this guidance pertains to exchanges of personal information between federal insitutions, exchanges
           of personal information with public insitutions in other jurisdictions should include similar sections to the
           ISA template but refect the applicable laws of the respective jurisdictions.

           A summary of the ISA mus be made available to the public as outlined in section 4.2.26 of the Directive
           on Privacy Practices. For greater transparency, insitutions can consider disclosing the full ISA through the
           Open Government portal or on the insitution‚Äôs website. For more information on using the Open
           Government portal, please contact the open government team at open-ouvert@tbs-sct.gc.ca.


           ISA components
                   In this section

           The following sections provide further guidance to support parties using the provided ISA template and
           annexes. Insitutions using their own template can sill beneft from the guidance, but the terminology and
           sructure may not be exactly as seen here.


           1. Preamble
           The preamble section of the ISA should outline the reasons for the arrangement, explain why information
           is being shared insead of being collected directly from the individual(s), describe the potential outcomes
           that the arrangement seeks to achieve, and provide relevant background information.

           Federal insitutions requesing to receive personal information should be able to clearly identify the
           purpose for which the information is needed. The information may be needed for an adminisrative
           purpose where it will be used in a decision-making process that will afect the individuals involved, such as
           for:

                   Authentication and verifcation: where personal information is used to compare and confrm the


https://www.canada.ca/en/treasury-board-secretariat/services/access-information-privacy/privacy/guidance-preparing-information-sharing-agreements-involving-personal-information.html[11/6/2024 12:58:26 PM]
Guidance on Preparing Information Sharing Agreements Involving Personal Information - Canada.ca


                   identity of individuals prior to granting access to programs and services, physical areas or information
                   Eligibility to a program or a service: where personal information is used for determining or verifying
                   eligibility for programs, adminisering program payments or overpayments, or issuing or denying
                   permits/licences
                   Compliance/regulatory activities: where the information is used for detecting possible abuses of
                   programs, services or harassment where the consequences are adminisrative in nature (such as,
                   fne, discontinuation of benefts, audit of personal fles or claims)
                   Criminal invesigations and enforcement/national security: where the information is used for
                   purposes related to invesigations and enforcement in a criminal context or associated with national
                   security or anti-terrorism activities


           2. Provisions
           For clarity and to ease an insitution‚Äôs ability to track an ISA, it is recommended that ISAs provide details
           as to the type and duration of the arrangement.

           There are diferent types of ISAs:

                   One-way disclosure: One insitution discloses personal information to a single insitution, without
                   receiving information itself
                   Two-way disclosure: Two insitutions party to the ISA receive and disclose personal information to
                   each other
                   Multi-party, one-way disclosure: An insitution discloses personal information to two or more other
                   insitutions, without receiving information itself
                   Multi-party two-way disclosure: More than two insitutions are involved in the collection, use and
                   disclosure of personal information between each other

           There are diferent durations for ISAs:

                   One-time disclosure: Insitutions share personal information on a single occasion. In such a case,
                   information can be provided only once, and the ISA should clearly indicate an entry into force date
                   and an expiry date.
                   Determinate period disclosure: Information may be disclosed routinely throughout a determined
                   period of time. The ISA mus clearly defne an entry into force date and an expiry date. Where the
                   defned period exceeds fve years, a review clause should be added to ensure that the ISA is
                   reviewed on a periodic basis to be determined between the parties.
                   Unspecifed period disclosure: Personal information may be disclosed routinely throughout an
                   unspecifed period of time. The ISA should include an entry into force date and a clause on periodic
                   review.


           3. Legal authority for collection and disclosure

           Collection

https://www.canada.ca/en/treasury-board-secretariat/services/access-information-privacy/privacy/guidance-preparing-information-sharing-agreements-involving-personal-information.html[11/6/2024 12:58:26 PM]
Guidance on Preparing Information Sharing Agreements Involving Personal Information - Canada.ca



           Section 4 of the Privacy Act prohibits federal insitutions from collecting personal information unless it is
           directly related to a program or activity of the government insitution.

           The Privacy Act is not the single source of law that sets out rules for federal insitutions regarding the
           collection, use and disclosure of personal information.

           Other acts of Parliament contain satutory provisions that specifcally authorize, prohibit or regulate the
           sharing of personal information. For example, the Statisics Act authorizes Statisics Canada to enter into
           agreements with provincial satisical agencies, federal, provincial or municipal government insitutions or
           other corporations.

           When frs considering sharing personal information, parties mus satisfy themselves that it is lawful. This
           means that all insitutions involved in the proposed sharing mus ensure that they have their own satutory
           authority to carry out the proposed information sharing (collecting and/or disclosing) activities.

           Notifcation
           With confrmed legal authority, insitutions are required to notify individuals of a collection of personal
           information. As outlined in subsection 5(2) of the Privacy Act, parties shall notify the program participants
           at the point of collection about the need to share their personal information as a condition of program
           enrolment. Notifcations mus include the elements described in the Directive on Privacy Practices. For
           example, under many beneft programs, it may be necessary that personal information be shared between
           insitutions to determine the eligibility to program benefts.

           When drafting the ISA, make sure that:

                   individuals are notifed of the new collection/disclosure at the point of collection, if possible
                   a privacy notice has been agreed upon by all parties to the ISA and meets policy requirements

           Note: Privacy notices should inform the individual why the information is being collected, of any satutory
           authority for the collection, how it will be used, and who it will be shared with and why.

           Disclosure
           Section 8 of the Privacy Act sates that personal information under the control of a government insitution
           cannot be disclosed without the consent of the individual to whom the information relates, unless the
           disclosure is permitted in any of the 13 circumsances under subsection 8(2). All subsection 8(2)
           disclosure provisions are discretionary and any disclosure of personal information pursuant to this section
           mus be considered on a case-by-case basis.

           In certain cases, the sections of the Privacy Act that authorize disclosure do not specify who mus make
           the decision. Although there is no rule about who that should be, it is important that an appropriate ofcial
           be identifed as having made the decision to authorize the disclosure for the purposes of accountability
           and transparency.

           The sharing of personal information could, in some situations, implicate the Canadian Charter of Rights


https://www.canada.ca/en/treasury-board-secretariat/services/access-information-privacy/privacy/guidance-preparing-information-sharing-agreements-involving-personal-information.html[11/6/2024 12:58:26 PM]
Guidance on Preparing Information Sharing Agreements Involving Personal Information - Canada.ca


           and Freedoms (the Charter). The fact that a disclosure is specifcally authorized under other acts of
           Parliament does not automatically ensure compliance with the Charter. It is recognized that the
           interrelationship between these areas of law is quite complex and legal advice will be necessary.

           When drafting section 3 of the ISA, make sure that:

                   your ISA collects only personal information that is ‚Äúdirectly related to a program or an activity of your
                   insitution‚Äù
                   you have consulted your legal services to esablish the collection authority
                           Consent (described below) is not sufcient authority to collect personal information
                   you have consulted the TBS Privacy Policy suite, including Directive on Social Insurance Number if
                   you intend to collect or disclose the SIN
                   you have determined whether the arrangement qualifes for a disclosure provision under
                   subsection 8(2)
                   You have verifed whether your own insitution‚Äôs legislation is further resricting the disclosure of
                   personal information than subsection 8(2) of the Privacy Act


           4. Accuracy of personal information and compliance with sandards

           Accuracy
           Parties to an ISA should base adminisrative decisions on current and up-to-date personal information.
           This is especially important where the impact of an adminisrative decision on individuals might be high.
           Providing unreliable, inaccurate or incomplete information to other insitutions can create potentially
           serious problems for those who rely on the information, as well as those who are the subjects of the
           inaccuracies.

           When drafting, make sure the ISA:

                   contains an obligation to notify the other party if the insitution become aware of inaccurate personal
                   information that has been shared
                   requires the other party to review any adminisrative actions taken based on inaccurate information
                   provided
                   includes a process to treat any requess for correction made by the individual

           Applicable sandards
           Insitutions mus make every reasonable efort to ensure that all personal information disclosed ‚Äì which
           includes all associated metadata used to describe it ‚Äì is relevant and meets applicable sandards (such as
           for quality). This is particularly important when personal information will be used in a decision-making
           process that afects individuals. Prior to sharing the information, parties mus clearly esablish the type,
           extent and quality of the personal information and associated metadata to be shared, as well as the
           method of transmitting it. Steps required to ensure that the personal information is appropriate for the
           intended use may need to be addressed by the parties before entering into any arrangement to share it.


https://www.canada.ca/en/treasury-board-secretariat/services/access-information-privacy/privacy/guidance-preparing-information-sharing-agreements-involving-personal-information.html[11/6/2024 12:58:26 PM]
Guidance on Preparing Information Sharing Agreements Involving Personal Information - Canada.ca



           One example of the importance of personal information that complies with sandards is when it includes
           criminal intelligence. In those cases, the disclosing federal insitution could indicate the reliability of the
           information involved by providing background information ‚Äì including if it was observed, heard from a
           source or obtained by electronic intercept, and its origin. In this example, the ISA would have to include
           caveats to address any concerns about the quality of the personal information or metadata describing it.

           Paragraph 4.3 of the ISA template includes provisions with regards to the sandards that the insitution is
           following to ensure that the personal information is of high quality.

           When drafting the ISA, make sure that:

                   the format of the personal information, including its associated metadata, is specifed in the ISA so
                   that it is ‚Äúreadable‚Äù by the other party. The exchanged information and metadata should be inter-
                   operable between the parties and sandardized in accordance with the Policy on Service and Digital
                   and the Standard for Managing Metadata
                   personal information received from external parties, governmental or otherwise, has been profled
                   and validated prior to its use or reuse. This practice involves complying with any applicable
                   enterprise-level sandards needed to enable their sructural and semantic interoperability
                   information exchanges will be adequately recorded or logged on the subject‚Äôs fle by the disclosing
                   insitution, when reasonable, in case of a need to correct inaccurate information


           5. Use
           Section 7 of the Privacy Act prohibits the use of personal information by a federal government insitution
           without the consent of the individual to whom it relates, except for the purpose for which it was collected or
           for a use consisent with that purpose.

           Secondary use
           Any prohibitions on secondary uses should be agreed to prior to entering into any arrangement to share
           personal information. Where appropriate, procedures to seek approval for additional use should be
           articulated in the ISA, taking into consideration relevant laws, regulations, or policies applicable to each
           party to the ISA.

           When drafting, make sure the ISA:

                   identifes how the personal information to be shared under the ISA will be used
                   describes any planned secondary uses and is updated when new secondary uses are identifed
                   specifes that secondary uses are limited to those that are in accordance with section 7 of the Privacy
                   Act
                   describes any limitations agains subsequent or secondary use of the information, taking into
                   consideration relevant laws, regulations, or policies applicable to each party to the ISA
                   includes clauses defning procedures to seek approval to further use, along with any other
                   appropriate conditions


https://www.canada.ca/en/treasury-board-secretariat/services/access-information-privacy/privacy/guidance-preparing-information-sharing-agreements-involving-personal-information.html[11/6/2024 12:58:26 PM]
Guidance on Preparing Information Sharing Agreements Involving Personal Information - Canada.ca




           6. Disclosure
           Subsection 8(2) of the Privacy Act describes the circumsances under which personal information under
           the control of a government insitution may be disclosed without the consent of the individual to whom the
           information pertains. Such disclosures are discretionary and are subject to any other act of Parliament.

           Of note, paragraph 8(2)(j) of the Privacy Act authorizes the head of a government insitution to disclose
           personal information to any person or body for research or satisical purposes, if the head of the
           government insitution who has control of the records:

                   is satisfed that the purpose for which the information is disclosed cannot reasonably be
                   accomplished unless the information is provided in a form that would identify the individual to whom it
                   relates and
                   obtains from the person or body a written undertaking that the information will not subsequently be
                   disclosed in a form that could reasonably be expected to identify the individual to whom it relates

           Secondary disclosure
           Any prohibitions on secondary disclosures should be agreed to prior to entering into any arrangement to
           share personal information. Where appropriate, procedures to seek approval for additional disclosure
           should be articulated in the ISA, taking into consideration relevant laws, regulations, or policies applicable
           to each party to the ISA.

           When drafting, make sure the ISA:

                   identifes how the personal information to be shared under the ISA will be disclosed
                   describes any planned secondary disclosures and is updated when new secondary disclosures are
                   identifed

           When determining whether a potential disclosure is aligned with subsection 8(2) of the Privacy Act, it is
           recommended that insitutions engage privacy and legal experts.


           7. Access rights
           ISAs are subject to access to information and personal information requess. As such, the ISA should
           include clauses about the process that will be used to answer such requess, whether parties want to be
           consulted, how the consultation process ought to be conducted and which party should be responsible for
           addressing the reques.

           When drafting the ISA, make sure that:

                   it contains clauses to esablish a consultation procedure to respond to Access to Information Act or
                   Privacy Act requess
                   it includes clauses requiring the parties to inform with each other when they receive an access to
                   information reques and intend on disclosing information


https://www.canada.ca/en/treasury-board-secretariat/services/access-information-privacy/privacy/guidance-preparing-information-sharing-agreements-involving-personal-information.html[11/6/2024 12:58:26 PM]
Guidance on Preparing Information Sharing Agreements Involving Personal Information - Canada.ca




           8. Method of exchange and frequency of sharing
           Defning the method and frequency for sharing information is important to ensure that the collection and
           disclosure of the personal information occurs as anticipated by both parties and is in line with legal and
           policy requirements. In addition, defning the method and frequency of exchange will better prepare
           insitutions to react quickly should information be accessed or handled improperly.

           Information can either be ‚Äúpushed‚Äù (insigated by the disclosing party) or ‚Äúpulled‚Äù (insigated by the
           receiving party). Regardless of the method, insitutions should transfer the relevant information to the
           other party only in the manner, times and dates provided for in the ISA, rather than giving ongoing access
           to the database in which personal information is sored.

           When drafting, make sure the ISA:

                   esablishes the process and the frequency by which personal information will be exchanged between
                   the parties has been agreed upon by the parties. Consult Annex D ‚Äì Transmission and safeguarding
                   of information for examples of how this could be articulated
                   includes alternative processes and technologies for sharing in case the primary process or
                   technology is temporarily unavailable


           9. Information management
           The information collected, used, disclosed or disposed of mus be managed in accordance with legislation,
           the Policy on Service and Digital, the Policy on Privacy Protection and their supporting policy insruments.
           This is further outlined in Annex D ‚Äì Transmission and safeguarding of information.

           The Directive on Service and Digital requires insitutions to use digital sysems as the preferred means for
           managing information. Leveraging enterprise sysems where possible can increase the efciency of
           information exchange.

           Government insitutions mus have their information management practices and controls defned,
           documented, implemented, assessed, monitored and maintained throughout all sages of the information
           life cycle to provide reasonable assurance that information is adequately protected in a manner that
           respects legal and other obligations, and balances the risk of injury and threats with the cos of applying
           safeguards.

           Parties should take seps to ensure that their information management practices are aligned with those
           with whom personal information is being or will be shared. The Government of Canada Enterprise
           Architecture Framework sets out enterprise architecture requirements to ensure that information
           management practices are aligned across the government.

           When drafting the ISA, consider these bes practices:

                   Use digital sysems to create, collect, manage, use and share personal information
                   Clearly defne roles, responsibilities and accountabilities for personal information in the federal


https://www.canada.ca/en/treasury-board-secretariat/services/access-information-privacy/privacy/guidance-preparing-information-sharing-agreements-involving-personal-information.html[11/6/2024 12:58:26 PM]
Guidance on Preparing Information Sharing Agreements Involving Personal Information - Canada.ca


                   insitution via departmental governance sructure, both at the working and senior levels
                   Ensure, when required, that personal information datasets are updated at appropriate intervals
                   Regularly assess the value and utility of personal information assets to ensure that collection and use
                   of personal information is limited to what is required for a program or activity


           10. Security management
           Appropriate security controls are essential to support the trused delivery of programs and services. In the
           context of personal information sharing, it is essential that measures are in place to enable the secure
           transfer and sorage of personal information.

           In accordance with the Policy on Government Security and section 4.5.1 of the Directive on Security
           Management, federal government employees are responsible for adhering to government security policy
           and departmental security practices, including safeguarding information and assets under their control,
           whether working on-site or of-site.

           The ISA should include clauses to explain that the information shared amongs the parties is treated in
           accordance with the security designation of the information and on a ‚Äúneeds to know‚Äù basis so that only
           authorized employees have access to the relevant information at the appropriate time.

           Any security measures that are appropriate to ensure the security of the personal information during and
           after transmission, such as encryption, password protection, or authentication should be fully documented
           in the ISA. Tagging or watermarking the information shared, especially when it is sensitive information, is
           one way of ensuring that the information will be treated in a manner that respects the terms and conditions
           of the ISA.

           The nature of the safeguards used to protect personal information from both external and internal risks will
           vary depending on the sensitivity of the information that has been collected; the amount, disribution and
           format of the information; the method of sorage; and the harm or injury that would arise from a breach of
           security. More sensitive information will be safeguarded by a higher level of protection. To this end,
           government insitutions as defned under section 11.1 of the Financial Adminisration Act should follow the
           requirements of the Policy on Government Security the Policy on Service and Digital, and other
           associated sandards and directives. Insitutions that are not subject to the FAA should refer to their own
           internal security policies and procedures.

           Moreover, the safeguards should take into account actions to be taken to respond to security incidents or
           privacy breaches, including the notifcation of afected parties. For more information, consult Appendix B
           of the Directive on Privacy Practices.

           Often these measures are included in a separate schedule attached to the ISA to allow for fexible
           amendment whenever the measures are changed. Annex D ‚Äì Transmission and safeguarding of
           information, and Annex E ‚Äì Attesation of consideration of security requirements, provide sample text.

           When drafting the ISA, consider:


https://www.canada.ca/en/treasury-board-secretariat/services/access-information-privacy/privacy/guidance-preparing-information-sharing-agreements-involving-personal-information.html[11/6/2024 12:58:26 PM]
Guidance on Preparing Information Sharing Agreements Involving Personal Information - Canada.ca


                   whether personal information should be headed or watermarked ‚ÄúReceived in confdence pursuant to
                   (title of the ISA).‚Äù This could also assis in identifying the source of any unauthorized dissemination
                   how insitutions entering into an ISA ensure that the personal information being exchanged receives
                   the same security classifcation on either side (see Appendix J of the Directive on Security
                   Management for more details. Appendix F of that Directive contains a series of security requirements
                   for insitutions entering into arrangements that should be respected by federal insitutions in the core
                   public service)
                   whether the ISA will exchange personal information with insitutions who are not subject to the
                   Directive on Security Management, such as Crowns corporations, and whether additional details
                   should be included in the ISAs so obligations are well undersood by all parties


           11. Addressing suspected security incidents and privacy breaches
           Efective reporting and management of security incidents and privacy breaches are essential to ensuring
           that each Party to the ISA is compliant with privacy and security requirements. The ISA template and
           Annex G ‚Äì Protocol for suspected security incidents including privacy breaches requires the notifcation of
           all parties to the ISA of any suspected privacy breach. The OPI Preliminary Breach Report Form can be
           used for the purpose of this notifcation.

           Many privacy breaches are security incidents. Therefore, it is essential that privacy ofcials in each
           insitution coordinate their response with security ofcials. Similarly, all security incidents involving
           information assets should be brought to the attention of privacy ofcials, who are bes placed to determine
           whether information is personal in nature.

           Additional policy requirements include esablishing procedures for notifying individuals afected by privacy
           breaches. Documenting in the ISA which insitution will notify individuals in what circumsances could help
           sreamline the breach management process.

           More details on managing security incidents can be found in the Directive on Security Management and
           its Appendix G: Mandatory Procedures for Security Event Management Control, and further guidance on
           managing privacy breaches can be found in Annex B of the Directive on Privacy Practices and the Privacy
           Breach Management Toolkit .

           When drafting the ISA, make sure that:

                   the ISA includes breach requirements in the event of accidental or unauthorized access, disclosure,
                   use, modifcation and deletion of personal information (see ISA template Annex G ‚Äì Protocol for
                   suspected security incidents including privacy breaches)
                   the ISA requires each Party to follow their insitution‚Äôs plans in the event of a breach. Additionally,
                   once a material breach has been determined, insitutions mus use the Privacy Act Material Breach
                   Reporting form for reporting to TBS and the Ofce of the Privacy Commissioner (OPC)
                   both privacy and security ofcials have been included in the contact information provided in the ISA
                   a clause about coordination between insitutional privacy and security ofcials has been included in


https://www.canada.ca/en/treasury-board-secretariat/services/access-information-privacy/privacy/guidance-preparing-information-sharing-agreements-involving-personal-information.html[11/6/2024 12:58:26 PM]
Guidance on Preparing Information Sharing Agreements Involving Personal Information - Canada.ca


                   the ISA


           12. Retention period and disposition

           Retention of personal information
           The Library and Archives Canada Act outlines broad legislative direction as it relates to the retention of
           government records. Additionally, the Privacy Act and its Regulations require that personal information
           used by a government insitution for an adminisrative purpose be retained by that government insitution
           for at leas two years following the las adminisrative use of the information, unless the individual consents
           to its earlier disposition.

           Under the Directive on Service and Digital, each government insitution is responsible for esablishing,
           implementing and maintaining retention periods for information resources of business value. It is
           incumbent upon each government insitution to undersand and apply any legislation regarding the
           retention and disclosure of information and, more specifcally, its own legislation when setting those
           retention periods.

           The parties to an ISA should indicate a retention period for the personal information shared under the ISA.
           Parties should ensure that the retention period is no less than the two years provided under section 7 of
           the Privacy Regulations.

           Disposition of personal information
           The disposition of personal information is similarly governed by the Privacy Act and the Library and
           Archives of Canada Act. However, the requirements of disposal are less prescriptive and are left to policy
           insruments to defne. For this reason, consulting with your insitution‚Äôs information management experts is
           recommended.

           Information exchanged in the course of an ISA that is subject to a litigation hold mus not be disposed of
           before the end of the litigation or the end of all appeal possibilities. Insitutions should consult with their
           legal services if they fnd themselves in such a situation.

           The ISA should explain how the parties plan to dispose of the personal information.

           Depending on the sensitivity of the personal information shared, insitutions may require to be notifed
           when desruction has taken place. This would require a record of desruction or a log of the disposition of
           any shared personal information.

           Such record of desruction or log could contain the following information and be made available to the
           other party immediately upon its reques:

                   details of the records that were disposed of (such as, fle name, fle number, date(s) of the records)
                   the method of desruction (paper copy shredded or electronic copy deleted from all fles)
                   the date of desruction (day, month, year)
                   the name and position title of the person who carried out the desruction of the records


https://www.canada.ca/en/treasury-board-secretariat/services/access-information-privacy/privacy/guidance-preparing-information-sharing-agreements-involving-personal-information.html[11/6/2024 12:58:26 PM]
Guidance on Preparing Information Sharing Agreements Involving Personal Information - Canada.ca



           The parties to an ISA may also want to use ISA Annex F ‚Äì Certifcate of desruction of personal
           information and data elements to attes to that the personal information and data in their possession was
           desroyed and disposed of.

           When drafting, make sure the ISA includes:

                   a retention period of the personal information and, if required, clauses for obtaining consent from
                   individuals for early disposal
                   an indication of whether the personal information is expected to be returned to the insitution or
                   desroyed by the receiving insitution at the conclusion of the ISA
                   whether a record of desruction will be used by the insitutions party to the ISA
                   a process for identifying and communicating any litigation holds, personal information requess or
                   access to information requess that would prevent the desruction of any personal information for all
                   party to the ISA


           13. Compliance monitoring and audits
           Audits help ensure that parties to an ISA adhere to the terms and conditions of the arrangement. Audit
           provisions also help to srengthen security and privacy controls by encouraging parties to ensure that their
           information and privacy controls are as efective as possible to meet all their compliance requirements.

           When drafting the ISA, determine:

                   whether the parties want to use management audits to evaluate their respective information
                   management practices and to assess their compliance with the requirements of the ISA
                   whether the parties agree to provide a copy of their respective management audit reports and any
                   action plans to each other within a prescribed time of their completion
                   whether clauses in the ISA should reques specifc audits according to a pre-determined schedule


           14. Transparency
           Section 4.2.27 of the Directive on Privacy Practices requires that, where possible by security
           requirements, parties entering into an ISA publish a summary containing the following details, as outlined
           in Annex K ‚Äì Summary of the Arrangement to be published:

                   title
                   name of the program
                   identity of the federal insitutions who are party to the ISA
                   date of entry into force
                   date of termination and/or review (depending on the type/length of ISA)
                   purpose (including the reason for, type and length of the ISA)
                   legal authorities
                   personal information being disclosed and collected (including the personal information bank title and



https://www.canada.ca/en/treasury-board-secretariat/services/access-information-privacy/privacy/guidance-preparing-information-sharing-agreements-involving-personal-information.html[11/6/2024 12:58:26 PM]
Guidance on Preparing Information Sharing Agreements Involving Personal Information - Canada.ca


                   number)

           The Privacy Act also requires personal information under the control of the insitution to be described in a
           personal information bank (PIB). When collecting personal information through an ISA, insitutions mus
           engage with their privacy experts to identify the relevant PIB and validate whether it requires any changes
           or updates resulting from the ISA.

           When drafting the ISA, determine:

                   what are the security requirements and any other confdentiality or legal considerations when making
                   the ISA or a summary of the ISA publicly available. In mos cases, a summary of the ISA should be
                   possible, even if some details need to be withheld
                   what is the parties‚Äô preferred platform for public disclosure. In addition to the requirements under the
                   Directive on Privacy Practices to publish annual updates in Info Source, parties should also consider
                   using exising platforms such as the Government of Canada‚Äôs Open Government portal for public
                   disclosures. In the event that insitutions consider the Open Government portal, they are encouraged
                   to contact the open government team at open-ouvert@tbs-sct.gc.ca and to comply with TBS policies
                   and directives on information management and communications, such as the Policy on Service and
                   Digital and the Directive on Open Government.
                   whether you have consulted your privacy experts to determine whether an update to a PIB is needed


           15. Notice
           Parties to an ISA should provide each other, as soon as practicable, notice of any change in legislation,
           regulation, policy, technology or funding relating to their respective programs that may impact their ability
           to fulfll their obligations as described in the Arrangement.

           All notices should be made in writing by the designated ofcials who signed the ISA or their delegate as
           outlined in Annex H of the ISA template and should be clear and concise with the objective of
           communicating specifc changes afecting the ISA and potentially the ability to meet obligations.

           When drafting notices, insitutions should consider including the following details:

                   Which clause(s) of the ISA are impacted
                   A clear description of the events leading to the change
                   Details of whether the change is likely to cause delay
                   Details of when the change is expected to take efect
                   What mitigation measures have been or will need to be taken
                   In cases where insitutions are using cos recovery, details of whether the cos recovery amount is
                   likely to be afected


           16. Financial arrangement
           Some insitutions may want to include cos-recovery provisions in their ISAs with other insitutions.



https://www.canada.ca/en/treasury-board-secretariat/services/access-information-privacy/privacy/guidance-preparing-information-sharing-agreements-involving-personal-information.html[11/6/2024 12:58:26 PM]
Guidance on Preparing Information Sharing Agreements Involving Personal Information - Canada.ca


           Paragraph 16 of the ISA template and Annex J provides more details on examples of these types of
           clauses.

           Parties should familiarize themselves with fnancial policy requirements and confrm they have the
           appropriate authorities to initiate cos recovery. The Directive on Charging and Special Financial
           Authorities and the FAA should help parties in determining their authorities for cos recovery, in
           consultation with their insitution‚Äôs legal services.

           Other considerations include determining whether the arrangement is a multi-year arrangement where the
           coss would carry on throughout the life of the arrangement, or if the arrangement is for a specifc fscal
           year where the revenues can only be re-spent during the fscal year for which the coss were incurred.

           A process and timing for the cos recovery will need to be agreed upon by the parties to the ISA.

           Example 1: The [Disclosing Insitution] will send invoices three times per year. The frs invoice will be
           sent in Augus to cover information provided in April and July, and the second invoice will be sent in mid-
           February (to ensure payment by March 31) to cover information provided in October and January. If
           required, the [Disclosing Insitution] will reconcile coss with the actual expenditures to date and, if
           necessary, proceed with an adjusment for the variance.

           Example 2: The [Disclosing Insitution] will send an invoice in September of each year, following the
           annual provision of information, scheduled for Augus 15 of each year. The [Receiving Insitution] agrees
           to reimburse the [Disclosing Insitution] for these coss. If required, the [Disclosing Insitution] will reconcile
           coss with the actual expenditures to date and, if necessary, proceed with an adjusment for the variance.


           17. Review
           In cases where the duration of an ISA exceeds a period of fve years, parties to an ISA should include a
           clause to ensure that the ISA is reviewed on a periodic basis to be determined between the parties. See
           paragraph 17 ‚Äì Review of the ISA template for an example of such clause. More frequent reviews can be
           considered where needed.


           18. Amendments
           Amendments to the terms of an ISA or the related annexes should be made in writing and executed by the
           designated ofcials or their authorized representatives. Refer to paragraph 18 of the ISA template for
           suggesed wording. Parties to the ISA should include clauses in the ISA defning the amendment process
           to the ISA or its annexes.

           In insances where an amendment changes the collection, use, disclosure to third parties, transfer
           between parties, disposal or other aspects of the management of the personal information, the insitutions
           mus change their related PIBs. If the changes are subsantial, the insitutions mus also provide TBS with
           the updated PIB as well as a privacy impact assessment, as outlined in section 4.1.7 of Directive on
           Privacy Practices.



https://www.canada.ca/en/treasury-board-secretariat/services/access-information-privacy/privacy/guidance-preparing-information-sharing-agreements-involving-personal-information.html[11/6/2024 12:58:26 PM]
Guidance on Preparing Information Sharing Agreements Involving Personal Information - Canada.ca


           Should amendments to the terms of an arrangement result in new consisent uses or new disclosures, the
           insitutions mus modify their PIBs accordingly and notify the OPC of the new consisent uses, as outlined
           in section 4.1.17 of the Directive on Privacy Practices.

           Coming into force
           An ISA comes into force on the efective date agreed upon by the parties and continues to be in force
           unless it is terminated in accordance with the procedure set out in paragraph 19 of the template or it is
           replaced by another agreement.


           19. Termination
           The termination of an ISA or of an annex should be done in writing, either following mutual consent of both
           parties, or by one of the parties providing a termination notice to the other. Notice mus be provided to the
           insitution‚Äôs designated ofcial for the arrangement. Despite the termination of the ISA, the conditions
           associated with the sharing of the personal information and the related clauses normally continue to apply
           to the information that has already been exchanged. Parties to the ISA should include clauses defning
           how the information should be disposed of at the conclusion of the ISA. Where cos recovery provisions
           are included in an ISA, the obligation for account reconciliation and the issuance of a fnal invoice, when
           appropriate and subject to the termination of the arrangement would also continue.

           Upon termination of an ISA, the parties should advise their ATIP ofces and update their related PIBs.

           When drafting the ISA, make sure that:

                   the ISA has a clause to ensure that it is reviewed on a pre-determined, periodic basis (review of ISAs
                   at leas every fve years from their signature date or more frequently is highly encouraged)
                   the ISA includes provisions to allow the parties to terminate the ISA immediately and may reques the
                   return of personal information that has been shared
                   the ISA contains clauses where amendments to the ISA and the annexes are made in writing and
                   executed by the signatories of each insitution or their representatives in the form of an ofcial letter
                   the ISA contains amendment process clauses and termination clauses.


           20. Confict resolution
           In the event of quesions, challenges or disagreements related to any issue connected to an ISA, it is
           recommended that clauses be included to provide a mechanism for confict resolution. Paragraph 21 of
           the ISA template provides sample text.

           In cases where conficts are unable to be resolved, the head of the insitution or the Deputy Miniser (when
           applicable) would typically be the ultimate level of confict resolution.

           When drafting the ISA, determine:

                   whether the ISA needs to contain confict resolution clauses to address interpretation quesions and
                   challenges, or disagreement associated with the ISA

https://www.canada.ca/en/treasury-board-secretariat/services/access-information-privacy/privacy/guidance-preparing-information-sharing-agreements-involving-personal-information.html[11/6/2024 12:58:26 PM]
Guidance on Preparing Information Sharing Agreements Involving Personal Information - Canada.ca




           21. Designated ofcials
           An ISA should include the date, names, titles and signatures of the designated ofcial of all parties
           involved in the ISA. You may want to refer to paragraph 21 of the ISA template and relevant sections of
           annexes A, B and H for sample language.

           The level of the delegation to sign ISAs depends on the sensitivity of information, the magnitude of any
           cos recovery involved, and the powers conferred through legislation. Arrangements deemed to be low risk
           or low cos are often signed at the director or the director general level.

           When the personal information is more sensitive or when the coss are higher, either the head of the
           insitution or a deputy miniser or assisant deputy miniser equivalent may wish to sign.

           Broad ISAs that act as overarching arrangements comprised of smaller annexed arrangements are often
           signed by heads of insitutions or deputy minisers (when applicable). The signature of amendments to
           annexes in umbrella ISAs could be delegated to an authorized representative who would be responsible
           for the program area. This allows for greater fexibility should the content of those annexes need to be
           amended.

           Typically, the signing authority for the respective parties to the ISAs is equivalent but this may difer if a
           specifc authority is identifed in legislation. For example, if the deputy miniser is signing the ISA for
           Party A, the respective deputy miniser or equivalent should be the signatory for Party B. This practice
           extends to interjurisdictional ISAs whereby the provincial or territorial signing authority would be at the
           same level as their federal counterpart.


           Defnitions
           The defnitions of the terms ‚Äúadminisrative purpose,‚Äù ‚Äúhead of insitution,‚Äù ‚Äúpersonal information,‚Äù and
           ‚Äúpersonal information bank‚Äù shown below are consisent with the manner in which these four terms are
           defned in section 3 of the Privacy Act. The other defnitions have been adapted for the specifc use of this
           guidance document.

           adminisrative purpose (fns adminisratives)
           is the use of personal information about an individual ‚Äúin a decision-making process that directly afects
           that individual‚Äù (section 3). This includes mos uses of personal information for confrming identity
           (authentication and verifcation purposes) and for determining eligibility of individuals for government
           programs

           aggregated data (donn√©es agr√©g√©es)
           describes data in satisics that is combined from several measurements, or in economics, describes high-
           level data that is composed of a multitude or combination of other more individual data. In all cases,
           ‚Äúaggregated data‚Äù should have been generalized in such a way that it cannot be linked to an individual, for
           example, by using a range of ages rather than specifc ages



https://www.canada.ca/en/treasury-board-secretariat/services/access-information-privacy/privacy/guidance-preparing-information-sharing-agreements-involving-personal-information.html[11/6/2024 12:58:26 PM]
Guidance on Preparing Information Sharing Agreements Involving Personal Information - Canada.ca


           authorized representative (repr√©sentant autoris√©)
           is a representative authorized by the designated ofcial for carrying out specifc terms and conditions of
           the arrangement and who can amend the annexes of the ISA, except for a new use of personal
           information

           consent (consentement)
           is the informed, voluntary agreement of an individual for the indirect collection or for the disclosure,
           retention and subsequent uses of personal information collected from the individual for a legally
           authorized purpose

           consisent use (usage compatible)
           is a use that has a reasonable and direct connection to the original purpose(s) for which the information
           was obtained or compiled. This means that the original purpose and the proposed purpose are so closely
           related that the individual would expect that the information would be used for the consisent purpose,
           even if the use is not spelled out

           data (donn√©es)
           is the set of values of subjects with respect to qualitative or quantitative variables representing facts,
           satisics or items of information in a formalized manner suitable for communication, reinterpretation or
           processing

           de-identifcation (d√©personnalisation)
           is personal information that has been modifed through a process to remove identifers to a degree that is
           appropriate in the circumsances. De-identifed information carries a residual risk of re-identifcation

           designated ofcial (fonctionnaire d√©sign√©)
           is the person designated by a federal insitution as the signatory to an ISA that will have the overall
           adminisrative responsibility for the ISA and its related annexes

           head (responsable d‚Äôinsitution f√©d√©rale)
           is the miniser, in the case of a department or minisry of sate. In any other case, it is the person
           designated by the Privacy Act Heads of Government Insitutions Designation Order. If no such person is
           designated, the chief executive ofcer of the government insitution, whatever their title, is the head

           non-adminisrative purpose (fns non adminisratives)
           is the use of personal information for a purpose that is not related to any decision-making process that
           directly afects the individual. This includes the use of personal information for research, satisical, audit
           and evaluation purposes

           notice (avis)
           is the indication given to another insitution about possible changes to an ISA

           privacy notice (avis de confdentialit√©)
           is a verbal or written notice informing an individual of the purpose of a collection of personal information
           and of the government insitution‚Äôs authority for collecting, including creating, using and disclosing the
           information. The notice, which mus reference the PIB described in Info Source, also informs the individual
           of their right to access, and reques the correction of, the personal information and of the consequences of
           refusing to provide the information requesed



https://www.canada.ca/en/treasury-board-secretariat/services/access-information-privacy/privacy/guidance-preparing-information-sharing-agreements-involving-personal-information.html[11/6/2024 12:58:26 PM]
Guidance on Preparing Information Sharing Agreements Involving Personal Information - Canada.ca


           program or activity (programme ou activit√©)
           is, for the purposes of the appropriate collection, use or disclosure of personal information by government
           insitutions subject to this policy, a program or activity that is authorized or approved by Parliament.
           Parliamentary authority is usually contained in an act of Parliament or subsequent regulations.
           Parliamentary authority can also be in the form of approval of expenditures proposed in the Esimates and
           as authorized by an appropriation act. Also included in this defnition are any activities conducted as part
           of the adminisration of the program

           personal Information (renseignements personnels)
           is ‚Äúinformation about an identifable individual that is recorded in any form.‚Äù See section 3 of the Privacy
           Act for additional information

           personal information bank (PIB) (fchiers de renseignements personnels)
           is a description of personal information that is organized and retrievable by a person‚Äôs name or by an
           identifying number, symbol or other particular assigned only to that person. The personal information
           described in the PIB has been used, is being used, or is available for use for an adminisrative purpose
           and is under the control of a government insitution

           social insurance number (SIN) (num√©ro d‚Äôassurance sociale (NAS))
           is a number suitable for use as a fle number or account number or for data-processing purposes, as
           defned in subsection 28.1(3) of the Department of Employment and Social Development Act. For
           purposes of paragraph 3(c) of the Privacy Act, the SIN is an identifying number and is therefore considered
           to be personal information

           sensitive personal information (renseignements personnels sensibles)
           while virtually any personal information may be sensitive in certain contexts (for example, disclosure of a
           home address may expose an individual to risk for personal or professional reasons), there are certain
           categories of personal information that are considered sensitive for all or mos individuals. These include
           medical, fnancial information, criminal hisory, or widely used personal identifers such as the SIN or other
           information, the disclosure of which could be injurious to the individual to whom it relates (such as, identity
           theft, fraud, emotional disress or negative efects on an individual‚Äôs career, reputation, fnancial position,
           safety, health or well-being)


           Reference documents and useful links
           Federal legislation and useful links
                   Access to Information Act
                   Canadian Charter of Rights and Freedoms
                   Library and Archives of Canada Act
                   Privacy Act
                   Privacy Regulations


           Policies, directives and guidance
                   Directive on Open Government


https://www.canada.ca/en/treasury-board-secretariat/services/access-information-privacy/privacy/guidance-preparing-information-sharing-agreements-involving-personal-information.html[11/6/2024 12:58:26 PM]
Guidance on Preparing Information Sharing Agreements Involving Personal Information - Canada.ca


                   Directive on Privacy Impact Assessment
                   Directive on Privacy Practices
                   Directive on the Social Insurance Number
                   Directive on Departmental Security Management
                   Guide to Integrated Risk Management
                   Policy on Access to Information
                   Policy on Privacy Protection
                   Policy on Service and Digital




           Date modifed: 2024-04-23




           Government of Canada

           All contacts                                                  Departments and agencies                                      About government




           Jobs                                                          Taxes                                                         Canada and the world

           Immigration and citizenship                                   Environment and natural resources                             Money and fnance

           Travel and tourism                                            National security and defence                                 Science and innovation

           Business                                                      Culture, hisory and sport                                     Indigenous peoples

           Benefts                                                       Policing, jusice and emergencies                              Veterans and military

           Health                                                        Transport and infrasructure                                   Youth




           Social media ‚Ä¢ Mobile applications ‚Ä¢ About Canada.ca ‚Ä¢ Terms and conditions                                 ‚Ä¢ Privacy




https://www.canada.ca/en/treasury-board-secretariat/services/access-information-privacy/privacy/guidance-preparing-information-sharing-agreements-involving-personal-information.html[11/6/2024 12:58:26 PM]
       Module 4

      DATA        Summary Modules on

PRIVACY AND
                  Litigating Digital Rights
                  and Freedom of
      DATA        Expression Online

PROTECTION
                          Module 4: Data privacy and data protection




                 Published by Media Defence: www.mediadefence.org
 This module was prepared with the assistance of ALT Advisory: https://altadvisory.africa/


                                      December 2020


     This work is licenced under the Creative Commons Attribution-NonCommercial 4.0
International License. This means that you are free to share and adapt this work so long as
you give appropriate credit, provide a link to the license, and indicate if changes were made.
 Any such sharing or adaptation must be for non-commercial purposes and must be made
      available under the same ‚Äúshare alike‚Äù terms. Full licence terms can be found at
                 https://creativecommons.org/licenses/by-ncsa/4.0/legalcode.
                                    Module 4: Data privacy and data protection


                                               TABLE OF CONTENTS


INTRODUCTION .................................................................................................................. 1
THE RIGHT TO PRIVACY .................................................................................................... 1
DATA PROTECTION............................................................................................................ 3
‚ÄòTHE RIGHT TO BE FORGOTTEN‚Äô ...................................................................................... 5
ENCRYPTION AND ANONYMITY ON THE INTERNET ....................................................... 8
GOVERNMENT-LED DIGITAL SURVEILLANCE ................................................................ 9
CONCLUSION .................................................................................................................... 12
                           Module 4: Data privacy and data protection



MODULE 4
DATA PRIVACY AND DATA PROTECTION

ÔÇ∑     The right to privacy is gaining prominence with increasing data flows and the
      concomitant need for the protection of personal information.

ÔÇ∑     In the African context, there are multiple instruments, including the AU
      Convention on Cyber Security and Personal Data Protection (Malabo Convention),
      which govern data protection.

ÔÇ∑     Importantly, states should ensure that their domestic legislation details principles
      for the lawful processing of personal information and that they keep step with
      data protection developments.

ÔÇ∑     Allied to data protection are the concepts of the ‚Äòright to be forgotten‚Äô, encryption
      and government-led surveillance.

ÔÇ∑     Notably, the disclosure of journalistic sources as a result of state surveillance has
      a negative impact on freedom of expression and journalistic freedom.




INTRODUCTION

The right to privacy and the concomitant requirement to protect personal information has
garnered significant attention with the dawn of the information age. While the internet and
online information-sharing and data collection increase at an exponential rate, legislative
developments have failed to keep pace and adequately protect personal information.
However, with time, African states and regional and continental bodies have begun to adopt
data protection-related instruments and regulations in an attempt to remedy and vindicate the
privacy rights of their citizens.

This module focuses on data protection in Africa and the related concepts of the ‚Äòright to be
forgotten‚Äô, encryption and surveillance.

THE RIGHT TO PRIVACY

There is an increasing recognition that the right to privacy plays a vital role in and of itself and
in facilitating the right to freedom of expression. For instance, reliance on the right to privacy
allows individuals to share views anonymously in circumstances where they may fear being
censured for those views, it allows whistle-blowers to make protected disclosures, and it
enables members of the media and activists to communicate securely beyond the reach of
unlawful government interception.

The right to privacy is contained in article 17 of the International Covenant on Civil and Political
Rights (ICCPR), which provides:



                                                                                                1
                             Module 4: Data privacy and data protection



        ‚Äú(1)    No one shall be subjected to arbitrary or unlawful interference with his privacy,
                family, home or correspondence, nor to unlawful attacks on his honour and
                reputation.
        (2)     Everyone has the right to the protection of the law against such interference or
                attacks.‚Äù

Although not contained in the African Charter on Human and Peoples‚Äô Rights (ACHPR), the
right to privacy of children is contained in article 10 of the African Charter on the Rights and
Welfare of the Child (ACRWC), which provides that:

       ‚ÄúNo child shall be subject to arbitrary or unlawful interference with his privacy, family home
       or correspondence, or to the attacks upon his honour or reputation, provided that parents
       or legal guardians shall have the right to exercise reasonable supervision over the
       conduct of their children. The child has the right to the protection of the law against such
       interference or attacks.‚Äù

The right to privacy has also been recognised in other regional and sub-regional instruments
in the context of data protection, which is discussed further below. Moreover, almost all
African states guarantee this right under their domestic constitutions.1

Interestingly, in 2017, the Supreme Court of India declared that the right to privacy is protected
as an intrinsic part of the right to life and personal liberty, and as part of the fundamental
freedoms guaranteed by Part III of the Constitution of India.2 As such, although the
Constitution of India does not expressly contain a right to privacy, the right can nevertheless
be read when considered in the context of the other rights and freedoms that are
constitutionally guaranteed. Although this has not been tested in the context of the ACHPR,
there is arguably scope to read the right to privacy into other provisions of the African Charter.

As with the right to freedom of expression, a limitation of the right to privacy must comply with
the three-part test for a justifiable limitation.           According to the South African
                       3
Constitutional Court:

        ‚ÄúA very high level of protection is given to the individual‚Äôs intimate personal sphere of life
        and the maintenance of its basic preconditions and there is a final untouchable sphere of
        human freedom that is beyond interference from any public authority. So much so that,
        in regard to this most intimate core of privacy, no justifiable limitation thereof can take
        place. But this most intimate core is narrowly construed. This inviolable core is left behind
        once an individual enters into relationships with persons outside this closest intimate

1 At the domestic level, more than 50 African constitutions, inclusive of amendments and recent

reviews, include reference to the right to privacy. Singh and Power, ‚ÄòThe privacy awakening: The
urgent need to harmonise the right to privacy in Africa‚Äô African Human Rights Yearbook 3 (2019) 202
at p 202, http://www.pulp.up.ac.za/images/pulp/books/journals/AHRY_2019/Power%202019.pdf.
2 Justice K.S. Puttaswamy and Another v Union of India and Others, Petition No. 494/2012, 24 August

2017 (accessible at:
http://supremecourtofindia.nic.in/supremecourt/2012/35071/35071_2012_Judgement_24-Aug-
2017.pdf).
3 NM and Others v Smith and Others, [2007] ZACC 6, 4 April 2007 at para 33 (accessible at:

https://www.saflii.org/za/cases/ZACC/2007/6.html), citing with approval Bernstein and Others v Bester
NNO and Others, [1996] ZACC 2, 27 March 1996 at para 77.




                                                                                                         2
                              Module 4: Data privacy and data protection


        sphere; the individual‚Äôs activities then acquire a social dimension and the right of privacy
        in this context becomes subject to limitation.‚Äù

Set out below, we consider specific aspects of the right to privacy and the impact that the
internet has had on the enjoyment of this right.

DATA PROTECTION

Data protection laws are aimed at protecting and safeguarding the processing of personal
information (or personal data). This refers to any information relating to an identified or
identifiable natural person ‚Äî i.e. the data subject ‚Äî by which the data subject can be
identified, directly or indirectly, in particular by reference to an identification number or to one
or more factors specific to his or her physical, physiological, mental, economic, cultural or
social identity. A data controller, which can typically be either a public or private body, refers
to the person or entity responsible for processing the personal information about the data
subject.

Data protection is one of the primary measures through which the right to privacy is given
effect. There have already been a number of African states that have enacted data protection
laws, and more that are in the process of doing so.4 In addition to giving effect to the right to
privacy, data protection legislation also has a key role to play in facilitating trade amongst
states, as many data protection laws restrict cross-border data transfers in circumstances
where the state receiving the information does not provide an adequate level of data
protection.

In relation to data protection of personal information, General Comment No. 16 on article 17
of the ICCPR (General Comment No. 16) provides as follows:5

        ‚ÄúThe gathering and holding of personal information on computers, data banks and other
        devices, whether by public authorities or private individuals or bodies, must be regulated
        by law. Effective measures have to be taken by States to ensure that information
        concerning a person‚Äôs private life does not reach the hands of persons who are not
        authorized by law to receive, process and use it, and is never used for purposes
        incompatible with the Covenant. In order to have the most effective protection of his
        private life, every individual should have the right to ascertain in an intelligible form,
        whether, and if so, what personal data is stored in automatic data files, and for what
        purposes. Every individual should also be able to ascertain which public authorities or
        private individuals or bodies control or may control their files. If such files contain incorrect
        personal data or have been collected or processed contrary to the provisions of the law,
        every individual should have the right to request rectification or elimination.‚Äù




4 At present, there are 21 states in the African Union (AU) that have enacted comprehensive privacy

laws: Angola, Benin, Burkina Faso, Cape Verde, Chad, C√¥te d‚ÄôIvoire, Equatorial Guinea, Egypt,
Gabon, Ghana, Kenya, Lesotho, Madagascar, Mali, Mauritius, Morocco, Senegal, Seychelles, South
Africa, Togolese Republic and Tunisia. There are a further four states that have shown indications of
being close to adopting legislation: Niger, Tanzania, Uganda and Zimbabwe. See
https://dataprotection.africa/ for more information.
5 General Comment No. 16 at para 10.




                                                                                                            3
                             Module 4: Data privacy and data protection


Most comprehensive data protection laws typically make provision for the following principles:6

ÔÇ∑     Personal information must be processed fairly and lawfully, and must not be processed
      unless the stipulated conditions are met.
ÔÇ∑     Personal information must be obtained for a specified purpose (or purposes), and must
      not be further processed in any manner incompatible with that purpose.
ÔÇ∑     Personal data must be adequate, relevant and not excessive in relation to the purpose
      (or purposes) for which it is processed.
ÔÇ∑     Personal information must be accurate and, where necessary, kept up to date.
ÔÇ∑     Personal information must not be kept for longer than is necessary for the purpose of
      collection.
ÔÇ∑     Personal information must be processed in accordance with the rights of data subjects
      provided for under the data protection law.
ÔÇ∑     Appropriate technical and organisational measures must be taken against unauthorised
      or unlawful processing of personal data and against accidental loss or destruction of, or
      damage to, personal data.
ÔÇ∑     Personal data must not be transferred to another country that does not ensure an
      adequate level of protection for the rights and freedoms of data subjects in relation to
      the processing of personal information.

There are a number of African regional instruments that deal with data protection:

ÔÇ∑     AU Convention on Cyber Security and Personal Data Protection 20147
      (AU Convention or ‚ÄúMalabo Convention‚Äù): This instrument, aimed at a continental level,
      includes provisions relating to data protection, e-transactions, cybercrimes and
      cybersecurity. The provisions relating to data protection are contained in Chapter II, and
      contain the conditions for the lawful processing of personal information, as well as the
      rights afforded to data subjects. Although it has not entered into force as yet, it may
      potentially in future be a binding legal instrument on data protection in Africa.
ÔÇ∑     Draft EAC Legal Framework for Cyberlaws 20088 (EAC Legal Framework): This
      instrument covers topics relating to data protection, electronic commerce, data security
      and consumer protection. It is not intended to be a model law but instead provides
      guidance and recommendations to states to assist with informing the development of
      their laws. Data protection is dealt with briefly at paragraph 2.5 of the EAC Legal
      Framework.
ÔÇ∑     Supplementary Act on Personal Data Protection within ECOWAS 2010 9
      (ECOWAS Supplementary Act): This instrument is designed to be directly transposed
      into a domestic context, and, in a similar vein to the AU Convention, provides in detail

6 Information Commissioner‚Äôs Office, ‚ÄòData protection principles‚Äô (accessible at: https://ico.org.uk/for-

organisations/guide-to-data-protection/data-protection-principles/).
7 Accessible at: https://au.int/sites/default/files/treaties/29560-treaty-0048_-

_african_union_convention_on_cyber_security_and_personal_data_protection_e.pdf. At present, it
has been ratified by one state, and signed by a further ten states.
8 Accessible at:

http://repository.eac.int:8080/bitstream/handle/11671/1815/EAC%20Framework%20for%20Cyberlaws
.pdf?sequence=1&isAllowed=y.
9 Accessible at: http://www.statewatch.org/news/2013/mar/ecowas-dp-act.pdf.




                                                                                                       4
                            Module 4: Data privacy and data protection


      for the conditions for lawful processing of personal information and the rights of data
      subjects.
ÔÇ∑     SADC Data Protection Model Law 201310 (SADC Model Law): This instrument is a
      model law that can be utilised in a national context by member states. It seeks to ensure
      the harmonisations of information and communications technologies (ICT) policies, and
      recognises that ICT developments impact the rights and protection of personal data,
      including in government and commercial activities. In addition to setting out the
      conditions for lawful processing of personal information and the rights of data subjects,
      it also deals with whistle-blowing, providing that the data protection authority must
      establish rules giving authorisation for and governing the whistleblowing system which
      preserve the data protection principles, including the principles of fairness, lawfulness,
      purpose-specification, proportionality and openness.

In addition to giving effect to the right to privacy, data protection laws also typically facilitate a
right of access to information. In this regard, most data protection laws provide for data
subjects to request, and be given access to, the information being held about them by a
controller. This mechanism can enable data subjects to ascertain whether their personal
information is being processed in accordance with the applicable data protection laws, and
whether their rights are indeed being upheld.

‚ÄòTHE RIGHT TO BE FORGOTTEN‚Äô11

The so-called ‚Äòright to be forgotten‚Äô ‚Äî which is perhaps better described as ‚Äòthe right to
erasure‚Äô or ‚Äòthe right to be de-listed‚Äô ‚Äî entails a right to request that commercial search
engines or other websites that gather personal information for profit, such as Google, should
remove links to private information when asked. The right to be forgotten progresses from the
right of data subjects contained in many data protection laws that personal information held
about a person should be erased in circumstances where it is inadequate, irrelevant or no
longer relevant, or excessive in relation to purposes for which it was collected.

In 2014, the Court of Justice of the European Union (CJEU) handed down an important ruling
in the case of Google Spain v Gonzalez.12 Mr Gonzalez, a Spanish national, lodged a
complaint in 2010 with the Spanish information regulator. The cause of Mr Gonzalez‚Äôs
complaint was that, when an internet user entered his name into Google‚Äôs search engine, the
user would obtain links to pages of the Spanish newspaper from 1998 referring to attachment
proceedings against him for the recovery of certain debts. Mr Gonzalez requested that the
personal data relating to him be removed or concealed because the proceedings against him
had been fully resolved and the reference to him was therefore now entirely irrelevant.


10 Accessible at: https://www.itu.int/en/ITU-D/Projects/ITU-EC-

ACP/HIPSSA/Documents/FINAL%20DOCUMENTS/FINAL%20DOCS%20ENGLISH/sadc_model_law
_data_protection.pdf.
11 For more on this topic see Media Defence ‚ÄúTraining Manual on Digital Rights and Freedom of

expression Online: Litigating digital rights and online freedom of expression in East, West and
Southern Africa (accessible at: https://www.mediadefence.org/wp-content/uploads/2020/06/MLDI-
Training-Manual-on-Digital-Rights-and-Freedom-of-Expression-Online.pdf).
12 Google Spain SL and Another v Agencia Espa√±ola de Protecci√≥n de Datos (AEPD) and Another,

Case No. C-131/12, 13 May 2014 (accessible at: https://eur-lex.europa.eu/legal-
content/EN/TXT/?uri=CELEX%3A62012CJ0131).




                                                                                                  5
                            Module 4: Data privacy and data protection


Before the CJEU, relying to the EU data protection law in effect at the time, the claim was
upheld. The CJEU noted that the very display of personal information on a search results
page constitutes processing of such information,13 and there was no reason why a search
engine should not be subject to the obligations and guarantees laid out under the law.14
Further, it was noted that the processing of personal information carried out by a search engine
can significantly affect the fundamental rights to privacy and to the protection of personal data
when a search is carried out of a person‚Äôs name, as it enables any internet user to obtain a
structured overview of information relating to that individual and establish a profile of the
person.15 According to the CJEU, the effect of the interference ‚Äúis heightened taking into
account the important role played by the internet and search engines in modern society, which
render the information contained in such a list of results ubiquitous.‚Äù16

With regard to de-listing, the CJEU held that the removal of links from the list of results could,
depending on the information at issue, have effects on legitimate internet users potentially
interested in having access to that information.17 This would require a fair balance to be struck
between that interest and the data subject‚Äôs fundamental rights, taking into account the nature
of the information, its sensitivity for the data subject‚Äôs private life, and the interest of the public
in having that information, which may vary according to the role played by the data subject in
public life.18

The CJEU went on to hold that a data subject is permitted to request that information about
him or her no longer be made available to the general public by its inclusion in a list of search
results where, having regard to all the circumstances, the information appears to be
inadequate, irrelevant or no longer relevant, or excessive in relation to purposes of the
processing carried out by the operator of the search engine.19 In such circumstances, the
information and links concerned in the list of results must be erased.20

The right to be forgotten has also been recognised in domestic contexts. For instance, Italy‚Äôs
Supreme Court of Cassation has held that the public interest in an article diminished after two
and a half years, and that sensitive and private information should not be available to the
public indefinitely.21 The case is currently being litigated before the European Court of Human
Rights.22 The Belgian Court of Cassation has also recognised the right to be forgotten.23


13 Id at para 57.

14 Id at para 58.

15 Id at para 80.

16 Id.

17 Id at para 81.

18 Id.

19 Id. at para 94.

20 Id. at para 94.

21 Plaintiff X v PrimaDaNoi, Case No. 13161, 22 November 2015 (accessible at:

https://globalfreedomofexpression.columbia.edu/cases/plaintiff-x-v-primadanoi/).
22 European Court of Human Rights, Application no. 77419/16 (2020) (accessible at:

https://hudoc.echr.coe.int/fre#%7B%22itemid%22:%5B%22001-201483%22%5D%7D).
23 P.H. v O.G., Case No. 15/0052/F, 29 April 2016 (accessible at:

https://www.huntonprivacyblog.com/wp-content/uploads/sites/18/2016/06/download_blob.pdf). For a
discussion of the case, see Hunton & Williams, ‚ÄòBelgian Court of Cassation rules on right to be



                                                                                                   6
                            Module 4: Data privacy and data protection



There are, however, limits to the ambit of the right to be forgotten. In 2017, the CJEU was
seized with a request for a preliminary ruling in the case of Camera di Commercio, Industria,
Artigianato e Agricoltura di Lecce v Salvatore Manni.24 Mr Manni, relying on the Gonzalez
decision, sought an order requiring the Chamber of Commerce to erase, anonymise or block
any data linking him to the liquidation of his company contained in the companies register.
The CJEU declined to uphold Mr Manni‚Äôs request, and held that in light of the range of possible
legitimate uses for data in companies registers and the different limitation periods applicable
to such records, it was impossible to identify a suitable maximum retention period.
Accordingly, the CJEU declined to find that there is a general right to be forgotten from public
company registers.

Furthermore, other jurisdictions have refused to uphold a right to be forgotten against search
engines. In Brazil, for example, it was held that search engines cannot be compelled to
remove search results relating to a specific term or expression;25 similarly, the Supreme Court
of Japan declined to enforce the right to be forgotten against Google, finding that deletion ‚Äúcan
be allowed only when the value of privacy protection significantly outweighs that of information
disclosure‚Äù.26

According to the Global Principles of Freedom of Expression and Privacy (Global Principles),27
the right ‚Äî to the extent that it is recognised in a particular jurisdiction ‚Äî should be limited to
the right of individuals under data protection law to request search engines to delist inaccurate
or out-of-date search results produced on the basis of a search for their name,28 and should
be limited in scope to the domain name corresponding to the country where the right is
recognised and the individual has established substantial damage.29 It states further that de-
listing requests should be subject to ultimate adjudication by a court or independent
adjudicatory body with relevant expertise in freedom of expression and data protection law. 30




forgotten‚Äô, 1 June 2016 (accessible at: https://www.huntonprivacyblog.com/2016/06/01/belgian-court-
of-cassation-rules-on-right-to-be-forgotten/).
For more on the right to be forgotten, see NT1 & NT2 v Google LLC in the UK (2018) (accessible at:
https://www.judiciary.uk/wp-content/uploads/2018/04/nt1-nt2-v-google-press-summary-180413.pdf).
24 Case No. C-385-15, 9 March 2017 (accessible at:

https://curia.europa.eu/juris/document/document.jsf?text=&docid=188750&pageIndex=0&doclang=EN
&mode=lst&dir=&occ=first&part=1&cid=446798).
25 Ministra Nancy Andrighi v Google Brasil Internet Ltd and Others, 2011/0307909-6, 26 June 2012

(accessible at: https://www.internetlab.org.br/wp-content/uploads/2017/02/STJ-REsp-1316921.pdf).
26 The Japan Times, ‚ÄòTop court rejects ‚Äòright to be forgotten‚Äô demand‚Äô, 1 February 2017 (accessible at:

https://www.japantimes.co.jp/news/2017/02/01/national/crime-legal/top-court-rejects-right-forgotten-
demand/#.WqZQXehubIV).
27 The Global Principles (accessible at:

https://www.article19.org/data/files/medialibrary/38657/Expression-and-Privacy-Principles-1.pdf)
were developed by civil society, led by ARTICLE19, in cooperation with high-level experts from
around the world.
28 Principle 18(1) of the Global Principles.

29 Id at principle 18(4).

30 Id at principle 18(2).




                                                                                                    7
                             Module 4: Data privacy and data protection


ENCRYPTION AND ANONYMITY ON THE INTERNET31

Encryption refers to a mathematical process of converting messages, information or data into
a form unreadable by anyone except the intended recipient, and in doing so protecting the
confidentiality and integrity of content against third party access or manipulation. 32 With a
‚Äúpublic key encryption‚Äù ‚Äî the dominant form of end-to-end security for data in transit ‚Äî the
sender uses the recipient‚Äôs public key to encrypt the message and its attachments, and the
recipient uses her or his own private key to decrypt them.33 It is also possible to encrypt data
at rest that is stored on one‚Äôs device, such as a laptop or hard drive.34

Anonymity can be defined either as acting or communicating without using or presenting one‚Äôs
name or identity, or as acting or communicating in a way that protects the determination of
one‚Äôs name or identity, or using an invented or assumed name that may not necessarily be
associated with one‚Äôs legal or customary identity.35 Anonymity may be distinguished from
pseudo-anonymity: the former refers to taking no name at all, whilst the latter refers to taking
an assumed name.36

Encryption and anonymity are necessary tools for the full enjoyment of digital rights, and enjoy
protection by virtue of the critical role that they play in securing the rights to freedom of
expression and privacy. As described by the United Nations Special Rapporteur (UNSR) on
freedom of expression:37

        ‚ÄúEncryption and anonymity, separately or together, create a zone of privacy to protect
        opinion and belief. For instance, they enable private communications and can shield an
        opinion from outside scrutiny, particularly important in hostile political, social, religious
        and legal environments. Where States impose unlawful censorship through filtering and
        other technologies, the use of encryption and anonymity may empower individuals to
        circumvent barriers and access information and ideas without the intrusion of authorities.
        Journalists, researchers, lawyers and civil society rely on encryption and anonymity to
        shield themselves (and their sources, clients and partners) from surveillance and
        harassment. The ability to search the web, develop ideas and communicate securely
        may be the only way in which many can explore basic aspects of identity, such as one‚Äôs

31 For more on this topic see Media Defence ‚ÄúTraining Manual on Digital Rights and Freedom of

expression Online: Litigating digital rights and online freedom of expression in East, West and
Southern Africa (accessible at: https://www.mediadefence.org/wp-content/uploads/2020/06/MLDI-
Training-Manual-on-Digital-Rights-and-Freedom-of-Expression-Online.pdf).
32 Report of the UNSR on Freedom of Expression, ‚ÄòReport on anonymity, encryption and the human

rights framework‚Äô, A/HRC/29/32, 22 May 2015 (UNSR Report on Anonymity and Encryption) at para 7
(accessible at: http://www.ohchr.org/EN/Issues/FreedomOpinion/Pages/CallForSubmission.aspx). For
further discussion and resources, see UCI Law International Justice Clinic, ‚ÄòSelected references:
Unofficial companion report to Report of the Special Rapporteur (A/HRC/29/32) on encryption,
anonymity and freedom of expression‚Äô (accessible at:
http://www.ohchr.org/Documents/Issues/Opinion/Communications/States/Selected_References_SR_
Report.pdf).
33 Id.

34 Id.

35 Electronic Frontier Foundation, Anonymity and encryption, 10 February 2015 at p 3 (accessible at:

https://www.ohchr.org/Documents/Issues/Opinion/Communications/EFF.pdf).
36 Id.

37 UNSR Report on Anonymity and Encryption above n 30 at para 12.




                                                                                                        8
                             Module 4: Data privacy and data protection


         gender, religion, ethnicity, national origin or sexuality. Artists rely on encryption and
         anonymity to safeguard and protect their right to expression, especially in situations
         where it is not only the State creating limitations but also society that does not tolerate
         unconventional opinions or expression.‚Äù

Encryption and anonymity are especially useful for the development and sharing of opinions
online, particularly in circumstances where persons may be concerned that their
communications may be subject to interference or attack by state or non-state actors. These
are therefore specific technologies through which individuals may exercise their rights.
Accordingly, restrictions on encryption and anonymity must meet the three-part test to justify
the restriction.

According to the UNSR on freedom of expression, while encryption and anonymity may
frustrate law enforcement and counter-terrorism officials and complicate surveillance, state
authorities have generally failed to provide appropriate public justification to support the
restriction or to identify situations where the restriction has been necessary to achieve a
legitimate goal.38 Outright prohibitions on the individual use of encryption technology
disproportionately restricts the right to freedom of expression as it deprives all online users in
a particular jurisdiction of the right to carve out a space for opinion and expression, without
any particular claim of the use of encryption being for unlawful ends.39 Likewise, state
regulation of encryption may be tantamount to a ban, for example through requiring licences
for encryption use, setting weak technical standards for encryption or controlling the import
and export of encryption tools.40

The UNSR on freedom of expression has called on states to promote strong encryption and
anonymity, and noted that decryption orders should only be permissible when it results from
transparent and publicly-accessible laws applied solely on a targeted, case-by-case basis to
individuals (not to a mass of people), and subject to a judicial warrant and the protection of
due process rights of individuals.41

GOVERNMENT-LED DIGITAL SURVEILLANCE42

Communications surveillance encompasses the monitoring, intercepting, collecting, obtaining,
analysing, using, preserving, retaining, interfering with, accessing or similar actions taken with
regard to information that includes, reflects, arises from or is about a person‚Äôs communications
in the past, present, or future.43 This relates to both the content of communications and
metadata. In respect of the latter, it has been noted that the aggregation of information ‚Äî

38 Id. at para 36.

39 Id. at para 40.

40 Id. at para 41.

41 Id. at paras 59-60.

42 For more on this topic see Media Defence ‚ÄúTraining Manual on Digital Rights and Freedom of

expression Online: Litigating digital rights and online freedom of expression in East, West and
Southern Africa (accessible at: https://www.mediadefence.org/wp-content/uploads/2020/06/MLDI-
Training-Manual-on-Digital-Rights-and-Freedom-of-Expression-Online.pdf).
43 Necessary and proportionate: International principles on the application of human rights to

communications surveillance, 2014 (Necessary and Proportionate Principles) at p 4 (accessible at:
https://necessaryandproportionate.org/files/2016/03/04/en_principles_2014.pdf).




                                                                                                       9
                            Module 4: Data privacy and data protection


commonly referred to as ‚Äòmetadata‚Äô ‚Äî may give an insight into an individual‚Äôs behaviour,
social relationships, private preferences and identity. Taken as a whole, it may allow very
precise conclusions to be drawn concerning the private life of the person.

General Comment No. 16 provides that ‚Äú[s]urveillance, whether electronic or otherwise,
interceptions of telephonic, telegraphic and other forms of communication, wire-tapping and
recording of conversations should be prohibited‚Äù.44 Surveillance ‚Äî both bulk (or mass)
collection of data45 or targeted collection of data ‚Äî interferes directly with the privacy and
security necessary for freedom of opinion and expression, and must be considered against
the three-part test to assess the permissibility of the restriction.46 In the digital age, ICTs have
enhanced the capacity of governments, corporations and individuals to conduct surveillance,
interception and data collection, and have meant that the effectiveness in conducting such
surveillance is no longer limited by scale or duration.47

In a resolution adopted by the UN General Assembly (UNGA) on the right to privacy in the
digital age, the UNGA emphasised that unlawful or arbitrary surveillance and/or interception
of communications, as well as the unlawful or arbitrary collection of personal data are highly
intrusive acts, violate the right to privacy, can interfere with the right to freedom of expression
and may contradict the tenets of a democratic society, including when undertaken on a mass
scale.48 It noted further that surveillance of digital communications must be consistent with
international human rights obligations and must be conducted on the basis of a legal
framework, which must be publicly accessible, clear, precise, comprehensive and non-
discriminatory.49

In order to meet the condition of legality, many states have taken steps to reform their
surveillance laws to allow for the powers required to conduct the surveillance activities.
According to the Necessity and Proportionate Principles, communications surveillance should
be regarded as a highly intrusive act, and in order to meet the threshold of proportionality, the
state should be required at a minimum to establish the following information to a competent
judicial authority prior to conducting any communications surveillance:50

ÔÇ∑     There is a high degree of probability that a serious crime or specific threat to a legitimate
      aim has been or will be carried out.


44 General Comment No. 16 at para 8.

45 Revelations be whistle-blowers, such as Edward Snowden, have revealed that the National

Security Agency in the USA and the General Communications Headquarters in the United Kingdom
had developed technologies allowing access to much global internet traffic, calling records in the
United States, individuals‚Äô electronic address books and huge volumes of other digital
communications content. These technologies are deployed through a transnational network
comprising strategic intelligence relationships between governments and other role-players. This is
referred to as bulk or mass surveillance. See 2016 Report of the OHCHR at para 4.
46 2016 Report of the UNSR on Freedom of Expression at para 20.

47 Report of the OHCHR at para 2.

48 UNGA, ‚ÄòResolution on the right to privacy in the digital age‚Äô, A/C.3/71/L.39/Rev.1, 16 November

2016 (2016 UN Resolution on Privacy) (accessible at:
http://www.un.org/ga/search/view_doc.asp?symbol=A/C.3/71/L.39/Rev.1).
49 Id.

50 Above at n 43, Principle 5.




                                                                                                  1
                                                                                                  0
                            Module 4: Data privacy and data protection


ÔÇ∑     There is a high degree of probability that evidence relevant and material to such a
      serious crime or specific threat to a legitimate aim would be obtained by accessing the
      protected information sought.
ÔÇ∑     Other less invasive techniques have been exhausted or would be futile, such that the
      technique used is the least invasive option.
ÔÇ∑     Information accessed will be confined to that which is relevant and material to the serious
      crime or specific threat to a legitimate aim alleged.
ÔÇ∑     Any excess information collected will not be retained, but instead will be promptly
      destroyed or returned.
ÔÇ∑     Information will be accessed only by the specified authority and used only for the
      purpose and duration for which authorisation was given.
ÔÇ∑     The surveillance activities requested and techniques proposed do not undermine the
      essence of the right to privacy or of fundamental freedoms.

Surveillance constitutes an obvious interference with the right to privacy. Further, it also
constitutes an interference on the right to hold opinions without interference and the right to
freedom of expression. With particular reference to the right to hold opinions without
interference, surveillance systems, both targeted and mass, may undermine the right to form
an opinion, as the fear of unwilling disclosure of online activity, such as search and browsing,
likely deters individuals from accessing information, particularly where such surveillance leads
to repressive outcomes.51

The interference with the right to freedom of expression is particularly apparent in the context
of journalists and members of the media who may be placed under surveillance as a result of
their journalistic activities. As noted by the Secretary-General of the UN, this can have a
chilling effect on the enjoyment of media freedom, and renders it more difficult to communicate
with sources and share and develop ideas, which may lead to self-censorship.52 The use of
encryption and other similar tools have become essential to the work of journalists to ensure
that they are able to conduct their work without interference.

The disclosure of journalistic sources and surveillance can have negative consequences for
the right to freedom of expression due to a breach of an individual‚Äôs confidentiality in their
communications.53 This is the same for cases concerning the disclosure of anonymous user
data. Once confidentiality is undermined, it cannot be restored. It is, therefore, of utmost
importance that measures that undermine confidentiality are not taken arbitrarily.

The importance of source protection has been well-established. For example, in Bosasa
Operation (Pty) Ltd v Basson and Another, the South Africa High Court held that journalists


51 UNSR Report on Anonymity and Encryption at para 21.

52 Report of the Secretary-General on the UN to the UNGA, ‚ÄòReport on the safety of journalists and

the issue of impunity‚Äô, A/70/290, 6 August 2015 (2015 Report of the UN Secretary-General) at paras
14-16 (accessible at: https://documents-dds-
ny.un.org/doc/UNDOC/GEN/N15/247/06/PDF/N1524706.pdf?OpenElement).
53 For more, see Big Brother Watch v United Kingdom in the ECtHR (2018) (accessible at:

https://globalfreedomofexpression.columbia.edu/cases/big-brother-watch-v-united-kingdom/) and
amaBhungane Centre for Investigative Journalism v Minister of Justice in South Africa (2019)
(accessible at: http://www.saflii.org/za/cases/ZAGPPHC/2019/384.html).



                                                                                                     1
                                                                                                     1
                             Module 4: Data privacy and data protection


are not required to reveal their sources, subject to certain exceptions.54 The court stated in
this regard that:

        ‚ÄúIf indeed freedom of the press is fundamental and sine qua non for democracy, it is essential
        that in carrying out this public duty for the public good, the identity of their sources should not
        be revealed, particularly, when the information so revealed, would not have been publicly
        known. This essential and critical role of the media, which is more pronounced in our nascent
        democracy, founded on openness, where corruption has become cancerous, needs to be
        fostered rather than denuded.‚Äù55

Surveillance activities carried out against journalists have the risk of fundamentally
undermining the source protection to which journalists are otherwise entitled.56

CONCLUSION

As more of the world moves online, data protection is becoming increasingly necessary. In
an African context, some headway has been made with 21 African states having privacy laws
in place. However, with the rapid growth in data harvesting, legislators are some way behind
in fully protecting and promoting data privacy and data protection. As we move forward, digital
rights activists have a significant role to play in ensuring that states keep step with data
protection developments and enact legislative frameworks that fully protect and promote
peoples‚Äô rights to privacy.




54 [2012] ZAGPJHC 71, 26 April 2012 (accessible at:

http://www.saflii.org/za/cases/ZAGPJHC/2012/71.html).
55 Id. at para 38.

56 According to principle 9 of the Global Principles, states should provide for the protection of the

confidentiality of sources in their legislation and ensure that:
    ÔÇ∑   Any restriction on the right to protection of sources complies with the three-part test under
        international human rights law.
    ÔÇ∑   The confidentiality of sources should only be lifted in exceptional circumstances and only by a
        court order, which complies with the requirements of a legitimate aim, necessity, and
        proportionality. The same protections should apply to access to journalistic material.
    ÔÇ∑   The right not to disclose the identity of sources and the protection of journalistic material
        requires that the privacy and security of the communications of anyone engaged in journalistic
        activity, including access to their communications data and metadata, must be protected.
        Circumventions, such as secret surveillance or analysis of communications data not
        authorised by judicial authorities according to clear and narrow legal rules, must not be used
        to undermine source confidentiality.
    ÔÇ∑   Any court order must only be granted after a fair hearing where sufficient notice has been
        given to the journalist in question, except in genuine emergencies.




                                                                                                        1
                                                                                                        2
                                                                                                      MARCH 2021




                                                                                            GUIDELINES FOR THE
Reporting a Privacy Breach to the IPC                                                           HEALTH SECTOR
If you are a health information custodian under
Ontario‚Äôs health privacy law, the Personal Health
Information Protection Act (PHIPA) and you experience
a privacy breach, you may be required to notify the
Information and Privacy Commissioner of Ontario (IPC).
This guidance explains what types of breaches must be
reported to the IPC.
Custodians are only required to notify the IPC if the breach falls into the
categories explained below.1
The categories are not mutually exclusive; more than one can apply to a
single incident. You must report the breach to the IPC at the first
reasonable opportunity if at least one of the situations applies. These
categories are set out in the regulation, and you can find the complete
wording in the appendix of this document..


It‚Äôs important to remember that even if you don‚Äôt need to report
the breach to the IPC, you have a duty to notify individuals whose
privacy has been breached. You must also count every breach in
your annual statistics report to the IPC.




1   A coroner to whom Ontario Health provides personal health information under
    subsection 55.9.1 (1) of PHIPA must, with respect to that information, comply with a
    number of obligations as if the coroner were a custodian, including the obligation to
    notify the IPC of the breaches described here.
SITUATIONS WHERE YOU MUST NOTIFY THE IPC
1. Use or disclosure without authority
There may be situations where you or another person uses or discloses
personal health information in your custody or control without authority.
You must report such breaches to the IPC where the person committing
the breach either knew or should have known that their actions were not
permitted under the law. That person could be your employee, a health
care practitioner with privileges, a third party (such as a service provider),
or even someone with no relationship to you.
One example is where an employee looks at the personal health
information of their neighbour, a friend‚Äôs child, or a celebrity, for a non-
work related purpose. This is called ‚Äúsnooping.‚Äù
Whether done maliciously or out of curiosity or even concern, snooping is
a type of unauthorized use of information. Regardless of the motive, you
must report this type of breach to the IPC.
By contrast, you generally do not need to notify the IPC when the breach
is accidental, for example, if information is inadvertently sent by email or
courier to the wrong person, or a letter is placed in the wrong envelope.
Also, you do not need to notify the IPC when a person who is permitted
to access patient information accidentally accesses the wrong patient
record. However, you must report even accidental privacy breaches if
they fall into one of the other categories below.

2. Stolen information
If you believe personal health information was stolen you must report it to
the IPC.
A typical example would be where someone has stolen paper records, a
laptop, a USB drive, or other electronic device. Another example would
be where personal health information is subject to a ransomware or other
malware attack. You must report these types of breaches to the IPC.
You do not need to notify the IPC if the stolen information was de-
identified or encrypted.

3. Further use or disclosure without authority after a breach
Following an initial privacy breach, you may become aware that the
information was or will be further used or disclosed without authority. If
this is the case, you must report it to the IPC.
For example, an employee accidentally sends a letter containing a
patient‚Äôs personal health information to the wrong person. Although the
person returned the letter to you, you learn that they kept a copy and are
threatening to make the information public. Even if you did not report the
initial incident, you must notify the IPC of this situation.




       TECHNOLOGY
REPORTING A PRIVACY FACT SHEET:
                    BREACH      PROTECTING
                            TO THE          AGAINST
                                   IPC: GUIDELINES   RANSOMWARE
                                                   FOR THE HEALTH SECTOR         2   2
Another example is where you learn an employee wrongfully accessed a
patient‚Äôs personal health information and subsequently used this
information to market products or services or to commit fraud (for
example, health care or insurance fraud). You would also need to report
this breach.

4. Pattern of similar breaches
Even if a privacy breach is accidental or insignificant, you must report it
to the IPC if it is part of a pattern of similar breaches. Such a pattern may
reflect systemic issues that need to be addressed, such as inadequate
training or procedures.
For example, you discover that a letter to a patient inadvertently included
information relating to a different patient. Over a few months, the same
mistake is repeated several times because an automated process for
generating letters has been malfunctioning for some time. You should
report this to the IPC.
Use your judgment in deciding if a privacy breach is an isolated incident
or part of a pattern. Consider, for instance, the time between the
breaches and their similarities. Keeping track of privacy breaches in a
uniform way can help you identify patterns.

5. Disciplinary action against a college member
If you are required to report an employee or another agent to a health
regulatory college because of a breach, you must also report the breach
to the IPC.
Where the agent is a member of a college, you must notify the IPC of a
privacy breach if:
    ‚Ä¢   you terminate, suspend or discipline them as a result of the breach
    ‚Ä¢   they resign, and you believe this action is related to the breach
Where a health care practitioner with privileges or otherwise affiliated
with you is a member of a college, you must notify the IPC of a privacy
breach if:
    ‚Ä¢   you revoke, suspend or restrict their privileges or affiliation as a
        result of the breach
    ‚Ä¢   they give up or voluntarily restrict their privileges or affiliation with
        you, and you believe this action is related to the breach
For example, a doctor reveals on social media that a well-known
individual is receiving services from your hospital. You formally discipline
the doctor by placing a written reprimand in their personnel file. Or, an
employee resigns, and you suspect the resignation relates to their recent
breach of patient privacy. You should report these breaches to the IPC.
Similar requirements apply to health care practitioners who are employed
to provide health care for a board of health.




       TECHNOLOGY
REPORTING A PRIVACY FACT SHEET:
                    BREACH      PROTECTING
                            TO THE          AGAINST
                                   IPC: GUIDELINES   RANSOMWARE
                                                   FOR THE HEALTH SECTOR            3   3
6. Disciplinary action against a non-college member
Not all agents of a custodian are members of a college. If an agent is not
a member, you must still notify the IPC in the same circumstances that
would have triggered notification to a college.
For example, a registration clerk has an unpleasant encounter with a
patient and posts information about the patient on social media. You
suspend the clerk for a month. Although the clerk is not a member of a
college, you must still report this privacy breach.

7. Significant breach
Even if none of the above six circumstances apply, you must notify the
IPC if the privacy breach is significant. To decide whether a breach is
significant, you must consider all the relevant circumstances, including
whether:
    ‚Ä¢   the information is sensitive
    ‚Ä¢   the breach involves a large volume of information
    ‚Ä¢   the breach involves many individuals‚Äô information
    ‚Ä¢   more than one custodian or agent was responsible for the breach
For example, you are a health care practitioner and you accidentally
disclose a patient‚Äôs mental health assessment to other practitioners
through a group email, rather than to just the patient‚Äôs physician. This
information is highly sensitive and you disclosed it to a number of
persons to whom you did not intend to send the information. Or, you post
detailed information on a website about a group of patients receiving
specialized treatment for an unusual health issue. It comes to your
attention that while you did not use any patients‚Äô names, others can
easily identify them. This breach involves many patients, whose
information has potentially been made widely available. You should report
these types of breaches to the IPC.
Note that even breaches that cause no particular harm may still be
significant. For example, the recipients of a misdirected group email that
contains a patient‚Äôs mental health assessment may, upon realizing the
mistake, delete the email and successfully contain the breach. Containing
the breach might minimize or eliminate the potential for harm to the
patient, but the breach may still be significant in that it reveals an
underlying problem in your information policies and practices.


UNAUTHORIZED COLLECTION BY MEANS OF THE EHR
Custodians may collect, use, and disclose personal health information by
means of the electronic health record (EHR) according to the rules set out
in Part V.1 of PHIPA.
In the EHR context, custodians must comply with the breach notification
requirements set out elsewhere in PHIPA, as well as an additional



       TECHNOLOGY
REPORTING A PRIVACY FACT SHEET:
                    BREACH      PROTECTING
                            TO THE          AGAINST
                                   IPC: GUIDELINES   RANSOMWARE
                                                   FOR THE HEALTH SECTOR     4   4
requirement: if personal health information is collected without authority
by means of the EHR, the custodian responsible for the collection must,
in certain circumstances, notify the IPC. That is, if the unauthorized
collection by means of the EHR had been a use or disclosure under any
of the seven circumstances described in this document, the custodian
must notify the IPC at the first reasonable opportunity.
For example, one of the circumstances described in this document is that
the use or disclosure is part of a pattern. This means that the custodian
must notify the IPC if an unauthorized collection by means of the EHR is
part of a pattern.


HOW TO REPORT A BREACH TO THE IPC
You must submit the report at the first reasonable opportunity, either by
mail or online at www.ipc.on.ca.
You will need to describe:
    ‚Ä¢   the circumstances of the breach (for example, how the information
        came to be stolen, lost, or disclosed without authority, how many
        individuals were affected, how the breach was discovered)
    ‚Ä¢   whether and how you notified the affected individuals
    ‚Ä¢   the nature of the personal health information that was stolen, lost,
        used or disclosed without authority, or collected by means of the
        EHR without authority
    ‚Ä¢   the steps you took to contain, investigate, and remediate the
        breach and prevent future breaches (with the understanding some
        of this work may still be ongoing)
The IPC will review the information you provide and may request
additional information. In some cases, the IPC may decide to conduct an
investigation, while in other cases no further action will be taken.


ANNUAL STATISTICS
PHIPA requires custodians to submit statistics on the number of privacy
breaches each year, including all thefts, losses, unauthorized uses or
disclosures, or unauthorized collections by means of the EHR of personal
health information.2
Note that these statistics include privacy breaches that did not meet the
threshold for reporting the breach to the IPC. An accidental privacy
breach that is isolated and limited in scope ‚Äî misdirected
correspondence, for example ‚Äî may not have been reported to the IPC
when it happened, but should still be counted for annual statistical
2   A coroner to whom Ontario Health provides personal health information under
    subsection 55.9.1 (1) of PHIPA must, in respect of that information, comply with this
    annual statistics requirement, with any necessary modification, as if the coroner were a
    custodian.



       TECHNOLOGY
REPORTING A PRIVACY FACT SHEET:
                    BREACH      PROTECTING
                            TO THE          AGAINST
                                   IPC: GUIDELINES   RANSOMWARE
                                                   FOR THE HEALTH SECTOR                       5   5
reporting. For more information about submitting annual statistics, please
see Annual Reporting of Privacy Breach Statistics to the
Commissioner available on the IPC‚Äôs website.
You should have a system in place to record all privacy breaches. This will
help you track ongoing issues, patterns, and changes, and help you meet
your annual statistics reporting obligations.




       TECHNOLOGY
REPORTING A PRIVACY FACT SHEET:
                    BREACH      PROTECTING
                            TO THE          AGAINST
                                   IPC: GUIDELINES   RANSOMWARE
                                                   FOR THE HEALTH SECTOR      6   6
APPENDIX
Excerpts from Ontario Regulation 329/04 under PHIPA

Section 6.3

(1) The following are the circumstances in which a health information
custodian is required to notify the Commissioner for the purposes of
subsection 12 (3) of the Act:
1. The health information custodian has reasonable grounds to believe
   that personal health information in the custodian‚Äôs custody or control
   was used or disclosed without authority by a person who knew or
   ought to have known that they were using or disclosing the
   information without authority.
2. The health information custodian has reasonable grounds to believe
   that personal health information in the custodian‚Äôs custody or control
   was stolen.
3. The health information custodian has reasonable grounds to believe
   that, after an initial loss or unauthorized use or disclosure of personal
   health information in the custodian‚Äôs custody or control, the personal
   health information was or will be further used or disclosed without
   authority.
4. The loss or unauthorized use or disclosure of personal health
   information is part of a pattern of similar losses or unauthorized uses
   or disclosures of personal health information in the custody or control
   of the health information custodian.
5. The health information custodian is required to give notice to a
   College of an event described in section 17.1 of the Act that relates to
   a loss or unauthorized use or disclosure of personal health
   information.
6. The health information custodian would be required to give notice to a
   College, if an agent of the health information custodian were a
   member of the College, of an event described in section 17.1 of the
   Act that relates to a loss or unauthorized use or disclosure of personal
   health information.
7. The health information custodian determines that the loss or
   unauthorized use or disclosure of personal health information is
   significant after considering all relevant circumstances, including the
   following:
     i.   Whether the personal health information that was lost or used or
          disclosed without authority is sensitive.
     ii. Whether the loss or unauthorized use or disclosure involved a
         large volume of personal health information.
     iii. Whether the loss or unauthorized use or disclosure involved many
          individuals‚Äô personal health information.



       TECHNOLOGY
REPORTING A PRIVACY FACT SHEET:
                    BREACH      PROTECTING
                            TO THE          AGAINST
                                   IPC: GUIDELINES   RANSOMWARE
                                                   FOR THE HEALTH SECTOR       7   7
     iv. Whether more than one health information custodian or agent was
         responsible for the loss or unauthorized use or disclosure of the
         personal health information.
(2) In this section,
         ‚ÄúCollege‚Äù means a College as defined in subsection 17.1 (1) of the
         Act.
(3) A health information custodian shall notify the Commissioner of the
existence of a circumstance set out in subsection (1) at the first
reasonable opportunity.

Section 18.3

(1) A health information custodian is required to notify the Commissioner
for the purposes of clause 55.5 (7) (b) of the Act under any circumstance
where the custodian would be required to notify the Commissioner if the
collection by means of the electronic health record had been for a use or
disclosure to which section 6.3 of this Regulation applied.
(2) The health information custodian shall inform the Commissioner of an
unauthorized collection to which subsection (1) applies at the first
reasonable opportunity.

Subsection 18.10 (4)

If personal health information about an individual is collected without
authority by a coroner by means of the electronic health record, the
coroner shall,
‚Ä¶
(b) notify the Commissioner of the unauthorized collection at the first
reasonable opportunity, if any circumstance exists where the coroner
would be required to notify the Commissioner if the coroner were a
custodian to which subsection 18.3 (1) of this Regulation applied.




       TECHNOLOGY
REPORTING A PRIVACY FACT SHEET:
                    BREACH      PROTECTING
                            TO THE          AGAINST
                                   IPC: GUIDELINES   RANSOMWARE
                                                   FOR THE HEALTH SECTOR      8   8
        ‚Ä¢




Privacy and EHR Information
Flows in Canada
Common understandings of
the Pan-Canadian Health Information Privacy Group




                              Pan-Canadian
                              Health Information
                              Privacy Group




June 30, 2010
Acknowlegements
This document is the result of the dedicated efforts of the
members of the Health Information Privacy (HIP) Group, the
support of Canada Health Infoway and the contributions of
subject matter experts whose presentations, research, feedback
and other input have enriched this paper. A list of HIP Group
members is included in Appendix D.




                               Privacy and EHR Information Flows in Canada   2
Table of Contents
Acknowlegements                                                                                 2	
 ¬†
Executive Summary                                                                               5	
 ¬†
Introduction and Background                                                                     7	
 ¬†
Notes on Terminology                                                                          10	
 ¬†

Section 1: Pan-Canadian common understandings to support
trans-jurisdictional disclosures of EHR information within Canada                              11	
 ¬†
A) Foundational Principles                                                                    13	
 ¬†
Support for appropriate and privacy-protective trans-jurisdictional
   disclosures within Canada                                                                  13	
 ¬†
B) Trans-jurisdictional disclosure principles                                                 15	
 ¬†
Disclosure principles                                                                         15	
 ¬†
Identity management                                                                           15	
 ¬†
Respecting patient/individuals‚Äô wishes                                                        16	
 ¬†
Patient information/notices about EHRs                                                        17	
 ¬†

Section 2: Trans-jurisdictional disclosures of EHR information for
secondary uses                                                                                19	
 ¬†
Context                                                                                       20	
 ¬†
Scope and terminology                                                                         21	
 ¬†
De-identification of personal health information                                              22	
 ¬†
Review and assessment processes in trans-jurisdictional disclosure
   requests                                                                                   24	
 ¬†
Patient notification respecting trans-jurisdictional disclosures for
   secondary uses                                                                             26	
 ¬†
Governance of trans-jurisdictional disclosures for secondary uses                             26	
 ¬†

Section 3: Accountability for information governance in the EHR                               28	
 ¬†
Accountability at the jurisdictional level                                                    29	
 ¬†
Accountability at the organizational level                                                    31	
 ¬†
Accountability and trans-jurisdictional disclosures of personal
   health information                                                                         32	
 ¬†




                                                Privacy and EHR Information Flows in Canada      3
Appendices
Appendix A: Summary list of Common Understandings

Appendix B: Examples of Jurisdictional EHR Governance Models

Appendix C: Potential Options for the Structure and Roles of a
Pan-Canadian Body for Privacy and Related Information Governance

Appendix D: List of HIP Group Members, 2008-2010

Appendix E: List of presentations


Background Papers
MacPherson, Don and Ross Fraser, Jurisdictional Scan of Patient Notices,
November 2008
http://www2.infoway-
inforoute.ca/Documents/Jurisdictional_Scan_of_Patient_Notices_EN_FINAL.pdf

Fraser, Ross and Don Willison, Tools for De-Identification of Personal Health Information,
September 2009
http://www2.infoway-inforoute.ca/Documents/Tools_for_De-identification_EN_FINAL.pdf

El Emam, Khaled, Practices for the Review of Data Requests and the Disclosure of Health
Information by Health Ministries and Large Data Custodians, June 2010
http://www2.infoway-
inforoute.ca/Documents/Practices_for_the_review_of_data_requests_June_2010_EN_FINAL.pdf

Sawatsky, Elaine, Information Sharing Agreements for Disclosure of EHR Data
within Canada, January 2010
http://www2.infoway-
inforoute.ca/Documents/ISA_report_for_HIP_Group_January_2010_EN_Final.pdf


Other References
White Paper on Information Governance of the Interoperable Electronic Health Record:
http://www2.infoway-
inforoute.ca/Documents/Information%20Governance%20Paper%20Final_20070328_EN.pdf

Conceptual Privacy Impact Assessment of Canada‚Äôs
Electronic Health Record Solution Blueprint Version 2:
http://www2.infoway-inforoute.ca/Documents/CHI_625_PIA_rj13.pdf

CSA Model Code for the Protection of Personal Information:
http://www.csa.ca/cm?c=Page&childpagename=CSA%2FLayout&cid=1239124810319&pagename=
CSA%2FRenderPage

Pan-Canadian Health Information Privacy and Confidentiality Framework:
http://www.hc-sc.gc.ca/hcs-sss/pubs/ehealth-esante/2005-pancanad-priv/index-eng.php




                                             Privacy and EHR Information Flows in Canada     4
Executive Summary
Health information is currently disclosed from a trustee or custodian in one jurisdiction to a
trustee or custodian in another jurisdiction in Canada for care and treatment and for secondary
uses. It is important to jurisdictions that such trans-jurisdictional disclosures and collections
continue to be supported in the new interoperable electronic health record (iEHR) environment
in a privacy-protective manner.

The White Paper on Information Governance of the Interoperable Electronic Health Record and
the Conceptual Privacy Impact Assessment of Canada‚Äôs Electronic Health Record Solution
Blueprint Version 2 identified privacy-related information governance issues that were non-
technical in nature but were needed to support the flow of EHR information from one
jurisdiction to another. At its 2007 Annual General Meeting, Canada Health Infoway‚Äôs members
(the Deputy Ministers of Health of Canada‚Äôs fourteen jurisdictions) asked for assistance in
addressing these common issues.

As a result, the Pan-Canadian Privacy Forum on EHR Information Governance was established
in November, 2007, to share information and approaches. The Privacy Forum is comprised of
one representative from the office of each jurisdiction‚Äôs Privacy Commissioner or Ombudsman
and one from each Ministry of Health across Canada.

Subsequently, the Pan-Canadian Health Information Privacy (HIP) Group was formed in
December 2008, composed of the Ministry representatives of the Privacy Forum, to further the
work and thinking on these issues.

As its first contribution, the HIP Group has put forward 33 ‚Äòcommon understandings‚Äô to support
appropriate and privacy protective trans-jurisdictional disclosures of EHR information for care
and treatment and for secondary uses. The common understandings represent principles that the
HIP Group believes should be adopted consistently across jurisdictions.

The common understandings reflect the general consensus of the HIP Group members in that
most members generally agreed with the statements. Those members not in complete agreement
with a particular common understanding (sometimes because of their jurisdiction‚Äôs legislative
environment) did not actively oppose it. Quebec participated in and contributed to the HIP
Group: however, the differences in its legislative framework and EHR approach precluded it
from being able to support all of the common understandings.

There is no intention to bind jurisdictions; rather, the paper emphasizes jurisdictional
responsibility for decisions in these areas. The common understandings can, however,
be valuable in promoting consistency and informing jurisdiction work on health information
privacy legislation, associated health information or ehealth policies, information sharing
agreements and business/technical requirements for EHR systems.




                                               Privacy and EHR Information Flows in Canada          5
The 33 common understandings (the full list can be found in Appendix A) encompass:
   ‚Ä¢ Foundational understandings: which set the stage for appropriate, privacy protective
     trans-jurisdictional disclosures of personal health information in a multi-jurisdictional
     EHR context.
   ‚Ä¢ Understandings related to trans-jurisdictional disclosure and collection of EHR information
     within Canada: which set out some basic principles for EHR information flows across
     jurisdictions (e.g., once information has been disclosed to another jurisdiction the
     information becomes subject to the receiving jurisdiction‚Äôs data protection laws.)
   ‚Ä¢ Understandings related to patient control of their personal health information in the EHR
     and patient notices about EHRs: which set out principles for handling information that
     patients have chosen to ‚Äòmask‚Äô, as well as some key messages for patient notification tools.
   ‚Ä¢ Understandings related to trans-jurisdictional disclosures of EHR information for
     secondary uses: which address topics such as the use of de-identified information; the need
     for privacy risk assessments; patient information about trans-jurisdictional disclosures for
     secondary uses; and governance of trans-jurisdictional disclosures for secondary use.
   ‚Ä¢ Understandings related to accountability for information governance of the iEHR: which
     speak to the importance of jurisdictional EHR governance structures including a privacy
     and information governance component and being accountable to a Minister.

The final two common understandings relate to the need for a single integrated pan-Canadian group
to discuss, address and coordinate common privacy related information governance issues.
The current HIP Group includes the core representation and activities of such a structure, but
there is a need for discussion about the potential evolution of the HIP Group‚Äôs mandate and
composition. A range of alternatives is provided for consideration.




                                                Privacy and EHR Information Flows in Canada         6
Introduction and Background
Jurisdictions are at various points in developing and implementing the information systems that
make up their EHR network. They must manage their internal EHR priorities and expectations
within budget and other resource restraints and are also working to ensure that appropriate
supporting legislation and/or policies are in place. These are massive undertakings requiring an
internal, jurisdictional focus.

The vision of the interoperable EHR (iEHR), however, involves information being available
where and when a patient is treated or follow-up care provided. While most health care activity
occurs within a patient‚Äôs home jurisdiction, patient care can take place in a jurisdiction other than
the patient‚Äôs home jurisdiction. Examples include emergency care, some specialized care, specific
services such as the reading of diagnostic images, and regular care for residents of many rural,
remote, northern and border communities. Patient information is currently disclosed by a trustee
or custodian in one jurisdiction and collected by a trustee or custodian in another for
these purposes.

Health information is also used for many purposes other than care and treatment, such as
managing the health system, processing claims, improving health care and patient safety,
expanding knowledge about illness and disease, strengthening the effectiveness and efficiency of
health care delivery, and supporting public health initiatives. Information currently flows within
jurisdictions and in some cases to other jurisdictions for these purposes as well.

It is important to jurisdictions that all of the above trans-jurisdictional disclosures and collections
continue to be supported in the new EHR environment and that they be accomplished in a
coordinated and privacy-protective manner.

As work on the iEHR has progressed, a number of privacy related information governance issues1
have been identified, notably in the following:
   ‚Ä¢ The 2007 White Paper on Information Governance of the Interoperable Electronic Health Record
      identified a number of issues around trust and accountability, the privacy rights of patients
      and other topics for policy makers to consider in developing policies and related non-
      technical measures to support the interoperability of the iEHR.
   ‚Ä¢ The 2008 Conceptual Privacy Impact Assessment of Canada‚Äôs Electronic Health Record Solution
      Blueprint Version 2 concluded that the Blueprint strongly supported patient privacy and that
      properly implemented, EHR Infostructure initiatives underway presented an unprecedented
      opportunity to bolster privacy. The report also made observations on policy-related
      information governance issues.




1
  The term ‚Äúinformation governance‚Äù in this paper refers to the rules, requirements and mechanisms involved in
managing personal health information in the EHR as it relates to privacy, although there may be some overlap with
other areas, e.g., data governance, system governance and corporate governance.



                                                        Privacy and EHR Information Flows in Canada                 7
At Infoway‚Äôs 2007 Annual General Meeting, its members (the Deputy Ministers of Health of
Canada‚Äôs fourteen jurisdictions) recognizing that work needed to get underway on trans-
jurisdictional issues in preparation for the future, asked for assistance in addressing the issues that
had been raised. As a result, the Pan-Canadian Privacy Forum on EHR Information Governance
was established in November, 2007, to share information and approaches.2 Subsequently, the
Pan-Canadian Health Information Privacy (HIP) Group, composed of the Ministry
representatives of the Privacy Forum, was established in December 2008 to further the work and
thinking on these issues and to share that knowledge.

As its first contribution, the HIP group has developed a series of common understandings on a
number of topics related to the trans-jurisdictional disclosure of information from the EHR.
These understandings, which are set out in this paper, are a mix of high level and more
prescriptive principles that the HIP Group believes should be adopted consistently across
jurisdictions to support trans-jurisdictional disclosures of personal information in a manner that is
respectful of privacy and the differing approaches adopted by the jurisdictions. It is important to
note that:
     ‚Ä¢ The common understandings were considered within the context of current jurisdictional
       legislation, the principles of the Canadian Standards Association Model Code for the Protection
       of Personal Information and the Pan-Canadian Personal Health Information Privacy and
       Confidentiality Framework.3
     ‚Ä¢ The common understandings reflect the general consensus of the HIP Group members.
       For the purpose of this work, this means that most members generally agreed with the
       statements and that those members not in complete agreement with a particular common
       understanding (sometimes because of their legislative environment) did not actively oppose
       it. Quebec participated in and contributed to the HIP Group: however, the differences in
       its legislative framework and EHR approach precluded it from being able to support all of
       the common understandings.
     ‚Ä¢ Although the focus is on trans-jurisdictional disclosures of EHR information within
       Canada, some of the common understandings speak to the jurisdictional level. In part this
       is because jurisdictions recognize the desirability of consistent practices at the jurisdictional
       level, and in part, it is because trust in each other‚Äôs information handling practices is
       required for jurisdictions to be comfortable making trans-jurisdictional disclosures.
     ‚Ä¢ The HIP Group recognizes that EHR privacy issues go hand-in-hand with security and
       technology issues, and that several of the common understandings have technical and
       security implications. This paper, however, is focused on the privacy aspects of trans-
       jurisdictional disclosures.


2
  The Privacy Forum includes representatives from each federal/provincial/territorial Ministry of Health and Privacy
Oversight body. It provides a collegial setting for jurisdictions to share knowledge and experiences, and leverage their
collective wisdom to facilitate the development of common solutions that can be considered by jurisdictions when
making policy choices.
3
  The Framework was created for the Advisory Committee on Information and Emerging Technologies and endorsed by
the Federal/Provincial/Territorial Conference of Deputy Ministers of Health. Saskatchewan and Quebec did not endorse
the framework document. The Framework can be found at: http://www.hc-sc.gc.ca/hcs-sss/pubs/ehealth-
esante/2005-pancanad-priv/index-eng.php




                                                          Privacy and EHR Information Flows in Canada                  8
    ‚Ä¢ There is no intention to bind jurisdictions. The authors of this paper understand fully that
      jurisdictions are responsible for the laws, policies and systems developed and implemented
      within their boundaries and that within their laws, jurisdictions will determine how to
      interface with another jurisdiction. The common understandings are meant only to
      facilitate inter-jurisdictional thinking on these topics and promote consistency in approach,
      thereby facilitating the controlled and appropriate disclosure of personal health information
      across jurisdictions in authorized circumstances. The participation of ministry officials in
      the development of the common understandings is in no way to be interpreted as binding
      the jurisdictions to these positions.
    ‚Ä¢ The paper does not address every information governance issue that has been raised, or
      every aspect of an issue.

An earlier draft of this paper was shared for informal comment by HIP Group members
internally within their jurisdictions, internally within Canada Health Infoway and the Canadian
Institute for Health Information, and with the pan-Canadian Privacy Forum.

Purpose and structure of this paper
This paper is meant to serve as a vehicle for sharing and discussing with other stakeholders
(e.g., clinicians, policy makers, system designers) the HIP Group‚Äôs common understandings.
It is structured as follows:

Section 1
focuses on trans-jurisdictional disclosures of personal health information from the EHR within
Canada, as well as patient control of their personal health information in the EHR and patient
notices about EHRs

Section 2
focuses specifically on trans-jurisdictional disclosures of EHR information for secondary uses

Section 3
looks at accountability for information governance of the iEHR.




                                               Privacy and EHR Information Flows in Canada        9
Notes on Terminology
Different jurisdictions use different terms for EHR-related activities in legislation and in the field.
Some of these terms, such as ‚Äòcustodian‚Äô and ‚Äòtrustee‚Äô, are defined in a jurisdiction‚Äôs legislation.
Other commonly-used terms are not set out in legislation but are descriptive ‚Äì such as
‚Äòinformation flows‚Äô, ‚Äòsharing‚Äô and ‚Äòviewing‚Äô.

Jurisdictions will of course continue to use terms as defined in their own legislation and practice.
However, the HIP Group has agreed to use common definitions for certain terms in order to
facilitate their discussions and they are used throughout the paper as described below.

       ‚Ä¢ Disclosure and indirect collection: Information that ‚Äòflows‚Äô from, is ‚Äòshared‚Äô by or ‚Äòmade
         available‚Äô by a custodian or trustee in one jurisdiction for ‚Äòviewing‚Äô by an authorized health
         care provider or organization in another jurisdiction constitutes a disclosure of that
         information by the first jurisdiction and an indirect collection by the second jurisdiction.4
       ‚Ä¢ Access: ‚ÄòAccess‚Äô is often defined in jurisdictional legislation to refer to a person‚Äôs ability to
         view or receive copies of their own information. The term can also refer to activities under
         various access to information/freedom of information statutes. In other contexts,
         including the iEHR context, it often refers to any action that involves an authorized
         individual being able to view, use, or modify a record. If the term ‚Äòaccess‚Äô is used with no
         qualifier, it refers to the third sense of the word. The paper uses qualifiers, e.g., ‚Äúpatient
         access to his or her information‚Äù, or ‚Äúaccess under access to information legislation‚Äù to
         refer to the other senses of the term.
       ‚Ä¢ Masking: Jurisdictions use various terms ‚Äì e.g., ‚Äòconsent directives‚Äô, ‚Äòdisclosure directives‚Äô,
         ‚Äòexpressed wishes‚Äô ‚Äì in legislation or policy, to describe how a patient can exercise a
         measure of control to restrict access to, use and/or disclosure of his or her personal
         information. Masking is the function used to operationalize this principle of patient
         control.5 (Note: while recognizing that Ontario uses the term ‚Äòlock‚Äô or ‚Äòlock box‚Äô to
         describe this activity, the HIP group agreed to use the term ‚Äòmasking‚Äô in its discussions.)
       ‚Ä¢ Secondary use: While the HIP Group recognizes that there is ongoing debate about the
         division between primary and secondary use, in this paper, secondary use refers to the
         utilization of health information for any purpose other than the provision of direct care
         and treatment.
       ‚Ä¢ Accountability: This refers to responsibility for decisions related to the collection,
         access, use, disclosure, retention and overall protection of personal health information.
         It encompasses accountability to the patient for the protection of his or her personal health
         information; as well as accountability to a Minister or other body for the good management
         of information in the iEHR.




4
    Note that all transactions within Alberta Netcare are ‚Äúuses‚Äù, not disclosures
5
    Note that this relates to control over the INFORMATION in the record, not consent for treatment.



                                                           Privacy and EHR Information Flows in Canada   10
Section 1:
Pan-Canadian common understandings
to support trans-jurisdictional disclosures of
EHR information within Canada




                                Privacy and EHR Information Flows in Canada   11
Section 1:
Pan-Canadian common understandings to support trans-jurisdictional
disclosures of EHR information within Canada

All jurisdictions in Canada have a variety of statutes in place that enable and govern the collection,
use and disclosure of personal health information for care and treatment and for secondary uses.
Many jurisdictions are also putting in place supporting legislation and/or policies to reflect the
new EHR context.

As noted in a paper prepared for an Infoway project on trans-jurisdictional disclosures of health
information,6 all jurisdictions currently send personal health information (in electronic and non
electronic formats) across jurisdictional boundaries for care and treatment purposes.
For example:
    ‚Ä¢ Residents in communities that straddle jurisdictional boundaries may receive care on both
       sides of the border
    ‚Ä¢ Residents in rural and remote communities ‚Äì residents of northern communities and
       territories in particular ‚Äì travel to other jurisdictions for regular, non-specialized care and
       treatment
    ‚Ä¢ Regional centres of excellence provide specialized services for the treatment of serious
       conditions ‚Äì such as cancer treatment, cardiac care and organ transplants
    ‚Ä¢ Medical services are provided to temporary residents
    ‚Ä¢ Emergency department services are provided to out-of-jurisdiction visitors
    ‚Ä¢ Files are transferred to another jurisdiction due to a change of residence
    ‚Ä¢ Services, such as radiology services, or telehealth services, are provided by clinicians who
       reside in another jurisdiction
    ‚Ä¢ Services are provided by clinicians who are temporarily outside of the jurisdiction (e.g.,
       clinicians at conferences but continuing to follow their patients‚Äô treatments).

Information is also disclosed to other jurisdictions for secondary uses, notably research, but also
for billing and related administrative purposes, public health surveillance, and other authorized
secondary uses.

Currently, information in a non-electronic (paper, tape, verbal) or electronic (CD, flash drive,
DVD, disc) format flows physically (carried by patient, telephone, post, courier) or semi-
electronically (faxed) between jurisdictions. There are also examples of electronic information
(such as radiology imaging and e-mail information) flowing electronically (via systems such as
Picture Archiving and Communication systems (PACs) and secure e-mail and store-and-forward




6
  Canada Health Infoway, Trans-jurisdictional Flows of EHR Data in Canada, v.2, July, 2009 http://www2.infoway-
inforoute.ca/Documents/Trans-Jurisdictional_Flows_of_EHR_Data_in_Canada_v2.pdf



                                                        Privacy and EHR Information Flows in Canada               12
systems) from an institution or medical office in one jurisdiction to an institution or medical
office in another.

These flows or disclosures essentially reflect the existing environment, rather than the shift that
the iEHR represents. Generally, the patient‚Äôs care provider in the home jurisdiction chooses what
information to disclose to the out-of-jurisdiction clinician. It could be a letter of introduction
plus the patient‚Äôs full file and results or it could be little more than a referral sheet.

Such disclosures are driven by medical imperatives and are often based on historical patterns
established in a jurisdiction‚Äôs health system. Some of these flows are governed by agreements
between jurisdictions.

The iEHR world represents a different situation, one that is still evolving. It could be that, once
the out-of-jurisdiction requestor is sufficiently authorized (via rules built into the system) the
home jurisdiction‚Äôs system could make visible (disclose) to the out-of-jurisdiction requestor an
EHR screen (e.g., the shared record summary in the case of an emergency; or perhaps more detail
if it is a specialist referral). Rather than being based on a one-to-one communication between
clinicians, the disclosure could be transacted on the basis of rules built into the system related to
the authority of the individual to request the information, the identification and authentication of
the patient, the authority of the sending jurisdiction to make out-of-jurisdiction disclosures, the
specific information in question and any rules that may be in place regarding masked or locked
data. And it is possible that, rather than being comprised solely of the information the clinician
chooses to disclose from the patient‚Äôs file, the information disclosed could involve a series of
standardized screens containing standard sets of information.

Trans-jurisdictional disclosures for secondary uses in the iEHR context are also expected to work
quite differently. While potential scenarios are still in early stages, one given is that data for
secondary uses will not be disclosed directly from the live iEHR or EHR and those seeking such
information will not be provided access to live EHR systems. (See Section 2 for discussion of
common understandings relating to secondary use of EHR data).

The following common understandings of the HIP Group support appropriate and privacy-
protective trans-jurisdictional disclosures in this new iEHR context and are flexible enough to
take into account the various approaches jurisdictions are taking to privacy and the development
and implementation of their EHR systems. It is recognized that some of these common
understandings may appear self-evident; however the HIP Group felt it important to include the
following six foundational principles to set the stage for subsequent common understandings.

A) Foundational Principles

Support for appropriate and privacy-protective trans-jurisdictional
disclosures within Canada
It is a common understanding that:
____________________________________________________

    1. Jurisdictions support appropriate (i.e., authorized, necessary) and privacy-protective trans-
    jurisdictional disclosures of personal health information.


                                                       Privacy and EHR Information Flows in Canada   13
    2. Jurisdictions make EHR technology/system choices that meet legislative requirements,
    while striving for pan-Canadian interoperability to support trans-jurisdictional disclosures.
EHR governance structures
It is a common understanding that:
____________________________________________________

    3. For jurisdictions to be comfortable making disclosures of the personal health information
    of their residents to other jurisdictions, they require confidence in other jurisdictions‚Äô laws,
    regulations and practices that relate to how that personal health information will be handled
    and protected.7 Jurisdictional EHR governance structures that include a privacy and
    information governance component are one element of this trust framework. (See Section 3
    for a discussion of this issue.)
    4. A pan-Canadian structure is also important for the coordination of information
    governance issues related to trans-jurisdictional disclosures of EHR information. (See Section
    3 for further discussion.)
Authorities for disclosures to other jurisdictions within Canada
Each jurisdiction currently has a different mix of legislation, policy and precedent/practice
governing its management of personal health information. This mix may include agreements or
related tools that function in tandem with or in place of legislation (when legislation does not
exist) to authorize disclosures. Personal health information is currently collected, used and
disclosed within and across jurisdictions within this mix, and this will continue to be the case in
the EHR context. Because most privacy and health information privacy legislation and
associated policies are subject to regular review, the legal and policy framework evolves over time
to take into account advances in privacy and other issues.

It is a common understanding that:
____________________________________________________

    5. Jurisdictions disclose EHR information in compliance with the appropriate authority
    framework that may include legislation, policies (that provide guidance to legislation or act in
    the place of legislation where none exists) and agreements.
    6. The longer-term vision is for each jurisdiction to have legislation and/or policies in place
    that clearly authorize appropriate trans-jurisdictional disclosures from the EHR, as well as the
    privacy and security of personal health information.




7
  E.g., Quebec law prohibits disclosures to another jurisdiction if that jurisdiction does not have equivalent privacy
protection of personal health information.



                                                            Privacy and EHR Information Flows in Canada                  14
B) Trans-jurisdictional disclosure principles

Disclosure principles
It is a common understanding that:
____________________________________________________

       7. When a custodian or trustee in one jurisdiction provides personal health information to a
       custodian or trustee in a second jurisdiction, it is a disclosure from one jurisdiction and an
       (indirect) collection by the second jurisdiction, even if the information is only ‚Äúviewed‚Äù, but
       not recorded, in the second jurisdiction. (Note that this means that multiple custodians could
       have custody and/or8 control over the same information in different jurisdictions.)
       8. A disclosing jurisdiction must follow its legislation and policies for disclosure to a second
       jurisdiction, and the jurisdiction to which the information is disclosed must follow its
       legislation and policies for (indirect) collection.
       9. Once information is disclosed to a custodian or trustee in a second jurisdiction (and
       thereby has been indirectly collected by a custodian or trustee in the second jurisdiction),
       it becomes subject to the information handling legislation and policies of the
       second jurisdiction.
       10. All EHR information disclosed from a custodian or trustee in one jurisdiction to a
       custodian or trustee in a second jurisdiction should be protected by reasonable safeguards,
       and in compliance with applicable legislative requirements in the receiving jurisdiction,
       whether or not the information is recorded. Where legislation does not refer to unrecorded
       personal health information, such information may be protected by policy or by professional
       ethical obligations.
Identity management
As identified in a paper9 examining trans-jurisdictional disclosures of information for care and
treatment, the ability to unambiguously identify a patient and a provider involved in such
disclosures is of great importance, not just from an operational perspective, but even more so,
from the standpoint of privacy and patient safety. This issue is beyond the scope of this paper
and needs to be addressed in a wider context; however, the common understanding that follows is
intended to underscore its importance.

It is a common understanding that:
____________________________________________________

       11. Processes should be in place for uniquely identifying patients and providers in trans-
       jurisdictional disclosures and collections of information.




8
    The use of ‚Äúand/or‚Äù reflects the existence of jurisdictional differences regarding custody and control

9
  Canada Health Infoway, Trans-jurisdictional Flows of EHR Data in Canada, v.2, July, 2009 http://www2.infoway-
inforoute.ca/Documents/Trans-Jurisdictional_Flows_of_EHR_Data_in_Canada_v2.pdf



                                                              Privacy and EHR Information Flows in Canada         15
C) Patient control of their personal health information

Respecting patient/individuals‚Äô wishes
It is a common understanding that:
____________________________________________________

     12. As in the paper-based environment, jurisdictions recognize the value of including all
     relevant and necessary information in the EHR. Jurisdictions also support patients‚Äô rights to
     exercise a measure of control10,11 over the use and disclosure of their personal health
     information for care and treatment, and strive to respect the control a patient has put on this
     information in trans-jurisdictional disclosures.
     13. When a patient seeks care in another jurisdiction, whether in an emergency, for planned
     care or for another reason, the control a patient has exercised over his or her information in
     the home jurisdiction should be respected in the second jurisdiction to the extent possible
     given the legal framework and technology in use in the second jurisdiction:
             ‚Ä¢ Except where otherwise permitted, if personal health information has been
               masked,12 it should not be disclosed to another jurisdiction. In these situations, the
               care provider in the jurisdiction requesting the information must be advised that
               information has been masked and is not being disclosed.
             ‚Ä¢ However, where permitted because the patient has provided consent or the
               situation meets a jurisdiction‚Äôs override criteria, information may be unmasked and
               disclosed to the requesting jurisdiction. In these situations, both the disclosing and
               collecting jurisdictions should log the transactions. The collecting jurisdiction
               should make efforts to re-mask the information in accordance with its legal
               framework and technology currently in place in the jurisdiction, and the patient
               should be notified of the results.




10
   Jurisdictions use various terms ‚Äì e.g., consent directives, disclosure directives, expressed wishes -- to describe how a
patient can exercise the principle of patient control to restrict access to, use and/or disclosure of his or her personal
health information. (Note: this relates to the INFORMATION in the record, not consent for treatment)
11
   Note that based on the current legislation in Quebec, patients have the right to opt out of the province‚Äôs EHR, but if
they participate, they do not have the right to mask any information in it.
12
   If legislative provisions allow for patient control and a request for masking has been made, but existing systems do
not have the capability to support masking, the information should not be disclosed to another jurisdiction unless the
patient has provided consent or the situation meets the jurisdiction‚Äôs override criteria.



                                                           Privacy and EHR Information Flows in Canada                  16
Patient information/notices about EHRs
In helping patients become well-informed about EHRs, jurisdictions must decide how much
information to make available and how to best deliver the information. Jurisdictions wish to
provide sufficient and relevant information to the patient in a way that does not hamper clinical
workflow. The common understandings that follow seek to find the balance among these issues.

It is a common understanding that:
____________________________________________________

     14. The information included in patient notices13 is a jurisdiction‚Äôs responsibility and will
     depend on its approach to health care delivery. Notices about the EHR should include
     information about trans-jurisdictional disclosures, in addition to information on topics such
     as, but not limited to the following:
            ‚Ä¢ What information is collected
            ‚Ä¢ The purpose of collection (i.e., for care and treatment) and whether that
              information may also be used for other purposes such as determining payment for
              services provided, health system analysis, quality assurance reviews, education and
              research, under specified conditions
            ‚Ä¢ Who is authorized to see patient information
            ‚Ä¢ How patient information will be protected
            ‚Ä¢ That if patient information is disclosed to another jurisdiction, it will be subject to
              the second jurisdiction‚Äôs information handling laws and policies, which may be
              different from the approach in the patient‚Äôs home jurisdiction
            ‚Ä¢ Where to go for more detailed information and how/where to register an inquiry or
              complaint, whether their complaint refers to an incident in their home jurisdiction
              or another jurisdiction in Canada.

     15. Where jurisdictions have legislative provisions and their EHR systems are capable of
     offering patient control of their information, patient notices and discussions with a patient
     requesting masking should include the following messages:
            ‚Ä¢ That patients have a right to request masking as well as unmasking of some or all
              of their information
            ‚Ä¢ The clinical implications and other limits of masking
            ‚Ä¢ How to request that their information be masked or unmasked
            ‚Ä¢ In which situations, such as emergencies, that legislation or policy allows their
              information to be unmasked without their consent, and whether or not in these
              situations, their information will be remasked automatically or whether they need to
              request remasking




13
  MacPherson, D. and R. Fraser, ‚ÄúJurisdictional Scan of Patient Notices‚Äù
http://www2.infoway-inforoute.ca/Documents/Jurisdictional_Scan_of_Patient_Notices_EN_FINAL.pdf



                                                       Privacy and EHR Information Flows in Canada   17
       ‚Ä¢ Which, if any, other provisions in law or policy (in the absence of legislation) can
         override personal masking requests, for example, that their unmasked information
         in de-identified form may be used for secondary purposes
       ‚Ä¢ That if they seek care in another jurisdiction, their information will be subject to
         the second jurisdiction‚Äôs masking policies, which may be different from the
         approach in the home jurisdiction.

16. Neither patients nor health care providers are expected to be experts about other
jurisdictions‚Äô EHR systems or health information privacy laws. Jurisdictions will need to
work together to put in place practical and simple processes to point patients and providers
towards sources of information about the information handling laws and policies of
other jurisdictions.




                                           Privacy and EHR Information Flows in Canada         18
Section 2:
Trans-jurisdictional disclosures of EHR
information for secondary uses




                               Privacy and EHR Information Flows in Canada   19
Section 2:
Trans-jurisdictional disclosures of EHR information
for secondary uses

Context
The term ‚Äòsecondary use‚Äô has been widely used to refer to the utilization of health information for
any purpose other than the provision of direct care and treatment. Most14 secondary uses of
health information relate to work that benefits the health of Canadians, but not through direct
care and treatment. Recently other terms have also come into play, most notably ‚Äòhealth system
use‚Äô, which denotes using health information for clinical program management (including quality
improvement and decision support), health system management (e.g., analysis, planning,
monitoring), population health surveillance, and research. This paper will continue to use the
term ‚Äòsecondary use‚Äô.

The value of using health information for secondary uses has long been recognized and
legislatively authorized, irrespective of the presence of the EHR. These secondary uses have been
shown to improve the health care experience, expand knowledge about disease, illness and
treatment, strengthen the effectiveness and efficiency of health care delivery and support public
health initiatives.

Disclosures of personal health information for secondary use must be made in compliance with
legislative authorities. Health information and privacy statutes commonly list authorized
disclosures for secondary uses that trustees or custodians have the discretion to make without the
consent of the individual (although few statutes use the term ‚Äòsecondary use‚Äô;15 instead using
terms such as ‚Äòpermitted uses‚Äô or ‚Äòauthorized uses‚Äô). The lists of authorized uses and associated
disclosures are relatively consistent across jurisdictions. Where lists are not specifically set out,
the legislation may indicate that the information may be used or disclosed for a purpose that is
consistent with the purpose for which it was collected.

In keeping with this longstanding recognition of the value and appropriateness of secondary use
of health information, numerous Canadian commissions and reports (most recently, Romanow16
and Kirby17) have affirmed that part of the value of the EHR initiative would be the potential for
using the stored information for research and related purposes.




14
   A few secondary uses of health information have little or no relation to healthcare and are authorized under other
statutes. Examples of these include mandatory reporting of gunshot wounds in some jurisdictions or complying with a
warrant or subpoena. This paper is not focusing on these secondary uses.
15
   Of note Federal Privacy legislation does not recognize this terminology.
16
   Romanow Q.C., Roy, J. Commissioner, Building on Values: The Future of Health Care in Canada, November 2002;
Chapter 3, Information, Evidence & Ideas. pp. 75-89 http://www.cbc.ca/healthcare/final_report.pdf
17
   The Honourable Michael J Kirby, The Health of Canadians ‚Äì The Federal Role, Final Report, Volume Six,
Recommendations for Reform, October 2002, Part V, Chapter Ten
 http://www.parl.gc.ca/37/2/parlbus/commbus/senate/Com-e/soci-e/rep-e/repoct02vol6-e.htm



                                                         Privacy and EHR Information Flows in Canada                20
Like care and treatment, most other uses of health information take place within a jurisdiction and
this will continue to be the case in the EHR context. Jurisdictions, however, do currently disclose
information to other jurisdictions for secondary uses, even though practical issues, such as
allowing remote access only within the jurisdiction, can make such disclosures difficult to
operationalize.18 The EHR environment must continue to allow for the appropriate and privacy-
protective use and disclosure of health information for secondary uses not only within, but also
across jurisdictions.
(Note: this paper assumes that those seeking EHR information for purposes other than direct care and treatment of a patient will not be
provided access to ‚Äúlive‚Äù data, that is, to the EHR itself or to point-of-service systems connected to the EHR.)

It bears repeating that although the focus is on trans-jurisdictional disclosures, some of the
common understandings below speak to the jurisdictional level. In part this is because
jurisdictions recognize the desirability of consistent practices at the jurisdictional level, and in part,
it is because trust in other jurisdictions is required for trans-jurisdictional disclosures of
information for secondary purposes.

Scope and terminology
The privacy of personal health information involved in secondary use is a complex topic being
examined in many quarters. The HIP Group is limiting its examination to trans-jurisdictional
disclosures without consent of EHR information that is identifiable or potentially re-identifiable
(PHI or potential PHI), for clinical program management, health system administration and
research. This scope is summarized below:

 In scope                                                              Out of scope

 Trans-jurisdictional disclosures                                      Uses and disclosures within a jurisdiction

 Disclosures without consent                                           Disclosures for which consent is required
                                                                       or sought

 EHR information                                                       Information from source systems

 Information that is identifiable or potentially                       Anonymous or aggregated information
 re-identifiable ‚Äì PHI or potential PHI

 Clinical program management, health system                            Population health surveillance
 administration and research
                                                                       Secondary uses unrelated to health


Within this scope, the HIP Group‚Äôs focus is on de-identification of personal health information,
review and assessment processes, patient notification and governance.




18
  El Emam, K, Practices for the Review of Data Requests and the Disclosure of Health Information by Health Ministries
and Large Data Custodians.
http://www2.infoway-inforoute.ca/Documents/Practices_for_the_review_of_data_requests_June_2010_EN_FINAL.pdf



                                                                 Privacy and EHR Information Flows in Canada                         21
De-identification of personal health information
The disclosure of identifiable information ‚Äì information that on its own or in combination with
other available information could identify an individual ‚Äì raises privacy risks. Privacy concerns
diminish as information becomes increasingly unidentifiable.

There is a spectrum of identifiability that illustrates a gray zone rather than a sharp cut-off
between what is identifiable and what is truly de-identified. One aspect of the spectrum has to do
with the format of the information ‚Äì record level information is data at the level of an individual
person, and even if these data do not directly identify the person, they are more vulnerable than
aggregate data, which are data that have been averaged or grouped into ranges across multiple
records. The following illustrates three basic levels of the identifiability spectrum.

Identifiable information is:
       ‚Ä¢ information that includes data elements that directly identify an individual, such as name,
         health number, etc., or
       ‚Ä¢ record-level information that includes data elements such as full postal code, gender, date
         of birth and/or unique occupation, that in combination can be readily used to identify an
         individual even when direct identifiers such as name, have been removed.
Information is de-identified when:
       ‚Ä¢ direct identifiers have been removed (or replaced with pseudonyms), and
       ‚Ä¢ data elements that could be used to identify an individual, such as postal code, gender and
         date of birth, have been removed, generalized (e.g., removing the last three digits of a postal
         code), put into ranges (e.g., 10-year age category) or otherwise manipulated with the intent
         that the information not be re-identifiable, and
       ‚Ä¢ no other data set can reasonably be expected to be available to combine with the data and
         re-identify the individual.
Information is anonymous when, for example:
       ‚Ä¢ it is aggregated, and
       ‚Ä¢ the aggregation satisfies rules about small cell size,19 and
       ‚Ä¢ no other data set can reasonably be expected to be available to combine with the data and
         re-identify the individual.




19
     Aggregate information that does not meet aggregation rules about small cell size is potentially identifiable



                                                             Privacy and EHR Information Flows in Canada            22
It is a common understanding that:
____________________________________________________

     17. Trans-jurisdictional disclosures for secondary uses should, as a general rule, involve
     aggregated or de-identified information. The disclosing jurisdiction is responsible for the
     aggregation or de-identification procedures before disclosing the information.
     18. In some situations legislation authorizes or requires the disclosure of
     identifiable information.20
As outlined in a paper21 on de-identification tools prepared for the HIP Group, ever increasing
computing power and availability of online databases for data linkage make it more and more
difficult to de-identify information with confidence that the potential for re-identification is low,
while keeping its informative value for analysis and research. The paper describes a number of
tools that are available to de-identify information. The use of the tools, however, requires
considerable technical and statistical expertise and it does not appear that they are consistently or
widely used at this time.

It is a common understanding that:
____________________________________________________

     19. Those entities and individuals responsible for handling requests for trans-jurisdictional
     disclosures of EHR information for secondary uses should be knowledgeable about de-
     identification, up-to-date on de-identification tools and techniques, and able to apply them.
Even when de-identification tools are used, it will still be necessary to assess the risk of the
proposed disclosure and to outline the responsibilities and obligations of the data requestor.
This is particularly important since de-identification and re-identification techniques and strategies
are constantly evolving, and it is therefore not possible to guarantee that de-identified data will
never be able to be re-identified.

It is a common understanding that:
____________________________________________________

     20. De-identification techniques should work hand in hand with risk assessment processes,22
     agreements (which set out obligations and conditions for management of health information
     being used for secondary purposes), security practices and other safeguards to minimize the
     privacy risks of disclosing information for secondary uses.




20
   For example, Ontario‚Äôs PHIPA authorizes such disclosures.
21
   Fraser, R. and D. Willison, ‚ÄúTools for De-Identification of Personal Health Information‚Äù
http://www2.infoway-inforoute.ca/Documents/Tools_for_De-identification_EN_FINAL.pdf
22
   Ibid.




                                                           Privacy and EHR Information Flows in Canada   23
Review and assessment processes in trans-jurisdictional
disclosure requests
The EHR system will hold ever increasing volumes of personal health information and trans-
jurisdictional requests for portions of that information for research and other secondary uses
(including those that are not related to health care) can be expected to increase over time. The
potential for supporting valuable research is great but so too are the potential privacy risks.

It is a common understanding that:
____________________________________________________

    21. Jurisdictions need to put in place processes to enable appropriate and privacy protective
    trans-jurisdictional disclosures of EHR data for secondary uses. It is recognized that some
    jurisdictions may not have the capacity to undertake these processes, and could work with
    other jurisdictions or bodies in this regard.
Custodians of EHR holdings will need to manage trans-jurisdictional requests for EHR
information for secondary uses and their risks in a manner that engenders trust with the public.
Robust and sensitive reviews of such requests can help to strike the balance between protecting
the privacy of individuals, and providing requesters useful information for activities that will
benefit Canadians in general.

It is a common understanding that:
____________________________________________________

    22. Requests for disclosure of identifiable or potentially re-identifiable information from the
    EHR to individuals or organizations in another jurisdiction for research, clinical program
    management and health system administration, should, in addition to complying with
    Research Ethics Board processes, undergo an assessment of privacy risks at the outset and as
    required over time. Special consideration should be given to requests for readily identifiable
    data or for record-level data (individual records), to ensure the need for such data is
    authorized and justified.
    23. The formality of the assessment process should be commensurate to the potential privacy
    risk related to the project at hand.
The completion of a questionnaire or checklist may be sufficient to assess projects whose privacy
risks appear minimal, while a more formal process may be required for one where the privacy
risks appear more substantial. For example, the review of requests for disclosure of particularly
sensitive information, such as information about abortion procedures, is likely to require a more
in-depth review than a request for disclosure of aggregate information about diabetes treatment
regimes. Part of the goal of the process is to embed an organizational and cultural predisposition
towards considering privacy at the outset of any potential trans-jurisdictional disclosures of EHR
information.




                                                       Privacy and EHR Information Flows in Canada    24
Ideally, processes should be consistent across the country. Various risk assessment tools and
processes already exist,23 including Alberta‚Äôs Alberta Research Ethics Community Consensus
Initiative (ARECCI) guidelines24 and the Privacy Analytics Re-identification Risk Assessment and
De-identification Tool.25 High level core elements of an assessment process would include:
     ‚Ä¢ Understanding of the project and purpose of the disclosure
     ‚Ä¢ Compliance with legislative, policy or other authorities and requirements for the disclosure
       for secondary use
     ‚Ä¢ Existence of and conformity with a data sharing or similar agreement or arrangement
     ‚Ä¢ Correspondence of the information requested to that needed for the purpose
     ‚Ä¢ Identifiability of the information requested
     ‚Ä¢ Sensitivity of the information (extent to which its exposure could cause harm,
       embarrassment, etc. to an individual or group)
     ‚Ä¢ Potential for exposure of the information (e.g., level of de-identification, potential for
       linkage with other datasets for re-identification, data security; access controls)
     ‚Ä¢ Risk management elements (e.g., how data will be disclosed (e.g., on-site or remote access)
       requirement for review of final products or outputs, compliance audits).




23
   Ibid.
24
   These guidelines assist in the determination of whether or not a project should be considered ‚Äúresearch‚Äù (and subject
to a REB) and also assess the privacy risk for both research and non-research (quality assurance and evaluation-type
projects) http://www.ahfmr.ab.ca/arecci/screening/
25
   http://www.ehealthinformation.ca/index.asp



                                                         Privacy and EHR Information Flows in Canada                 25
Patient notification respecting trans-jurisdictional disclosures for secondary uses
It is a common understanding that:
____________________________________________________

    24. Patient notices should include general information on trans-jurisdictional disclosures of
    information for secondary uses.
More information should also be available to patients wishing more detail. Relevant
messages include:
   ‚Ä¢ Examples of the types of trans-jurisdictional disclosures that are made for secondary uses
   ‚Ä¢ Legislation which allows for these disclosures without seeking patient consent
   ‚Ä¢ That information disclosed for these secondary uses is generally de-identified
   ‚Ä¢ That even if patients have masked their identifiable information, the information in de-
     identified form may be disclosed to another jurisdiction for secondary uses
   ‚Ä¢ That patients have a right to ask for a report of the trans-jurisdictional disclosures that have
     been made of their identifiable information for secondary uses
   ‚Ä¢ That once the information is in a second jurisdiction it is subject to the data protection
     provisions of the second jurisdiction.

It is a common understanding that:
____________________________________________________

    25. Records must be kept of trans-jurisdictional disclosures of identifiable information for
    secondary uses so that reports can be made to a patient upon request.
Governance of trans-jurisdictional disclosures
for secondary uses
Although legislatively authorized disclosures for secondary uses are somewhat consistent across
jurisdictions, there are some differences in legislation and practice which may have an impact on
disclosing EHR information to another jurisdiction for secondary purposes.

For example, British Columbia‚Äôs Freedom of Information and Protection of Privacy Act allows the
disclosure by public bodies of residents‚Äô data outside the country for specific purposes only, while
the E-Health (Personal Health Information Access and Protection) Act only allows the disclosure outside
of Canada to assess and address threats to public health. The province expects these restrictions
to be respected should the data of BC residents be disclosed to another jurisdiction in Canada. In
Nova Scotia, the Personal Information International Disclosure Protection Act restricts the ability of a
public body to access or store personal information outside of the country unless certain
conditions are met. Jurisdictions will need to determine if their restrictions can be formally or
informally imposed on a collecting jurisdiction.

It is a common understanding that:
____________________________________________________

    26. Jurisdictions must comply with their statutes and policies before disclosing EHR
    information to other jurisdictions for secondary uses, and for collecting information from
    another jurisdiction.



                                                       Privacy and EHR Information Flows in Canada    26
     27. Once the information is collected by another jurisdiction, the information handling rules
     in the collecting jurisdiction take effect.
Agreements are one of the tools available to help bridge differing approaches among jurisdictions
and provide confidence in making disclosures of EHR information across borders in Canada.
Such agreements set out legislative authority and obligations, manage risk, define due diligence
and manage service expectations. A paper prepared for the HIP Group discusses these types of
agreements and provides samples and templates.26

It is a common understanding that:
____________________________________________________

     28. Agreements setting out the rules under which information is to be disclosed to another
     jurisdiction for secondary uses should be in place to formalize recurring and ad hoc
     disclosures of EHR information for these uses. Such agreements may provide details not set
     out in legislation or provide guidance in the absence of legislation.
An agreement between jurisdictions should include, among other elements:
     ‚Ä¢ Purpose of the agreement
     ‚Ä¢ Legislative or other authorities for the agreement and for the information disclosure and
       subsequent collection
     ‚Ä¢ Allowable information uses and processing covered by the agreement, and any restrictions,
       such as onward disclosures
     ‚Ä¢ The identification of accountable bodies or persons
     ‚Ä¢ Level of identifiability of the information being disclosed
     ‚Ä¢ Any further uses or onward disclosures of the information that are or are not authorized,
       for example, data linkage protocols
     ‚Ä¢ The controls to which the information is subject, such as privacy risk assessments, security
       measures and audit provisions
     ‚Ä¢ Process requirements, such as the term of the agreement and program changes that trigger
       a review of the agreement.
As noted above, there is a desire for some degree of consistency across Canada in practices and
processes around disclosures for secondary use. Moreover, as experience grows with trans-
jurisdictional disclosures of EHR information for secondary uses, it is expected that new issues
will arise.

It is a common understanding that:
____________________________________________________

     29. Issues around secondary uses of EHR information would benefit from
     pan-Canadian deliberation and the development of recommendations for consideration by all
     jurisdictions in an effort to promote a degree of consistency in approach across the country.



26
  Sawatsky, Elaine, ‚ÄúInformation Sharing Agreements for Disclosure of EHR Data Within Canada‚Äù
http://www2.infoway-inforoute.ca/Documents/ISA_report_for_HIP_Group_January_2010_EN_Final.pdf




                                                       Privacy and EHR Information Flows in Canada   27
Section 3:
Accountability for information governance
in the EHR




                              Privacy and EHR Information Flows in Canada   28
Section 3:
Accountability for information governance in the EHR

This section of the paper focuses on the principle of accountability in relation to privacy of
personal health information held in the EHR. In this context, accountability refers to
responsibility for decisions related to the collection, access (in all the senses of the term), use,
disclosure, retention and overall protection of personal health information. It encompasses
patients‚Äô access to their own information and accountability to the patient for the protection of
his or her personal health information; as well as accountability to a Minister or other body for
the good management of information in the iEHR.

Accountability at the jurisdictional level
Health information and general privacy legislation in most Canadian jurisdictions identify who can
be a custodian (or trustee or other term) and the obligations of a custodian. These statutes also
generally require that a body authorized as a custodian identify an individual to be accountable for
compliance with the privacy-related obligations set out in the statutes. Jurisdictions also have
oversight bodies which are mandated to protect privacy rights and to oversee compliance with
jurisdictional privacy legislation.

Ministers of Health (or equivalents27) are ultimately responsible for the delivery of health services
within their boundaries, for the EHR being created to support health service delivery and for the
management of information that will be collected and stored in the EHR systems. However
many bodies, such as health authorities, hospitals and individual health care providers, are
accountable for those individual parts of the overall EHR system over which they have control.
In an interoperable EHR environment, such control can be more complicated to identify because:
    ‚Ä¢ health information from multiple custodians can be held in a single repository
    ‚Ä¢ health information stored in the repositories may be transformed to agreed
       upon coding structures
    ‚Ä¢ information stored in the repositories may be accessed by multiple providers.




27
  This is to ensure appropriate representation of federal departments, such as DND or VAC which deliver health
services but do not report to a Minister of Health.



                                                        Privacy and EHR Information Flows in Canada              29
Jurisdictions are taking a variety of approaches as they deem appropriate to their EHR
governance structures and accountability. (Appendix B illustrates examples of various
jurisdictional EHR accountability structures.) Some have centralized models, others are
decentralized; they may be based within or outside a ministry. Some jurisdictions may be covered
by another jurisdiction‚Äôs EHR and EHR governance structure. Yet some common themes are
emerging, for example:
     ‚Ä¢ Consensus-building through consultation and stakeholder involvement
     ‚Ä¢ Information governance through, for example, data stewardship committees
     ‚Ä¢ Transparency through steering committees or councils
     ‚Ä¢ Reporting to government from arm‚Äôs length organization or committees
     ‚Ä¢ Flexibility to allow for evolution over time.

It is a common understanding that:
____________________________________________________

    30. Whatever approach is taken to overall governance of a jurisdiction‚Äôs EHR:
            ‚Ä¢ It should include a privacy and information governance component
            ‚Ä¢ It should be ultimately accountable to the Minister of Health/e-Health or
              equivalent and have some form of formal standing ‚Äì e.g., established via Ministerial
              authority, Order, legislation, etc.
            ‚Ä¢ It should be clearly articulated where accountability resides for the EHR system as a
              whole and its parts so that patients, providers and oversight bodies within and
              outside the jurisdiction know to whom they can turn if they wish to access their
              own information or make a correction to it, or if they encounter a problem.

The HIP Group believes that the privacy and information governance component could have
responsibilities that include, in compliance with legislation and policy:
    ‚Ä¢ Setting and ensuring compliance with policies and rules for collection, access, use,
      disclosure and retention of PHI in the EHR
    ‚Ä¢ Exercising authority for decisions related to the secondary uses of information within the
      jurisdiction in a transparent fashion, setting policy directions for secondary uses and being
      responsible for compliance with the body‚Äôs decisions by recipients of EHR data for
      secondary purposes
    ‚Ä¢ Advising the ministry on or negotiating on behalf of/in collaboration with the ministry,
      information sharing agreements with other jurisdictions for trans-jurisdictional disclosures
      and collections of EHR information
    ‚Ä¢ Setting common requirements for information sharing/manager agreements or other tools
    ‚Ä¢ Serving as the breach investigation coordinator in situations where the source of a breach is
      unclear or involves multiple sources. This could include notifying patients as required or
      appropriate, and notifying the relevant oversight body as required
    ‚Ä¢ Maintaining up-to-date standards and jurisdictional practices relating to privacy and
      security, as well as information technology related to the EHR



                                                       Privacy and EHR Information Flows in Canada   30
     ‚Ä¢ Representing the jurisdiction on any pan-Canadian body for EHR privacy and related
       information governance, serving as a point of contact between jurisdictions for discussion
       of certain issues and liaising with oversight bodies
     ‚Ä¢ Acting as a resource for organizations in privacy and information governance issues
       for the EHR
     ‚Ä¢ Responding to patient complaints and patient access requests related to the EHR that
       cannot be dealt with or have been escalated by the entity or entities concerned
     ‚Ä¢ Managing public education initiatives, for example, serving as the coordinating body for
       patient notification templates, best practices, etc., and engaging the public in the continuing
       development and refinement of the EHR.

Accountability at the organizational level
Within jurisdictions, organizations, (including points of service, entities, custodians, facilities,
providers, registries and others) collect, use and disclose personal health information in
accordance with specific policies and procedures that comply with jurisdictional laws and policies.
Many of these organizations are the primary point of contact for patients regarding privacy issues.

Organizations are generally required (by law or policy) to have a process for overall privacy
compliance, including dealing with individual complaints, as well as with requests for access to
personal health information by the individual and the correction of errors.

It is a common understanding that:
____________________________________________________

     31. Organizations should revisit their privacy compliance process in the EHR context to
     ensure that it remains transparent and accessible to patients/clients and providers.
     Organizations also need to revisit their privacy roles and responsibilities, for example:
             ‚Ä¢ Ensuring that patients are provided contact information for persons who can
               respond to their privacy questions or complaints related to the EHR
             ‚Ä¢ Exercising operational responsibility for staff, training and the implementation of
               policies and procedures for technical, physical and administrative safeguards for the
               protection of personal health information in the EHR28
             ‚Ä¢ Complying with jurisdictional legislation and policy, proposed secondary uses and
               disclosures of personal health information from the EHR
             ‚Ä¢ Establishing clear relationships with the jurisdictional EHR governance structure.




28
   The Canadian Medical Protective Association has, for example, issued an Electronic Records Handbook to provide
members with an overview of the issues associated with EHRs and EMRs. It can be downloaded at
http://www.cmpa-acpm.ca/cmpapd04/docs/submissions_papers/com_electronic_records_handbook-e.cfm




                                                        Privacy and EHR Information Flows in Canada                 31
Accountability and trans-jurisdictional disclosures
of personal health information
The matter of accountability can be even more challenging when EHR health information is
disclosed from one jurisdiction to another. In the trans-jurisdictional context it must be
recognized that jurisdictional statutes differ, that one jurisdiction‚Äôs laws do not have power in
another jurisdiction, and that disclosures for care and treatment as well as for secondary uses can
be expected to increase over time. Despite the complexity of the EHR environment, patients,
providers, and oversight bodies still need to know:
    ‚Ä¢ who is accountable for the information systems
    ‚Ä¢ to whom the individual patient can turn if they wish access to their own information
    ‚Ä¢ who is responsible for investigating and resolving any problems that arise.

The common understandings in this paper address some but not all of the issues related to
supporting appropriate and privacy-protective trans-jurisdictional disclosures, and it is expected
that as time goes on, new trans-jurisdictional issues will arise needing coordination across
jurisdictions. Since privacy needs to be embedded in every aspect of the EHR, architects, subject
matter experts in privacy, security specialists and other information technology specialists need to
work together on these issues so that each can bring their particular expertise to the table.

It is a common understanding that:
____________________________________________________

    32. Even though accountability for the privacy of personal health information held in the
    EHR will continue to rest with the jurisdictions, a pan-Canadian coordinating group is needed
    to discuss, address and coordinate common information governance issues including those
    related to trans-jurisdictional disclosures/collections of information. Such a group will
    continue to be needed as the EHR is rolled out and maintained across the country.
    33. The group should be a single integrated structure that brings together the many
    specializations (including but not limited to expertise in privacy, security, and related IT issues
    and standards) needed to manage information and information technologies in a privacy
    sensitive manner and to create a culture of privacy that will engender the trust and support of
    patients, clinicians and the public at large in the EHR system.

The current HIP Group, whose establishment was supported by jurisdictional Deputy Ministers,
includes the core representation and activities of such a pan-Canadian structure. The mandate of
the HIP Group, however, focuses on discussion and information sharing, rather than on
addressing and coordinating issues. There is a need for further discussion about the potential
evolution of the HIP Group‚Äôs mandate and composition. Appendix C illustrates a range of
options that could be considered by jurisdictions as a starting point for discussions. In the
meantime, the current HIP Group will move on to consider additional topics in information
governance of the iEHR.




                                                       Privacy and EHR Information Flows in Canada   32
Appendix A:
Summary List of Common Understandings

Section 1:
Pan-Canadian common understandings to support trans-jurisdictional
disclosures of EHR information within Canada

A) Foundational Principles
It is a common understanding that:
____________________________________________________

    1. Jurisdictions support appropriate (i.e., authorized, necessary) and privacy-protective trans-
    jurisdictional disclosures of personal health information. (Page 13)
    2. Jurisdictions make EHR technology/system choices that meet legislative requirements,
    while striving for pan-Canadian interoperability to support trans-jurisdictional disclosures.
    (Page 13)
    3. For jurisdictions to be comfortable making disclosures of the EHR personal health
    information of their residents to other jurisdictions, they require confidence in other
    jurisdictions‚Äô laws, regulations and practices that relate to how that personal health
    information will be handled and protected.29 Jurisdictional EHR governance structures that
    include a privacy and information governance component are one element of this trust
    framework. (See Section 3 for a discussion of this issue.) (Page 14)
    4. A pan-Canadian structure is also important for the coordination of information
    governance issues related to trans-jurisdictional disclosures of EHR information. (See Section
    3 for further discussion.) (Page 14)
    5. Jurisdictions disclose EHR information in compliance with the appropriate authority
    framework that may include legislation, policies (that provide guidance to legislation or act in
    the place of legislation where none exists) and agreements. (Page 14)
    6. The longer-term vision is for each jurisdiction to have legislation and/or policies in place
    that clearly authorize appropriate trans-jurisdictional disclosures from the EHR, as well as the
    privacy and security of personal health information. (Page 14)




29
   E.g., Quebec law prohibits disclosures to another jurisdiction if that jurisdiction does not have equivalent privacy
protection of personal health information.



                                                            Privacy and EHR Information Flows in Canada                   33
B) Trans-jurisdictional disclosure principles
It is a common understanding that:
____________________________________________________

     7. When a custodian or trustee in one jurisdiction provides personal health information to a
     custodian or trustee in a second jurisdiction, it is a disclosure from one jurisdiction and an
     (indirect) collection by the second jurisdiction, even if the information is only ‚Äúviewed‚Äù, but
     not recorded, in the second jurisdiction. (Note this means that multiple custodians could
     have custody and/or control of the same information in different jurisdictions.) (Page 15)
     8. A disclosing jurisdiction must follow its legislation and policies for disclosure to a second
     jurisdiction, and the jurisdiction to which the information is disclosed must follow its
     legislation and policies for (indirect) collection. (Page 15)
     9. Once information is disclosed to a custodian or trustee in a second jurisdiction
     (and thereby has been indirectly collected by a custodian or trustee in the second jurisdiction),
     it becomes subject to the information handling legislation and policies of the
     second jurisdiction. (Page 15)
     10. All EHR information disclosed from a custodian or trustee in one jurisdiction to a
     custodian or trustee in a second jurisdiction should be protected by reasonable safeguards,
     and in compliance with applicable legislative requirements in the receiving jurisdiction,
     whether or not the information is recorded. Where legislation does not refer to unrecorded
     personal health information, such information may be protected by policy or by professional
     ethical obligations. (Page 15)
     11. Processes should be in place for uniquely identifying patients and providers in trans-
     jurisdictional disclosures and collections of information. (Page 15)

C) Patient control of their personal health information
     12. As in the paper-based environment, jurisdictions recognize the value of including all
     relevant and necessary information in the EHR. Jurisdictions also support patients‚Äô rights to
     exercise a measure of control30,31 over the use and disclosure of their personal health
     information for care and treatment, and strive to respect the control a patient has put on this
     information in trans-jurisdictional disclosures. (Page 16)




30
   Jurisdictions use various terms ‚Äì e.g., consent directives, disclosure directives, expressed wishes -- to describe how a
patient can exercise the principle of patient control to restrict access to, use and/or disclosure of his or her personal
health information. (Note: this relates to the INFORMATION in the record, not consent for treatment)
31
   Note that based on the current legislation in Quebec, patients have the right to opt out of the province‚Äôs EHR, but if
they participate, they do not have the right to mask any information in it.



                                                           Privacy and EHR Information Flows in Canada                  34
    13. When a patient seeks care in another jurisdiction, whether in an emergency, for planned
    care or for another reason, the control a patient has exercised over his or her
    information in the home jurisdiction should be respected in the second jurisdiction to the
    extent possible given the legal framework and technology in use in the second jurisdiction:
            ‚Ä¢ Except where otherwise permitted, if personal health information has been
              masked,32 it should not be disclosed to another jurisdiction. In these situations, the
              care provider in the jurisdiction requesting the information must be advised that
              information has been masked and is not being disclosed.
            ‚Ä¢ However, where permitted because the patient has provided consent or the
              situation meets a jurisdiction‚Äôs override criteria, information may be unmasked and
              disclosed to the requesting jurisdiction. In these situations, both the disclosing and
              collecting jurisdictions should log the transactions. The collecting jurisdiction
              should make efforts to re-mask the information in accordance with its legal
              framework and technology currently in place in the jurisdiction, and the patient
              should be notified of the results. (Page 16)
    14. The information included in patient notices33 is a jurisdiction‚Äôs responsibility and will
    depend on its approach to health care delivery. Notices about the EHR should include
    information about trans-jurisdictional disclosures, in addition to information on topics such
    as, but not limited to the following:
            ‚Ä¢ What information is collected
            ‚Ä¢ The purpose of collection (i.e., for care and treatment) and whether that
              information may also be used for other purposes such as determining payment for
              services provided, health system analysis, quality assurance reviews, education and
              research, under specified conditions
            ‚Ä¢ Who is authorized to see patient information
            ‚Ä¢ How patient information will be protected
            ‚Ä¢ That if patient information is disclosed to another jurisdiction, it will be subject to
              the second jurisdiction‚Äôs information handling laws and policies, which may be
              different from the approach in patient‚Äôs home jurisdiction
            ‚Ä¢ Where to go for more detailed information and how/where to register an inquiry or
              complaint, whether their complaint refers to an incident in their home jurisdiction
              or another jurisdiction in Canada. (Page 17)




32
   If legislative provisions allow for patient control and a request for masking has been made, but existing systems do
not have the capability to support masking, the information should not be disclosed to another jurisdiction unless the
patient has provided consent or the situation meets the jurisdiction‚Äôs override criteria.
33
   MacPherson, D. and R. Fraser, ‚ÄúJurisdictional Scan of Patient Notices‚Äù
http://www2.infoway-inforoute.ca/Documents/Jurisdictional_Scan_of_Patient_Notices_EN_FINAL.pdf



                                                          Privacy and EHR Information Flows in Canada                     35
15. Where jurisdictions have legislative provisions and their EHR systems are capable of
offering patient control of their information, patient notices and discussions with a patient
requesting masking should include the following messages:
        ‚Ä¢ That patients have a right to request masking as well as unmasking of some or all of
           their information
        ‚Ä¢ The clinical implications and other limits of masking
        ‚Ä¢ How to request that their information be masked or unmasked
        ‚Ä¢ In which situations, such as emergencies, that legislation or policy allows their
           information to be unmasked without their consent, and whether or not in these
           situations, their information will be remasked automatically or whether they need to
           request remasking
        ‚Ä¢ Which, if any, other provisions in law or policy (in the absence of legislation) can
           override personal masking requests, for example, that their unmasked information
           in de-identified form may be used for secondary purposes
        ‚Ä¢ That if they seek care in another jurisdiction, their information will be subject to the
           second jurisdiction‚Äôs masking policies, which may be different from the approach in
           the home jurisdiction. (Page 17)
16. Neither patients nor health care providers are expected to be experts about other
jurisdictions‚Äô EHR systems or health information privacy laws. Jurisdictions will need to work
together to put in place practical and simple processes to point patients and providers towards
sources of information about the information handling laws and policies of other
jurisdictions. (Page 18)




                                             Privacy and EHR Information Flows in Canada       36
Section 2:
Trans-jurisdictional disclosures of EHR information
for secondary uses

De-identification of personal health information
It is a common understanding that:
____________________________________________________

     17. Trans-jurisdictional disclosures for secondary uses should, as a general rule, involve
     aggregate or de-identified information. The disclosing jurisdiction is responsible for the
     aggregation or de-identification procedures before disclosing the information. (Page 23)
     18. In some situations legislation authorizes or requires the disclosure of identifiable
     information.34 (Page 23)
     19. Those entities and individuals responsible for handling requests for trans-jurisdictional
     disclosures of EHR information for secondary uses should be knowledgeable about de-
     identification, up-to-date on de-identification tools and techniques, and able to apply them.
     (Page 23)
     20. De-identification techniques should work hand in hand with risk assessment processes,35
     agreements (which set out obligations and conditions for management of health information
     being used for secondary purposes), security practices and other safeguards to minimize the
     privacy risks of disclosing information for secondary uses. (Page 23)

Review and assessment processes in trans-jurisdictional disclosure requests
It is a common understanding that:
____________________________________________________

     21. Jurisdictions need to put in place processes to enable the appropriate and privacy
     protective trans-jurisdictional disclosures of EHR data for secondary uses. It is recognized
     that some jurisdictions may not have the capacity to undertake these processes, and could
     work with other jurisdictions or bodies in this regard. (Page 24)
     22. Requests for disclosure of identifiable or potentially re-identifiable information from the
     EHR to individuals or organizations in another jurisdiction for research, clinical program
     management and health system administration, should, in addition to complying with
     Research Ethics Board processes, undergo an assessment of privacy risks at the outset and as
     required over time. Special consideration should be given to requests for readily identifiable
     data or for record-level data (individual records), to ensure the need for such data is
     authorized and justified. (Page 24)
     23. The formality of the assessment process should be commensurate to the potential privacy
     risk related to the project at hand. (Page 24)



34
  For example, Ontario‚Äôs PHIPA authorizes such disclosures.
35
  Fraser, R. and D. Willison, ‚ÄúTools for De-Identification of Personal Health Information‚Äù
http://www2.infoway-inforoute.ca/Documents/Tools_for_De-identification_EN_FINAL.pdf




                                                          Privacy and EHR Information Flows in Canada   37
Patient notification respecting trans-jurisdictional disclosures for secondary use
It is a common understanding that:
____________________________________________________

    24. Patient notices should include general information on trans-jurisdictional disclosures of
    information for secondary uses. (Page 26)
    25. Records must be kept of trans-jurisdictional disclosures of identifiable information for
    secondary uses so that reports can be made to a patient upon request. (Page 26)

Governance of trans-jurisdictional disclosures for secondary use
It is a common understanding that:
____________________________________________________

    26. Jurisdictions must comply with their statutes and policies before disclosing EHR
    information to other jurisdictions for secondary uses, and for collecting information from
    another jurisdiction. (Page 26)
    27. Once the information is collected by another jurisdiction, the information handling rules
    in the collecting jurisdiction take effect. (Page 27)
    28. Agreements setting out the rules under which information is to be disclosed to another
    jurisdiction for secondary uses should be in place to formalize recurring and ad hoc
    disclosures of EHR information for these uses. Such agreements may provide details not set
    out in legislation or provide guidance in the absence of legislation. (Page 27)
    29. Issues around secondary uses of EHR information would benefit from pan-Canadian
    deliberation and the development of recommendations for consideration by all jurisdictions in
    an effort to promote a degree of consistency in approach across the country. (Page 27)




                                                       Privacy and EHR Information Flows in Canada   38
Section 3:
Accountability for information governance in the EHR

Accountability at the jurisdictional level
It is a common understanding that:
____________________________________________________

    30. Whatever approach is taken to overall governance of a jurisdiction‚Äôs EHR:
            ‚Ä¢ It should include a privacy and information governance component
            ‚Ä¢ It should be ultimately accountable to the Minister of Health/e-Health or
              equivalent and have some form of formal standing ‚Äì e.g., established via Ministerial
              authority, Order, legislation, etc.
            ‚Ä¢ It should be clearly articulated where accountability resides for the EHR system as a
              whole and its parts so that patients, providers and oversight bodies within and
              outside the jurisdiction know to whom they can turn if they wish to access their
              own information or make a correction to it, or if they encounter a problem.
              (Page 30)

Accountability at the organizational level
It is a common understanding that:
____________________________________________________

    31. Organizations should revisit their privacy compliance process in the EHR context to
    ensure that it remains transparent and accessible to patients/clients and providers.
    Organizations also need to revisit their privacy roles and responsibilities. for example:
            ‚Ä¢ Ensuring that patients are provided contact information for persons who can
              respond to their privacy questions or complaints related to the EHR.
            ‚Ä¢ Exercising operational responsibility for staff, training and the implementation of
              policies and procedures for technical, physical and administrative safeguards for the
              protection of personal health information in the EHR.36
            ‚Ä¢ Complying with jurisdictional legislation and policy, proposed secondary uses and
              disclosures of personal health information from the EHR.
            ‚Ä¢ Establishing clear relationships with the jurisdictional EHR governance structure.
              (Page 31)




36
   The Canadian Medical Protective Association has, for example, issued an Electronic Records Handbook to provide
members with an overview of the issues associated with EHRs and EMRs. It can be downloaded at http://www.cmpa-
acpm.ca/cmpapd04/docs/submissions_papers/com_electronic_records_handbook-e.cfm




                                                       Privacy and EHR Information Flows in Canada              39
Accountability and trans-jurisdictional disclosures of personal health information
It is a common understanding that:
____________________________________________________

    32. Even though accountability for the privacy of personal health information held in the
    EHR will continue to rest with the jurisdictions, a pan-Canadian coordinating group is needed
    to discuss, address and coordinate common information governance issues including those
    related to trans-jurisdictional disclosures/collections of information. Such a group will
    continue to be needed as the EHR is rolled out and maintained across the country. (Page 32)
    33. The group should be a single integrated structure that brings together the many
    specializations (including but not limited to expertise in privacy, security, and related IT issues
    and standards) needed to manage information and information technologies in a privacy
    sensitive manner and to create a culture of privacy that will engender the trust and support of
    patients, clinicians and the public at large in the EHR. (Page 32)




                                                       Privacy and EHR Information Flows in Canada   40
Appendix B:
Examples of Jurisdictional EHR Governance Models

MANITOBA EHEALTH PROGRAM GOVERNANCE (2009)


                                                   MHHL
                                              (Deputy Minister)




                                              eHealth Provincial
                                              Programs Council




                                              Manitoba eHealth                                 Advisory
                                                  Program                                     Committees




     MHHL               WRHA              Brandon             CCMB                DSM               Other
                                            RHA                                                      HAs




Manitoba eHealth Program governance (2009)

The Manitoba eHealth Program reports to the Deputy Minister through a Provincial Program Council
that is composed of senior executives of the major stakeholders as shown in the above chart.37
The Council was first created in late 2008 and has 8 members ‚Äì the CEOs of the six stakeholders,
the Assistant Deputy Minister responsible for Corporate Programs at Manitoba Health and the
CIO of the Manitoba eHealth Program.

The Program is administratively housed within the Winnipeg Regional Health Authority
(WRHA), and is subject to all WRHA policies and processes. The CIO has a direct reporting line
to the WRHA through its Chief Operating Officer, as well as to the Deputy Minister through
the Council.




37
   MHHL = Manitoba Health and Healthy Living; WRHA = Winnipeg Regional Health Authority; CCMB = CancerCare
Manitoba; DSM = Diagnostic Services Manitoba; HA = Health Authorities



                                                     Privacy and EHR Information Flows in Canada             41
Advisory committees

Telehealth and EHR Services
The developing Telehealth and EHR Services portfolio is being supported by three separate
advisory committees: Registry Integrity Unit Advisory Committee; Primary Care Information
System Advisory Committee; and the MBTelehealth Provincial Strategy and Investment
Committee. In each committee stakeholders assist with input into key program decisions and
ensure that services being developed are relevant to the key program users.

eHealth Clinician & Research Committees
           ‚Ä¢ eHealth Clinical Advisory Committee
             Mandate: to provide advice and counsel to Manitoba eHealth regarding clinical
             applications
             Membership: 40 member group including 20 physicians
           ‚Ä¢ eHealth Research Advisory Committee
             Mandate: to advise Manitoba eHealth regarding access to health care information
             for
             secondary use
             Membership: 16 member group including representatives from the University of
             Manitoba and the MCHP
           ‚Ä¢ Clinical Design Team
             Mandate: to provide input into the clinical content of the EPR including
             Order Sets and Alerts
             Membership: diverse
           ‚Ä¢ Physician Champions
             Mandate: to provide local expertise and liaise with respective institutions and
             programs
             Membership: 3 physicians
           ‚Ä¢ Core Clinical Design Team
             Mandate: to establish regional principles regarding clinical content in the EPR
              application.
              Chair: Manager, Nursing Informatics Strategy, Manitoba eHealth

Privacy and security
The Privacy and Security Advisory Committee is responsible for providing recommendations on
privacy and security matters related to the provincial iEHR and other Manitoba eHealth
provincial projects.

The intent of the Provincial Privacy and Security Advisory Committee is to support the
appropriate sharing of personal health information where and when required in a manner that
protects the confidentiality, privacy, security and integrity of that information in accordance with
The Personal Health Information Act (PHIA).




                                                 Privacy and EHR Information Flows in Canada       42
The membership includes privacy officers representing urban and rural regional health authorities,
the Director of the provincial Information Protection Centre, an internal auditor, a clinician, a
representative from Manitoba Health. A representative from the provincial Ombudsman‚Äôs Office
attends meetings as an observer.

Standards
Two committees govern standards: The Provincial Information Standards Committee and the
Provincial Architecture Management Committee.

The Health Information Standards Committee (HISC-MB) provides strategic and tactical
direction related to various aspects of the Information Standards Lifecycle including validating
and maintaining the Enterprise Information Model, identifying and managing the health
information standards policies and establishing and communicating health information standards
priorities, process and procedures.

The Architecture Management Committee (AMC) is responsible for providing the architecture
direction for Manitoba eHealth Provincial Projects, RHA projects, and the EHR initiative. The
AMC will establish best practices, principles, and standards in the areas of application,
information, security, and technology architectures through the creation and maintenance of the
Enterprise Architecture (EA).

Manitoba eHealth Program Oversight Committee
The Program is supported directly by MHHL and the Manitoba Government through a separate
operating funding envelope. Its overall mandate is subject to review by an Oversight Committee,
reporting directly to the Minister of Health, and is chaired by the Deputy Minister. As well as the
Deputy, the Committee is composed of the Deputy Minister of Science, Technology, Energy and
Mines, the Chair of the Program Council as well as one member from the Council. The
Committee meets twice per year to review the progress and status of the program as well as to
review its plans for the future.




                                                Privacy and EHR Information Flows in Canada       43
ALBERTA (2010)
PROVINCIAL IM/IT GOVERNANCE STRUCTURE




                                                             Alberta Health       Alberta Health          Alberta Medical
                              Minister of Health               & Wellness         Services Board               Assn.




                                                                                    Tri-Lateral
                                                                                    Agreement
                                                                                     Structure


  Medication Domain
       Steering
                             Health Information
  Committee (DRAFT)         Executive Committee
                                                                        POSP                         PCI
                                                                      Committee                   Committee

     DI Steering
     Committee



                                                   Operations Mgmt                                  EHRDSC
    SHR Steering                 EHR Sponsors                            IM/IT Strategy
     Committee                    Committee          Committee             Committee
                                                       (DRAFT)



   Personal Health
   Portal Steering                                                   IT Governance Process Checklist
     Committee                                                       ÔÉº   Project Approval
                                                                     ÔÉº   Budgeting
                                                                     ÔÉº   Project Monitoring & Review
                                                                     ÔÉº   Policy & Standards Setting
                                                                     ÔÉº   Maintenance
                                                                     ÔÉº   Development
                                                                     ÔÉº   Talent Management
                                             Health IM/IT            ÔÉº   Vendor Management
       HISCA          Integrated Clinical                            ÔÉº   Service Performance Management
                        Working Group        Architecture            ÔÉº   Benefits Capture
                                            Working Group            ÔÉº   Change Management
                                                                     ÔÉº   Deployment




                                                Privacy and EHR Information Flows in Canada                   44
NEWFOUNDLAND AND LABRADOR EHR GOVERNANCE STRUCTURE
(AS OF JANUARY, 2010)




                                           Minister
                                     Department of Health
                                    and Community Services



                                       Board of Directors
                                      NL Centre for Health
                                          Information



                                              CEO
                                      NL Centre for Health
                                          Information



                                         Senior Director
                                      Clinical Information
                                            Programs
                                      NL Centre for Health
                                          Information




        Picture Archiving and        Pharmacy Network and            Client Registry
       Communications System           Provider Registry           Governance Advisory
         (PACS) Governance            Governance Advisory              Committee
         Advisory Committee               Committee




The Newfoundland and Labrador Centre for Health Information (the Centre) is a crown agency
with a legislated mandate to assist individuals, communities, health professionals and policy
makers in making informed decisions by providing a comprehensive province-wide information
system. In 2007, the Department of Health and Community Services gave the Centre the
mandate to govern the EHR.
The Centre is governed by a board of fifteen representatives appointed by Cabinet and reporting
to the Minister of Health and Community Services. The Board of Directors represents a balance
of the stakeholders in the EHR: the health system, health professionals and the community. The
CEO establishes committees to advise on the strategic and operational governance of specific
domains. Each committee has its own Terms of Reference and meets regularly to consider all
aspects relevant to their component of the EHR. The membership of these committees
represents the users of the specific domain.




                                              Privacy and EHR Information Flows in Canada     45
Appendix C:
Potential Options for the Structure and Roles of a Pan-Canadian Body
for EHR Privacy and Related Information Governance

Common understandings #32 and 33 relate to the need for a single integrated pan-Canadian
coordinating group to discuss, address and coordinate common privacy related information
governance issues as the EHR continues to be implemented across the country. The following
illustrates a range of potential structures and responsibilities for such a body that could be
considered by jurisdictions as a starting point for discussions.

Committee or sub-committee of the Conference of Deputy Ministers of Health
   ‚Ä¢ A pan-Canadian committee or sub-committee of the Conference of Deputy Ministers of
     Health (CDM) that includes a stakeholder advisory structure and makes recommendations
     for CDM approval. Examples of its potential responsibilities could include
          ‚Äì Serving as a resource to the CDM for matters related to EHR privacy and
            information governance issues
          ‚Äì Making recommendations towards the vision and processes for using EHR
            information from multiple jurisdictions for purposes other than care and treatment
            in a privacy-protective manner.
          ‚Äì Serving as a mediator as requested, on privacy and information governance issues
            that are national in scope or involve more than one jurisdiction.

Pan-Canadian committee with broad, formal stakeholder representation
   ‚Ä¢ A pan-Canadian committee with terms of reference, a degree of secretariat support, and
     broad, formal stakeholder representation (e.g., such as an advisory council linked to the
     committee or the inclusion on the committee of representatives from clinical, IT and other
     relevant domains), that provides information and updates to jurisdictions and considers
     jurisdictional suggestions regarding issues to explore or consider. It could, for example:
          ‚Äì Contribute to national and international standards initiatives that involve or have
              implications for privacy and the protection of personal health information
          ‚Äì Manage the creation of information materials that describe the interoperable EHR
              system for public information at the pan-Canadian level

New, independent organization
   ‚Ä¢ A new organization established by MOU or in legislation to manage privacy and related
     information governance issues on behalf of jurisdictions, and to report to the Council of
     Ministers of Health.




                                              Privacy and EHR Information Flows in Canada        46
Pan-Canadian Committee
   ‚Ä¢ A pan-Canadian ministry committee with terms of reference and a degree of secretariat
     support, that meets regularly to share ideas, invite knowledge from stakeholders and
     consider common approaches to issues. The current HIP Group fits this category.
     Examples of its responsibilities could include:
         ‚Ä¢ Coordinating jurisdictional input:
              ‚Äì Into privacy reviews by oversight bodies that involve more than one
                 jurisdiction
              ‚Äì To respond to questions and issues on pan-Canadian issues.
         ‚Ä¢ Maintaining and further developing pan-Canadian common understandings on
            privacy-related information governance matters that are national in scope or that
            cross jurisdictional boundaries/involve more than one jurisdiction.
         ‚Ä¢ Serving as a resource for jurisdictional EHR information governance structures.

Network of interested representatives
   ‚Ä¢ An informal network of interested ministry representatives and/or representatives from
     jurisdictional privacy and information governance mechanisms that communicates and
     shares ideas and solutions on an ad hoc basis, sharing information, for example, on:
          ‚Äì Emerging privacy related information governance issues
          ‚Äì Relevant new technologies and tools relating to EHR privacy
          ‚Äì Privacy and security breaches that involve more than one jurisdiction.




                                              Privacy and EHR Information Flows in Canada       47
Appendix D:
List of HIP Group Members 2008-2010

Joan Roch, Co-Chair (2008-2010)                    Sara Smallwood (from 2010)
Chief Privacy Strategist,                          Chief Privacy Officer, Corporate Privacy
Canada Health Infoway                              Office, New Brunswick Department
                                                   of Health
Lucy McDonald, Co-Chair (2008-2010)
Chief Privacy Officer/Corporate Secretary,         Diane Bois (2008-2010)
Newfoundland & Labrador Centre for                 Responsable et coordonnatrice des affaires
Health Information                                 juridiques, Minist√®re de la sant√© et des
                                                   services sociaux de Qu√©bec
Wendy Robillard , Co-Chair (from 2010)
Member from 2008-2010                              Carol Appathurai (2008-2009)
Senior Manager, Information, Compliance            Director, PHIPA Review Project, Ontario
and Access Unit, Alberta Health & Wellness         Ministry of Health and Long Term Care
                                                   Alison Blair (2009-2010)
Jurisdictional Members
                                                   Director, Information Management Strategy
Dave Morgan (from 2010)                            and Policy Branch, Ontario Ministry of
Privacy Officer ‚Äì Secondary Uses,                  Health and Long Term Care
Newfoundland & Labrador Centre for
Health Information                                 Karen Waite (2009-2010)
                                                   Chief Privacy and Security Officer,
Michelle MacDonald (2008-2010)                     eHealth Ontario
A/Manager, Privacy and Health
Information, Nova Scotia Department of             Heather McLaren (2008-2010)
Health Health Promotion & Protection               Chief Privacy and Risk Officer,
                                                   Manitoba e-Health
Marina Fay (2008-2010)
Privacy & Access Coordinator / Health              Christine E. Underwood (2008-2010)
Information Specialist, Corporate Services,        Manager, IT/IM Privacy/Data Access Unit,
Prince Edward Island Department of Health          Saskatchewan Ministry of Health

Fran White (2008-2009)                             Brenda Hudey (from 2010)
Chief Privacy Officer, Corporate Privacy           Health Information Solutions Centre,
Office, New Brunswick Department                   Saskatchewan Ministry of Health
of Health                                          Deborah McGinnis (2008-2010)
Andrea MacKenzie (2009-2010)                       A/Executive Director, eHealth Privacy
Senior Policy Advisor, Corporate Privacy           Security and Legislation Office,
Office, New Brunswick Department                   British Columbia Ministry of Health
of Health                                          Lynda Ehrlich (2008-2010)
                                                   Senior Policy Analyst, Policy & Program
                                                   Development, Yukon Health and Social
                                                   Services




                                              Privacy and EHR Information Flows in Canada       48
Lisa Cardinal (2008-2010)
Director, Policy, Planning & Evaluation,
Northwest Territories Department of Health
and Social Services
Martin Joy (from 2010)
A/Director Health Information, Nunavut
Department of Health & Social Services
Philippe Tousignant (2008-2010)
Director, Access to Information & Privacy
Division, Health Canada
Tina McKinnon
Director, Health Information, Nunavut
Department of Health & Social Services
(2008-2009);
Senior Policy Advisor, Access to
Information and Privacy, Health Canada
(2009-2010)




                                             Privacy and EHR Information Flows in Canada   49
Ex officio members
Mimi Lepage
Chief Privacy Officer & General Counsel,
Canadian Institute for Health Information
Louis Barr√©
Vice President, Strategy, Planning and
Outreach, Canadian Institute for Health
Information

Canada Health Infoway
Stanley Ratajczak
Group Director, Security & Privacy
Architecture
Agnes Wong
Director, Professional Practice & Clinical
Informatics
Brian Foran
Privacy Specialist
Lorri MacKay
Research Specialist
Christina Northcott
Events Planner, Corporate Affairs
Jeannie O‚ÄôRegan
Director, Conferences and Events
Shannon Byck
Manager, Conferences and Events




                                             Privacy and EHR Information Flows in Canada   50
Appendix E:
List of Presentations

EHR Blueprint Version 2
http://www2.infoway-inforoute.ca/Documents/EHRS-Blueprint-v2-Exec-Overview.pdf
Ron Parker & Stanley Ratajczak, Canada Health Infoway

Electronic Health Information and Privacy Survey: What Canadians Think
http://www2.infoway-inforoute.ca/Documents/EKOS_Final%20report_EN.pdf
Mary Lysyk, Health Canada

Conceptual Privacy Impact Assessment (PIA) on Canada‚Äôs Electronic Health Record Solution
Blueprint Version 2
http://www2.infoway-inforoute.ca/Documents/CHI_625_PIA_rj13.pdf
Joan Roch, Canada Health Infoway

The Canadian Health Information Management Association (CHIMA)
Marci MacDonald, Halton Healthcare Services

The De-identification of Personal Health Information
Related research and presentations available at
http://www.ehealthinformation.ca/index.asp
Khaled El Emam, University of Ottawa

Panorama: The Pan Canadian Public Health Communicable Disease
Surveillance and Management Project
Krystyna Hommen, BC Ministry of Health Services

The Quebec Health Record
Diane Bois, Minist√®re de la sant√© et des services sociaux de Qu√©bec

CIO Forum
Mike Barron, Newfoundland and Labrador Centre for Health Information

Jurisdictional Scan of Patient Notices
Inventory of Health Information Governance Terminology
Jurisdictional EHR Governance Approaches
Don MacPherson, Anzen Consulting; Ross Fraser, Sextant Software

Jurisdictional Consent Models and Functionality
Ross Fraser, Sextant Software; Stanley Ratajczak, Canada Health Infoway

Infoway‚Äôs iEHR Tech Project II Consent Track: Architecture and standards for Consent Directives
Ross Fraser, Sextant Software




                                                  Privacy and EHR Information Flows in Canada     51
Saskatchewan‚Äôs Patient Notification Work
Christine Underwood, Saskatchewan Ministry of Health

Health Data Warehouse and Secondary Uses:
Newfoundland and Labrador Centre for Health Information
Lucy McDonald, NLCHI; Linda Weaver & Daniel Desch√™nes, Emergis

Review of Selected Provincial Health Policy Centres
Michelle MacDonald & Suellen Murray, N.S. Department of Health

De-identification Process for Secondary Use
Related research and presentations available at http://www.ehealthinformation.ca/index.asp
Khaled El Emam, University of Ottawa

Consumer Health Solutions: Infoway Pre-Implementation Certification Initiative
Shelagh Maloney, Canada Health Infoway

Pan-Canadian iEHR & Privacy: Data Sharing of Personal Health Information
LCol Jim Kirkland, Federal Healthcare Partnership

Personal Health Records
Dennis Giokas, Canada Health Infoway

CIHI Health System Use Project
Louis Barre, Canadian Institute for Health Information

De-identification Tools Report
Ross Fraser, Sextant Software; Don Willison, McMaster University
http://www2.infoway-inforoute.ca/Documents/Tools_for_De-identification_EN_FINAL.pdf


Use of Data from the Electronic Health Record for Health Research
Don Willison, McMaster University

Information Sharing Agreements inventory work
Elaine Sawatsky
http://www2.infoway-inforoute.ca/Documents/ISA_report_for_HIP_Group_January_2010_EN_Final.pdf


Public Engagement Strategies
Peter MacLeod, MASS LPB

Breach Protocols in Alberta
Wendy Robillard, Alberta Health and Wellness




                                                    Privacy and EHR Information Flows in Canada   52
 CIRCLE OF
  CARE
Sharing Personal Health Information
     for Health-Care Purposes




                     Ann Cavoukian, Ph.D.
                     Information and Privacy Commissioner,
                     Ontario, Canada
THE Information and Privacy Commissioner of Ontario, canada
would like to thank the following organizations for their
participation in this brochure:

College of Physicians and Surgeons of Ontario
Ontario Association of Community Care Access Centres
Ontario Association of Non-Profit Homes and Services for Seniors
Ontario Hospital Association
Ontario Long Term Care Association
Ontario Medical Association
Ontario Ministry of Health and Long-Term Care
The term ‚Äúcircle of care‚Äù is not a defined term in the
Personal Health Information Protection Act, 2004 (PHIPA).
It is a term commonly used to describe the ability of certain
health information custodians to assume an individual‚Äôs
implied consent to collect, use or disclose personal health
information for the purpose of providing health care, in
circumstances defined in PHIPA.

The purpose of this brochure is to clarify the circumstances in which a

health information custodian may assume implied consent and the options

available to a health information custodian where consent cannot be

assumed to be implied. Throughout the brochure, appropriate application

of the assumed implied consent provisions of PHIPA will be illustrated using

a variety of health-care scenarios involving a fictional 61-year-old gentleman

named David Mann. It should be noted that the assumed implied consent

provisions of PHIPA apply equally to paper-based and electronic records of

personal health information.




    In an appointment with his family physician, David Mann complains of memory
    loss, disorientation, speech problems and mood swings.
    The family physician examines David and asks him a series of questions relating
    to his medications, his health history and the health history of his family. The
    family physician also conducts a mini-mental state examination and provides
    David with a requisition for blood and urine testing and for magnetic resonance
    imaging. The family physician indicates that she will refer David to both a
    neurologist and geriatrician for further assessments.
Circumstances When you may assume
       Consent to be Implied
        A health information custodian may only assume an
individual‚Äôs implied consent to collect, use or disclose personal health
  information if all of the following six (6) conditions are satisfied.
                                                                                        1
                                          1
          The health information custodian must fall within a
           category of health information custodians that are
              entitled to rely on assumed implied consent.

Most health information custodians may rely on assumed implied consent to collect,
use and disclose personal health information for the purpose of providing health care
or assisting in the provision of health care to an individual.

A health information custodian is a person or organization described in PHIPA with
custody or control of personal health information as a result of, or in connection
with, the performance of its powers, duties or work. For example, health information
custodians include:

‚úö   health care practitioners
‚úö   long-term care homes
‚úö   community care access centres
‚úö   hospitals, including psychiatric facilities
‚úö   specimen collection centres, laboratories, independent health facilities		
‚úö   pharmacies
‚úö   ambulance services
‚úö   Ontario Agency for Health Protection and Promotion

However, it is important to note that some health information custodians are not
entitled to rely on assumed implied consent. For example, these include:

‚úö   an evaluator within the meaning of the Health Care Consent Act, 1996
‚úö   an assessor within the meaning of the Substitute Decisions Act, 1992
‚úö   the Minister or Ministry of Health and Long-Term Care
‚úö   the Minister or Ministry of Health Promotion
‚úö   the Canadian Blood Services
                                       2




                                                                                            2
         The personal health information to be collected, used or
         disclosed by the health information custodian must have
          been received from the individual, his or her substitute
        decision-maker or another health information custodian.

The personal health information to be collected, used or disclosed must have been
received from the individual to whom the personal health information relates, from
his or her substitute decision-maker or from another health information custodian.

Personal health information is defined in PHIPA as identifying information relating
to the physical or mental health of an individual, the provision of health care to an
individual, the identification of the substitute decision-maker for the individual and
the payments or eligibility of an individual for health care or coverage for health care,
including the individual‚Äôs health number.

A substitute decision-maker is a person authorized under PHIPA to consent on behalf
of an individual to the collection, use or disclosure of personal health information.

If the personal health information to be collected, used or disclosed was received
from a third party, other than the substitute decision-maker for the individual or
another health information custodian, consent cannot be assumed to be implied. For
example, a health information custodian may not rely on assumed implied consent if
the personal health information was received from an employer, insurer or educational
institution.
David‚Äôs family physician provides the neurologist and geriatrician with a referral
letter summarizing David‚Äôs symptoms, health history, and family health history,
along with the results of his examination.
Can the family physician disclose and can the neurologist and geriatrician collect
this personal health information based on assumed implied consent?
Yes. The family physician, neurologist and geriatrician may assume implied
consent. The family physician received the personal health information directly
from David and the neurologist and geriatrician received the information directly
from another health information custodian, the family physician, for the purpose
of providing health care to David.
                                       3
       The health information custodian must have received the
        personal health information that is being collected, used




                                                                                        3
       or disclosed for the purpose of providing or assisting in the
                provision of health care to the individual.

The personal health information to be collected, used or disclosed must have been
received for the purpose of providing health care or assisting in the provision of
health care to the individual to whom it relates. A health information custodian may
not rely on assumed implied consent if the personal health information was received
for other purposes, such as research, fundraising, marketing or providing health care
or assisting in providing health care to another individual or group of individuals.
The geriatrician to whom the referral is made is a co-investigator in a research
study involving familial predisposition to Alzheimer‚Äôs disease. In the course of
the research study, while reviewing the list of study participants, the geriatrician
notices the name ‚ÄúDavid Mann.‚Äù The geriatrician reviews the research file of
David Mann and determines, based on a comparison with the information
contained in the referral letter, that it is the same David Mann.
The geriatrician photocopies the records of personal health information contained
in the research file and places them in the clinical file for use at an appointment
with David scheduled for November 13.
Can the geriatrician use the personal health information in this way based on
assumed implied consent?
No. The geriatrician may not assume implied consent because the personal health
information in the research file was not received for the purpose of providing
health care or assisting in the provision of health care to David, but rather, for
research purposes.




Following the appointment with David on November 13, the geriatrician would
like to contact the laboratory for the results of the blood and urine testing ordered
by David‚Äôs family physician. The geriatrician would also like to contact the
pharmacy where David indicated he routinely fills his prescriptions in order to
obtain a list of all current medications.
Can the laboratory and pharmacy disclose and can the geriatrician collect this
personal health information based on assumed implied consent?
Yes. The laboratory, pharmacy and geriatrician may assume implied consent. The
personal health information was received by the laboratory and pharmacy, and will
be received by the geriatrician, for the purpose of providing health care to David.
                                      4
       The purpose of the collection, use or disclosure of personal
        health information by the health information custodian
       must be for the provision of health care or assisting in the
               provision of health care to the individual.

The collection, use or disclosure must be for the purposes of providing health care or




                                                                                          4
assisting in the provision of health care to the individual to whom the personal health
information relates. A health information custodian may not rely on assumed implied
consent if the collection, use or disclosure is for other purposes, such as research,
fundraising, marketing or providing health care or assisting in the provision of health
care to another individual or group of individuals.
Several years pass and David‚Äôs cognitive abilities continue to decline. Based on a
diagnosis of probable Alzheimer‚Äôs disease and the growing loss of David‚Äôs functional
abilities, David‚Äôs geriatrician makes a referral to the local Community Care Access
Centre. For purposes of assessing David‚Äôs eligibility and service levels, the case
manager at the local Community Care Access Centre contacts David‚Äôs family
physician to obtain further information about David‚Äôs health history, current
medications and treatment.
Can the Community Care Access Centre collect and can the family physician
disclose this personal health information based on assumed implied consent?
Yes. The Community Care Access Centre is collecting this personal health
information and the family physician is disclosing this personal health information
for the purpose of providing health care or assisting in the provision of health
care to David.
Ultimately, the local Community Care Access Centre facilitates the placement of
David into a long-term care home.
One morning, following breakfast at the long-term care home, David falls and
is transferred to the hospital by ambulance with a suspected hip fracture.
The next day David‚Äôs former spouse, a nurse in the labour and delivery unit of
the hospital, is advised by their son that David was admitted. The nurse looks
at David‚Äôs electronic health record to determine the reason for admission. The
nurse signed a confidentiality agreement with the hospital.
Can the nurse use the personal health information in this way based on assumed
implied consent?
No. The nurse may not assume implied consent to use the personal health
information because she is not providing health care or assisting in the provision
of health care to David.
Following a physical examination and X-ray, it is confirmed that David has a
hip fracture and David undergoes a surgical procedure. A week later, David is
discharged from hospital and returns to the long-term care home.
Two days following discharge, a nurse at the long-term care home notices
small red, swollen and pus-filled bumps on David‚Äôs skin. David also complains
of fever, chills and shortness of breath. Following laboratory testing, David is
diagnosed with MRSA infection. Since the infection may have been acquired at
the hospital, the nurse would like to disclose the fact that David has MRSA to the
hospital to prevent or reduce the risk of a possible outbreak.
Can this personal health information be disclosed to the hospital by the nurse at
the long-term care home?
Yes. PHIPA permits a health information custodian to disclose personal health
information without consent if there are reasonable grounds to believe that it
is necessary to eliminate or reduce a significant risk of serious bodily harm to
a person or group of persons. The nurse, however, may not rely on assumed
implied consent because the disclosure is not for the purposes of providing health
care or assisting in providing health care to David.
                                        5
         In the context of disclosure, the disclosure of personal
        health information by the health information custodian
           must be to another health information custodian.

A health information custodian may not assume an individual‚Äôs implied consent in
disclosing personal health information to a person or organization that is not a health
information custodian, regardless of the purpose of the disclosure.




     David is planning to attend an outing away from the long-term care home and will
     be accompanied by his cousin and the spouse of his cousin.
     On the Wednesday prior to the outing, the spouse of David‚Äôs cousin contacts the

                                                                                          5
     long-term care home. She would like information about the medications David
     is currently taking, including the frequency and dose, and ‚Äúany other information
     about his condition‚Äù that will assist her in ‚Äúhelping David.‚Äù
     Can the long-term care home disclose this personal health information based on
     assumed implied consent?
     No. The long-term care home may not assume implied consent because the spouse
     of David‚Äôs cousin is not a health information custodian within the meaning of
     PHIPA.
                                      6
           The health information custodian that receives the
         personal health information must not be aware that the
        individual has expressly withheld or withdrawn his or her
               consent to the collection, use or disclosure.

PHIPA permits an individual to expressly withhold or withdraw consent to the
collection, use or disclosure of his or her personal health information, unless the
collection, use or disclosure is permitted or required by PHIPA to be made without
consent. In most circumstances, if an individual decides to withhold or withdraw
consent, PHIPA requires the receiving health information custodians or their agents to
be notified if the disclosing health information custodian is prevented from disclosing
all of the information that is considered to be reasonably necessary for the provision
of health care.

For further information about the ability of an individual to expressly withhold or
withdraw consent to the collection, use or disclosure of personal health information
for health-care purposes, and the obligations on health information custodians in this
context, please refer to the Lock-box Fact Sheet produced by the Information and
Privacy Commissioner of Ontario, which is available at www.ipc.on.ca.
                                                                                          6
David must visit the orthopedic clinic of the hospital for follow up related to his
hip fracture. The orthopedic clinic is staffed by physiotherapists, occupational
therapists, physicians and nurses.
David‚Äôs current spouse, who is his substitute decision-maker, learns that his
former spouse, who was a nurse in the labour and delivery unit of the hospital,
now works as a nurse in the orthopedic clinic. David‚Äôs current spouse wants to
ensure that the former spouse and her colleagues do not view David‚Äôs electronic
health record. David‚Äôs current spouse requests the hospital to ensure that only the
orthopedic surgeon and the physiotherapist providing health care to David are
permitted to view his electronic health record.
Can David‚Äôs current spouse make this request?
Yes. David has been determined to be incapable of consenting to the collection,
use and disclosure of personal health information and his current spouse is
his substitute decision-maker for these purposes. As the substitute decision-
maker, David‚Äôs current spouse may expressly withhold or withdraw consent to
the collection, use and disclosure of David‚Äôs personal health information. The
hospital, as a health information custodian, must comply with this decision
unless the collection, use or disclosure is required or permitted by PHIPA to be
made without consent.
                                                7
Factors to be Considered in Relying on Assumed Implied Consent

In general, a health information custodian must not collect, use or disclose personal
health information if other information will serve the purpose and must not collect,
use or disclose more personal health information than is reasonably necessary for
that purpose. These general limiting principles apply even where a health information
custodian is entitled to rely on an individual‚Äôs assumed implied consent.

Options Available When you Cannot Assume consent to be Implied

When consent cannot be assumed to be implied, health information custodians
should consider other options. Depending on the circumstances, a health information
custodian may be permitted to collect, use or disclose personal health information
without consent, with the implied consent of the individual to whom the personal
health information relates or with the express consent of that individual. PHIPA
distinguishes between implied consent and assumed implied consent. In the case of
implied consent, health information custodians must ensure that all of the elements
of consent are fulfilled; whereas in the case of assumed implied consent, health
information custodians may assume that all of the elements of consent are fulfilled,
unless it is not reasonable to do so in the circumstances.

Without Consent
Health information custodians may collect, use or disclose personal health information
without consent if the collection, use or disclosure is permitted or required by PHIPA to
be made without consent1. For example, health information custodians are permitted
to disclose personal health information without consent to a medical officer of health
if the disclosure is made for purposes of the Health Protection and Promotion Act. In
addition, in certain circumstances set out in sections 37(1)(a), 38(1)(a) and 50(1)(e) of



1	Sections 36 and 37 of PHIPA, respectively, set out the circumstances in which personal health information
                                                                                                               7




   may be collected and used without consent and sections 38 - 48 and section 50 set out the circumstances
   in which personal health information is permitted or required to be disclosed without consent.
PHIPA, health information custodians may use or disclose personal health information
without consent where it is reasonably necessary for the provision of health care and
the individual has not expressly instructed otherwise.

Implied Consent
Health information custodians may imply an individual‚Äôs consent to collect and use
personal health information for most purposes. They may also imply consent to
disclose personal health information to another health information custodian for the
purpose of providing or assisting in the provision of health care to the individual.
However, subject to limited exceptions, health information custodians cannot
rely on implied consent when disclosing personal health information to a person
or organization that is not a health information custodian. This exception applies
regardless of the purpose of the disclosure.

In order to rely on implied consent, health information custodians must be satisfied
that all the required elements of consent are fulfilled.

Express Consent
In all other circumstances, health information custodians may only collect, use or
disclose personal health information with the express consent, (i.e., verbal or written
consent) of the individual to whom the personal health information relates or his or
her substitute decision-maker.

In order to rely on express consent, health information custodians must be satisfied
that all of the required elements of consent are fulfilled.
Elements of Consent

The consent of an individual for the collection, use or disclosure of personal
health information by a health information custodian:

‚úö	Must be a consent of the individual or his or her substitute decision-
    maker;
‚úö Must be knowledgeable;
‚úö	Must relate to the information that will be collected, used or disclosed;
    and
‚úö   Must not be obtained through deception or coercion.

For consent to be knowledgeable, it must be reasonable to believe that
the individual knows the purpose of the collection, use or disclosure and
knows that he or she may give or withhold consent.

It is reasonable to believe that an individual knows the purpose of the
collection, use or disclosure if the health information custodian posts or
makes readily available a notice describing these purposes where it is
likely to come to the individual‚Äôs attention or provides the individual with
such a notice. Although health information custodians are not required
to provide notice in those circumstances where consent may be assumed
to be implied, health information custodians are encouraged to do so as
a best practice.
                                                                 The Commissioner would like to gratefully acknowledge the excellent contribution of
                                                                 Manuela Di Re, Health Law Legal Counsel and Debra Grant, Senior Health Specialist,
                                                                 Office of the Information and Privacy Commissioner of Ontario, Canada, in the
                                                                 preparation of this paper.
PRINT: Clockwork Productions Inc.
design: Bus Stop Design + Communications www.busstopdesign.com




                                                                 Information and Privacy Commissioner, Ontario, Canada
                                                                 2 Bloor Street East, Suite 1400
                                                                 Toronto, Ontario M4W 1A8
                                                                 Tel: 416 326 3333
                                                                 1 800 387 0073
                                                                 Fax: 416 325 9195
                                                                 TTY: 416 325 7539
                                                                 www.ipc.on.ca

                                                                 September 2, 2009
CONSENT & CAPACITY UNDER THE PERSONAL
HEALTH INFORMATION PROTECTION ACT, 2004
(PHIPA), IN THE CONTEXT OF THE LONG TERM
      CARE HOMES ACT, 2007 (LTCHA)


                        PHIPA Summit, 2010
                      November 28-
                               28-29, 2010

                        Lise Hendlisz, Counsel
                Ministry of Health and Long Term Care
                       Lise.hendlisz@ontario.ca

 The contents of this presentation should not be construed as legal advice. For any issues
       of interpretation of PHIPA or the LTCHA please consult your legal counsel.
                 TO WHOM AND WHAT DOES
                      PHIPA APPLY?

 Primarily, to Health Information Custodians (Custodians) that
        collect, use, or disclose (C/U/D) personal health information
        (PHI), and to their Agents.*

 It also applies, to some extent, to non-Custodians,when they
        receive PHI from a Custodian: these are called ‚ÄúRecipients‚Äù
        under PHIPA.

* All underlined terms are defined in PHIPA




                                                                        2
                 WHO ARE HEALTH INFORMATION
                       CUSTODIANS?
      Health care practitioners*                          A service provider under the Long-Term
                                                            Care Act*
      A person who operates a:
        ‚Ä¢ Hospital or independent health facility*         A Community Care Access Corporation*
        ‚Ä¢ A long-term care home within the
             meaning of the Long-Term Care                 Medical officers or Boards of Health*
             Homes Act, 2007, a placement co-
             ordinator described in subsection 40(1)       Evaluators or Assessors under the Health
             of that Act, or a care home within the         Care Consent Act or the Substitute
             meaning of the Residential Tenancies           Decisions Act
             Act *
        ‚Ä¢ Pharmacy*                                        Minister/Ministry of Health and Long-
        ‚Ä¢ Laboratory*                                       Term Care
        ‚Ä¢ Ambulance service*
        ‚Ä¢ Home for special care*                           Any other persons ‚Äúprescribed‚Äù under the
        ‚Ä¢ A centre, program or service for                  regulations (eg. Ministry of Health
             community health or mental health              Promotion, Ontario Agency for Health
             whose primary purpose is the provision         Protection and Promotion)
             of health care*

*Custodians who can rely on ‚Äúassumed implied consent‚Äù


                                                                                                       3
                    WHO ARE AGENTS?
 ‚ÄúAgent‚Äù means a person who acts for or on behalf of a Custodian, and
    not for the Agent‚Äôs own purposes. It includes persons who aren‚Äôt
    employees, and those who aren‚Äôt paid for their services - - if they are
    given access to PHI in Custodian‚Äôs custody or control, for the
    Custodian‚Äôs purposes;

 Ministry of Health and Long Term Care (MOHLTC) is a HIC; MOHLTC
    employees are its PHIPA Agents;

 LTCH inspectors, and the ‚ÄúDirector‚Äù appointed under the LTCHA, are
    PHIPA agents of the MOHLTC;

 The flow of PHI between /among Agents and their Custodian is not a
    disclosure of PHI; it‚Äôs a use (e.g. when PHI is discussed/transferred
    between an LTCH inspector and the Director it‚Äôs a ‚Äúuse‚Äù, not a
    ‚Äúdisclosure‚Äù of PHI).

                                                                              4
             WHAT IS PERSONAL HEALTH
                  INFORMATION?
   PHI is defined very broadly in the Act. It means ‚Äúidentifying information about an individual in
    oral or recorded form‚Äù that, (for example):

     ‚óè    relates to an individual‚Äôs physical or mental health (includes family health history);
     ‚óè    relates to the provision of health care to the individual, including the identification of a
          person as a provider of health care to the individual;
     ‚óè    relates to individual‚Äôs payments for health care or eligibility for OHIP or ODB;
     ‚óè    is a plan of service within the meaning of the Home Care and Community Service Act;
     ‚óè    relates to the donation of a body part or bodily substance;
     ‚óè    is a health number;
     ‚óè    Identifies an individual‚Äôs substitute decision maker (SDM)

   Mixed Record Rule: expands the definition of PHI to include all identifying information in a
    record that also contains information of the type listed above. For example, a resident‚Äôs file
    may be mixed because it not only contains her PHI, but also includes the name, address and
    email address of her friend who is her ‚Äúemergency contact‚Äù. The entire record is considered
    PHI and it is the resident‚Äôs record of PHI- - even though it also contains friend‚Äôs personal
    information.



                                                                                                    5
  WHAT IS IDENTIFYING INFORMATION?

 Information that clearly identifies Mrs. X as a resident of Home Y;
   and

 Information that could reasonably be used, either alone or in
   combination with other information to identify Mrs. X. For
   example, a record that does not contain Mrs. X‚Äôs name, but
   describes a incident involving her, that occurred at Home Y, is
   nevertheless ‚Äúidentifying information‚Äù, if it‚Äôs a high profile incident
   that was reported in the media. The record, together with the
   media reports, could easily be combined to reveal/deduce Mrs.
   X‚Äôs identity. Therefore, a reader would know that all the
   information in the record about the incident relates to Mrs. X.




                                                                             6
          WHAT DOES CAPACITY MEAN
                UNDER PHIPA?
   PHIPA refers to ‚Äúcapacity‚Äù only for the purpose of consenting the C/U/D of PHI;
    capacity to consent to other transactions, or for other purposes, is beyond the
    scope of PHIPA;

   PHIPA assumes that individuals are capable of consenting, regardless of their age;

   An individual‚Äôs age does not determine capacity for PHIPA purposes;

   Section 21(1) establishes the PHIPA capacity test:

     ‚óè   An individual is capable of consenting to the C/U/D of PHI if the individual is
         able,

         (a)   to understand the information that is relevant to deciding whether to
               consent to the C/U/D, as the case may be; and
         (b)   to appreciate the reasonably foreseeable consequences of giving, not
               giving, withholding or withdrawing the consent.


                                                                                           7
     WHAT IS CONSENT UNDER PHIPA?
PHIPA refers to three types of consent: Express, Implied and Assumed Implied

Express and Implied Consent

    Where PHIPA requires that Custodian obtain individual‚Äôs consent to the C/U/D of their PHI,
     PHIPA provides that express or implied consent are equally valid.
     Exception: where PHIPA states that the consent must be express. (s.18(3): disclosure to a non-
     Custodian, or to a Custodian for non-health care purposes)

    An express consent may be oral or written; best practice is that it be written.

    An implied consent is one that is apparent from the individual‚Äôs actions, or the surrounding
     circumstances. (e.g. X provides PHI on application form in order to obtain a benefit or
     participate in a program; Y receives a Notice stating Custodian‚Äôs intention to disclose PHI for a
     given purpose unless individuals withdraw their consent to the disclosure, and Y does not
     withdraw her consent).

    When Custodian obtains individual‚Äôs consent, or receives a document purporting to record such
     consent, Custodian is ‚Äúentitled to assume‚Äù that consent is valid, unless it‚Äôs not reasonable to
     assume so. (i.e. Custodian has information indicating the individual no longer consents)




                                                                                                         8
                     NOTICE OF PURPOSE
   Consent (whether implied or express) must meet 4 conditions:
     1. be a consent of the individual;
     2. be knowledgeable;
     3. relate to the information; and
     4. not be obtained through deception or coercion.

   Consent is considered ‚Äúknowledgeable‚Äù under PHIPA if it‚Äôs reasonable in the
    circumstances to believe that the individual knows two things:
     1. The purpose(s) of the C/U/D; and
     2. that s/he can withhold their consent.

   Custodian can assume consent is ‚Äúknowledgeable‚Äù. (i.e. that individual is aware of these
    2 facts) if Custodian has ‚Äúposted or made readily available‚Äù a Notice explaining this
    information where it‚Äôs likely to come to the individual‚Äôs attention. (i.e. wall poster,
    brochure)

   Notice can be relied on for both express or implied consent. (e.g. include it in an
    application or a consent form that individuals must sign, or post it on office wall where
    they are sure to see it)

   A Notice isn‚Äôt required when PHIPA permits the C/U/D at issue without consent. (e.g.
    disclosure to the OPP, or pursuant to a summons/court order)

                                                                                                9
               ASSUMED IMPLIED CONSENT
Assumed Implied Consent is the third type of consent referred to in PHIPA;

    Can only be relied on by particular category of Custodians: generally speaking, Custodians who
     provide health care directly to individual, as described in ‚Äúparagraphs 1,2, 3 or 4 of the definition of
     health information custodian‚Äù. (See slide 3)

    Includes long term care homes.

    When this type of Custodian receives PHI about an individual from: the individual, the individual‚Äôs
     substitute decision maker or from any other Custodian ‚Äì and it‚Äôs for the purpose of providing health
     care to that individual ‚Äì the Custodian is ‚Äúentitled to assume‚Äù that it has the individual‚Äôs implied
     consent to collect the PHI at issue, AND use or disclose it ‚Äì for the purpose of providing health care
     or assisting in providing health care to the individual.

    Exception: if Custodian is ‚Äúaware that the individual has expressly withheld or withdrawn their
     consent‚Äù.

    Is a very significant provision that allows different Custodians involved in providing direct care to X,
     to exchange PHI about X without having to get X‚Äôs express or implied consent, and without having to
     give X a Notice of Purpose, as described in slide 9.

    CCACs, long term care homes and physicians can rely on this provision to disclose and share PHI
     about long term care home applicants/residents - - as long as purpose of disclosure is to provide or
     assist in providing health care to individual.


                                                                                                           10
              INDIVIDUAL HAS RIGHT OF
             ACCESS TO HIS/HER OWN PHI
   Every individual has a right of access to his/her record of PHI, unless a PHIPA exemption applies to
    that information;

   Disclosure of PHI to individual whose PHI is in the record is notionally based on consent: the
    individual is ‚Äúconsenting‚Äù to the Custodian‚Äôs disclosure of the PHI to him/herself;

   If a record is ‚Äúdedicated primarily to PHI about the individual‚Äù requesting access, the individual has a
    right to the whole record;

   However, if disclosure of A‚Äôs record of PHI will necessarily result in disclosure of sensitive PHI about
    B, consult with legal counsel on the issue;

   If a record is ‚ÄúNOT dedicated primarily to PHI about the individual‚Äù requesting access, s/he has a
    right only to the part of the record that contains his/her PHI, if it can be severed. (e.g. a record about
    an incident at a home may contain information about several residents. Requester can only get
    portion of record containing his/her PHI - - and no PHI about other residents);

   Whether a record is/is not ‚Äúdedicated primarily to‚Äù PHI about an individual can be difficult to
    determine, and very fact specific. Will have to use case by case approach. Consult with your legal
    counsel on this issue.


                                                                                                           11
              SUBSTITUTE DECISION
                MAKERS (SDMs)
 An SDM is a person authorized under PHIPA to consent on an
   individual‚Äôs behalf to the C/U/D of the individual‚Äôs PHI. An SDM
   ‚Äústands in the shoes of‚Äù the individual.

 PHIPA identifies who can be an SDM for:
    (a) a capable individual
    (b) a child
    (c) a deceased individual
    (d) an incapable individual

 Different persons are listed as possible SDMs for each of these 4
   categories.


                                                                      12
            WHO CAN ACT AS AN SDM?

It depends on who they are acting for:

1.   If acting for a Capable Individual?
     Any capable individual who is 16 or over, and who has been authorized in writing
     by the individual to act on his/her behalf. Mr. A can authorize another individual,
     Mr. B, to act on Mr. A‚Äôs behalf - - as long as Mr. B is 16 or over, and capable.
     Under PHIPA, B can act as A‚Äôs SDM, for the purpose of consenting to the C/U/D of
     A‚Äôs PHI.

2.   If acting for a child?
     A parent who has custody (not just access rights) of the child

3.   If acting for a deceased individual?
     The individual‚Äôs estate trustee, or, if no trustee, then the person who has assumed
     responsibility for the administration of the deceased‚Äôs estate.




                                                                                       13
               WHO CAN ACT AS SDM FOR AN
                 INCAPABLE INDIVIDUAL?
     A person who is already an SDM within the meaning of:

      1.     s. 9 of the Health Care Consent Act, 1996 if the purpose of the C/U/D is related to a decision about a
             treatment under Part II of that Act;
      2.     s. 39 of the Health Care Consent Act, 1996 if the purpose of the C/U/D is related to a decision about
             admission to a care facility under Part III of that Act; or
      3.     s. 56 of the Health Care Consent Act, 1996 if the purpose of the C/U/D is related to a decision about a
             personal assistance service under Part IV of that Act.

If consent is not related to such a precise decision, or if no Health care Consent Act SDM appointed, then:

     A person described in s. 26 of PHIPA, ranked in that order:

      1.     The individual‚Äôs guardian of the person or of property;
      2.     The individual‚Äôs attorney for personal care or for property;
      3.     The individual‚Äôs representative appointed by the Consent and Capacity Board;
      4.     The individual‚Äôs spouse or partner;
      5.     A child or parent of the individual, or a children‚Äôs aid society or other person who is lawfully entitled
             to give or refuse consent in the place of the parent. Does not include a parent who has only a right of access
             to the individual. If a children‚Äôs aid society or other person is lawfully entitled to consent in the place
             of the parent, this paragraph does not include the parent.
      6.     A parent of the individual with only a right of access to the individual.
      7.     A brother or sister of the individual.
      8.     Any other relative of the individual.


                                                                                                                       14
                REQUESTS BY SDMs FOR
                   RECORDS OF PHI
 An SDM ‚Äústands in the shoes of‚Äù the individual, and therefore has the same
    right of access as individual to the individual‚Äôs PHI.

 Where PHIPA authorizes an individual to make an access request, the
    individual‚Äôs SDM is also authorized to make the request;

 If PHIPA or LTCHA permits or requires a Custodian to disclose to an
    individual his/her record of PHI, then that record can be disclosed to the
    individual‚Äôs SDM;

 SDMs described in previous slides can request/receive a record of PHI
    about an individual for whom they are acting as an SDM.




                                                                                 15
              EXAMPLES OF PERMITTED
               DISCLOSURES TO SDMs
 Mr. X, who is capable, but infirm, wants his daughter to take care of
    all his paperwork. He has authorized her, in writing, to ‚Äúact on my
    behalf‚Äù. This is sufficient for PHIPA purposes. Record of PHI can be
    disclosed to her. Authorization need not refer specifically to ‚Äúaccess
    requests or disclosures‚Äù under PHIPA;

 Ms. Y is deceased; her will names her daughter J., as the executor
    of her estate. J. has provided copy of will;. J. is the estate trustee.
    Her mother‚Äôs PHI can be disclosed to J.




                                                                              16
             EXAMPLES OF PERMITTED
           DISCLOSURES TO SDMs, WHEN
             INDIVIDUAL IS INCAPABLE
   Mrs. P. has asked for a record of her husband‚Äôs PHI. Mr. P‚Äôs PHI can be disclosed to his wife,
    if Mr. P has no:
       1. guardian of person/property
       2. attorney for personal care/property
       3. individual representative appointed by the Consent and Capacity Board
     because these 3 categories of persons rank higher than ‚Äúspouse‚Äù in s. 26 SDM list (see slide 14).

   Ms. M.‚Äôs adult son, S., has asked for his mother‚Äôs record of PHI. Ms. M‚Äôs spouse, Mr. M., is incapable. Even
    though ‚Äúspouse‚Äù ranks higher than ‚Äúchild‚Äù in s. 26 SDM list, since the spouse is incapable, he cannot act as
    SDM. So Ms. M‚Äôs PHI can be disclosed to S, her son, as long as the persons in the 3 categories above do
    not exist for Ms. M.

   If, in the above example, the spouse was capable, would the son have a right of access to the PHI?
    Section 26(5) provides that if son is ‚Äúpresent‚Äù, and there is no person who falls into the 3 categories above,
    the PHI can be disclosed to the son if the son asserts that he ‚Äúbelieves that‚Äù Mr, M.‚Äúwould not object‚Äù to him
    obtaining the PHI.

   Mr. P. has no spouse, and no person who falls within the 3 categories above. He has 2 children who are
    fighting about who gets to see their father‚Äôs record of PHI. What to do? Ultimately PGT may make the
    decision about disclosure. Consult with your legal counsel.


                                                                                                              17
                 LTCHA PROVISIONS PERMIT
           DISCLOSURES OF PHI WITHOUT
                   CONSENT
   Sections 36-50 of PHIPA describe purposes/circumstances where Custodians can C/U/D PHI without the
    individual‚Äôs consent.

   In particular, ss. 36(1)(h), 37(1)(k) and 43(1)(h) of PHIPA provide that a Custodian may (respectively), C/U/D PHI
    without individual‚Äôs consent where that disclosure is ‚Äúpermitted or required by law‚Äù.

   The LTCHA is a ‚Äúlaw‚Äù that contains many provisions requiring licencees to disclose PHI to MOHLTC inspectors or
    the Director, and permitting or requiring the MOHLTC to collect and sometimes disclose PHI. For example:

     ‚óè Licensee must forward complaints re: resident care, and results of licensee‚Äôs investigation of complaints, to
          the Director;
     ‚óè Any person (including physicians) who suspects improper treatment of resident must report this to Director;
     ‚óè Ministry inspectors have extensive powers of inspection and can therefore see/take copies of resident files
     ‚óè Licensee must submit reports to Director, on any resident-related issue, at Director‚Äôs request.

   In addition, licensees and MOHLTC have obligation to post/publish inspection reports and orders‚Äì which may
    include PHI.

   To protect residents‚Äô privacy, this ‚Äúposting/publishing‚Äù obligation is modified by s. 301 of the LTCHA regulation: it
    provides that only ‚Äúa version that has been edited by an inspector so as to provide only the finding and summary of
    the evidence supporting the finding‚Äù‚Äù should be posted/published. The resident‚Äôs name and room number are
    severed from the inspection report, and any other identifying information ‚Äì to the extent that this is possible ‚Äì prior
    to posting/publication.


                                                                                                                         18
              MORE INFORMATION?

 Text of the Personal Health Information Protection Act, 2004 and
   regulations:
               http://www.e-laws.gov.on.ca

 Related Ministry of Health and Long-Term Care documents:
              http://www.health.gov.on.ca

 Related Information and Privacy Commissioner / Ontario
   documents:
             http://www.ipc.on.ca




                                                                 19
                                  Information and Privacy Commissioner of Ontario




Number 9
October 2005


                                     Long-term Care Homes
                              Consent and Access under the
                     Personal Health Information Protection Act, 2004
               The Personal Health Information                 2. Consent for Collection, Use
               Protection Act, 2004 (PHIPA) requires           and Disclosure of Personal Health
               health information custodians to obtain         Information
               the consent of the individual or his or
               her substitute decision-maker prior to the      a. O b t a i n i n g C o n s e n t W h e r e t h e
               collection, use or disclosure of personal          Resident is Capable
               health information unless PHIPA provides
               otherwise. PHIPA further requires health        Where the resident is capable and PHIPA
               information custodians to provide an            requires consent prior to the collection,
               individual or his or her substitute             use or disclosure of personal health
               decision-maker with a right to request access   information, the consent must be obtained
               to records of personal health information       from the resident or from any capable person
               about the individual.                           sixteen years of age or older authorized
                                                               in writing by the resident to provide the
                                                               consent on his or her behalf. A resident is
               1. Long-Term Care Homes as                      capable of consenting to the collection, use
               Health Information Custodians                   or disclosure of personal health information
               Approved charitable homes for the aged          if the resident is able to:
               within the meaning of the Charitable
               Institutions Act, homes or joint homes            ‚Ä¢ Understand information relevant to the
               within the meaning of the Homes for the             decision of whether to consent to the
               Aged and Rest Homes Act and nursing                 collection, use or disclosure of personal
               homes within the meaning of the Nursing             health information, and
               Homes Act are defined as health information
                                                                 ‚Ä¢Appreciate the reasonably
               custodians and therefore must comply with
                                                                  foreseeable consequences of giving, not
               PHIPA, including the provisions related to
                                                                  giving, withholding or withdrawing
               consent and access to records of personal
                                                                  consent.
               health information. For purposes of this
               Fact Sheet, these charitable homes, homes       Long-term care homes may presume that
               or joint homes and nursing homes will be        a resident is capable of consenting to the
               collectively referred to as ‚Äúlong-term care     collection, use or disclosure of his or her
               homes.‚Äù                                         personal health information unless it is
                                                               unreasonable to do so.
INFORMATION
AND PRIVACY
COMMISSIONER OF
ONTARIO


b. Obtaining Consent Where the Resident is            ‚Ä¢ A sibling of the resident; or
   Incapable
                                                      ‚Ä¢ Any other relative of the resident.
Where a resident is incapable of
                                                    A person listed may consent on behalf of the
consenting to the collection, use or disclosure
                                                    resident only where there is no person ranked
of his or her personal health information, the
                                                    higher on the above list or where the person
following persons (in the following order of
                                                    ranked higher in the list is incapable, unavailable
priority) may provide consent on behalf of
                                                    or unwilling to make a decision of whether or
the resident:
                                                    not to consent.
    ‚Ä¢ A substitute decision-maker under
                                                    c. Obtaining Consent Where the Resident is
      section 9, section 39 and section 56 of
                                                       Deceased
      the Health Care Consent Act, 1996 if
      the purpose of the collection, use or         Where a resident has died, the estate trustee or
      disclosure is necessary for or ancillary      the person who assumed responsibility for the
      to a decision about treatment under Part      administration of the estate if the estate does
      II, a decision about admission to a care      not have an estate trustee, may consent to the
      facility under Part III or a decision about   collection, use or disclosure of personal health
      a personal assistance service under Part      information on behalf of the resident.
      IV of the Health Care Consent Act, 1996
      respectively;
                                                    3. Rights of Access to Records of
    ‚Ä¢ The resident‚Äôs guardian of the person         Personal Health Information
      or guardian of property, if the consent       A resident or his or her substitute decision-
      relates to the guardian‚Äôs authority           maker has a right of access to records of
      to make a decision on behalf of the           personal health information about the
      resident;                                     resident in the custody or control of a long-
                                                    term care home, subject to certain exceptions
    ‚Ä¢ The resident‚Äôs attorney for personal
                                                    set out in sections 51(1) and 52(1) of PHIPA
      care or attorney for property, if the
                                                    such as:
      consent relates to the attorney ‚Äôs
      authority to make a decision on behalf of      ‚Ä¢ Where the record of personal health
      the resident;                                    information is subject to a legal privilege;

    ‚Ä¢ The resident‚Äôs representative appointed        ‚Ä¢ Where another Act or a court order
      by the Consent and Capacity Board;               prohibits the disclosure of the record of
                                                       personal health information to the resident;
    ‚Ä¢ The resident‚Äôs spouse or partner;
                                                       or
    ‚Ä¢ A child or parent of the resident;
                                                     ‚Ä¢ Where granting access could result in a risk
    ‚Ä¢ A parent of the resident with only a right       of serious harm to the treatment or recovery
      of access;                                       of the resident or a risk of serious bodily
                                                       harm to the resident or another person.

2
                                                                                           INFORMATION
                                                                                            AND PRIVACY
                                                                                       COMMISSIONER OF
                                                                                               ONTARIO


The provisions in the Nursing Homes Act, the         extended in limited circumstances for a further
Homes for the Aged and Rest Homes Act and            thirty days upon written notice to the resident
the Charitable Institutions Act which enable a       or his or her substitute decision-maker.
resident or his or her substitute decision-maker
to request access to a resident‚Äôs plan of care, do   Where a long-term care home denies access,
not limit the right of the resident or his or her    the long-term care home must provide the
substitute decision-maker to request access to       requester with a written notice in accordance
any other record of personal health information      with section 54(1) (c) or 54(1) (d) of PHIPA,
in the custody or control of a long-term care        depending on the nature of the exception relied
home subject to the exceptions in PHIPA.             on by the long-term care home in refusing
                                                     the request for access.
a. Persons Who May Make a Request for
                                                     Nothing in PHIPA prevents long-term care
   Access
                                                     homes from granting the resident or his or her
Where the resident is capable, a request for         substitute decision-maker access to a record
access may be made by the resident or any            of personal health information, where no
capable person sixteen years of age or older         exceptions to the right of access are applicable,
authorized in writing by the resident to make        in circumstances where the resident or his or
the request for access on his or her behalf.         her substitute decision-maker makes an oral as
                                                     opposed to a written request for access.
Where a resident is incapable, please refer to
the list of persons under the heading ‚ÄúObtaining
                                                     However, in order to trigger the formal process,
Consent Where the Resident is Incapable‚Äù to
                                                     the request for access must be in writing.
determine who may make a request for access
on behalf of the incapable resident.                 c. Fee for Access
Where a resident has died, the estate trustee
or the person who assumed responsibility for         Where a request for access is granted and a
the administration of the estate if the estate       long-term care home makes a record of personal
does not have an estate trustee, may make a          health information available or provides a
request for access to records of personal health     copy to the resident or his or her substitute
information of the resident.                         decision-maker, the long-term care home is
                                                     entitled to charge the resident or his or her
                                                     substitute decision-maker a fee that does not
b. Responding to a Request for Access
                                                     exceed the amount that is reasonable to enable
When granting a request for access, a long-term      the long-term care home to recover its costs,
care home must make the record available for         provided the long-term care home first gives
examination or, upon request, provide the            the resident or his or her substitute decision-
resident or his or her substitute decision-maker     maker an estimate of the fee.
with a copy of the record as soon as possible
but no later than thirty days after receipt of
the request. The thirty day time limit may be


                                                                                                     3
Fact Sheet                                              Communications Department
                                                        Information and Privacy Commissioner of Ontario
                                                        2 Bloor Street East, Suite 1400                   30% recycled
                                                        Toronto, Ontario CANADA                              paper
is published by the Office of the Information and
                                                        M4W 1A8
Privacy Commissioner of Ontario.                        Telephone: 416-326-3333 ‚Ä¢ 1-800-387-0073
                                                        Facsimile: 416-325-9195
If you have any comments regarding this news¬≠¬≠letter,   TTY (Teletypewriter): 416-325-7539
wish to advise of a change of address, or be added      Website: www.ipc.on.ca
to the mailing list, contact:                           Cette publication, intitul√©e ¬´ Feuille-info ¬ª,
                                                        est √©galement disponible en fran√ßais.
Preventing/Reducing Unauthorized Access
to Personal Health Information
A Shared Commitment to Preventing Unauthorized Access


The health and safety of patients, and the quality of health      contravenes PHIPA. This includes unauthorized access,
care provided, are paramount considerations in hospital           such as when personal health information is viewed by a
records management. Giving employees and other staff              health care professional who is not providing, or assisting
(as agents of health information custodians) quick and            in the provision, of health care to the individual involved
easy access to records of personal health information             (usually a patient).
required for the delivery of health care services is essential,
particularly when an individual requires urgent attention.        A privacy breach will have significant consequences for the
                                                                  individual, the hospital and the agent of the hospital.
Hospitals are required to take reasonable measures to
ensure that its agents do not abuse their access rights and       Protection of privacy is a shared responsibility across all
privileges, and to ensure that they understand both their         levels of an organization. Agents need to know that the
obligations to protect privacy and the security of personal       hospital‚Äôs management and senior executive team are
health information and the consequences of failing to             adamant about preventing unauthorized access ‚Äì they take
do so.                                                            their responsibilities under PHIPA very seriously.

Since the introduction of the Personal Health Information         Together the Ontario Hospital Association and the Office
Protection Act (PHIPA) in 2004, hospitals and other               of the Information and Privacy Commissioner have
health information custodians have made tremendous                developed this resource to help hospitals ensure that they
strides to ensure that the privacy of individuals and the         have a robust approach to preventing unauthorized access
confidentiality of their personal health information remains      ‚Äì one that is implemented consistently within and across
a top priority.                                                   their organizations.

Privacy breaches occur when personal health information
in the custody or control of a hospital is collected, used,
disclosed, retained or disposed of in a manner that




Pat Campbell                                                      Ann Cavoukian, Ph.D.
President and CEO                                                 Information and Privacy Commissioner
Ontario Hospital Association                                      Province of Ontario
Preventing/Reducing Unauthorized Access to
Personal Health Information

The Ontario Hospital Association (OHA) and the                         What is Personal Health Information (PHI)?
Information and Privacy Commissioner of Ontario (IPC),
have collaborated to develop this Primer for hospitals                 PHI is identifying information collected about a person,
to highlight best practices for safeguarding personal                  either orally or in writing. It includes information about an
health information (PHI) to prevent/reduce incidents of                individual‚Äôs health or health care history in relation to:
unauthorized access.
                                                                       ‚Ä¢   The individual‚Äôs physical or mental condition,
Specifically this Primer will:                                             including family health history;

‚Ä¢   Define authorized and unauthorized access;                         ‚Ä¢   The provision of health care, including the
                                                                           identification of persons providing care;
‚Ä¢   Discuss the implications and consequences of
    unauthorized access;                                               ‚Ä¢   Long-term care services;

‚Ä¢   Outline ways your organization can prevent                         ‚Ä¢   Payment or eligibility for health care or eligibility for
    unauthorized access; and                                               coverage for health care;

‚Ä¢   Describe how personal privacy should be protected in               ‚Ä¢   Donation of body parts or bodily substances or
    your organization.                                                     information that is derived from the testing or
                                                                           examination of such parts or substances;
The rules for the collection, use, disclosure, retention
and disposal of PHI by health information custodians                   ‚Ä¢   The individual‚Äôs health card number; or
(e.g., hospitals) and their agents, (e.g., nurses, physicians,
pharmacists and data entry clerks) are set out in the                  ‚Ä¢   The identity of a individual‚Äôs substitute decision-maker.
Personal Health Information Protection Act (PHIPA). PHIPA
also provides individuals with a right to access their PHI,            Identifying information includes health information
subject to limitations, and to request a correction of these           that could identify an individual when used alone or
records.                                                               in conjunction with other information. Identifying
                                                                       information about an agent of your hospital (e.g.,
                                                                       employee) that is not maintained to provide care is
                                                                       not PHI.

                                                                       PHI is among the most sensitive personal information
                                                                       about an individual. As such, the collection, use, disclosure,
                                                                       retention, and disposal of PHI requires a high level of
                                                                       protection by organizations and individuals who have such
                                                                       information in their custody or control.




                                                                   1
                                 Preventing/Reducing Unauthorized Access to Personal Health Information
Authorized Access to Personal Health
                                                                      The IPC has issued a number of orders pertaining to
Information
                                                                      unauthorized access. In two separate cases (H0-002 and
                                                                      H0-010), a hospital patient‚Äôs PHI was accessed by an
PHIPA provides that PHI is only permitted to be accessed
                                                                      employee of the hospital who was not providing care to
for authorized purposes, including the collection, use and
                                                                      the individual. In H0-010, the IPC ordered the hospital
disclosure of PHI by health care professionals within an
                                                                      to undertake 10 remedial actions to prevent similar
individual‚Äôs ‚Äúcircle of care.‚Äù While ‚Äúcircle of care‚Äù is not
                                                                      privacy breaches in the future.
explicitly defined in PHIPA, it is commonly used to describe
the ability of certain health information custodians to
assume an individual‚Äôs implied consent to collect, use or
disclose PHI for the purposes of providing health care.             Hospitals may learn about a privacy breach through
                                                                    internal reporting/auditing, when contacted by the IPC
The circle of care generally refers to those on the health          after a formal complaint has been received, or when the
care team who are actually involved in the care or treatment        IPC investigates on its own initiative.
of a patient. Members of an individual‚Äôs circle of care can
imply consent, or assume they have the individual‚Äôs implied         The Consequences of Unauthorized Access
consent, to collect, use and disclose the individual‚Äôs PHI
for such care, unless the individual expressly withholds or         A privacy breach in the form of an unauthorized access to
withdraws consent.                                                  PHI can have significant consequences for the individual,
                                                                    the hospital and the agent of the hospital (i.e., the hospital
                                                                    employee).
  For more information on circle of care see
  Circle of Care: Sharing Personal Health Information               Consequences for the individual
  for Health-Care Purposes.
                                                                    A person whose PHI is accessed without authorization may
                                                                    be deterred from seeking testing or treatment in the future,
Unauthorized Access to Personal Health                              and may withhold or falsify information provided to health
                                                                    care professionals for fear of unauthorized access. The
Information
                                                                    unauthorized access of sensitive PHI may also result in the
Hospitals are required to take reasonable precautions to            individual being discriminated against or stigmatized and
safeguard PHI from theft, loss and unauthorized use, and            may cause them economic or psychological harm.
to notify the affected individual at the first reasonable
opportunity if PHI is stolen, lost or accessed by an                Consequences for the hospital employee
unauthorized person.                                                Agents who access PHI without authorization may be
                                                                    subject to disciplinary action by the hospital at which they
A privacy breach occurs whenever PHI in the custody or              are employed, including a verbal warning, disciplinary
control of a hospital is collected, used, disclosed, retained       letter, suspension or even termination. In addition, the
or disposed of in a manner that contravenes PHIPA. This             actions of the agent may be reported to their licensing
includes unauthorized use or access to PHI, such as when            body or regulatory college which could have significant
PHI is viewed by a health care professional where there is          implications for the individual‚Äôs ability to work in the
no health care requirement to view such information about           future.
that person.




                                                                2
                              Preventing/Reducing Unauthorized Access to Personal Health Information
For example, the Ontario College of Nurses disciplined                 Health and Central Health in Newfoundland and Labrador
a member for unauthorized accesses of records of PHI.                  for unauthorized access to PHI. While these cases have
The affected individual was the ex-spouse of the                       yet to be heard by the Newfoundland Supreme Court, the
member‚Äôs partner.1                                                     impact on the organizations, from a reputational and cost
                                                                       perspective, will likely be significant.
In addition to having her employment terminated, the
member was found to have failed to meet the standards of
                                                                       Preventing/Reducing Unauthorized Access:
practice of the College and to have engaged in conduct
                                                                       A Multi-Pronged Approach
that would reasonably be regarded by members of the
profession as dishonourable and unprofessional. The
                                                                       As noted above, the consequences of a privacy breach
member had her certificate of registration suspended for
                                                                       can be devastating for individuals, hospitals, and agents
six weeks and was required, for 12 months after the order
                                                                       of the hospital (i.e., hospital employees). Organizations
was made, to notify all new employers of the decision, the
                                                                       should ensure they have a robust approach to preventing
order and the penalties imposed.
                                                                       unauthorized access that is implemented consistently across
                                                                       the organization.
A breach may also result in prosecution by the provincial
Attorney General. In Alberta, a medical office clerk pleaded
                                                                       A privacy policy cannot, by itself, protect PHI. It
guilty to charges of unauthorized access of another person‚Äôs
                                                                       must be operationalized throughout the activities of
PHI and was fined $10,000 by an Alberta court.
                                                                       an organization.2 Protection of privacy is a shared
                                                                       responsibility across all levels of an organization. Agents
Consequences for the organization
                                                                       need to know that the hospital‚Äôs management and senior
                                                                       executive team are serious about preventing/reducing
A privacy breach may result in:
                                                                       unauthorized access.

‚Ä¢    Damage to the hospital‚Äôs reputation;
                                                                       To prevent/reduce unauthorized access, the OHA and the
‚Ä¢    An IPC order;                                                     IPC recommend this multi-pronged approach:

‚Ä¢    Lost time and resources to contain, investigate, and
                                                                       a) Education and Training
     remediate the breach;

‚Ä¢    Prosecution by the Attorney General; or                           Organizations should ensure that all agents, including
                                                                       board-appointed professional staff with privileges,
‚Ä¢    Legal action taken against the hospital.
                                                                       maintenance staff, and security staff, are appropriately
                                                                       trained upon hiring or upon the granting of access
The IPC‚Äôs two orders related to unauthorized access both
                                                                       privileges. This training should be refreshed regularly
garnered significant media and public attention and set
                                                                       (preferably annually). Education and training programs
out specific remediation that the hospital was required to
                                                                       should include:
complete.

                                                                       ‚Ä¢    An overview of role-based access rules, and the
In other provinces, the scope of repercussions has
                                                                            definitions of a privacy breach and unauthorized access;
expanded outside of the privacy commissioner‚Äôs office. In
early 2012, four class action law suits were brought against
the Western Health Regional Health Authority, Eastern

                                                                       2   A Policy is Not Enough: It Must Be Reflected in Concrete Practices.
1   http://www.cno.org/CNO/PageTypes/DisciplineDecision.                   http://www.ipc.on.ca/images/Resources/pbd-policy-not-enough.pdf
    aspx?id=1464&epslanguage=en.                                           Accessed: October 10, 2012.

                                                                   3
                                 Preventing/Reducing Unauthorized Access to Personal Health Information
‚Ä¢   Details of the hospital‚Äôs planned response to
    unauthorized access, including a report to the                       Hospital privacy policies should be accessible and
    employee‚Äôs regulatory college (where appropriate),                   written in plain language. Employees should be aware
    and notice to the affected parties;                                  of and commit to following these policies.


‚Ä¢   Potential disciplinary action that the hospital will take
                                                                     c) Importance of a Privacy Breach Protocol
    in the event of unauthorized access;
                                                                     Your organization should ensure that a written policy
‚Ä¢   Notice that random audits will be conducted;                     is in place for responding to all privacy breaches. Your
                                                                     organization‚Äôs privacy breach protocol should include:
‚Ä¢   Appropriate contact information for reporting privacy
    breaches; and                                                    ‚Ä¢    Zero tolerance of unauthorized access (both in policy
                                                                          and practice);
‚Ä¢   Understanding the IPC‚Äôs role (i.e., a review of orders
    issued to date on unauthorized access).                          ‚Ä¢    Immediate temporary suspension of an individual‚Äôs
                                                                          access to records (and potentially employment)
New agents (e.g., employees) should be provided with                      pending the outcome of an internal investigation;
copies of all privacy policies upon hire. Hospitals should
consider building in confidentiality acknowledgments                 ‚Ä¢    Policy on containment and notification of affected
to initial employment contracts. Regular performance                      parties;
evaluations should include a renewal of confidentiality
acknowledgments and build an obligation to protect privacy           ‚Ä¢    Notice of the unauthorized access to your
into the evaluation process.                                              organization‚Äôs privacy officer, human resources
                                                                          department and the Information and Privacy
b) Communications                                                         Commissioner‚Äôs Office;

Successful communications strategies can include:                    ‚Ä¢    Provision of all information regarding the breach and
                                                                          hospital‚Äôs response to breach to affected individuals;
‚Ä¢   Log-on notices to staff asking them to acknowledge
    appropriate access and reminders about random audits;            ‚Ä¢    Discipline (where necessary and appropriate) following
                                                                          an investigation; and
‚Ä¢   Posters with zero tolerance messaging, screensavers
    with reminders about privacy, fact sheets, and questions         ‚Ä¢    Process for conducting a full audit.
    and answer documents;
                                                                     You can also refer to the IPC‚Äôs guidance document, What to
‚Ä¢   A confidential system for whistleblowers;                        do When Faced With a Privacy Breach: Guidelines for the
                                                                     Health Sector.
‚Ä¢   Regular communications from Senior Management
    re-affirming a commitment to privacy (i.e., through
                                                                     d) Audit and Evaluation
    newsletters, email, staff meetings); and

                                                                     Hospitals need to ensure that auditing is in place for all
‚Ä¢   Role-modeled commitment from Senior Management
                                                                     systems containing PHI and that processes are in place to
    (i.e., attendance at training and education sessions).
                                                                     review system control and audit logs to prevent and identify
                                                                     unauthorized use or access to PHI.


                                                                 4
                               Preventing/Reducing Unauthorized Access to Personal Health Information
Hospitals should be able to generate complete and accurate
audit logs that identify:

‚Ä¢    All agents who log-in to a system;

‚Ä¢    Any and all user log-in identifications;

‚Ä¢    All log-in attempts during a specified period of time
     and/or for a particular agent;

‚Ä¢    Each agent who accessed an individual‚Äôs PHI through a
     specified period of time;

‚Ä¢    Those individuals for whom an agent accessed PHI
     through the hospital during a specified period of time;
     and

‚Ä¢    Any occasion on which the PHI for an individual who
     has requested restrictions (i.e., lockbox) was accessed
     and by whom.


    When to conduct audits:

    1. On a regular basis (monthly) with random patients,
       prominent patients when they are admitted and
       staff members when they are admitted;

    2. Prior to any operational or technical changes to
       the electronic system or environmental
       circumstances impacting risk; and

    3. In response to any suspected or actual privacy
       breach.




                                                                  5
                                Preventing/Reducing Unauthorized Access to Personal Health Information
200 Front Street West, Suite 2800
Toronto, Ontario M5V 3L1
www.oha.com
                                  ORDER HO-14
                                 Complaint HA13-108

                               London Health Sciences Centre

                                        March 6, 2015

Summary: The complaint arises from a fee of $117 charged by the London Health Sciences
Centre to a lawyer for copies of his client‚Äôs records of personal health information. The
complaint asserts that the fee paid exceeds the amount that the London Health Sciences Centre
was entitled to charge under the Personal Health Information Protection Act (the Act). In
response, the Office of the Information and Privacy Commissioner/Ontario opened a complaint
file to determine if the fee was in compliance with the Act.

The London Health Sciences Centre submitted that the request was for ‚Äúdisclosure‚Äù under
section 35(2) of the Act, and therefore it could charge more than would be permitted in
response to a request for ‚Äúaccess‚Äù under section 54(11) of the Act. The lawyer for the
complainant submitted that this was a request for ‚Äúaccess‚Äù falling squarely within the fee
framework previously accepted by this Office. This order concludes that it is not necessary for
me to determine whether the request by the lawyer for copies of his client‚Äôs records of personal
health information is a request for ‚Äúaccess‚Äù or a request for ‚Äúdisclosure‚Äù under the Act because,
in either event, the issue for determination is whether the fee charged amounts to ‚Äúreasonable
cost recovery‚Äù as that term is used in sections 54(11) and 35(2) of the Act. This order
determines that the phrase ‚Äúreasonable cost recovery‚Äù should be given a consistent
interpretation throughout the Act. Therefore, this order finds that the fee charged by the
London Health Sciences Centre exceeds ‚Äúreasonable cost recovery‚Äù and orders that it be
reduced.

Statutes considered: Personal Health Information Protection Act, 2004, ss. 2, 3, 4, 35(2) and
54(11); Public Hospitals Act ss. 1, 3, 4 and 32.1.

Orders Considered: Order HO-009.




                              [IPC Order HO-14/March 6, 2015]
                                          -2-

Cases Considered: R. v. Clark, [2005] 1 S.C.R. 6; R. v. J.H., 2002 CanLII 41069; R v.
Zeolkowski, [1989] 1 S.C.R. 1378; Re Rizzo & Rizzo Shoes Limited [1998] 1 S.C.R. 27; and
Thomson v. Canada (Deputy Minister of Agriculture), [1992] 1 S.C.R. 385.

BACKGROUND:

[1]     A request was made to the London Health Sciences Centre under the Personal
Health Information Protection Act, 2004 (the Act) by a lawyer for copies of his client‚Äôs
records of personal health information from January 1, 2006 to August 26, 2013. In
making the request, the lawyer provided the London Health Sciences Centre with a
document entitled ‚ÄúAuthorization to Release Personal Information‚Äù (the Authorization)
signed by his client. In the Authorization, his client (the complainant) directed and
instructed the London Health Sciences Centre to release to his lawyer ‚Äúany and all
information which they may require in connection with my physical condition and
clinical records.‚Äù In his correspondence requesting the records of personal health
information, the lawyer for the complainant referenced this office‚Äôs Order HO-009
(discussed below), enclosed a cheque for $30 representing the ‚Äúreasonable cost
recovery charge for providing these personal medical records‚Äù and promised to pay
further reasonable copying charges.

[2]     The London Health Sciences Centre responded to the request by acknowledging
receipt of the request and issuing an invoice. The invoice charged $200 for responsive
records in paper format, and credited the complainant‚Äôs lawyer for the $30 already paid,
for a total amount due of $170.

[3]   The complainant‚Äôs lawyer filed a complaint with this office, stating that the fee
exceeded the amount that the London Health Sciences Centre was entitled to charge
under the Act and was contrary to the findings made in Order HO-009.

[4]    The London Health Sciences Centre subsequently reduced the fee to $117. It
issued a revised invoice which clarified that the fee related to 112 pages of records of
personal health information in paper format. The lawyer paid the fee and copies of the
records were provided to the complainant without prejudice to the right of the
complainant‚Äôs lawyer to pursue this complaint.

CONDUCT OF THE REVIEW:

[5]   Following unsuccessful attempts at mediation, this complaint was moved to the
review stage of the complaint process where an adjudicator conducts a review under
the Act. I began the review by issuing a Notice of Review, asking the London Health
Sciences Centre to submit its representations addressing the facts and issues set out in
the Notice of Review.

[6]    After reviewing the representations from the London Health Sciences Centre, I
then issued a Notice of Review to the complainant‚Äôs lawyer, asking him to submit



                             [IPC Order HO-14/March 6, 2015]
                                            -3-

representations addressing the facts and issues set out in the Notice of Review and to
respond to the representations of the London Health Sciences Centre, a copy of which
was enclosed.

ISSUES AND SUMMARY OF CONCLUSIONS:

A. Are the records at issue ‚Äúrecords‚Äù of ‚Äúpersonal health information‚Äù as defined in
   sections 2 and 4 of the Act?

B. Is the London Health Sciences Centre a ‚Äúhealth information custodian‚Äù as defined in
   section 3(1) of the Act?

C. Is the request by the lawyer for copies of his client‚Äôs records of personal health
   information a request for ‚Äúaccess‚Äù or a request for ‚Äúdisclosure‚Äù under the Act?

D. Does the fee of $117 exceed ‚Äúreasonable cost recovery‚Äù as that term is used in the
   Act? If the answer is yes, what would qualify as ‚Äúreasonable cost recovery‚Äù in this
   case?

[7]   I conclude below that:

A. The records at issue are records of personal health information as defined in
   sections 2 and 4 of the Act.

B. The person who operates the London Health Sciences Centre is a health information
   custodian as defined in paragraph 4(i) of section 3(1) of the Act.

C. It is not necessary for me to determine whether the request by the lawyer for copies
   of his client‚Äôs records of personal health information is a request for ‚Äúaccess‚Äù or a
   request for ‚Äúdisclosure‚Äù under the Act because, in either event, the issue for
   determination is whether the fee charged amounts to ‚Äúreasonable cost recovery.‚Äù I
   conclude that ‚Äúreasonable cost recovery‚Äù should be given a consistent interpretation
   throughout the Act.

D. The fee of $117 exceeds ‚Äúreasonable cost recovery‚Äù as that term is used in the Act.
   I find that ‚Äúreasonable cost recovery‚Äù in this case is $53.


DISCUSSION:

Issue A:     Are the records at issue ‚Äúrecords‚Äù of ‚Äúpersonal                    health
             information‚Äù as defined in sections 2 and 4 of the Act ?

[8]   Section 2 of the Act defines a record as:




                               [IPC Order HO-14/March 6, 2015]
                                            -4-

      ‚Ä¶a record of information in any form or in any medium, whether in
      written, printed, photographic or electronic form or otherwise, but
      does not include a computer program or other mechanism that can
      produce a record.

[9]   Section 4(1) of the Act states:

      In this Act,

             ‚Äúpersonal health information‚Äù, subject to subsections (3) and
             (4), means identifying information about an individual in oral
             or recorded form, if the information,

                     (a) relates to the physical or mental health of
                     the individual, including information that
                     consists of the health history of the individual‚Äôs
                     family,

                     (b)    relates to the providing of health care to
                     the individual, including the identification of a
                     person as a provider of health care to the
                     individual,

                     (c)    is a plan of service within the meaning
                     of the Home Care and Community Services
                     Act, 1994 for the individual,

                     (d)     relates to payments or eligibility for
                     health care, or eligibility for coverage for health
                     care, in respect of the individual,

                     (e)    relates to the donation by the individual
                     of any body part or bodily substance of the
                     individual or is derived from the testing or
                     examination of any such body part or bodily
                     substance,

                     (f)    is the individual‚Äôs health number, or

                     (g)    identifies an individual‚Äôs       substitute
                     decision-maker. [Emphasis added]

[10] Identifying information is defined in section 4(2) of the Act as information that
identifies an individual or for which it is reasonably foreseeable in the circumstances
that it could be used, either alone or with other information, to identify an individual.



                               [IPC Order HO-14/March 6, 2015]
                                           -5-

[11] Neither party to this review disputes that the records at issue in this review are
records of personal health information. I agree.

[12] The complainant‚Äôs lawyer requested records ‚Äútouching on [the complainant‚Äôs]
care and treatment.‚Äù The Authorization directed and instructed the London Health
Sciences Centre to provide the complainant‚Äôs lawyer with:

      ‚Ä¶all information which they may require in connection with [the
      complainant‚Äôs] physical condition and clinical records, including but not
      limited to all x-rays, clinical notes, treatment plans, charts and diagrams,
      hospital records, medical reports, reports on diagnostic tests, medical
      opinions, other health provider notes‚Ä¶

[13] The records at issue contain identifying information about the complainant and
the information contained in the records relates to the physical or mental health of the
complainant and relates to the provision of health care to the complainant. As a result,
I find that the records at issue are records of personal health information as defined in
sections 2 and 4 of the Act.

Issue B:     Is the London Health Sciences Centre a ‚Äúhealth information
             custodian‚Äù as defined in section 3(1) of the Act ?

[14] The term ‚Äúhealth information custodian‚Äù is defined in section 3(1) of the Act,
which reads in part as follows:

      In this Act,

      ‚Äúhealth information custodian‚Äù, subject to subsections (3) to (11), means
      a person or organization described in one of the following paragraphs who
      has custody or control of personal health information as a result of or in
      connection with performing the person‚Äôs or organization‚Äôs powers or
      duties or the work described in the paragraph, if any:

      [‚Ä¶]

             4. A person who operates one of the following facilities,
                programs or services:

                     i. A hospital within the meaning of the Public
                        Hospitals Act, a private hospital within the
                        meaning of the Private Hospitals Act, a
                        psychiatric facility within the meaning of the
                        Mental Health Act or an independent health
                        facility within the meaning of the
                        Independent Health Facilities Act.




                              [IPC Order HO-14/March 6, 2015]
                                             -6-

       [‚Ä¶]

[15]   Section 1 of the Public Hospitals Act defines ‚Äúhospital‚Äù as follows:

       ‚Äúhospital‚Äù means any institution, building or other premises or place that
       is established for the purposes of the treatment of patients and that is
       approved under this Act as a public hospital;

[16] Section 4(2) of the Public Hospitals Act empowers the Minister of Health and
Long-Term Care (the Minister) to approve the operation or use of an institution,
building or other premises or place as a hospital. Section 4(2) of the Public Hospitals
Act provides:

       No institution, building or other premises or place shall be operated or
       used for the purposes of a hospital unless the Minister has approved the
       operation or use of the premises or place for that purpose.

[17] Neither party to this review disputes that the London Health Sciences Centre is a
health information custodian within the meaning of the Act.

[18] Pursuant to section 32.1(2) of the Public Hospitals Act, ‚Äú[t]he Minister shall
maintain a list of hospitals and their classifications and grades.‚Äù The London Health
Sciences Centre is included on the list of general hospitals under the Public Hospitals
Act maintained on the website of the Ministry of Health and Long-Term Care. 1

[19] I find that the London Health Sciences Centre is a ‚Äúhospital within the meaning
of the Public Hospitals Act‚Äù and that the person who operates the London Health
Sciences Centre is a heath information custodian within the meaning of paragraph 4(i)
of section 3(1) of the Act.

Issue C:      Is the request by the lawyer for copies of his client‚Äôs records of
              personal health information a request for ‚Äúaccess‚Äù or a request
              for ‚Äúdisclosure‚Äù under the Act ?

[20] The parties disagree on whether the request made by the lawyer for copies of
the records of personal health information of his client, the complainant, is a request for
‚Äúaccess‚Äù or a request for ‚Äúdisclosure‚Äù under the Act.

[21] The London Health Sciences Centre argues that the request is a request for
‚Äúdisclosure.‚Äù The Centre submits that a lawyer making a request for records of personal
health information on behalf of a client is not a substitute decision-maker within the

1
 http://www.health.gov.on.ca/en/common/system/services/hosp/
and
http://www.health.gov.on.ca/en/common/system/services/hosp/southwest.aspx#london




                                [IPC Order HO-14/March 6, 2015]
                                           -7-

meaning of the Act. Therefore, it argues, the request was not a request for ‚Äúaccess‚Äù but
rather a request for ‚Äúdisclosure.‚Äù

[22] The complainant‚Äôs lawyer argues that the request for copies of the records of
personal health information relating to his client, the complainant, is a request for
‚Äúaccess‚Äù and not a request for ‚Äúdisclosure.‚Äù While the complainant‚Äôs lawyer concurs with
the London Health Sciences Centre that a lawyer acting on behalf of a client is not a
substitute decision-maker within the meaning of the Act, he asserts that a lawyer is a
representative or agent who is acting solely for and on behalf of his or her client, the
individual to whom the records of personal health information relate, to enable the
client to obtain his or her records for the use and benefit of the client and not the use
and benefit of the lawyer. Therefore, he submits that the request in this complaint is for
‚Äúaccess‚Äù and not ‚Äúdisclosure.‚Äù

[23] Having considered the parties‚Äô representations on all issues, I conclude that it is
not necessary for me to determine whether the request by the lawyer for copies of his
client‚Äôs records of personal health information is a request for ‚Äúaccess‚Äù or a request for
‚Äúdisclosure‚Äù under the Act. Whether it is ‚Äúaccess‚Äù or ‚Äúdisclosure‚Äù under the Act, the
same fee framework applies. A health information custodian is prohibited from charging
a fee that exceeds the amount of reasonable cost recovery.

[24] Section 35(2) of the Act governs the fee a health information custodian is
permitted to charge to disclose personal health information. It states:

      When disclosing personal health information, a health information
      custodian shall not charge fees to a person that exceed the prescribed
      amount or the amount of reasonable cost recovery, if no amount is
      prescribed. [Emphasis added]

[25] Similarly, sections 54(10) and (11) of the Act govern the fee that a health
information custodian is permitted to charge for access to records of personal health
information. Those sections state:

      (10) A health information custodian that makes a record of personal
      health information or a part of it available to an individual under this Part
      or provides a copy of it to an individual under clause (1)(a) may charge
      the individual a fee for that purpose if the custodian first gives the
      individual an estimate of the fee.

      (11) The amount of the fee shall not exceed the prescribed amount or the
      amount of reasonable cost recovery, if no amount is prescribed.
      [Emphasis added]

[26] Sections 35(2) and 54(11) both state that the fee shall not ‚Äúexceed the
prescribed amount or the amount of reasonable cost recovery, if no amount is



                              [IPC Order HO-14/March 6, 2015]
                                          -8-

prescribed.‚Äù No regulations prescribe the amount of the fee that may be charged by a
health information custodian when disclosing records of personal health information
under section 35(2) of the Act or providing access to records of personal health
information under section 54(11) of the Act. In the absence of such a regulation, the
issue under both section 35(2) and section 54(11) of the Act is whether the fee exceeds
the amount of ‚Äúreasonable cost recovery.‚Äù

[27] The Notice of Review issued to the parties asked them to submit representations
on how the prohibition on charging fees that ‚Äúexceed the prescribed amount or the
amount of reasonable cost recovery, if no amount is prescribed‚Äù in section 35(2) of the
Act, compares to the prohibition on charging fees that ‚Äúexceed the prescribed amount
or the amount of reasonable cost recovery, if no amount is prescribed‚Äù in section
54(11) of the Act. In response, the London Health Sciences Centre stated:

       In this case the fees charged were within the reasonable cost recovery
       range.

[28] The Notice of Review also asked for representations in response to the following
question: ‚Äú[i]f the amount of the fee charged differs depending on whether the request
was made directly by the individual to whom the personal health information relates or
by any other person, explain how the actions required to respond to the request, the
amount of time spent undertaking each action and the other costs incurred to respond
to the request would differ?‚Äù The London Health Sciences Centre responded:

       Disclosure requests are typically larger, more complicated and take a
       longer time to complete. Access fees are sometimes waived on a
       compassionate basis. Legal aid cases are completed at a reduced rate.

[29] For the reasons that follow, I reject differentiating between ‚Äúreasonable cost
recovery‚Äù for disclosure pursuant to section 35(2) of the Act and ‚Äúreasonable cost
recovery‚Äù for access pursuant to section 54(11) of the Act.

[30]   The modern rule of statutory interpretation, as articulated by Ruth Sullivan in
Sullivan on the Construction of Statutes, 5th ed. (Markham: LexisNexis Canada Inc.,
2008) and adopted by the Supreme Court of Canada in Re Rizzo & Rizzo Shoes Limited,
[1998] 1 S.C.R. 27 at para. 21, provides:

       Today there is only one principle or approach, namely, the words of an
       Act are to be read in their entire context, in their grammatical and
       ordinary sense harmoniously with the scheme of the Act, the object of the
       Act, and the intention of Parliament.

[31] In reading the words ‚Äúreasonable cost recovery‚Äù in their entire context,
harmoniously with the scheme and object of the Act and the intention of the legislature,




                             [IPC Order HO-14/March 6, 2015]
                                                -9-

the presumption of consistent expression should be considered.                  Professor Sullivan
explains the presumption of consistent expression as follows:

       It is presumed that the legislature uses language carefully and
       consistently so that within a statute or other legislative instrument the
       same words have the same meaning and different words have different
       meanings. Another way of understanding this presumption is to say that
       the legislature is presumed to avoid stylistic variation.

       [‚Ä¶]

       The presumption of consistent expression applies not only within statutes
       but across statutes as well, especially statutes or provisions dealing with
       the same subject matter.

[32] In fact, the Supreme Court of Canada acknowledged in R v. Zeolkowski, [1989] 1
S.C.R. 1378 at 1387 that ‚Äúgiving the same words the same meaning throughout a
statute is a basic principle of statutory interpretation.‚Äù

[33] In dealing with the presumption of consistent expression, the Supreme Court of
Canada in Thomson v. Canada (Deputy Minister of Agriculture), [1992] 1 S.C.R. 385 at
400- 401 held that ‚Äú[u]nless the contrary is clearly indicated by the context, a word
should be given the same interpretation or meaning whenever it appears in an act.‚Äù In
other words, consistency of expression is a presumption that may be rebutted by the
context in which an expression is used. In my view, the context does not rebut the
presumption of consistent expression in the circumstances of this complaint. In fact,
for the two reasons that follow, the context supports the conclusion that ‚Äúreasonable
cost recovery‚Äù has the same meaning for both access and disclosure.

[34] First, sections 35(2) and 54(11) of the Act relate to the same subject matter.
Both sections establish the fees that a health information custodian is permitted to
charge for releasing or making records of personal health information available under
the Act. 2 Both sections also state that the fee charged shall not exceed ‚Äúreasonable
cost recovery‚Äù if no amount is prescribed. Therefore, there is similarity in the contexts
in which the words or expression prohibiting the charging of fees that exceed the
amount of ‚Äúreasonable cost recovery‚Äù are used.      As stated by the Supreme Court of
Canada in R. v. Clark, [2005] 1 S.C.R. 6 at para. 51, the legislature ‚Äúcould not have
intended that identical words should have different meanings in‚Ä¶related provisions of
the very same enactment.‚Äù This is further supported by the Ontario Court of Appeal in
R. v. J.H., 2002 CanLII 41069 at para 44., which held (quoting Ruth Sullivan, Driedger
On the Construction of Statutes, 3rd ed. (Toronto: Butterworths: 1994) at 164) that the

2
  The phrase ‚Äúreasonable cost recovery‚Äù also appears in section 60(2)(e) of the Act. Section 60(2)(e)
permits the Commissioner to ‚Äúcopy any books, records or documents‚Äù as part of a review upon paying a
‚Äúreasonable cost recovery fee that the health information custodian or person being reviewed may
charge.‚Äù



                                  [IPC Order HO-14/March 6, 2015]
                                          - 10 -

presumption of consistent expression applies particularly ‚Äúwhere the provisions in which
the repeated words appear are close together or otherwise related.‚Äù

[35] Second, interpreting ‚Äúreasonable cost recovery‚Äù in the same manner under both
sections 35(2) and 54(11) of the Act avoids results that are inconsistent with a plain
reading of the Act. It would create an absurd result to permit different amounts to be
charged as ‚Äúreasonable cost recovery‚Äù pursuant to sections 35(2) and 54(11) of the
Act, in circumstances where the actual costs incurred by the health information
custodian to process a request are the same. If the representations of the London
Health Sciences Centre were to be accepted, namely that a request by a third party
with the consent of the individual is a request for ‚Äúdisclosure,‚Äù why should it be
permitted to charge $117 as ‚Äúreasonable cost recovery‚Äù under section 35(2) of the Act
when it could only charge $53 as ‚Äúreasonable cost recovery‚Äù under section 54(11)
based on the framework for ‚Äúaccess‚Äù adopted in Order HO-009? In both cases, the
costs incurred by the London Health Sciences Centre to process the request would be
the same. If the Centre was allowed to charge more simply because the request did
not come from the individual directly, third parties would simply ask individuals (or their
substitute decision-makers) to request access themselves, rather than sign consent
forms to be submitted to health information custodians by third parties. Divergent
‚Äúreasonable cost recovery‚Äù schemes could create financial incentives regarding how
records are requested and produced ‚Äì adding a layer of complexity that is neither
necessary nor desirable in the broader context of the Act. Divergent ‚Äúreasonable cost
recovery‚Äù schemes would also create needless and unproductive disputes between
parties as to what the ‚Äúreasonable cost recovery‚Äù should be for one type of person as
compared to another type of person.

[36] I conclude that the phrase ‚Äúreasonable cost recovery‚Äù has the same meaning in
sections 35(2) and 54(11). As a result, it is not necessary for me to determine whether
the request at issue is properly considered a request for ‚Äúaccess‚Äù or a request for
‚Äúdisclosure‚Äù under the Act. In the next issue, I address whether the $117 fee charged
by the London Health Sciences Centre exceeds ‚Äúreasonable cost recovery.‚Äù

Issue D:     Does the fee of $117.00 exceed ‚Äúreasonable cost recovery‚Äù as
             that term is used in the Act ? If the answer is yes, what would
             qualify as ‚Äúreasonable cost recovery‚Äù in this case?

[37] As noted above, both sections 35(2) and 54(11) of the Act prohibit a health
information custodian from charging a fee that exceeds ‚Äúthe prescribed amount‚Äù or the
‚Äúamount of reasonable cost recovery.‚Äù Given the absence of a regulation prescribing the
amount of the fee that may be charged, this office has the authority pursuant to Part VI
of the Act to conduct a review to determine whether the fee charged exceeds ‚Äúthe
amount of reasonable cost recovery‚Äù within the meaning of the Act.

[38] In Order HO-009, this office previously considered what is meant by ‚Äúreasonable
cost recovery‚Äù under section 54(11) of the Act.



                              [IPC Order HO-14/March 6, 2015]
                                         - 11 -

[39] The Notice of Review issued in this case asked the parties to submit
representations on the meaning of the phrase ‚Äúreasonable cost recovery‚Äù in sections
35(2) and 54(11) of the Act and on the application of Order HO-009 to the
circumstances of this complaint.

REPRESENTATIONS

The London Health Sciences Centre

[40] The London Health Sciences Centre did not record the time spent responding to
the request for records of personal health information at issue in this review.

[41] The London Health Sciences Centre asserts that it has conducted a ‚Äútrue costs
recovery‚Äù exercise and that the fee charged for 112 pages of records did not exceed
‚Äúreasonable cost recovery‚Äù because the $117 fee ‚Äúis within our true cost recovery
model.‚Äù Based upon that exercise, it has determined that reasonable cost recovery fees
are as follows:

      1-50 pages:         $30
      51-100:             $60
      101-200 pages:      $120
      201-300 pages:      $180
      301+ pages:         $300

[42] The London Health Sciences Centre further supports its $117 fee by reference to
a previous unrelated request from another party for 114 pages of records that was
included in its ‚Äútrue cost recovery‚Äù exercise. According to the London Health Sciences
Centre, the time to complete that previous request was more than 2 hours, made up as
follows:

      25 minutes for Opening and logging request
      15 minutes for retrieving the chart
      20 minutes of photocopying
      70 minutes for completing request, including processing & counting pages
      Total minutes; 2 hours, 10 minutes

[43] The London Health Sciences Centre calculated its ‚Äútrue cost‚Äù of responding to
that request to be $106.04:

      $76.55 Total HIM professional salary (including benefits)
      $18.61 Supply costs
      $10.88 Mail costs

[44] In respect of the application of Order HO-009, while the London Health Sciences
Centre asserts that Order HO-009 does not apply to the circumstances of this



                             [IPC Order HO-14/March 6, 2015]
                                          - 12 -

complaint, it does state that the fee charged was ‚Äúin compliance‚Äù with Order HO-009.

[45] As noted above, the London Health Sciences Centre also addressed why the
amount of the fee differs depending on whether the request was made directly by the
individual to whom the personal health information relates or by any other person. In
its submissions, the London Health Sciences Centre stated that: ‚ÄúDisclosure requests
are typically larger, more complicated and take a longer time to complete.‚Äù

The Com plainant

[46] The complainant‚Äôs lawyer submits that the fee of $117 exceeds ‚Äúreasonable cost
recovery.‚Äù The complainant‚Äôs lawyer states:

      ‚Ä¶ if the hospital was to follow the ‚Äúreasonable cost recovery‚Äù amounts
      originally ordered in decision HO-009 in November 2010, then the cost
      based on a $30.00 search and copy fee and $0.25 copying per page would
      amount to $58.00. The hospital wishes to charge double that amount -
      $117.00 ‚Äì and this is certainly not reasonable cost recovery.

[47] As set out below, in applying the fee schedule in Order HO-009 to the request in
this case, I calculate a fee owing of $53 for 112 pages.

ANALYSIS AND FINDINGS

[48] The expression ‚Äúreasonable cost recovery‚Äù is not defined in the Act. In
interpreting the meaning of the words ‚Äúreasonable cost recovery‚Äù in the Act, I will again
apply the modern rule of statutory interpretation that has been set out previously in this
order.

[49] Applying the modern rule, I find that ‚Äúreasonable cost recovery,‚Äù as that phrase
appears in the Act, was not intended to represent full cost recovery. In Order HO-009, I
previously considered whether ‚Äúreasonable cost recovery‚Äù as presented in section
54(11) means ‚Äútotal or actual costs‚Äù:

      Turning to the grammatical and ordinary sense of the words used, I note
      that the Act does not use the words ‚Äúactual cost recovery‚Äù or ‚Äúfull cost
      recovery‚Äù but rather the words ‚Äúreasonable cost recovery.‚Äù In my opinion,
      a plain and ordinary reading of the words ‚Äúreasonable cost recovery‚Äù in
      section 54(11) of the Act, having regard to their entire context, including
      the context of the scheme and stated purposes of the Act, suggests an
      intention that Custodians be entitled to recover something less than the
      actual or full costs associated with providing individuals access to their
      records of personal health information.




                              [IPC Order HO-14/March 6, 2015]
                                           - 13 -

[50] I acknowledge that the circumstances in Order HO-009 concerned an individual
seeking access to her own records of personal health information. That said, it is critical
to reiterate the finding in Order HO-009 that the words ‚Äúreasonable cost recovery‚Äù are
to be read ‚Äúin their entire context.‚Äù As noted above, it would create an absurd result to
permit different amounts to be charged as ‚Äúreasonable cost recovery‚Äù pursuant to
sections 35(2) and 54(11) of the Act.

[51] The plain and ordinary meaning of ‚Äúreasonable cost recovery‚Äù supports my
finding that this phrase does not mean full cost recovery. The costs that can be charged
by a health information custodian are explicitly limited to those which are ‚Äúreasonable.‚Äù
If the Legislature had intended that health information custodians should be able to
charge for all of their actual costs, the Act could have clearly said so. It does not.

[52] The use of the word ‚Äúreasonable‚Äù to describe cost recovery also suggests that
costs should not be excessive. Health information custodians should be encouraged to
develop file systems and train their agents in a manner that facilitates the efficient
processing of requests for, or disclosures of, records of personal health information.
Interpreting the Act as permitting a custodian to charge for its costs associated with the
processing of a request, whether or not those costs are reasonable, would provide little
incentive for it to become efficient in how it handles and processes these requests for
records of personal health information.

[53] The purposes of the Act also do not support the London Health Sciences Centre‚Äôs
claim that ‚Äúreasonable cost recovery‚Äù should reflect the ‚Äútrue costs‚Äù of responding to a
request for records of personal health information. Full cost recovery may erect a
barrier to individuals requesting their own records of personal health information or
others who request records of personal health information on their behalf. As noted in
Order HO-009, such barriers would be inconsistent with one of the purposes of the
legislation, as set out in section 1(b) of the Act: ‚Äúto provide a right of access to personal
health information, about themselves‚Ä¶.‚Äù

[54] In my view, the fee charged by the London Health Sciences Centre in this case is
not ‚Äúreasonable.‚Äù The London Health Sciences Centre has purportedly charged for its
‚Äútrue costs,‚Äù which I understand to mean the full costs of responding to the request for
the complainant‚Äôs records. As I stated above, I find that the Act does not permit ‚Äúfull
cost recovery.‚Äù

[55] Moreover, the amount of the fee charged by the London Health Sciences Centre
is not justified. It has provided no evidence as to how long it took to process the
request in this case. Instead, it seeks to justify the $117 fee by referencing an
unrelated request from another party for a similar number of pages of records.

[56] Even accepting that this was the actual time taken in the exercise conducted by
the London Health Sciences Centre, I find these times excessive, considering the
relatively modest nature and breadth of tasks required. I conclude that the fee



                               [IPC Order HO-14/March 6, 2015]
                                                - 14 -

calculated on the basis of this ‚Äútrue cost‚Äù recovery exercise does not amount to
reasonable cost recovery.

[57] In its representations, the London Health Sciences Centre raised statements
purportedly made by staff from this office clarifying that a request to release a record of
personal health information to a third party would be a request for disclosure and not a
request for access. 3 For the reasons already outlined, it is not necessary for me to
decide whether such a request is a request for ‚Äúaccess‚Äù or ‚Äúdisclosure‚Äù or to address
these purported facts. I note that although Order HO-009 was decided in the context
of an access request, it is my view that the Order is fully applicable to both access and
disclosure requests.

[58] After reviewing several proposed fee schemes, this office concluded in Order HO-
009 that the regulation proposed by the Minister on March 11, 2006 in The Ontario
Gazette, which prescribed the maximum amount of fees that a health information
custodian could charge an individual for access to records of personal health
information under the Act, ‚Äúprovides the best framework for determining ‚Äòthe amount of
reasonable cost recovery‚Äô as set out in section 54(11) of the Act.‚Äù I confirm that
decision again regarding reasonable cost recovery, for both access and disclosure.

[59] For the purposes of certainty, I will reproduce the fee framework adopted in
Order HO-009, with necessary modifications, to make clear that this framework
addresses requests for both access and disclosure:

         For the purposes of subsections 35(2) and 54(11) of the Act, the amount
         of the fee that may be charged shall not exceed $30 for any or all of the
         following:

               1. Receipt and clarification, if necessary, of a request for a
                  record.

               2.      Providing an estimate of the fee that will be payable
                       under subsection 54(10) of the Act in connection with
                       the request.

               3.      Locating and retrieving the record.

               4.      Review of the contents of the record for not more
                       than 15 minutes by the health information custodian
                       or an agent of the custodian to determine if the
                       record contains personal health information to which
                       access or disclosure may or shall be refused.

3
  In any event, as an administrative decision maker, I am not bound by the principle of stare decisis ‚Äì
TransCanada Pipelines Ltd. v. Beardmore (Township) (2000), 186 D.L.R. (4th) 403 at para. 129 (Ont.
C.A.).



                                   [IPC Order HO-14/March 6, 2015]
                                  - 15 -

      5.    Preparation of a response letter.

      6.    Preparation of the record for photocopying, printing
            or electronic transmission.

      7.    Photocopying the record to a maximum of the first 20
            pages or printing the record, if it is stored in
            electronic form, to a maximum of the first 20 pages,
            excluding the printing of photographs from
            photographs stored in electronic form.

      8.    Packaging of the photocopied or printed copy of the
            record for shipping or faxing.

      9.    If the record is stored in electronic form, electronically
            transmitting a copy of the electronic record instead of
            printing a copy of the record and shipping or faxing
            the printed copy.

      10.   The cost of faxing a copy of the record to a fax
            number in Ontario or mailing a copy of the record by
            ordinary mail to an address in Canada.

      11.   Supervising examination of the original record for not
            more than 15 minutes.

In addition to the fee charged above, fees for the services set out in
Column 1 of the Table below shall not, for the purposes of subsections
35(2) and 54(11) of the Act, exceed the amounts set out opposite the
service in Column 2 of the Table below.

   ITEM                         COLUMN 1                            COLUMN 2
     1.      For making and providing photocopies                25 cents for
             or computer printouts of a record                   each page
                                                                 after the first
                                                                 20 pages

     2.      For making and providing a paper copy of a          50 cents per
             record from microfilm or microfiche                 page
     3.      For making and providing a floppy disk or a          $10
             compact disk containing a copy of a record
             stored in electronic form
     4.      For making and providing a microfiche copy          50 cents per
             of a record stored on microfiche                    sheet




                      [IPC Order HO-14/March 6, 2015]
                          - 16 -

5.    For making and providing a copy of a
      microfilm of a record stored on microfilm
      that is,
         i. 16 mm                                     $25 per reel
         ii. 35 mm                                    $32 per reel
6.    For printing a photograph from a negative
      or from a photograph stored in electronic
      form, per print,
         i. measuring 4‚Äù x 5‚Äù                   $ 10
         ii. measuring 5‚Äù x 7‚Äù                        $ 13
         iii. measuring 8‚Äù x 10‚Äù                      $ 19
         iv. measuring 11‚Äù x 14‚Äù                      $ 26
         v. measuring 18‚Äù x 20‚Äù                       $ 32
7.    For making and providing a copy of a 35 mm $ 2
      slide
8.    For making and providing a copy of an audio $ 5
      cassette
9..   For making and providing a copy of a ¬º‚Äù,
      ¬Ω‚Äù or 8 mm video cassette,
         i. that is one hour or less in length        $ 20
          ii. that is more than one hour but not      $ 25
      more        than two hours in length
10.   For making and providing a copy of a
      ¬æ‚Äù video cassette,
          i. that is not more than 30 minutes in      $ 18
          length
          ii. that is more than 30 minutes but        $ 23
      not more than one hour in length
11.   For producing a record stored on medical        $5 per film
      film, including x-ray, CT and MRI films
12.   For the review by a health information          $45 for every
      custodian or an agent of the custodian of the   15 minutes
      contents of a record to determine if the        after the first
      record contains personal health information     15 minutes
      to which access or disclosure may or shall
      be refused
13.   For supervising examination of original         $6.75 for
      records                                         every 15
                                                      minutes



              [IPC Order HO-14/March 6, 2015]
                                         - 17 -

[60] In response to the London Health Sciences Centre‚Äôs claim that requests for
disclosure are ‚Äútypically large, more complicated, and take a longer time to
complete,‚Äù I note that the above fee framework permits a health information
custodian to charge additional amounts for other activities beyond those
encompassed in the $30 set fee where requests for records of personal health
information are more complex or are more voluminous records. For example, the
cost recovery framework allows additional costs to be charged where the records
requested are more than twenty pages (at a rate 25 cents per page). As a result, a
request for 1000 pages of records could result in a fee of $275.

[61] In applying the above framework to the records at issue in this review, I
find that the London Health Sciences Centre is entitled to charge a $53 fee. The
amount is calculated as follows: $30 for processing the request and
photocopying/printing the record to a maximum of the first 20 pages and the sum
of $0.25 per page for the remainder.

SUMMARY OF FINDINGS:

1. The records at issue are records of personal health information as defined in
   sections 2 and 4 of the Act.

2. The person who operates the London Health Sciences Centre is a health information
   custodian as defined in paragraph 4(i) of section 3(1) of the Act.

3. It is not necessary in the circumstances of this complaint for me to determine
   whether the request by the lawyer for copies of his client‚Äôs records of personal
   health information is a request for access or a request for disclosure under the Act.

4. The fee of $117 exceeds ‚Äúreasonable cost recovery‚Äù as that term is used in the Act.

ORDER:

 I do not uphold the fee of $117 and order the London Health Sciences Centre
 to reduce the fee to $53. I order the London Health Sciences Centre to refund
 $64 to the complainant‚Äôs lawyer.




Original Signed By:                                      March 6, 2015
 Brian Beamish
 Commissioner



                             [IPC Order HO-14/March 6, 2015]
Copyright ¬© 2004 Ontario Hospital Association, Ontario Hospital eHealth
Council, Ontario Medical Association, Office of the Information and
Privacy Commissioner/Ontario and Queen‚Äôs Printer for Ontario.
All rights reserved.



ISBN 0-88621-310-X
                         WARNING AND DISCLAIMER

This Toolkit has been prepared by McMillan Binch LLP and IBM Business
Consulting Services for the Ontario Hospital Association, for the ownership and
use of the Ontario Hospital Association and the Ontario Hospital eHealth Council,
the Ontario Medical Association, the Office of the Information and Privacy
Commissioner and the Ministry of Health and Long-Term Care, as a general
guide to assist hospitals and physicians to meet their obligations under the
Personal Health Information Protection Act, 2004.

‚Ä¢   This Toolkit is designed to assist in complying with the law and meeting the
    changing expectations of patients and the public.

‚Ä¢   The resource materials provided in this Toolkit are for general information
    purposes only. They should be adapted to the circumstances of each hospital
    or physician using the Toolkit.

‚Ä¢   This Toolkit reflects interpretations and practices regarded as valid when it
    was published based on available information at that time.

‚Ä¢   This Toolkit is not intended, and should not be construed, as legal or
    professional advice or opinion.

‚Ä¢   Hospitals or physicians concerned about the applicability of privacy
    legislation to their activities are advised to seek legal or professional advice
    based on their particular circumstances.

‚Ä¢   In addition, Ontario‚Äôs Information and Privacy Commissioner has an
    important role to play in providing further guidance on how the Personal
    Health Information Protection Act, 2004 is being applied and interpreted. You
    should monitor the Commission‚Äôs website at http://www.ipc.on.ca as well as
    that of the Ministry of Health and Long-Term Care at
    http://www.health.gov.on.ca/english/public/updates/archives/hu_03/priv_legis
    lation.html.

This is the first edition of the Toolkit. A second edition may be published in due
course. Your feedback on this first edition is appreciated.




                                                                                       i
     ACKNOWLEDGEMENTS

     Consultants
     This Toolkit was written and produced for the Ontario Hospital Association, the
     Ontario Medical Association, the Ontario Hospital E-Health Council, the
     Information and Privacy Commissioner/Ontario and the Ministry of Health and
     Long-Term Care by the following team drawn from McMillan Binch LLP and
     IBM Business Consulting Services:

     McMillan Binch LLP                     IBM Business Consulting Services
     Simon Chester                          Neil Stuart

     LYDIA WAKULOWSKY
      Principal Author

     Holly Agnew                            Erfa Alani
     Grant Carmichael                       Nigel Brown
     Sarah Diamond                          Janice Edgecombe
     Rebecca Huang                          Lane Ilersich
     Reema Kapoor                           Marta Yurcan
     Sharon Mitchell-Kamara
     Ginevra Saylor




ii
Steering Committee/Privacy Toolkit Working Group
We wish to acknowledge and thank the members of the Steering Committee and
Privacy Toolkit Working Group (PTWG) who contributed to the development of
this toolkit:

Steering Committee

Brian Beamish, Director, Policy and Compliance, Information and Privacy
Commissioner / Ontario
Elizabeth Carlton, Senior Advisor, Legislation & Policy, Ontario Hospital
Association
Susan Crozier (Chair of PTWG), Corporate Chief Privacy Officer, Royal Ottawa
Health Care
Nancy Gabor, eHealth Analyst, Ontario Hospital eHealth Council
Debra Grant, Senior Health Privacy Specialist, Information and Privacy
Commissioner / Ontario
Barb LeBlanc, Director, Health Policy, Ontario Medical Association
Michele Sanborn, Manager, Health Information Privacy, Ministry of Health and
Long-Term Care
Joanne Serraf, Project Manager, Privacy Toolkit, Ontario Hospital Association

Privacy Toolkit Working Group

Meredith Appleby, Privacy Officer, Shared Services West
Elaine Cathcart, Privacy Compliance Manager, Niagara Health System
Jane Dargie, Director, Privacy, Smart Systems for Health Agency
Kate Dewhirst, Legal Counsel, Centre for Addiction and Mental Health
Mary Gavel, Director, Risk Management and Patient Relations, Rouge Valley
Health System
Elizabeth Goff, Privacy Officer, St. Joseph's Health Centre (Toronto)
Dr. Ron Heslegrave, Chair, Research Ethics Board, University Health Network
and Mount Sinai Hospital
Tiffany Jay, Privacy Manager, University Health Network
Peter Lambert, Manager, Information Security, St. Michael's Hospital
Don Livingstone, Chief Medical Officer, Sunnybrook & Women's College Health
Sciences Centre


                                                                                iii
     Roberta MacDonald, Director, Information Systems / Chief Privacy Officer, St.
     Mary's General Hospital
     Sara McRae, Privacy Officer, Lakeridge Health
     Sharon Pfaff, Chief Privacy and Information Officer, Chatham-Kent Health
     Alliance
     Ruth Servos, Health Information Technician, Hotel Dieu Health Sciences
     Hospital, Niagara
     Joyce Seto, Director, Information and Information Technology, Cardiac Care
     Network of Ontario
     Sharon vanValkenburg, Director, Information Services, Temiskaming Hospital
     Pearl Veenema, Managing Director of Campaigns, University Health Network
     E. Jean Wright, Director, Information Systems, North Bay Psychiatric Hospital
     and North East Mental Health Centre

     Ministry of Health and Long-Term Care
     The Ontario Hospital Association also wishes to extend its appreciation to legal
     counsel of the Ministry of Health and Long Term Care for their time and efforts
     in reviewing the Toolkit:

     Legal Counsel

     Fannie Dimitriadis, Legal Counsel, Ministry of Health and Long Term Care
     Michael Orr, Legal Counsel, Ministry of Health and Long Term Care
     Halyna Perun, Legal Counsel, Ministry of Health and Long Term Care


     Funding for this Toolkit was provided by the Ontario Hospital Association, the
     Ontario Hospital E-Health Council, the Ontario Medical Association and the
     Ministry of Health and Long-Term Care.




iv
                                                                                  Table of Contents


General Privacy Compliance
Using This Toolkit ...................................................................................................5
Overview..................................................................................................................6
Application and Scope of the Act ............................................................................6
The Ten Privacy Principles......................................................................................7
What You Need to Do..............................................................................................9
Checklists, Templates and Tools ...........................................................................10
       Sample Written Statement of Information Practices

Contact Person
Key Points..............................................................................................................17
The Rule.................................................................................................................18
What You Need To Do ..........................................................................................18
   What You Should Do.......................................................................................19
Related Sections of the Act....................................................................................19
Checklists, Templates and Tools ...........................................................................19

Consent
Key Points..............................................................................................................25
The Rule.................................................................................................................26
What You Need To Do ..........................................................................................27
    Implied Consent ...............................................................................................27
        What is Implied Consent?
        When is Implied Consent Acceptable?
        Guidelines for Relying on Implied Consent
    Express Consent...............................................................................................28
        What Is Express Consent?
        When Is Express Consent Required?
        Guidelines for Obtaining Express Consent
    Withdrawal of Consent ....................................................................................30
    Conditional Consent.........................................................................................30
    When Consent Is Not Required .......................................................................30
        Collection
        Use
        Disclosure
Psychiatric Facilities ..............................................................................................35
Patient Capacity .....................................................................................................36


                                                                                                                              v
Table of Contents

             Children and Teenagers
             Dealing With Substitute Decision-Makers
      Related Sections of the Act....................................................................................39
      Checklists, Templates and Tools ...........................................................................40
             Decision Tree For Consent
             Sample Consent Form
             Sample Withdrawal of Consent Form

      Collection, Use and Disclosure
      Key Points..............................................................................................................49
      The Rule.................................................................................................................50
      Collection...............................................................................................................50
         What You Need To Do ....................................................................................50
               What You Should Do
      Use .........................................................................................................................51
         What You Need To Do ....................................................................................51
               What You Should Do
               Preventing Unauthorized Use by Authorized Users
               Use of Personal Health Information by the Circle of Care
               Videotaping, Audiotaping and Photographing Personal Health
                     Information
      Disclosure ..............................................................................................................57
         What You Need To Do ....................................................................................57
               What You Should Do
               Situations Involving Disclosure
               Disclosure Tables
               Mandatory Disclosure
               Disclosure for Health Related Programs and Legislation
               Disclosure to Lawyers, Insurance Companies, Adjusters, Investigators
               Disclosure to Legal Authorities and Law Enforcement
      Related Sections of the Act....................................................................................68
      Checklists, Templates and Tools ...........................................................................68
               Process Map for Disclosing Personal Health Information
               Sample Confidentiality Agreement
               Sample Consent to Disclose Personal Health Information Form

      Accessing Health Records
      Key Points..............................................................................................................77
      The Rule.................................................................................................................78
         What You Need To Do ....................................................................................78


vi
                                                                                  Table of Contents

       What You Should Do
       Fees for Providing Access
       Timeframe to Respond to a Request for Access
       Urgent Requests for Access
       Refusing a Request for Access
       Guidelines for Refusal of Access
       Failing to Respond to a Request for Access
Related Sections of the Act....................................................................................83
Checklists, Templates and Tools ...........................................................................84
       Sample Process Map ‚Äì Access to Personal Health Record
       Sample Form to Request Access to Personal Health Record
       Sample Checklist ‚Äì Process for Accessing a Personal Health Record
       Sample Letter for Extension to Comply with Request
       Sample Refusal of Access Letter

Correcting Health Records
Key Points..............................................................................................................97
The Rule.................................................................................................................98
What You Need To Do ..........................................................................................99
       Responding to Requests for Correction
       What You Should Do
       Where You Do Not Have To Make Corrections
       Timeframe for Responding to a Request for Correction
       Conflict Resolution: Refusing a Request for Correction
Related Sections of the Act..................................................................................102
Checklists, Templates and Tools .........................................................................102
       Process Map for Responding to Requests for Correction
       Sample Request Form for Correction to Personal Health Record

Dealing with Health Information
Key Points............................................................................................................111
Storage and Retention ..........................................................................................112
   The Rule.........................................................................................................112
       Storage
       Retention
What You Need To Do ........................................................................................112
       What You Should Do
Disposal................................................................................................................115
   The Rule.........................................................................................................115



                                                                                                                              vii
Table of Contents

       What You Need To Do ........................................................................................115
              What You Should Do
       Transfer ................................................................................................................116
          The Rule.........................................................................................................116
       What You Need To Do ........................................................................................116
              Transfer to Another Facility
              Transfer to a Successor
              Transfer to Archives
              What You Should Do
       Related Sections of the Act..................................................................................117
       Checklists, Templates and Tools .........................................................................117
              Summary of Retention Periods
              Supplementary Table A Retention Periods for Records Relating to Drugs
                    Dispensed under the Ontario Drug Benefit Plan
              Supplementary Table B Retention Periods Required for Patient Records
                    Relating to Dispensing of Drugs Under The Drugs and Pharmacies
                    Regulations Act

       Security ‚Äì Introduction...........................................................................125

       Security ‚Äì First Steps
       Key Points............................................................................................................135
       Security Program and Policy ...............................................................................136
           What You Should Do.....................................................................................136
              Small Office Applicability
       Roles and Responsibilities ...................................................................................138
           What You Should Do.....................................................................................138
              Small Office Applicability
       Information Inventory and Classification ............................................................139
           What You Should Do.....................................................................................139
              Small Office Applicability
       Checklists, Templates and Tools .........................................................................140
              Appendix A ‚Äì Roles and Responsibilities
              Appendix B ‚Äì Information Inventory and Classification

       Security ‚Äì People
       Key Points............................................................................................................147
       Personal Responsibilities for Security .................................................................148
          What You Should Do.....................................................................................148
          Physical Security............................................................................................148


viii
                                                                                   Table of Contents

       Small Office Applicability
Authentication and Authorization........................................................................150
   What You Should Do.....................................................................................150
       Small Office Applicability
Related Sections of the Act..................................................................................151
Checklists, Templates and Tools .........................................................................151
       Appendix A ‚Äì Personal Responsibilities for Security
       Appendix B ‚Äì Authentication and Authorization

Security ‚Äì Institutional Safeguards
Key Points............................................................................................................169
Perimeter Security................................................................................................170
    What You Should Do.....................................................................................170
       Physical Perimeter Security
       Electronic Access Points
       Small Office Applicability
Malicious Software ..............................................................................................172
    What You Should Do.....................................................................................172
       Small Office Applicability
Wireless and Portable Devices.............................................................................173
    What You Should Do.....................................................................................173
       Small Office Applicability
Related Sections of the Act..................................................................................174
Checklists, Templates and Tools .........................................................................174
       Appendix A ‚Äì Perimeter Security
       Appendix B ‚Äì Malicious Software
       Appendix C ‚Äì Wireless and Portable Devices

Sustaining Security
Key Points............................................................................................................195
Business Continuity .............................................................................................196
   What You Should Do.....................................................................................196
        Small Office Applicability
Development and Maintenance............................................................................198
   What You Need To Do ..................................................................................198
        Small Office Applicability
Audit ....................................................................................................................199
   What You Need To Do ..................................................................................199
        Small Office Applicability


                                                                                                                                ix
Table of Contents

      Recommended Standards.....................................................................................201
         What You Need To Know .............................................................................201
      Related Sections of the Act..................................................................................203
      Checklists, Templates and Tools .........................................................................203
             Appendix A ‚Äì Business Continuity
             Appendix B ‚Äì Development And Maintenance
             Appendix C ‚Äì Audit

      Research
      Key Points............................................................................................................219
      The Rule...............................................................................................................220
      What You Need To Do ........................................................................................221
         Collection.......................................................................................................221
         Use        .........................................................................................................221
         Disclosure ......................................................................................................222
         Research Plan.................................................................................................222
         Research Ethics Board Composition .............................................................223
         Research Ethics Board Duties........................................................................223
         Researcher Duties ..........................................................................................224
         Disclosure Under Other Acts .........................................................................225
         Transition Rules .............................................................................................225
         Express Consent.............................................................................................225
      Related Sections of the Act..................................................................................226
      Checklist, Templates and Tools...........................................................................226
             Research Approval Checklist
             Sample Application to Research Ethics Board
             Sample Information Sharing Agreement
             Sample Consent Form for Study Participant

      Fundraising
      Key Points............................................................................................................257
      The Rule...............................................................................................................258
      What You Need To Do ........................................................................................258
         Relying on Implied Consent ..........................................................................258
         Obtaining Express Consent............................................................................259
         Disclosing Information to the Hospital Foundation.......................................259
         Providing Information to Hired Fundraisers..................................................260
      Related Sections of the Act..................................................................................260
      Checklists, Templates and Tools .........................................................................260
             Fundraising Decision Tree


x
                                                                                 Table of Contents

           Sample Consent Form for Fundraising
           Sample Withdrawal of Consent Form for Fundraising

Managing Contracts and Agents
Key Points............................................................................................................269
The Rule...............................................................................................................270
What You Need To Do ........................................................................................271
   Due Diligence ................................................................................................272
   Contracts ........................................................................................................273
   Enforcement...................................................................................................273
   Information Sharing Agreements...................................................................274
   Dealing with Agents Operating Outside of Ontario ......................................274
Related Sections of the Act..................................................................................274
Checklists, Templates and Tools .........................................................................275
       Checklist for Agents Agreements
       Checklist for Information Sharing Agreements

Oversight
Key Points............................................................................................................279
Privacy Breaches..................................................................................................280
    What is a Privacy Breach? .............................................................................280
    Avoiding a Privacy Breach ............................................................................280
    Addressing a Privacy Breach .........................................................................281
        Containment
        Notification
        Additional Steps
    Reviewing a Privacy Complaint ....................................................................283
The Commissioner‚Äôs Role....................................................................................284
The Commissioner‚Äôs Powers ...............................................................................284
    Responding to Privacy Complaints................................................................284
    Initiating Privacy Reviews.............................................................................285
    Conducting Privacy Reviews.........................................................................285
    Result of the Review......................................................................................286
    Offence and Sanctions ...................................................................................287
Related Sections of the Act..................................................................................288
Checklists, Templates and Tools .........................................................................289
        Sample Inventory of Personal Health Information
        Commissioner Contact Information
        Decision Tree - Responding to Complaints About Privacy Breaches



                                                                                                                             xi
Table of Contents

      Glossary..........................................................................................................293

      Appendix of Forms ...................................................................................309
                 Sample Written Statement of Information Practices
                 Sample Confidentiality Agreement
                 Sample Consent to Disclose Personal Health Information Form
                 Sample Consent Form
                 Sample Withdrawal of Consent Form
                 Sample Form to Request Access to Personal Health Record
                 Sample Letter for Extension to Comply with Request
                 Sample Refusal of Access Letter
                 Sample Request Form for Correction to Personal Health Record
                 Sample Application to Research Ethics Board
                 Sample Consent Form for Study Participant

      Diagnostic Tool
      Overview..............................................................................................................335
         Purpose .........................................................................................................335
         Instructions.....................................................................................................335
      Survey Questions .................................................................................................336
         1 ‚Äì General Privacy Compliance ...................................................................336
         2 ‚Äì Contact Person .........................................................................................337
         3 ‚Äì Consent ....................................................................................................338
         4 ‚Äì Managing Health Information..................................................................339
         5 ‚Äì Accessing Health Records .......................................................................341
         6 ‚Äì Correcting Health Records.......................................................................342
         7 ‚Äì Dealing With Health Information ............................................................343
         8 ‚Äì Security, First Steps .................................................................................344
         9 ‚Äì Security, People .......................................................................................345
         10 ‚Äì Security, Institutional.............................................................................347
         11 ‚Äì Security, Sustaining Security .................................................................350
         12 ‚Äì Research.................................................................................................352
         13 ‚Äì Fundraising ............................................................................................353
         14 ‚Äì Managing Contracts & Agents...............................................................354
         15 ‚Äì Oversight................................................................................................355
      Score Sheet and Analysis.....................................................................................356
         Score Evaluation ............................................................................................358




xii
General Privacy Compliance
2
                                            General Privacy Compliance

Table of Contents


Using This Toolkit ...................................................................................................5

Overview..................................................................................................................6

Application and Scope of the Act ............................................................................7

The Ten Privacy Principles......................................................................................7

What You Need to Do..............................................................................................9

Checklists, Templates and Tools ...........................................................................10
       Sample Written Statement of Information Practices




                                                                                                                              3
General Privacy Compliance




4
                                 General Privacy Compliance



Using This Toolkit
A toolkit contains the tools needed to get the job
done. In this case, the job is to ensure you comply
with the Act. Legislation is not easy to read, let
alone interpret and apply to your own situation.
This Toolkit provides a variety of tools designed to
help you understand and apply the new privacy
legislation.

To get started, begin by reading:

              ‚Ä¢   General Privacy Compliance ‚Äì for a very general overview of
                  the Act
              ‚Ä¢   Consent ‚Äì to understand the foundation of privacy compliance
              ‚Ä¢   Security ‚Äì to understand how to protect personal health
                  information
              ‚Ä¢   Oversight ‚Äì to learn about the role and powers of the
                  Information and Privacy Commissioner/Ontario

Then proceed to the sections that affect your role:

              ‚Ä¢   Contact Person
              ‚Ä¢   Managing Health Information
              ‚Ä¢   Accessing Health Records
              ‚Ä¢   Correcting Health Records
              ‚Ä¢   Dealing With Health Information
              ‚Ä¢   Research
              ‚Ä¢   Fundraising
              ‚Ä¢   Managing Contracts and Agents

Then use the Diagnostic Tool:



              ‚Ä¢   to assess your state of privacy compliance




In each section you will find:

Key Points: A high level summary of the section.


                                                                                 5
General Privacy Compliance

     The Rule: A brief summary of the key requirements of the law.

     What You Need To Do: A brief summary of the key tasks that you must perform
     to comply with the law.

     What You Should Do: Recommended best practices for you to consider
     implementing.

     Related Sections of the Act: A list of the sections of the Act discussed in the
     section, should you want to refer directly to the Act for more information.

     Checklists, Templates and Tools: Tools to help you perform the tasks.

     To support your understanding of the content, there is a Glossary and Index at
     the end.




     Overview
     Starting November 1, 2004, you must comply with the Personal Health
     Information Protection Act, 2004, Ontario‚Äôs new health information privacy
     legislation. The Act regulates how you collect, use, retain, transfer, disclose,
     provide access to and dispose of patients‚Äô personal health information.

     The Act has a number of purposes:

     ‚Ä¢   to establish rules for the collection, use and disclosure of personal health
         information that protect the confidentiality of that information and the privacy
         of individuals, while facilitating the effective provision of health care,

     ‚Ä¢   with a few limited and specific exceptions, to provide individuals with a right
         to access and correct their personal health information,

     ‚Ä¢   to provide for independent review and resolution of complaints about personal
         health information, and

     ‚Ä¢   to provide effective remedies for contraventions of the Act.




6
                              General Privacy Compliance


Application and Scope of the Act
The Act applies to a variety of organizations and individuals within the health
care sector. These organizations and individuals are called health information
custodians, and include hospitals and health care practitioners. The Act also
applies to agents, who can be either organizations or individuals, and who are
authorized to act for or on a health information custodian‚Äôs behalf. The Act
regulates how health information custodians and their agents may collect, use,
retain, transfer, disclose, provide access to and dispose of patients‚Äô personal
health information. See the Glossary for a definition of the italicized terms.

This Toolkit uses the term ‚Äúyou‚Äù for the sake of clarity and brevity. The terms
‚Äúyou‚Äù and ‚Äúyour‚Äù describe the legal obligations of:

‚Ä¢   hospitals, who are health information custodians, and who have a broad
    institutional responsibility for privacy compliance,

‚Ä¢   physicians, who are health information custodians when operating their own
    private practice within a hospital (i.e., when they rent out office space at a
    hospital) and who are agents when acting for a hospital (i.e., when they treat
    patients in the hospital and contribute to patients‚Äô health records in that
    regard), and who have individual responsibility for privacy compliance, and

‚Ä¢   hospital professional staff members, administrative staff members, students
    and volunteers, who are agents of the hospital, and who also have individual
    responsibility for privacy compliance.

Each of these organizations and individuals (i.e., you) must make efforts (to the
extent reasonable given the circumstances) to fulfil the key tasks described in this
Toolkit, and to protect patients‚Äô privacy and the confidentiality of their personal
health information.

This Toolkit uses the term ‚Äúpatient‚Äù for the sake of clarity and brevity. The term
‚Äúpatient‚Äù should be read to include all individuals about whom you collect, use
and disclose personal health information.




The Ten Privacy Principles
The Act builds upon and codifies many of the existing high standards and
protections found in the common law, and various professional codes, policies
and guidelines.


                                                                                       7
General Privacy Compliance

     The Act is also based on ten privacy principles, which are derived from the
     Canadian Standards Association‚Äôs Model Code for the Protection of Personal
     Information. Most privacy legislation in the world is based on these ten privacy
     principles:

      Privacy Principle        Requirement

      Accountability           Designate a contact person to assist you in meeting your privacy
                               obligations, and to deal with any access requests, privacy related
                               inquiries and complaints, and Commissioner investigations

      Identifying Purposes     Inform your patients of the purposes for which their personal health
                               information is collected, used and disclosed, unless otherwise
                               exempted by the Act

      Consent                  Rely on implied consent, where appropriate, or obtain express consent
                               from your patients when collecting, using or disclosing their personal
                               health information, unless otherwise exempted by the Act

      Limiting Collection      Limit your collection of personal health information to that which is
                               necessary for the identified purposes or for purposes that the Act
                               permits or requires

      Limiting Use and         Limit your use and disclosure of personal health information to the
      Disclosure               identified purposes, unless you obtain further consent or your use or
                               disclosure is permitted or required by law

      Accuracy                 Take reasonable steps to ensure that your patients‚Äô personal health
                               information is as accurate, complete and up-to-date as is necessary for
                               the purposes for which you use or disclose it

                               Tell the person to whom you disclose information of limitations on the
                               accuracy, completeness or up-to-date character of the information

      Safeguards               Implement appropriate technical, administrative and physical
                               safeguards to protect your patients‚Äô privacy and the confidentiality of
                               their personal health information

                               Ensure your staff are informed of privacy and confidentiality
                               requirements

      Openness                 Develop and make available a written statement on your information
                               practices (e.g., your collection, use and disclosure of personal health
                               information)

      Access                   In a timely manner, give your patients access to, and the ability to
                               correct, their personal health records if they meet the requirements of
                               the Act



8
                                 General Privacy Compliance

    Privacy Principle        Requirement

    Challenging Compliance   Develop simple complaint procedures to allow individuals to challenge
                             your privacy practices




What You Need to Do
To comply with the Act, you must:

‚Ä¢     designate a contact person for the purposes of the Act,

‚Ä¢     identify the purposes for which you collect, use and disclose personal health
      information,

‚Ä¢     only collect, use or disclose your patients‚Äô personal health information if you
      have your patients‚Äô consent to do so or if the Act allows you to do so without
      consent,

‚Ä¢     only collect, use or disclose your patients‚Äô personal health information if no
      other information would serve your purpose,

‚Ä¢     only collect, use or disclose that amount of information necessary to serve
      your purpose and follow reasonable information practices to protect your
      patients‚Äô personal health information against theft, loss and unauthorized
      access, copying, modification, use, disclosure and disposal,

‚Ä¢     take reasonable steps to ensure that your patients‚Äô personal health information
      is as accurate, complete and up-to-date as needed for its use or disclosure,

‚Ä¢     establish and maintain appropriate information practices and tell your patients
      about these practices (note: the rest of this Toolkit will help you develop these
      information practices),

‚Ä¢     develop and make available a written statement on:

          ‚Äì your information practices (in general terms),
          ‚Äì your contact person‚Äôs contact information, and
          ‚Äì your access, correction, inquiry and complaints procedures,
‚Ä¢     develop procedures to:


                                                                                                     9
General Privacy Compliance

            ‚Äì identify when a use or disclosure of personal health information is
                beyond what is described in the written statement,

            ‚Äì notify affected patients about such a use or disclosure, and
            ‚Äì make and keep notes of such a use or disclosure in or linked to the
                affected patient‚Äôs personal health record,

     ‚Ä¢   train your staff, volunteers and others acting on your behalf to follow your
         information practices and your procedures, and

     ‚Ä¢   take reasonable steps to protect personal health information that you transfer
         to others (for example, including privacy clauses in your contracts with
         agents).




     Checklists, Templates and Tools

     ‚Ä¢   Sample Written Statement of Information Practices

     ‚Ä¢   For guidelines on establishing information practices, see the Security sections.

     ‚Ä¢   For guidelines on providing access to health records, see the Accessing Health
         Records section.

     ‚Ä¢   For guidelines on making corrections to health records, see the Correcting
         Health Records section.

     ‚Ä¢   For guidelines on responding to complaints, see the Oversight section.

     ‚Ä¢   For guidelines on managing others, see the Managing Contracts and Agents
         section.




10
                                    General Privacy Compliance

NOTE TO USER: When you use this Statement, you must ensure that you have included all
of your proposed uses and disclosures. If you use or disclose a patient‚Äôs personal health
information, without the patient‚Äôs consent, in a manner that is not described on the
Statement, you must: (a) inform the patient of this as soon as possible unless the patient
does not have a right of access to their personal health record, and (b) make and keep a note
of the use or disclosure in or linked to the affected patient‚Äôs personal health record.

   SAMPLE WRITTEN STATEMENT OF INFORMATION PRACTICES

Collection of Personal Health Information
We collect personal health information about you directly from you or from the person acting on
your behalf. The personal health information that we collect may include, for example, your
name, date of birth, address, health history, records of your visits to [the Hospital] and the care
that you received during those visits. Occasionally, we collect personal health information about
you from other sources if we have obtained your consent to do so or if the law permits.

Uses and Disclosures of Personal Health Information
We use and disclose your personal health information to:
‚Ä¢ treat and care for you,
‚Ä¢ get payment for your treatment and care (from OHIP, WSIB, your private insurer or others),
‚Ä¢ plan, administer and manage our internal operations,
‚Ä¢ conduct risk management activities,
‚Ä¢ conduct quality improvement activities (such as sending patients satisfaction surveys),
‚Ä¢ teach,
‚Ä¢ conduct research,
‚Ä¢ compile statistics,
‚Ä¢ fundraise to improve our healthcare services and programs,
‚Ä¢ comply with legal and regulatory requirements, and
‚Ä¢ fulfil other purposes permitted or required by law.
Your Choices                                       How to Contact Us
You may access and correct your personal           Our privacy contact person is z.
health records, or withdraw your consent for
some of the above uses and disclosures by          For more information about our privacy
contacting us (subject to legal exceptions).       protection practices, or to raise a concern you
                                                   have with our practices, contact us at:
Important Information
‚Ä¢ We take steps to protect your personal           [Address, fax, email, telephone number and
   health information from theft, loss and         website]
   unauthorized access, copying,                   You have the right to complain to the
   modification, use, disclosure and disposal.     Information and Privacy
‚Ä¢ We conduct audits and complete                   Commissioner/Ontario if you think we have
   investigations to monitor and manage our        violated your rights. The Commissioner can
   privacy compliance.                             be reached at:
‚Ä¢ We take steps to ensure that everyone who
                                                   [Address, fax, email, telephone number and
   performs services for us protect your
                                                   website]
   privacy and only use your personal health
   information for the purposes you have
   consented to.




                                                                                                      11
12
Contact Person
                                                                                          Contact Person

Table of Contents


Key Points..............................................................................................................17

The Rule.................................................................................................................18

What You Need To Do ..........................................................................................18
  What You Should Do.......................................................................................19

Related Sections of the Act....................................................................................19

Checklists, Templates and Tools ...........................................................................19




                                                                                                                              15
Contact Person




16
                                                              Contact Person


Key Points
‚Ä¢   You must designate a contact person to assist you in meeting your privacy
    obligations.

‚Ä¢   The contact person will be primarily responsible for privacy compliance
    activities within your facility.

‚Ä¢   This means responsibility for ensuring that your facility has:

       ‚Äì established and maintains appropriate information practices,
       ‚Äì developed access, correction, inquiry and complaints procedures, and
       ‚Äì developed and made available a written statement that describes your
           facility‚Äôs information practices, your contact person‚Äôs contact
           information, and your access, correction, inquiry and complaints
           procedures.

‚Ä¢   The contact person will deal with any access requests, privacy related
    inquiries and complaints, and Commissioner investigations.




                                                                                17
Contact Person


     The Rule
     You must designate a contact person to:




     What You Need To Do
     ‚Ä¢   Designate a contact person.

     ‚Ä¢   Assign responsibility to the contact person to develop procedures to respond
         to:

            ‚Äì patients who ask to review their personal health records,
            ‚Äì patients who ask to correct their personal health records, and


18
                                                             Contact Person

        ‚Äì inquiries or complaints about possible violations of the law.

What You Should Do
The duties listed under heading What You Need To Do are those required by the
Act. You may wish to broaden or complement the contact person‚Äôs duties to
include other work aimed at ensuring your overall compliance with the Act.

For example, the contact person could be responsible for:

‚Ä¢   making an assessment of the information you collect and how it is used and
    disclosed,

‚Ä¢   conducing privacy impact assessments and privacy audits of information use,

‚Ä¢   developing privacy policies, procedures and tools, and

‚Ä¢   informing and assisting agents on privacy matters.




Related Sections of the Act
2, 3, 4, 10, 11, 12, 13, 14, 15, 16, 17




Checklists, Templates and Tools

See the Sample Written Statement of Information Practices in the General Privacy
Compliance section.




                                                                                   19
20
Consent
22
                                                                                                                   Consent

Table of Contents

Key Points............................................................................................................. 25
The Rule................................................................................................................ 26
What You Need To Do ......................................................................................... 27
    Implied Consent ...............................................................................................27
        What is Implied Consent?
        When is Implied Consent Acceptable?
        Guidelines for Relying on Implied Consent
    Express Consent...............................................................................................28
        What Is Express Consent?
        When Is Express Consent Required?
        Guidelines for Obtaining Express Consent
    Withdrawal of Consent ....................................................................................30
    Conditional Consent.........................................................................................30
    When Consent Is Not Required .......................................................................30
        Collection
        Use
        Disclosure
Psychiatric Facilities ............................................................................................. 35
Patient Capacity .................................................................................................... 36
        Children and Teenagers
        Dealing With Substitute Decision-Makers
Related Sections of the Act................................................................................... 39
Checklists, Templates and Tools .......................................................................... 39
        Decision Tree For Consent
        Sample Consent Form
        Sample Withdrawal of Consent Form




                                                                                                                              23
Consent




24
                                                                                 Consent


Key Points
‚Ä¢   Generally, you need either express or implied consent before you may collect,
    use or disclose personal health information. When you collect, use and
    disclose personal health information for health care purposes, you can usually
    rely on implied consent. If the purpose is something other than health care,
    you must often obtain express consent. There are also specified circumstances
    where you may collect, use or disclose personal health information without
    consent.

‚Ä¢   To be a valid consent, the patient must have the capacity to consent. Where
    required, you must obtain consent from the patient‚Äôs substitute decision-maker
    if the patient does not have the capacity to consent.

‚Ä¢   To be a valid consent, the consent must be obtained voluntarily and directly
    from the patient (or substitute decision-maker), and the consent must be
    knowledgeable and related to the information in question.

‚Ä¢   Patients have the right to refuse, withdraw or place restrictions on their
    consent, if the purpose for which their personal health information is
    collected, used or disclosed requires consent.

‚Ä¢   The Act has special rules for dealing with children and teenagers.

‚Ä¢   The Mental Health Act has additional rules on mandatory and permitted
    disclosures without consent.




                                                                                      25
Consent


     The Rule

     Generally, you need either express or implied consent before you may collect, use
     or disclose personal health information. When you collect, use and disclose
     personal health information for health care purposes, you can usually rely on
     implied consent. If the purpose is something other than health care, you must
     often obtain express consent. There are also specified circumstances where you
     may collect, use or disclose personal health information without consent.

     To be a valid consent:

     ‚Ä¢   the patient must have the capacity to consent (for more information on
         capacity see page 36),

     ‚Ä¢   it must be obtained directly from the patient or someone with legal authority
         to consent for the patient (called a substitute decision-maker),

     ‚Ä¢   it must be related to the information in question,

     ‚Ä¢   it must be obtained voluntarily (without deception or coercion), and

     ‚Ä¢   it must be knowledgeable, meaning it must be reasonable to believe that the
         patient understands:

            ‚Äì why you are collecting, using or disclosing the information, and
            ‚Äì that the patient has the right to withhold or withdraw consent.

     Note: This section relates to consent to the collection, use and disclosure of
           personal health information, and not to consent to treatment. The Act has
           not changed the consent to treatment rules.




26
                                                                                               Consent



What You Need To Do

Implied Consent
There are many circumstances where you may rely on consent that can be implied
from your patients‚Äô behaviour. The following explains implied consent and when
you may rely upon it. It also sets out guidelines to ensure you are correctly
relying on implied consent.

What is Implied Consent?

Implied consent permits you to conclude from surrounding circumstances that a
patient would reasonably agree to the collection, use or disclosure of the patient‚Äôs
personal health information.

Example: If you ask patients for personal health information to open a record and
         they answer your questions, you can infer their consent to the
         collection of their information.

When is Implied Consent Acceptable?

You may rely upon your patients‚Äô implied consent if you are:

‚Ä¢   a health information custodian collecting, using and disclosing personal health
    information to provide health care,

    Note: A health information custodian who receives a patient‚Äôs personal
    health information from the patient, the substitute decision-maker or another
    health information custodian for the purpose of providing or assisting in
    providing health care to the patient may assume that it has the patient‚Äôs
    implied consent to collect, use and disclose the information for health care
    purposes, unless the health information custodian is aware that the patient has
    expressly withheld or withdrawn the consent.

‚Ä¢   collecting, using or disclosing names and mailing addresses‚Ä† for fundraising,
    or
‚Ä†
  The information set out in bold italicized text is based on draft Regulations that have not yet been
finalized. We will send you a replacement page with updated information once the Regulations
are finalized.




                                                                                                         27
Consent

     ‚Ä¢   providing names and location within the hospital to someone representing the
         patients‚Äô religious or other organization. See the discussion on Spiritual Care
         in the Managing Health Information section.

     Guidelines for Relying on Implied Consent

     To ensure that your reliance on implied consent is proper:

     ‚Ä¢   give your patients the information they need to understand why you are
         collecting their information and how you may use or disclose it,

     ‚Ä¢   do so by posting notices or placing brochures in high traffic areas and waiting
         rooms:

            ‚Äì describing why you collect, use and disclose personal health
                information, and

            ‚Äì informing patients that they may withhold or withdraw their consent
                and providing information on how they can do so,

     ‚Ä¢   if you have done this, and your patients have not withheld or withdrawn their
         consent, you may rely on your patients‚Äô implied consent.

     Remember: Consent may never be implied if patients specifically state that
               their personal health information may not be collected, used or
               disclosed.

     See page 11 for a sample notice.

     Express Consent
     The following explains express consent and when it is required. It also sets out
     guidelines for obtaining express consent.

     What Is Express Consent?

     Express consent is obtained when patients explicitly agree to the collection, use
     and disclosure of their personal health information.

     Express consent can be given in writing, orally, by telephone or electronically.

     When Is Express Consent Required?

     You must get your patients‚Äô express consent if you are:


28
                                                                                               Consent

‚Ä¢   disclosing personal health information to someone other than a health
    information custodian,

    Example: You must get express consent if you are disclosing personal health
             information to an employer or insurance provider.

‚Ä¢   disclosing personal health information to another health information custodian
    for a purpose other than providing or assisting in providing health care,

    Example: You must get express consent if you are disclosing personal health
             information to another health information custodian for the
             purpose of research (unless certain conditions and restrictions are
             met). See the Research section for additional guidance.

    Example: You must get express consent if you are disclosing personal health
             information to another hospital to establish common patterns in the
             use of a particular drug or therapy.

‚Ä¢   collecting, using or disclosing personal information (other than names and
    mailing addresses‚Ä†) for fundraising.

Guidelines for Obtaining Express Consent

When obtaining express consent:

‚Ä¢   tell your patients the specific reasons why you are collecting their information,
    how you will use their information, and under what circumstances you may
    disclose their information to others,

‚Ä¢   get to the point and explain why you are asking for consent in clear everyday
    language,

‚Ä¢   although you must tell your patients enough for them to give knowledgeable
    consent, do not overwhelm them with unnecessary medical or overly technical
    information,

‚Ä¢   create forms with clear explanations (when obtaining written consent),

‚Ä¢   only use and disclose personal health information for the purposes for which
    the patient consented or for purposes permitted without consent under the Act,


‚Ä†
  The information set out in bold italicized text is based on draft Regulations that have not yet been
finalized. We will send you a replacement page with updated information once the Regulations
are finalized.



                                                                                                         29
Consent

     ‚Ä¢   fully document oral consent and how you got it, and

     ‚Ä¢   tell your patients that they can put conditions on or withdraw their express
         consent any time.

     Remember: Although express consent is best from a risk management
               perspective, it is not always required.

     Withdrawal of Consent
     Patients may withdraw their consent at any time.

     Patients who want to withdraw their consent must notify you that they no longer
     consent to your collection, use and disclosure of their personal health information.

     A patient‚Äôs withdrawal has no effect on information you collected, used or
     disclosed before the patient withdrew consent, but has effect from the time it is
     received.

     A substitute decision-maker who consented on a patient‚Äôs behalf may also
     withdraw that consent at any time by notifying you.

     If the withdrawal of consent will compromise patient care, be sure to discuss the
     effect of the withdrawal with the patient and carefully document the withdrawal
     and these discussions in the patient‚Äôs health record.

     Conditional Consent
     Patients may place restrictions on their consent. For example, a patient may want
     you to share information with only a specific organization for a specific purpose.

     Such a restriction affects only collections, uses and disclosures that require
     consent or that are subject to the express instruction (‚Äúlock box‚Äù) provisions. See
     the Managing Health Information section for additional information on ‚Äúlock
     boxes‚Äù. Also, such a restriction cannot stop you from recording information as
     required by law, or established professional standards or institutional practice.

     When Consent Is Not Required
     There are situations where patient consent to the collection, use or disclosure of
     personal health information is not required.

     Examples of these situations are described below.



30
                                                                              Consent

Collection

You do not need your patients‚Äô consent to collect their personal health
information if you need the information to treat the patient, you can reasonably
rely on the information as accurate and there is no time to obtain consent.

You may indirectly collect personal health information without consent if:

‚Ä¢   the Commissioner specifically authorizes the collection,

‚Ä¢   you collect the information from a person who is permitted or required by law
    to disclose it to you, or

‚Ä¢   you are legally permitted or required to collect it indirectly.

Note: Collecting information ‚Äúindirectly‚Äù means collecting it from a source other
      than the patient or substitute decision-maker.

Use

You may use personal health information you collect with your patients‚Äô consent
for the purpose for which you gathered it. Where you have collected personal
health information with implied consent, you may use it for all purposes for which
it is reasonable to imply consent (see discussion above on implied consent).

Consent is not required to use the information to:

‚Ä¢   comply with a legal requirement,

‚Ä¢   plan, deliver or monitor health-related programs that you provide,

‚Ä¢   manage risk and errors, improve the quality of service or maintain programs,

‚Ä¢   educate those working with your patients so they can care for your patients,

    Example: During rounds with your students; however, you need consent to
             disclose personal health information at grand rounds held in other
             hospitals.

‚Ä¢   dispose of or alter information to ensure that others cannot link the
    information to a specific individual,

‚Ä¢   seek consent to additional collections, uses and disclosures when only
    patients‚Äô names and contact information are used,




                                                                                     31
Consent

     ‚Ä¢   participate in legal or administrative proceedings in which you are involved,
         or

     ‚Ä¢   collect payment for health care services you provided.

     You may also use information for research if certain conditions and restrictions
     are met (for more information, see the Research section).

     Note: When you provide personal health information to your agents, you are
           ‚Äúusing‚Äù the information. This use is not considered a ‚Äúdisclosure‚Äù under
           the Act.

     Disclosure

     You do not need consent to disclose personal health information to the following
     people or organizations:


     ‚Ä¢   other health care practitioners or groups of health care practitioners,
     ‚Ä¢   community service providers (defined in the Long-Term Care Act),
     ‚Ä¢   community care access corporations,
     ‚Ä¢   public or private hospitals,
     ‚Ä¢   psychiatric facilities,
     ‚Ä¢   independent health facilities,
     ‚Ä¢   homes for the aged, rest homes, nursing homes, care homes,
     ‚Ä¢   pharmacies,
     ‚Ä¢   laboratories,
     ‚Ä¢   ambulance services,
     ‚Ä¢   homes for special care,
     ‚Ä¢   centres, programs and services for community health or mental health whose
         primary purposes are providing health care,
     so long as:
             ‚Äì     the information is reasonably necessary to provide health care,
             ‚Äì     you cannot get consent when needed, and
             ‚Äì     the patient has not specifically told you not to disclose information in
                   certain circumstances.




32
                                                                                 Consent

‚Ä¢   a regulated health profession college for the purpose of administration or
    enforcement of the Drug and Pharmacies Regulation Act, the Regulated
    Health Professions Act or an Act named in Schedule 1 to that Act,

‚Ä¢   the Ontario College of Social Workers and Social Service Workers for the
    purpose of administration or enforcement of the Social Work and Social
    Service Work Act,

‚Ä¢   the Public Guardian and Trustee, the Children‚Äôs Lawyer, a children‚Äôs aid
    society, a Residential Placement Advisory Committee established under the
    Child and Family Services Act or the Registrar of Adoption Information
    appointed under that Act so that they can carry out their statutory functions,

‚Ä¢   the Archives of Ontario or, in certain limited circumstances, a prescribed
    person whose functions include collecting and preserving information (when
    the information is disclosed for that purpose),

‚Ä¢   someone auditing or reviewing an accreditation or accreditation application
    related to health care provision - but the record must not be removed from
    your premises,

‚Ä¢   a researcher following the guidelines in the Research section,

‚Ä¢   the Chief Medical Officer of Health or a Medical Officer of Health for
    purposes set out in the Health Protection and Promotion Act,

‚Ä¢   a public health authority similar to the Medical Officer of Health established
    by Canadian federal, provincial or territorial statute so long as information is
    disclosed for purposes similar to those in the Health Protection and
    Promotion Act,

‚Ä¢   the Minister on request to monitor or verify public health fund payments,

‚Ä¢   a body created under provincial government regulations that is analyzing or
    compiling statistics to manage, evaluate or monitor the resource allocation or
    planning for the health system and related service delivery (so long as that
    person has appropriate practices and procedures in place to protect patient
    privacy), except information concerning personal counselling or other
    information the regulations may exclude, and

‚Ä¢   the head of penal (or similar) institution where the patient is being detained in
    order to arrange treatment or make a decision regarding where the patient
    should be placed.




                                                                                        33
Consent

     You do not need your patients‚Äô consent to disclose their personal health
     information if:

     ‚Ä¢   you must do so to contact a relative, friend or potential substitute decision-
         maker of a patient who is injured, incapacitated or ill and unable to give
         consent personally,

     ‚Ä¢   you are disclosing information you collected to a person listed in the
         regulations who operates a registry intended to facilitate or improve health
         care provision or administer the storage or donation of body parts, organs or
         substances like blood or plasma,

     ‚Ä¢   you are permitted or required by law to provide the information,

     ‚Ä¢   the Minister or other health information custodian needs the information to
         decide whether to fund or pay a hospital or physician for health care provided,

     ‚Ä¢   the information is needed to determine eligibility for health care or other
         governmental benefits,

     ‚Ä¢   you reasonably believe the information is needed to prevent serious bodily
         harm or reduce a significant risk of it happening to any person,

     ‚Ä¢   you disclose personal health information to a potential purchaser of your
         practice to assess and evaluate your operations, and the potential purchaser
         agrees in writing to keep the information confidential and secure and keep the
         information no longer than necessary to reach a conclusion,

     ‚Ä¢   a health data institute that the Minister has approved will use the information
         to analyze how well the system and related service delivery works, and

     ‚Ä¢   you are doing so for the purpose of a proceeding or contemplated proceeding
         in which you or your agent (or former agent) is a party or witness, if the
         information is relevant to the proceeding.

     You may disclose patient‚Äôs personal health information to a person outside
     Ontario if:

     ‚Ä¢   you have consent or the Act allows you to do so,

     ‚Ä¢   the person receiving the information performs functions comparable to the
         functions performed by a person within Ontario to whom the Act permits
         disclosure,

     ‚Ä¢   you must disclose the information for health care purposes (unless the patient
         has expressly instructed you not to make the disclosure), or


34
                                                                                  Consent

‚Ä¢   the disclosure is reasonably necessary for payment of health care purposes.




Psychiatric Facilities
In addition to the disclosures permitted under the Act, the Mental Health Act
permits the officer in charge of a psychiatric facility to collect, use or disclose
personal health information with or without consent to:

‚Ä¢   assess, observe, examine or detain the patient in accordance with the Mental
    Health Act, and

‚Ä¢   comply with Part 21 of the Criminal Code (Mental Disorder) or an order
    under that part.

The officer in charge of a psychiatric facility may also disclose a personal health
record:

‚Ä¢   for proceedings before the Consent and Capacity Board at the request of a
    party to the proceeding,

‚Ä¢   to a physician who is considering, has or is renewing a community treatment
    order or who has been appointed to supervise and manage the community
    treatment order,

‚Ä¢   to an individual named in the community treatment plan as providing
    treatment, upon receiving written request of the physician or other named
    person,

‚Ä¢   to a person providing advocacy services in prescribed circumstances,

‚Ä¢   to the Public Guardian and Trustee to enable investigations, and

    Note: You must disclose personal health information to the Public Guardian
          and Trustee where you have a concern about adverse effects occurring
          to a mentally incapable individual, such as elder abuse. The Public
          Guardian and Trustee may access the patient‚Äôs health record for the
          purpose of investigating the allegation.

‚Ä¢   pursuant to a summons, unless the attending physician feels that such
    disclosure may result in harm to the patient or a third party, in which case, a
    court or other body may authorize the disclosure.




                                                                                       35
Consent

     Disclosure is also permitted for:

     ‚Ä¢   consultation among health care members named in a community treatment
         order, or

     ‚Ä¢   consultation between a physician and regulated health care professionals,
         social workers or others where a physician is considering issuing or renewing
         a community treatment order.

     If you have used a Mental Health Act Form 14 in the past to get patient consent,
     you should:

     ‚Ä¢   ensure that the previously obtained consent meets the requirements of the Act
         before you continue to rely on it, and

     ‚Ä¢   on a going-forward basis, get consent as set out in this Toolkit, unless your
         patient‚Äôs situation falls under the Mental Health Act exceptions or other
         exceptions to consent as set out in his Toolkit.

     For more information, refer to the Mental Health Act.




     Patient Capacity
     To be capable of consenting, a patient must be able to understand:

     ‚Ä¢   the information needed to make a decision on whether or not the patient
         should consent to the collection, use or disclosure of personal health
         information, and

     ‚Ä¢   the consequences of giving, withholding or withdrawing consent.

     A patient‚Äôs ability to consent may change from time to time. For example, a
     patient‚Äôs ability to consent may vary depending upon the patient‚Äôs condition and
     the type of information involved. You must consider the patient‚Äôs capacity every
     time you seek consent.

     You may presume a patient is capable of consenting unless you have reason to
     believe otherwise. For instance, if a patient says or does nothing to make you
     doubt the patient‚Äôs capacity when you are asking for consent, you may rely on the
     consent (whether you see them in person or speak to them on the telephone).

     If you determine that the patient does not have the capacity to consent and the
     patient has not applied for a review of your determination to the Consent and


36
                                                                                    Consent

Capacity Board, you should get the patient‚Äôs substitute decision-maker‚Äôs consent
instead. If a substitute decision-maker is acting on the patient‚Äôs behalf for matters
related to treatment, personal care service or admission to a care facility (under
the Health Care Consent Act, 1996), that person would also consent to the
collection, use or disclosure of the patient‚Äôs personal health information if the
information is related to the treatment, personal care service or admission to a
care facility.

When a patient is not capable of providing consent you may get consent (ranked
in order as listed) from the patient‚Äôs:

‚Ä¢   guardian (if the guardian has the authority to make such decisions),

‚Ä¢   attorney for personal care or attorney for property (if the attorney has the
    authority to make such decisions),

‚Ä¢   representative (appointed by the Consent and Capacity Board under the
    Health Care Consent Act, 1996 if the representative has the authority to give
    the consent),

‚Ä¢   spouse or partner,

‚Ä¢   child, custodial parent, or children‚Äôs aid society or other person legally
    entitled to give or withhold consent in place of a parent (Note: where this is
    the situation, the child‚Äôs parent cannot consent on behalf of the child),

‚Ä¢   parent with access rights,

‚Ä¢   brother or sister, and

‚Ä¢   any other relative (related by blood, marriage or adoption).

If the patient has died, you can get consent from the patient‚Äôs estate trustee or
someone who is in charge of administering the patient‚Äôs estate.

To consent for a patient, the person must be:

‚Ä¢   included in the list above,

‚Ä¢   available and capable of consenting,

‚Ä¢   at least 16 years old or the patient‚Äôs parent,

‚Ä¢   willing to assume responsibility for giving or refusing consent,




                                                                                         37
Consent

     ‚Ä¢   free of any court order or separation agreement prohibiting them from having
         access to or consenting for the patient, and

     ‚Ä¢   the highest ranked person on the list of potential substitute decision-makers
         who is available and capable of consenting.

     If a patient is not capable of consenting and you cannot find anyone capable of
     consenting on their behalf and willing to take on this role, contact the Public
     Guardian and Trustee who can consent for the patient.

     The Public Guardian and Trustee can also give consent if two or more equally
     high-ranking substitute decision-makers disagree about whether to consent. The
     Public Guardian and Trustee breaks the deadlock.

     Children and Teenagers

     Children of any age are presumed to have the capacity to consent to the
     collection, use and disclosure of their personal health information. Do not
     presume capacity if it is not reasonable to do so in the circumstances.

     For children under 16, a parent or other lawful guardian may consent to the
     collection, use or disclosure of personal health information even if the child has
     capacity, unless the information relates to:

     ‚Ä¢   treatment within the meaning of the Health Care Consent Act, 1996 about
         which the child has made his or her own decision, or

     ‚Ä¢   counselling in which the child has participated on his or her own under the
         Child and Family Services Act.

     When you need consent for the collection, use or disclosure of information about
     a child less than 16, you may either obtain it from that child, if capable, or the
     parent or other lawful guardian (but not the access parent, unless such a parent has
     been lawfully authorized in place of the custodial parent to make information
     decisions). If there is a conflict between the child and the parent, the capable
     child‚Äôs decision prevails with respect to the consent.

     Dealing With Substitute Decision-Makers

     When treating children and adults who cannot understand what it means to give or
     withhold consent to the collection, use or disclosure of personal health
     information, you must ask for a substitute decision-maker‚Äôs consent, where it is
     required.




38
                                                                                   Consent

When substitute decision-makers are involved:

‚Ä¢   The substitute decision-maker may be the patient‚Äôs legal guardian, attorney
    for personal care, spouse or partner, parent, child, sibling or other relative.

‚Ä¢   A substitute decision-maker should consent only if:

        ‚Äì the patient is incapable or the patient is capable and is 16 or older but
            has authorized in writing a substitute decision-maker to consent on the
            patient‚Äôs behalf,

        ‚Äì the substitute decision-maker can consent, and
        ‚Äì the substitute decision-maker is not prohibited by a court order or
            separation agreement from having access to the patient.

‚Ä¢   Always make sure substitute decision-makers understand and are willing to
    assume consent responsibilities by discussing the responsibilities with them.

‚Ä¢   Substitute decision-makers must consider the patient‚Äôs wishes and beliefs, the
    benefits to the patient, why the information will be collected, used or
    disclosed, and whether collecting the information is necessary.

If you do not believe that a substitute decision-maker properly considered the
required factors, you may apply to the Consent and Capacity Board (created under
the Health Care Consent Act) to determine whether the substitute decision-maker
met the requirements.



Related Sections of the Act
3, 5, 6, 9, 16(2), 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 32, 33, 36, 37, 38,
39, 42, 43, 44, 45, 46, 47



Checklists, Templates and Tools

‚Ä¢   Decision Tree for Consent
‚Ä¢   Consent Form
‚Ä¢   Withdrawal of Consent Form




                                                                                         39
Consent

          DECISION TREE FOR CONSENT




40
                                                                                                            Consent

NOTE TO USER: This Sample Consent Form provides a sample list of purposes for the collection, use and
disclosure of personal health information where express consent is required under the Act. You should consider
whether your intended purpose for the collection, use and disclosure of personal health information requires express
consent and amend this form to include all such purposes. If you choose to rely on express oral consent, no such
form is needed.


                                   SAMPLE CONSENT FORM

             Consent to the Collection, Use and Disclosure
                   of Personal Health Information
I, _________________________________, have reviewed the [Hospital]‚Äôs written statement
concerning the collection, use and disclosure of personal health information.
I understand that the [Hospital] is seeking my consent for it to collect, use and/or disclose my
personal health information from me or from the person acting on my behalf to:
          conduct patient satisfaction surveys,
          teach outside the [Hospital], and
          fundraise for the [Hospital]‚Äôs charitable activities.

I understand that the [Hospital] will only collect, use and disclose my personal health information
with my consent [as set out in its privacy policy] unless a particular collection, use or disclosure
is permitted or required by law without my consent.
I also understand that I can refuse to sign this consent form. I can also withdraw my consent any
time by writing to .
I hereby authorize [Hospital] to collect, use and disclose my personal health information for the
purposes that I have checked-off above.


Name:

Address:

Tel. Home:                                                         Tel. Work:

Signature:                                                               Date:




                                                                                                                       41
Consent
                                                                       FOR INTERNAL OFFICE USE ONLY
                                                                       Patient ID. No. _________




                       SAMPLE WITHDRAWAL OF CONSENT FORM

                                   Withdrawal of Consent
     I, ___________________________________, wish to withdraw my consent to any further use or
     disclosure by [Hospital/Physician name] of my personal health information for: (Please check
     all that apply)

         Conducting patient satisfaction surveys

         Teaching outside the [Hospital]

         Fundraising

     I wish to place the following conditions on any further use or disclosure of my personal health
     information:




     (Please specify conditions)



     This withdrawal of consent does not have retroactive effect nor does it affect the uses and
     disclosures of personal health information collected by [Hospital/Physician Name] where the
     uses and disclosures are permitted or required by law without consent.


     Name:

     Address:

     Tel. Home:                                               Tel. Work:

     Signature:                                                     Date:




42
Collection, Use and Disclosure
                                     Collection, Use and Disclosure

Table of Contents


Key Points............................................................................................................. 47

The Rule................................................................................................................ 48

Collection.............................................................................................................. 48
   What You Need To Do ....................................................................................48
       What You Should Do

Use ........................................................................................................................ 49
   What You Need To Do ....................................................................................49
         What You Should Do
         Preventing Unauthorized Use by Authorized Users
         Use of Personal Health Information by the Circle of Care
         Videotaping, Audiotaping and Photographing Personal Health
               Information

Disclosure ............................................................................................................. 55
   What You Need To Do ....................................................................................55
       What You Should Do
       Situations Involving Disclosure
       Disclosure Tables
       Mandatory Disclosure
       Disclosure for Health Related Programs and Legislation
       Disclosure to Lawyers, Insurance Companies, Adjusters,
           Investigators
       Disclosure to Legal Authorities and Law Enforcement

Related Sections of the Act................................................................................... 66

Checklists, Templates and Tools .......................................................................... 66
       Process Map for Disclosing Personal Health Information
       Sample Confidentiality Agreement
       Sample Consent to Disclose Personal Health Information Form




                                                                                                                                  45
Collection, Use and Disclosure




46
                        Collection, Use and Disclosure


Key Points
‚Ä¢   You must only collect, use and disclose your patients‚Äô personal health
    information in compliance with the law.

‚Ä¢   This means you must not collect, use or disclose patients‚Äô personal health
    information unless:

       ‚Äì you have the patients‚Äô consent and your collection, use or disclosure is,
           to the best of your knowledge, necessary for a lawful purpose, or

       ‚Äì the collection, use or disclosure is permitted or required by the Act.
‚Ä¢   You must identify the purposes for which you collect your patients‚Äô personal
    health information in your written statement.

‚Ä¢   You must not collect, use or disclose more personal health information than is
    reasonably necessary to meet your purposes. This limitation does not apply to
    collections, uses or disclosures required by law.

‚Ä¢   You must not collect, use or disclose personal health information if other
    information will serve your purposes.

‚Ä¢   If you collect personal health information in contravention of the Act, you
    must not use or disclose it unless you are required by law to do so.

‚Ä¢   You must not charge patients a fee for collecting or using their personal health
    information, unless permitted to do so under the Act.

‚Ä¢   When disclosing personal health information, you must not charge fees that
    exceed the prescribed amount or, if no amount is prescribed, a reasonable cost
    recovery charge.




                                                                                       47
Collection, Use and Disclosure


      The Rule

      Generally, you will collect, use and disclose personal health information for
      health care purposes. However, you might also collect, use and disclose personal
      health information for other purposes. These other purposes might include
      financial reimbursement, education, research, statistics, public health regulation
      compliance, litigation, quality improvement and other purposes permitted or
      required by law.

      The law generally says that you need either express or implied consent when you
      collect, use or disclose personal health information. When you collect, use and
      disclose personal health information for health care purposes, you can usually rely
      on implied consent. If the purpose is something other than health care, you must
      often obtain express consent. There are also specified circumstances where you
      may collect, use or disclose personal health information without consent.

      See the Consent section for more information and guidelines on consent.

      In addition, unless you can rely on an exemption under the law, you must:

      ‚Ä¢   identify the purposes for which you collect, use and disclose your patients‚Äô
          personal health information (this would be done in your written statement ‚Äì
          see the General Privacy Compliance section for guidelines on the written
          statement),

      ‚Ä¢   limit your collection, use and disclosure to information that is reasonably
          necessary to serve your purposes, and

      ‚Ä¢   not collect, use and disclose personal health information if other information
          will serve your purposes.




      Collection

      What You Need To Do
      ‚Ä¢   You must only collect your patients‚Äô personal health information in
          compliance with the law.



48
                        Collection, Use and Disclosure

‚Ä¢   You must identify the purposes for which you collect your patients‚Äô personal
    health information in your written statement.

*   See the Consent section for guidelines on when you may collect personal
    health information with implied or express consent or without consent.

*   See the General Privacy Compliance section for guidelines on the written
    statement.

What You Should Do

‚Ä¢   Define scopes of practice and job responsibilities for health care professionals
    and staff to identify who requires personal health information, what
    information they require, and for which purpose.

‚Ä¢   Inform health care professionals and staff:

       ‚Äì about who collects what personal health information (to limit
           duplication of collection),

       ‚Äì about the consent requirements for collection,
       ‚Äì about restricting the collection of personal health information to the
           purposes for which you have consent or which are permitted or
           required by law, and

       ‚Äì about restricting the collection of personal health information to the
           information that is required.

‚Ä¢   Regularly review your collection practices to ensure compliance with the Act.




Use

What You Need To Do
‚Ä¢   You must only use your patients‚Äô personal health information in compliance
    with the law.

‚Ä¢   You must identify the purposes for which you use personal health information
    in your written statement.



                                                                                       49
Collection, Use and Disclosure

      *   See the Consent section for guidelines on when you may use personal health
          information with implied or express consent or without consent.

      *   See the General Privacy Compliance section for guidelines on the written
          statement.

      What You Should Do

      ‚Ä¢   Develop policies on who is authorized to use your patients‚Äô personal health
          information and for which purpose.

      ‚Ä¢   Inform health care professionals and staff about:

             ‚Äì the purposes for which they may use personal health information,
             ‚Äì the consent requirements for use,
             ‚Äì when other information will suffice, and
             ‚Äì the need to restrict the use of personal health information to the
                 purposes for which you have consent or which are permitted or
                 required by the law.

      ‚Ä¢   Develop policies to address consent requirements.

      ‚Ä¢   Caution your agents with whom you share personal health information for
          non-health care purposes that:

             ‚Äì the information is only to be used for the purposes for which it was
                 shared, unless the use is permitted or required by the Act, and

             ‚Äì the information must be returned or disposed of securely once that
                 purpose has been fulfilled.

      ‚Ä¢   Consider whether de-identified information can be used to serve the same
          purpose (e.g., for research or quality of care purposes).



      Preventing Unauthorized Use by Authorized Users

      ‚Ä¢   Ensure staff members have clearly defined responsibilities related to the use
          of personal health information.




50
                        Collection, Use and Disclosure

‚Ä¢   Develop guidelines and inform internal agents (e.g., staff) that personal health
    information must only be accessed for authorized uses (for example, staff
    cannot look up information about family members, friends etc.).

‚Ä¢   Track and audit staff access to personal health information.

‚Ä¢   Ensure external agents (e.g., suppliers) who use personal health information
    can be held accountable and have an enforceable duty to keep the information
    secure.

‚Ä¢   Where reasonable, use non-disclosure agreements that require:

       ‚Äì limiting the use of personal health information to the purpose for
           which it was provided,

       ‚Äì de-identifying personal health information, where practical,
       ‚Äì putting in place physical, administrative and technological security
           measures to reduce the risk of unauthorized use and disclosure, and

       ‚Äì destroying or having a designated person destroy personal health
           information after the purpose has been met, if permitted by law.

Use of Personal Health Information by the Circle of Care




                                 Patient


                             Persons providing
                             care to the patient




                            Persons assisting in
                        providing care to the patient




                                                                                       51
Collection, Use and Disclosure



      ‚ÄúCircle of care‚Äù is not defined in the Act but refers to those in the health care
      team who are actually involved in the care or treatment of a particular patient.
      The term ‚Äúcircle of care‚Äù describes those:

      ‚Ä¢   health care practitioners and groups of health care practitioners,

      ‚Ä¢   public and private hospitals,

      ‚Ä¢   pharmacies,

      ‚Ä¢   laboratories,

      ‚Ä¢   ambulance services,

      ‚Ä¢   community care access corporations,

      ‚Ä¢   community service providers (defined in the Long-Term Care Act),

      ‚Ä¢   psychiatric facilities,

      ‚Ä¢   independent health facilities,

      ‚Ä¢   homes for the aged, rest homes, nursing homes, care homes and homes for
          special care, and

      ‚Ä¢   community health or mental health centres, programs and services whose
          primary purposes are providing health care,

      who provide health care or assist in providing health care to a particular patient.


      Members of a particular patient‚Äôs ‚Äúcircle of care‚Äù can provide health care to the
      patient, confidently assuming that they have consent to collect, use and disclose
      the patient‚Äôs personal health information for that care, unless they know that the
      patient has expressly withheld or withdrawn consent.

      For example:

      ‚Ä¢   In a hospital, the circle of care includes the attending physician and the health
          care team (e.g., residents, nurses, technicians and support staff assigned to the
          patient) who provide care or assist in providing care to the patient.




52
                         Collection, Use and Disclosure

‚Ä¢   But in a hospital, the circle of care does not include health care practitioners
    who do not provide care to the patient.

‚Ä¢   In a physician‚Äôs office, the circle of care includes the physician and any
    physicians who provide on-call services, the nurse and support staff who
    provide care or assist in providing care to the patient.

‚Ä¢   But in a physician‚Äôs office, the circle of care does not include the other
    physicians in a group practice who do not provide care to the patient.

Videotaping, Audiotaping and Photographing Personal Health
Information

You may videotape, audiotape or photograph personal health information for
patient care and medical educational purposes.

Patient Care

To assist in the provision of care you may videotape, audiotape or photograph a
procedure. When you do so, remember that the tape or photograph becomes part
of the patient‚Äôs health record and is to be treated in the same manner as other
personal health information.

When obtaining patient consent (whether express or implied), remember the
heightened sensitivity of the taped or photographed information from the patient‚Äôs
perspective. Best practices dictate that you obtain express consent to the
collection, use and disclosure of personal health information in this manner;
however, as with other circumstances, you may rely on implied consent. Once the
tape or photograph is made, make sure you have appropriate safeguards in place
to protect its confidentiality (See the Safeguards section for guidelines).

Medical Education

Procedures may be videotaped, audiotaped or photographed for general
educational purposes. If you plan to use the tape or photograph to educate your
agents to provide health care, you do not need to obtain your patients‚Äô consent.
However, it may be prudent for you to obtain express consent from your patients,
especially if you are facing them with a camera.

When obtaining consent, tell patients:

‚Ä¢   the specific personal health information that you intend to record,




                                                                                       53
Collection, Use and Disclosure

      ‚Ä¢   the exact educational purpose for which the information will be used (for
          instance, a clinical demonstration or a case study),

      ‚Ä¢   the intended audience (for example, undergraduate students or clinical
          practice rounds, meetings or video- or teleconferences),

      ‚Ä¢   that they can withhold or withdraw their consent at any time, and

      ‚Ä¢   that they will not benefit or suffer from their decision to withhold or withdraw
          consent.

      You should also ensure that the necessary safeguards are in place to protect the
      confidentiality of the videotapes, audiotapes and photographs (see the Safeguards
      section for guidelines).

      You should de-identify information about a patient when you disclose patient case
      details to health care practitioners for formal educational programs, where
      possible.

      If you disclose personal health information for educational purposes to health care
      practitioners or students who are not agents of your facility, you must obtain the
      affected patients‚Äô express consent. For example, if you disclose personal health
      information at grand rounds of another hospital, you must obtain express consent.
      If you are doing rounds in your own hospital, but outside guests (i.e., non-agents)
      have been invited to the rounds, you must obtain express consent.




54
                         Collection, Use and Disclosure


Disclosure

What You Need To Do
‚Ä¢   You must only disclose your patients‚Äô personal health information in
    compliance with the law.

‚Ä¢   You must identify the purposes for which you disclose personal health
    information in your written statement.

*   See the Consent section for guidelines on when you may disclose personal
    health information with implied or express consent or without consent.

*   See the General Privacy Compliance section for guidelines on the written
    statement.

What You Should Do

When assessing third party disclosure requests:

‚Ä¢   Inform all staff that disclosure requests must be evaluated on the basis of the
    type, purpose and requesting party, and whether other information can serve
    the purpose for which disclosure of personal health information is sought.

‚Ä¢   Inform all staff on the consent requirements for disclosure, including when
    personal health information can be disclosed without consent.

‚Ä¢   Develop a policy that incorporates the following steps:

       ‚Äì verifying the identity of the requesting party,
       ‚Äì seeking assistance from an appropriate resource, such as the contact
           person, legal counsel or a mental health practitioner if a request is
           unusual or if there is uncertainty about whether disclosure should be
           made,

       ‚Äì assessing whether further consultation is necessary and whether further
           legal processes may apply if the disclosure is required by law,

       ‚Äì including written consent in the patient‚Äôs personal health record, or
           documenting the date of consent, and date of disclosure in the patient‚Äôs
           personal health record, where express consent is necessary,


                                                                                      55
Collection, Use and Disclosure

              ‚Äì providing only a copy of the personal health record if a personal health
                  record is requested,

              ‚Äì designating a resource (e.g., the contact person) to be responsible for:
                  ‚Ä¢   understanding the rules governing disclosure of personal health
                      information, and

                  ‚Ä¢   recognizing when you need to consult with others.

      Situations Involving Disclosure

      Disclosure to Family Members or Friends

      If you are asked to disclose personal health information about a patient by the
      patient‚Äôs family member or friend you can only disclose the personal health
      information with the patient‚Äôs or substitute decision-maker‚Äôs consent.

      You must:

      ‚Ä¢   verify that the patient or substitute decision-maker has consented to the
          disclosure of the personal health information to the patient‚Äôs family members
          or friends,

      ‚Ä¢   understand the purpose for which the personal health information is being
          requested, and

      ‚Ä¢   only disclose personal health information for which you have consent to
          disclose and that serves the purpose for which the disclosure is requested.

      You should:

      ‚Ä¢   confirm the family member‚Äôs or friend‚Äôs identity, and

      ‚Ä¢   document the date of the request and the disclosure of the personal health
          information in the patient‚Äôs personal health record.

      You may disclose the following information if you provide the patient with an
      opportunity to object at the first reasonable opportunity after admission to your
      hospital and the patient does not do so:

      ‚Ä¢   whether or not the individual is a patient of your hospital,

      ‚Ä¢   the patient‚Äôs general health status (e.g. critical, poor, fair, stable or
          satisfactory), and


56
                          Collection, Use and Disclosure

‚Ä¢   the patient‚Äôs location in your hospital.

If the Patient is Deceased

You may disclose personal health information about any patients who die, or who
you reasonably believe have died, to:

‚Ä¢   identify a patient,

‚Ä¢   notify another person that a patient died and how they died, and

‚Ä¢   provide the immediate family with information they believe they need to
    make decisions about their own health care.

You can obtain consent from a deceased patient‚Äôs estate trustee to collect, use or
disclose any personal health information of a deceased patient where consent is
required. In this instance, you should verify the identity of the estate trustee by
reviewing the notarized ‚ÄúCertificate of Appointment of Estate Trustee with a
Will‚Äù or ‚ÄúCertificate of Appointment of Estate Trustee without a Will‚Äù. You
should also keep a copy of this certificate of appointment.

If the deceased patient does not have an estate trustee, you can obtain consent
from the person who has assumed this responsibility, if it is reasonable for you to
rely on the accuracy of the assertion made by that person, regarding their identity.

In-Patient Transfer

The physician at the sending facility and the physician at the receiving facility are
jointly responsible for determining what personal health information should be
provided when a patient is transferred from one facility to another. Where you
cannot obtain specific instructions from the physician or if you are facing an
emergency transfer, you should send a copy of the patient‚Äôs complete personal
health record. You should also ensure that the records are sealed in a container
and securely transferred. You should not transfer records if a patient has
withdrawn consent.

Volunteers

Your volunteers are considered to be your agents. Because volunteers may learn
personal health information about patients, you should provide volunteers with
training on the protection of patient privacy. You should also have them sign a
confidentiality agreement.




                                                                                        57
Collection, Use and Disclosure

      Spiritual Care

      In hospitals, spiritual or religious issues often arise. You may:

      ‚Ä¢   collect information about your patients‚Äô religious or other organizational
          affiliations but only with their consent, and

      ‚Ä¢   rely on your patients‚Äô implied consent if they provide you with information
          about their religious or other organizational affiliations, to provide or disclose
          the patient‚Äôs name and location in the hospital to a representative of the
          religious or other organizational body specified by the individual,

      but only if the patient has been given the opportunity to opt out of this disclosure
      and has not done so.

      If a hospital provides a religious program (such as a chaplain visiting program) to
      its patients, the staff member (e.g., the chaplain) who delivers the program may
      use personal health information about the hospital‚Äôs patients for the purposes of
      this program, without first obtaining consent.

      Disclosure to the Media

      The following practices may help you to address requests for information from
      the media:

      ‚Ä¢   Direct all media requests for patients‚Äô personal health information to your
          CEO or designate (e.g., your Communications Department).

      ‚Ä¢   Ensure members of the media identify themselves, the organizations they
          represent, and the specific personal health information they request.

      ‚Ä¢   Prohibit the media from taking photographs without consent and otherwise
          invading patient privacy.

      ‚Ä¢   Escort members of the media (through a staff member) when they are on your
          premises, and take steps to ensure they do not have access to personal health
          information, except with patient consent.

      ‚Ä¢   Clear media requests to visit your premises in advance through the CEO or
          designate to ensure proper arrangements are made.

      ‚Ä¢   Ensure members of the media wear a visitor‚Äôs badge when on your premises.

      ‚Ä¢   Ensure patients sign consent forms for any media photographs or disclosure of
          other personal health information.


58
                        Collection, Use and Disclosure

‚Ä¢   Develop a procedure on how to handle media inquiries outside of your regular
    business hours.


Lock Boxes

‚ÄúLock box‚Äù is not defined in the Act but it is an important concept about patients‚Äô
ability to control their own personal health information.

Patients have the right to expressly instruct you not to use specified personal
health information for health care purposes. Patients can also expressly instruct
you not to disclose specified personal health information to others (even to others
within their circle of care).

The term ‚Äúlock box‚Äù describes the limits that patients can place on the use and
disclosure of their personal health information.

If you disclose personal health information about a patient to another member of
the patient‚Äôs circle of care, but the patient has restricted (or locked) you from
disclosing all of the personal health information that you consider reasonably
necessary to provide health care, you must flag for the recipient that the
information is incomplete because the patient has ‚Äúlocked‚Äù it.

If you receive this kind of notice from another member of your patient‚Äôs circle of
care, you may choose to discuss the fact that information is restricted with the
patient. For example, you can talk about the impact of the restriction on
treatment. But you must obtain the patient‚Äôs express consent before accessing and
using the locked information.

Note, however, that a patient cannot restrict a use or disclosure that the Act
otherwise permits or requires. The Act trumps the lock box. For example, you
may disclose locked personal health information where, in your professional
opinion, you need to disclose the information to prevent serious bodily harm or to
reduce a significant risk of it happening to any person.

The lock box provisions take effect on November 1, 2004 when the Act comes
into force. However, under the Act, hospitals are not required to comply until
November 1, 2005.

You should use this time period to develop and implement appropriate procedures
to deal with lock box instructions.




                                                                                      59
Collection, Use and Disclosure

      Disclosure Tables

      The issue of disclosure is complex. The following tables provide a pictorial
      representation of the most common examples of disclosures to help you determine
      when disclosure must or can be made. See the Consent section for further
      information on disclosures that must or can be made without consent.

      Mandatory Disclosure

      The Act specifically permits the disclosure of personal health information for a
      number of purposes as required by other statutes. Consent is not required for
      these specific purposes. For example, you are required to provide the following
      information:

      To whom disclosure must      What information must be
      be made                      disclosed                                 Authority

      Aviation Medical Advisor     Information about flight crew             Aeronautics Act
      (note this is a mandatory    members, air traffic controllers or
      disclosure for a physician   other aviation licence holders who
      not for a hospital)          have a condition that may impact
                                   their ability to perform their job in a
                                   safe manner

      Chief Medical Officer of     Information to diagnose, investigate,     Health Protection and
      Health or Medical Officer    prevent, treat or contain                 Promotion Act
      of Health                    communicable diseases
                                                                             Personal Health
                                                                             Information Protection
                                                                             Act

      Chief Medical Officer of     Information to diagnose, investigate,     Public Hospitals Act
      Health or Medical Officer    prevent, treat or contain SARS
      of Health or a physician
      designated by the Chief
      Medical Officer of Health

      Children‚Äôs Aid Society       Information about a child in need of      Child and Family
                                   protection (e.g., abuse or neglect)       Services Act




60
                            Collection, Use and Disclosure

To whom disclosure must     What information must be
be made                     disclosed                                Authority

College of a regulated      Where there are reasonable grounds       Regulated Health
health care professional    to believe a health care professional    Professions Act
                            has sexually abused a patient, details
                            of the allegation, name of the health
                            care professional and name of the
                            allegedly abused patient

                            The patient‚Äôs name can only be
                            provided with consent

                            You must also include your name as
                            the individual filing the report.

College of a regulated      A written report, within 30 days,        Regulated Health
health care professional    regarding revocation, suspension,        Professions Act
                            termination or dissolution of a health
                            care professionals‚Äô privileges,
                            employment or practice for reasons
                            of professional misconduct,
                            incapacity or incompetence

College of Physicians and   Information about the care or            Public Hospitals Act
Surgeons of Ontario         treatment of a patient by the
                            physician under investigation            Notice must be given to
                                                                     the Chief of Staff and
                                                                     the administrator of the
                                                                     hospital

Coroner or designated       Facts surrounding the death of an        Coroners Act
Police Officer              individual in prescribed
                            circumstances (e.g., violence,
                            negligence or malpractice)

                            Information about a patient who
                            died while in the hospital after being
                            transferred from a listed facility,
                            institution or home

                            Information requested for the
                            purpose of an investigation

Minister of Health and      Information for data collection,         Public Hospitals Act
Long-Term Care              organization and analysis

Ontario Health Insurance    Information about the funding of         Public Hospitals Act
Plan                        patient services




                                                                                                61
Collection, Use and Disclosure

      To whom disclosure must      What information must be
      be made                      disclosed                              Authority

      Order, warrant, writ,        Information outlined on the warrant,   Personal Health
      summons or other process     summons, etc.                          Information Protection
      issued by an Ontario court                                          Act

      Physician assessor           Information to evaluate applications   Public Hospitals Act
      appointed by the Ministry    to the Underserviced Area Program
      of Health and Long-Term
      Care

      Registrar General            Births and deaths                      Vital Statistics Act

      Registrar of Motor           Name, address and condition of a       Highway Traffic Act
      Vehicles                     person who has a condition that may
      (note this is a mandatory    make it unsafe for them to drive
      disclosure for a physician
      not for a hospital)

      Subpoena issued by an        Information outlined in the            Personal Health
      Ontario court                subpoena                               Information Protection
                                                                          Act

      Trillium Gift of Life        For tissue donations or transplants    Trillium Gift of Life
      Network                      purposes, notice of the fact that a    Network Act
                                   patient died or is expected to die
                                   imminently (not in force yet)          Consent must be
                                                                          decided jointly with the
                                                                          Network to determine
                                                                          the need to contact the
                                                                          patient or substitute
                                                                          decision-maker

      Workplace Safety and         Information the Board requires         Workplace Safety and
      Insurance Board              about a patient receiving benefits     Insurance Act
                                   under the Workplace Safety and
                                   Insurance Act


      The following tables outline examples of where personal health information may
      be disclosed. See also the Consent section for additional information on
      permitted disclosures.




62
                             Collection, Use and Disclosure

Disclosure for Health Related Programs and Legislation

Person requesting
health record or patient                                         Consent       Authority to release
information                   Purpose                            Needed        information

Ambulance services            Administration/enforcement            No         Ambulance Act
operator or delivery agent    of the Ambulance Act
or the Minister

Cancer Care Ontario,          To analyze or compile                 No         Personal Health
Canadian Institute for        statistical information                          Information
Health Information,                                                            Protection Act
Institute for Clinical                                                         regulations‚Ä†
Evaluative Sciences or
Pediatric Oncology
Group of Ontario

Chief Medical Officer of      To report communicable                No         Health Protection
Health, Medical Officer       diseases                                         and Promotion Act
of Health or a physician
designated by the Chief
Medical Officer of Health

College of Pharmacists        Administration/enforcement            No         Drug
Investigator                  of the Drug                                      Interchangeability
                              Interchangeability and                           and Dispensing Fee
                              Dispensing Fee Act                               Act

College under the RHPA,       Administration/enforcement            No         Personal Health
or Social Work and Social     of the relevant statutes                         Information
Services Act, or Board of                                                      Protection Act
Regents under the
Drugless Practitioners
Act

Deputy Minister of            To review the information             No         Public Hospitals Act
Veteran‚Äôs Affairs or          about the care received by a
person with express           member of the Canadian
direction                     Armed Forces




‚Ä†
  The information set out in bold italicized text is based on draft Regulations that have not yet been
finalized. We will send you a replacement page with updated information once the Regulations
are finalized.



                                                                                                         63
Collection, Use and Disclosure

      Person requesting
      health record or patient                                      Consent   Authority to release
      information                 Purpose                           Needed    information

      Individual assessing        To assess capacity under the        No      Substitute Decisions
      patient capacity, who is    Substitute Decisions Act,                   Act; Health Care
      not providing care to the   Health Care Consent Act, or                 Consent Act;
      patient                     Personal Health                             Personal Health
                                  Information Protection Act                  Information
                                                                              Protection Act

      Minister Inspector          Administration/enforcement          No      Public Hospitals Act
                                  of the Public Hospitals Act

      Minister Inspector          Enforcement of the Drugs            No      Drugs and Pharmacy
                                  and Pharmacy Regulation                     Regulation Act
                                  Act

      Public Guardian and         To investigate an allegation        No      Public Hospitals Act;
      Trustee                     that a patient is unable to                 Personal Health
                                  manage their property                       Information
                                                                              Protection Act

      Public Guardian and         To carry out their duties and,      No      Personal Health
      Trustee, Children‚Äôs         for the PGT, to investigate                 Information
      Lawyer, Residential         serious adverse harm                        Protection Act
      Placement Advisory          resulting from alleged
      Committee, Registrar of     incapacity
      Adoption of Information,
      Childrens‚Äô Aid Societies




      Disclosure to Lawyers, Insurance Companies, Adjusters,
      Investigators

      Person requesting health
      record or patient                                            Consent    Authority to release
      information                    Purpose                       Needed     information

      Lawyers, Insurance             To assist a patient with       Yes       Express consent
      Companies, Adjusters on        a claim or proceeding
      behalf of a patient




64
                               Collection, Use and Disclosure

Person requesting health
record or patient                                               Consent   Authority to release
information                          Purpose                    Needed    information

Lawyers, Insurance                   To assist the third          No      Personal Health
Companies, Adjusters,                party with a                         Information
Investigators on behalf of a         proceeding                           Protection Act
third party, if the third party
is an agent or former agent of
the hospital/physician




Disclosure to Legal Authorities and Law Enforcement

Person requesting health
record or patient                                               Consent   Authority to release
information                       Purpose                       Needed    information

Head of penal or custodial        To assist with health care      No      Personal Health
institution or an officer in      or placement decisions                  Information
charge of a psychiatric                                                   Protection Act
facility where the patient
is being lawfully detained

Investigator or Inspector         To conduct an                   No      Personal Health
                                  investigation or inspection             Information
                                  authorized by a warrant or              Protection Act
                                  law

Police without a warrant          Legal authorities and law       Yes     Express consent
                                  enforcement

Police without a warrant          Where there are                 No      Personal Health
                                  reasonable grounds to                   Information
                                  believe that the disclosure             Protection Act
                                  is necessary for the
                                  purpose of eliminating or
                                  reducing a significant risk
                                  of serious bodily harm

Probation and Parole              Legal authorities and law       Yes     Express consent
Services                          enforcement




                                                                                                 65
Collection, Use and Disclosure


      Related Sections of the Act
      37, 38, 39, 40, 41, 42, 43, 43.1, 44, 45, 46, 47, 48




      Checklists, Templates and Tools

      ‚Ä¢   Process Map for Disclosing Personal Health Information

      ‚Ä¢   Sample Non-Disclosure Agreement

      ‚Ä¢   Sample Consent to Disclose Personal Health Information Form




66
  Collection, Use and Disclosure

 PROCESS MAP FOR DISCLOSING
PERSONAL HEALTH INFORMATION




                              67
Collection, Use and Disclosure

                           SAMPLE CONFIDENTIALITY AGREEMENT



      NOTE TO USER: Modify this sample agreement to suit your institution and your needs.
      Review with your lawyers before releasing.


      I acknowledge that I have read and understood the [z] policies and procedures on privacy,
      confidentiality and security.

      I understand that:

      ‚Ä¢   all confidential and/or personal health information that I have access to or learn through my
          employment or affiliation with [z] is confidential,

      ‚Ä¢   as a condition of my employment or affiliation with [z], I must comply with these policies
          and procedures, and

      ‚Ä¢   my failure to comply may result in the termination of my employment or affiliation with [z]
          and may also result in legal action being taken against me by [z] and others.

      I agree that I will not access, use or disclose any confidential and/or personal health information
      that I learn of or possess because of my affiliation with [z], unless it is necessary for me to do so
      in order to perform my job responsibilities. I also understand that under no circumstances may
      confidential and/or personal health information be communicated either within or outside of [z],
      except to other persons who are authorized by [z] to receive such information.

      I agree that I will not alter, destroy, copy or interfere with this information, except with
      authorization and in accordance with the policies and procedures.

      I agree to keep any computer access codes (for example, passwords) confidential and secure. I
      will protect physical access devices (for example, keys and badges) and the confidentiality of any
      information being accessed.

      I will not lend my access codes or devices to anyone, nor will I attempt to use those of others. I
      understand that access codes come with legal responsibilities and that I am accountable for all
      work done under these codes. If I have reason to believe that my access codes or devices have
      been compromised or stolen, I will immediately contact the [z].



            Name (Please Print)                     Signature                             Date




68
                               Collection, Use and Disclosure

           SAMPLE CONSENT TO DISCLOSE PERSONAL HEALTH
                       INFORMATION FORM


I                                         hereby authorize
                                                                    (Name of hospital/physician‚Äôs office)

to disclose the following personal health information:




     (Description of personal health information to be disclosed and dates of contact/hospitalization)

to


                      (Name and address of person/agency requesting information)

from the records of
                                       (Name of Patient)                               (Birth date)

Mailing Address of Patient:



I understand that this personal health information is to be used only by the
recipient for the purposes of:




Date:

I hereby waive any and all claims against [insert name of hospital/physician‚Äôs
office] in connection with the disclosure of this personal health information.

Witness:                                          Signed by:
                                                                (Patient or Substitute Decision-Maker)

Date:
                                                                      (Relationship to the Patient)




                                                                                                            69
70
Accessing Health Records
                                                   Accessing Health Records

Table of Contents


Key Points............................................................................................................. 75

The Rule................................................................................................................ 76
   What You Need To Do ....................................................................................76
      What You Should Do
      Fees for Providing Access
      Timeframe to Respond to a Request for Access
      Urgent Requests for Access
      Refusing a Request for Access
      Guidelines for Refusal of Access
      Failing to Respond to a Request for Access

Related Sections of the Act................................................................................... 81

Checklists, Templates and Tools .......................................................................... 82
       Sample Process Map ‚Äì Access to Personal Health Record
       Sample Form to Request Access to Personal Health Record
       Sample Checklist ‚Äì Process for Accessing a Personal Health
           Record
       Sample Letter for Extension to Comply with Request
       Sample Refusal of Access Letter




                                                                                                                              73
Accessing Health Records




74
                                   Accessing Health Records


Key Points
‚Ä¢   The term ‚Äúaccess‚Äù refers to access by patients or their substitute decision-
    makers.

‚Ä¢   Subject to a few exceptions, you must provide patients (or their substitute
    decision-makers) with access to their personal health records in a timely
    manner.

‚Ä¢   You must develop procedures to handle access requests.

‚Ä¢   You must make available a written statement telling patients and substitute
    decision-makers who to contact and what to do if they want to see their
    personal health record.

‚Ä¢   The Act does not prevent you from informally communicating with your
    patients or their substitute decision-makers.

‚Ä¢   The access rules in the Mental Health Act are repealed as of November 1,
    2004, and the transitional rules found in the Act apply. This means that as of
    November 1, 2004, you must comply with the access rules found in the Act
    (as opposed to those found in the Mental Health Act); however, if you
    received an access request before November 1, 2004, the access rules found in
    the Mental Health Act apply to that request.




                                                                                     75
Accessing Health Records


     The Rule

     Except under special circumstances, patients have the right to access their
     personal health records.

     Patients may request access to their personal health records orally or in writing.
     Oral requests are routine when the patient is still receiving care, and the Act does
     not prohibit you from responding to oral requests. The request must be in writing,
     however, to invoke the rights and procedural requirements set out in the Act.

     A substitute decision-maker can request access on a patient‚Äôs behalf because the
     right of access exists whether or not a patient has capacity. Substitute
     decision-makers will follow the same process to obtain access to the personal
     health record as the patient. See the Consent section for more information on
     substitute decision-makers.

     The Act does not prevent you from informally communicating with your patients
     or their substitute decision-makers. The access rules in the Mental Health Act are
     repealed as of November 1, 2004, and the transitional rules found in the Act
     apply. This means that as of November 1, 2004, you must comply with the access
     rules found in the Act (as opposed to those found in the Mental Health Act);
     however, if you received an access request before November 1, 2004, the access
     rules found in the Mental Health Act apply to that request.


     Note: The term ‚Äúaccess‚Äù refers to access by patients or their substitute decision-
     makers. The term ‚Äúdisclosure‚Äù refers to access by individuals other than patients
     or their substitute decision-makers. See the Collection, Use and Disclosure
     section for more information on disclosure to others. For the sake of brevity, the
     term ‚Äúrequestor‚Äù is used to describe patients and substitute decision-makers in this
     section of the Toolkit.

     What You Need To Do
     If you are asked for access to a personal health record:

     ‚Ä¢   Verify the patient‚Äôs identity or substitute decision-maker‚Äôs authority.

     ‚Ä¢   Determine if the request contains enough detail to let you reasonably find the
         record. If you need more information to find the record, work with your
         patient to obtain the information you require. If you cannot find the record
         after a reasonable search, tell the requestor so in writing.


76
                                    Accessing Health Records

‚Ä¢   Determine if one of the legal exceptions applies to providing access. See the
    table on pages 80 to 81 for a description of where you may refuse access.

‚Ä¢   If a legal exception applies:

       ‚Äì tell the requestor in writing that you are refusing access, in whole or in
           part, and why you are doing so,

       ‚Äì where possible, sever the record and provide access to the part of the
           record where no legal exception applies,

       ‚Äì tell the requestor about your complaints procedure, and that if the
           requestor is not satisfied with your resolution of the complaint, the
           requestor can complain to the Commissioner, and

       ‚Äì in some circumstances, you cannot even tell the requestor that a
           personal health record exists.

    Guidelines for refusing access are found at page 79.

‚Ä¢   If no legal exception applies and you can find the record, arrange to provide
    access. You can provide access by showing the requestor the original record.
    If you choose to show the requestor the original record, you should arrange for
    the requestor to be monitored while viewing the record to ensure that it is not
    altered in any way. You can also provide access by giving the requestor a
    copy of the record. You must provide a copy if the requestor asks for a copy.

‚Ä¢   If reasonably practical, answer any questions about any medical terms or
    abbreviations used in the record.

‚Ä¢   Put policies and procedures in place to handle access requests.

‚Ä¢   Make available a written statement telling patients and substitute decision-
    makers who to contact and what to do if they want to see their personal health
    record. See the General Privacy Compliance and Contact Person sections for
    more information on the written statement and the contact person.

What You Should Do

‚Ä¢   Train all staff to direct a requestor to your contact person if they want access
    to a personal health record.

‚Ä¢   Forward all access requests to your contact person. The contact person can
    also help to complete a request form for access to the personal health record.



                                                                                       77
Accessing Health Records

     ‚Ä¢   Document the date of the request for access in the patient‚Äôs personal health
         record.

     The Sample Checklist ‚Äì Process for Accessing a Personal Health Record provides
     a sample procedure for processing access requests.

     Fees for Providing Access

     You may charge a fee to provide access to a personal health record if you first
     give the requestor an estimate of the fee.

     The fee you charge cannot exceed either a prescribed amount or, if no amount is
     prescribed, a reasonable cost recovery charge.

     You may waive payment of all or any part of the fee if it is fair to do so.

     Note: The Regulations do not currently prescribe a fee.

     Timeframe to Respond to a Request for Access

     Respond to requests for access as soon as possible. If you need more than 30
     days to respond to a request for access, provide the requestor with written notice
     of an extension.

                 ‚Üê    Time to respond ‚Üí          ‚Üê Maximum Extension ‚Üí
            Date of                          30 days                       60 days
            Request                       from request                  from request

     An extension is only permitted if:

     ‚Ä¢   replying to the request within 30 days would reasonably interfere with your
         activities because locating the personal health record requires a complex
         search, or

     ‚Ä¢   the time required to undertake the necessary consultations would make it
         reasonably impractical to reply within 30 days.

     The written notice of an extension should explain:

     ‚Ä¢   when you will respond, and

     ‚Ä¢   why an extension is needed.

     An extension cannot exceed an additional 30 days.


78
                                   Accessing Health Records

If you do not provide access within the stated time period, the requestor can
assume that you have refused the request.

Urgent Requests for Access

If a requestor can satisfy you that the request is urgent, you must provide access
within the requested time period, if it is reasonable to do so.

Refusing a Request for Access

The table below lists circumstances where you may refuse access to personal
health records.

Even when a restriction to access exists, a requestor has a right to access personal
health information not related to the restriction. You must sever the restricted
information from the rest of the record, and give the requestor access to the
remaining information.

You should tell requestors that their request has been refused and, where
appropriate, give reasons for the refusal. There may be situations where access is
refused and you do not confirm or deny the existence of a record. This is context
specific and you may need to confer with a psychologist, legal counsel or the
Commissioner about how reasons for your refusal should be communicated.

You should tell requestors that they can complain about your refusal to the
Commissioner. You should also provide information on how to contact the
Commissioner.

Guidelines for Refusal of Access

In each of the following situations, you should provide access to the part of the
record that is not impacted by the reason for refusal and that can reasonably be
severed from the record.




                                                                                       79
Accessing Health Records


     Reason for Refusal of Access                     Follow-Up Notification to Requestor

                                                 State you are refusing   State you are refusing to
                                               the request (in whole or     confirm or deny the
                                                in part) and reason for    existence of any record
                                                       the refusal

     The record contains quality of care
     information                                         r


     The record contains information
     collected/created to comply with the
     requirements of a quality assurance
     program under the Health Professions                r
     Procedural Code that is Schedule 2 to
     the Regulated Health Professions Act

     The record contains raw data from
     standardized psychological tests or                 r
     assessments

     The record (or information in the
     record) is subject to a legal privilege
     that restricts disclosure to the                    r
     requestor

     Other legislation or court order
     prohibits disclosure to the requestor               r


     The information in the record was
     collected/created in anticipation of or
     use in a proceeding that has not                                                r
     concluded

     The information in the record was
     collected/created for an
     inspection/investigation/similar                                                r
     procedure authorized by law that has
     not concluded

     Granting access could reasonably be
     expected to result in a risk of serious
     harm to the patient or to others
     (Where this is suspected you may                                                r
     consult a physician or psychologist
     before deciding to refuse access)




80
                                         Accessing Health Records

Reason for Refusal of Access                    Follow-Up Notification to Requestor

                                           State you are refusing   State you are refusing to
                                         the request (in whole or     confirm or deny the
                                          in part) and reason for    existence of any record
                                                 the refusal

Granting access could lead to the
identification of a person who was
required by law to provide the                                                 r
information in the record

Granting access could lead to the
identification of a person who
provided the information in the record
in confidence (either explicitly or                                            r
implicitly) and it is considered
appropriate to keep the name of this
person confidential

The request for access is frivolous,
vexatious or made in bad faith                     r


The identity or authority of the
requestor cannot be proven by the                  r
requestor



Failing to Respond to a Request for Access

If you fail to respond to a request for access within the time required by the Act,
you will be deemed to have refused the request.

Requestors can complain to the Commissioner about your refusal of a request for
access. You will have to justify your decision to refuse access.

It is an offence under the Act to dispose of records so that you do not have to
respond to a request for access. You may face penalties if you commit this
offence.




Related Sections of the Act
51, 52, 53, 54, 72 (1)(d)



                                                                                                81
Accessing Health Records


     Checklists, Templates and Tools

     ‚Ä¢   Sample Process Map ‚Äì Access to Personal Health Record

     ‚Ä¢   Sample Form to Request Access to Personal Health Record

     ‚Ä¢   Sample Checklist ‚Äì Process for Accessing a Personal Health Record

     ‚Ä¢   Sample Letter for Extension to Comply with Request

     ‚Ä¢   Sample Refusal of Access Letter




82
            Accessing Health Records

       SAMPLE PROCESS MAP
‚Äì ACCESS TO PERSONAL HEALTH RECORD




                                     83
Accessing Health Records

                          SAMPLE FORM TO REQUEST ACCESS
                            TO PERSONAL HEALTH RECORD


     Information and Instructions

     We will provide you with access to your personal health record, unless a legal exception applies.
     We will review all health record access requests, and will make every effort to respond to your
     request in a timely fashion. Please complete Parts A and B of this Form. Part C is for our internal
     use. For information about our privacy protection practices, contact z at: [Address, fax, email
     and telephone number.]


     PART A: REQUESTOR INFORMATION

     Patient Contact Information:


     Last Name                                     First Name                                   Initials

     Mailing Address

     Telephone Number                              Date of Birth

     Hospital ID Number

     If you are a substitute decision-maker, your contact information:


     Last Name                                     First Name                                   Initials

     Mailing Address

     Telephone Number


     Note:    Include copies of documents that provide your authority as a substitute decision-maker.

     PART B: ACCESS REQUEST

     1.       Please describe what you need and include details that will help us locate the record (e.g.,
              dates, name of healthcare provider, etc.).




84
                                          Accessing Health Records

2.       How would you prefer to access this information? Please check off:

                  Receive hard copies of originals
                  Receive electronic copies of originals (please supply storage medium)
                  Examine originals in the facility



Signature                         Name (print)                       Date


PART C: RESPONSE TO ACCESS REQUEST (For Internal Use Only)

1.       Information Regarding Receipt and Initial Review of Request


         Date Request Received

2.       Information Regarding Response


         Date Response Issued

                  Access request granted
                  Access request not granted
                  Access request granted in part

         If complete access request was not granted, reason for refusing the request/part of the
         request.




3.       Information Regarding Extension

If an extension to the access request response was required, please indicate:


     Date of Extension                Reason for Extension               Date Patient Notified




4.       Processed by:


Signature                         Name (print)                       Title




                                                                                                   85
Accessing Health Records

                          SAMPLE CHECKLIST
          ‚Äì PROCESS FOR ACCESSING A PERSONAL HEALTH RECORD



              Give the requestor a form to request access to the health record (in whole or part) or
              inform them of what to include in a written request. An oral request may also be
              accepted. See Checklist for Information to Include in a Request Form.

              Verify the identity of the requestor.


                                                                           Verification of a Written
     Verification of an Oral             Verification of a Written         Request ‚Äì Substitute
     Request ‚Äì Patient                   Request ‚Äì Patient                 Decision-Maker

     Request photo-ID for                Ensure the following patient      Review information in the
     verification purposes if            information from the request      health record to ensure there is
     patient is not known                matches information in your       documentation that the
                                         registration system:              requestor is a substitute
     You should only accept                                                decision-maker
     requests by phone if you            ‚Ä¢   name
     know the patient                    ‚Ä¢   date of birth                 Request documentation
                                         ‚Ä¢   hospital ID number            (power of attorney) if there is
     If practical, call back to verify                                     no information in the health
     the patient‚Äôs identity              Check that a signature is         record
                                         included
                                                                           Verify if any parent requesting
                                                                           access for a minor is the
                                                                           custodial parent and that the
                                                                           parent is entitled to access


              Once you have the request, make sure you have enough information and any required
              payment to allow you to find the health record.

              If you need more information, follow up with the requestor.

              Fee estimates should be provided and agreed upon in advance. If payment is
              required, ensure the fee is included, otherwise follow up. Do not charge fees over a
              prescribed amount. If there is no prescribed amount, do not charge more than what you
              need to recover costs. You may waive any fees.

              Decide if there are any reasons to refuse access to the health record (in whole or part).
              See Guidelines for Refusal of Access.

              If the health record cannot be located, provide a written notice/form to the requestor
              advising them that the health record either does not exist or cannot be found.

              Assess if the request is urgent (required in fewer than 30 days of initial request).




86
                                 Accessing Health Records

Expedite request if the requestor can show that the need is urgent and you have enough
time to respond.

If you cannot meet the timeframe requested, advise the requestor.

Assess if the request for access can be provided within 30 days of receipt of the
request.

If an extension is needed, provide a written notice/form to the requestor advising of the
reason for the delay and length of the extension. The length of the extension must not
exceed 30 days, and is only permitted in certain circumstances.

If access is denied, provide a written notice to the requestor. Make sure the notice
reflects the appropriate response (see Table included in the Guidelines for Refusal of
Access).

Retrieve the health record.

Provide a copy of the record or schedule a convenient time for the requestor to look at
the requested health record (or copy) in a private and secure location.

Ensure you verify the identity of the requestor who comes in to examine the health
record. See Table above for guidelines.

Ensure the health record remains secure. Information in the health record should not
be removed, changed or otherwise tampered with. Supervise the requestor viewing the
health record to ensure the health record remains intact.

Where it is reasonable, respond to any questions about medical terms or abbreviations.

Verify the health record is intact and return the health record to its filing location, as
needed.

Document all requests, extensions, accesses and refusals to access the health record.
Ensure an event record is created when a requestor views an electronic health record.




                                                                                             87
Accessing Health Records

        SAMPLE LETTER FOR EXTENSION TO COMPLY WITH REQUEST



     [Name and Address of Health Care Facility]
     XXX Street
     City/Town, Ontario
     ABC 123

     Date

     Dear Sir/Madam,

     RE:     Request for Access to Personal Health Record of [Patient‚Äôs Name]

     Health Record #:

     An extension of _____ days is required to address your request to access the personal health
     record of the individual named above. While every effort is made to retrieve the information
     requested, this extension is required for the following reason:

     [Reason for extension]

     If you have any concerns or questions please contact _________________ (Contact Person). If
     they are unable to resolve your concerns, you may file a complaint with the Information and
     Privacy Commissioner/Ontario, who may be contacted at:

     [Contact information for the Information and Privacy Commissioner/Ontario]

     Sincerely,




     [Name, Title]




88
                                         Accessing Health Records

                    SAMPLE REFUSAL OF ACCESS LETTER



[Name and Address of Health Care Facility]
XXX Street
City/Town, Ontario
ABC 123

Date

Dear Sir/Madam,

RE:      Request for Access to Personal Health Record of [Patient‚Äôs Name]

Your request for access to the personal health record has been declined for the following reason:

[Reason for declining request]

If you have any questions or concerns please contact __________________ (Contact Person). If
we are unable to resolve your concerns, you may contact the Information and Privacy
Commissioner/Ontario, who may be contacted at:

[Contact information for the Information and Privacy Commissioner/Ontario]

Sincerely,



[Name, Title]




                                                                                                    89
90
Correcting Health Records
                                                  Correcting Health Records

Table of Contents

Key Points............................................................................................................. 95

The Rule................................................................................................................ 96

What You Need To Do ......................................................................................... 96
      Responding to Requests for Correction
      What You Should Do
      Where You Do Not Have To Make Corrections
      Timeframe for Responding to a Request for Correction
      Conflict Resolution: Refusing a Request for Correction

Related Sections of the Act................................................................................... 99

Checklists, Templates and Tools .......................................................................... 99
       Process Map for Responding to Requests for Correction
       Sample Request Form for Correction to Personal Health Record




                                                                                                                              93
Correcting Health Records




94
                                   Correcting Health Records


Key Points
‚Ä¢   Subject to a few exceptions, you must correct a patient‚Äôs health record at the
    patient‚Äôs request in a timely manner, if the requirements set out in the Act are
    met.
‚Ä¢   You must develop procedures to handle correction requests.
‚Ä¢   You must make available a written statement telling patients and substitute
    decision-makers who to contact and what to do if they want to correct their
    personal health record.
‚Ä¢   The correction rules in the Mental Health Act are repealed as of November 1,
    2004. This means that as of November 1, 2004, you must comply with the
    corrections rules found in the Act (as opposed to those found in the Mental
    Health Act).




                                                                                       95
Correcting Health Records


     The Rule

     If you have granted a patient access to his or her personal health record and the
     patient thinks that the record is not correct or complete for your purposes, the
     patient may ask you (in writing) to correct the record.
     If the patient makes an oral request for a correction, you may respond to it;
     however, only written requests invoke the rights and procedural requirements set
     out in the Act.
     With a few significant exceptions, you must make the requested correction if the
     patient can show to your satisfaction that the record is not correct or complete for
     your purposes and also gives you the information you need to make the
     correction.
     A substitute decision-maker can request a correction on a patient‚Äôs behalf because
     this right exists whether or not a patient has capacity. Substitute decision-makers
     will follow the same process to request a correction to the personal health record
     as the patient. See the Consent section for more information on substitute
     decision-makers. The correction rules in the Mental Health Act are repealed as of
     November 1, 2004. This means that as of November 1, 2004, you must comply
     with the corrections rules found in the Act (as opposed to those found in the
     Mental Health Act).




     What You Need To Do
     ‚Ä¢   Put policies and procedures in place to handle correction requests.
     ‚Ä¢   Make available a written statement telling patients and substitute decision-
         makers who to contact and what to do if they want to correct their personal
         health record. See the General Privacy Compliance and Contact Person
         sections for more information on the written statement and the contact person.

     Responding to Requests for Correction
     If you receive a written request for a correction, you should:
     ‚Ä¢   Verify the patient‚Äôs identity or substitute decision-maker‚Äôs authority.
     ‚Ä¢   Verify that the patient or substitute decision-maker has a right of access to the
         personal health record.


96
                                   Correcting Health Records

‚Ä¢   Ensure the request for correction relates to a personal health record created by
    you or your staff.
‚Ä¢   Determine who will validate the request and correct the personal health
    record. Confirm that this person has the knowledge, expertise and authority to
    validate and make the correction.
When making a correction you must:
‚Ä¢   record the correct information in the record, and
‚Ä¢   cross out the incorrect information (without obliterating it) or, if that is not
    possible, label the information as incorrect, remove it and store it separately
    from the record, and keep a link in the record that lets you trace the incorrect
    information.
If it is not possible to record the correct information in the record, you must put a
practical system in place to:
‚Ä¢   inform anyone who uses the record that the information in the record is
    incorrect, and
‚Ä¢   direct that person to the correct information.
Once you correct the personal health record:
‚Ä¢   tell the patient in writing how the correction was made, and
‚Ä¢   if the patient asks you to do so and to the extent reasonably possible, tell
    others in writing to whom you have disclosed the incorrect information of the
    correction, unless the correction cannot reasonably be expected to affect the
    ongoing provision of health care or otherwise benefit the patient.

What You Should Do
‚Ä¢   Develop procedures to determine who should assess requests for correction
    and make corrections.
‚Ä¢   Where practical, refer the request for correction to the author of the personal
    health record unless the author is not available, there is a question of
    competence or negligence in the creation of the record or if the patient has
    specifically requested that another practitioner assess the record.
‚Ä¢   If appropriate, refer the request to the most responsible physician.
‚Ä¢   Date and sign corrections.
‚Ä¢   Advise any members of the patient‚Äôs circle of care of the correction if it
    affects the patient‚Äôs current plan of care.



                                                                                        97
Correcting Health Records

     ‚Ä¢   Notify anyone who is currently using the personal health record of the
         correction.

     Where You Do Not Have To Make Corrections
     You do not have to correct a record:
     ‚Ä¢   that was not made by you/your facility and where you do not have sufficient
         knowledge, expertise and authority to correct the record (this would include
         your ability to validate the new information being provided),
     ‚Ä¢   if you reasonably believe that the request for correction is frivolous, vexatious
         or made in bad faith (requests should only be refused for these reasons in the
         rarest of cases),
     ‚Ä¢   if the patient has failed to demonstrate that the record is not correct or
         complete, or
     ‚Ä¢   if the patient has not given you the information you need to make the
         correction.
     You do not have to correct a professional opinion or observation made in good
     faith about a patient.

     Timeframe for Responding to a Request for Correction
     Respond to requests for correction as soon as possible and within 30 days of your
     receipt of the request. If you need more than 30 days to respond to a correction
     request, give the patient a written notice of an extension.

                 ‚Üê    Time to respond      ‚Üí      ‚Üê Maximum Extension ‚Üí
            Date of                          30 days                        60 days
            Request                       from request                   from request

     An extension is only permitted if:
     ‚Ä¢   replying to the request within 30 days would unreasonably interfere with your
         activities, or
     ‚Ä¢   the time required to undertake the necessary consultations would make it
         reasonably impractical to reply within 30 days.
     The written notice of an extension must describe:
     ‚Ä¢   when you will respond, and
     ‚Ä¢   why an extension is needed.




98
                                    Correcting Health Records

An extension cannot exceed an additional 30 days.
If you do not make a correction within the stated time period, the patient can
assume that you have refused the request.

Conflict Resolution: Refusing a Request for Correction
If you refuse a request, tell the patient in writing:
‚Ä¢    the reason for your refusal, and
‚Ä¢    that the patient can:
         ‚Äì prepare a brief written description of the correction that you refuse to
             make,
         ‚Äì require you to attach this document to the patient‚Äôs personal health
             record, and make you disclose the document whenever you disclose
             the information to which it relates,
         ‚Äì require you to make all reasonable efforts to disclose this document to
             anyone to whom the patient‚Äôs personal health record had been
             disclosed, unless the correction cannot reasonably be expected to
             affect the ongoing provision of health care or otherwise benefit the
             patient, and
         ‚Äì complain about your refusal to the Commissioner.
Note: The patient also has these rights when you are deemed to refuse a request.
If you refuse a request because you think it is frivolous, vexatious or made in bad
faith, tell the patient in writing:
‚Ä¢    the reason for your refusal, and
‚Ä¢    that the patient can complain about your refusal to the Commissioner.


Related Sections of the Act
55


Checklists, Templates and Tools

‚Ä¢    Process Map for Responding to Requests for Correction
‚Ä¢    Sample Request Form for Correction to Personal Health Record




                                                                                      99
Correcting Health Records

             PROCESS MAP FOR RESPONDING
             TO REQUESTS FOR CORRECTION




100
                                          Correcting Health Records

                 SAMPLE REQUEST FORM FOR CORRECTION
                     TO PERSONAL HEALTH RECORD

Information and Instructions
We will correct health record information if it is demonstrated, to our satisfaction, that the record
is not correct or complete for the purpose for which we collect, use or disclose the information.
We will make every effort to respond to your request in a timely fashion. Please complete Parts A
and B of this Form. Part C is for our internal use. For information about our privacy protection
practices, contact [z] at: [Address, fax, email and telephone no.]


PART A: REQUESTOR INFORMATION
Patient Contact Information:


Last Name                                             First Name                     Initials


Mailing Address


Telephone Number                            Date of Birth             Hospital ID Number

If you are a substitute decision-maker, your contact information:


Last Name                                             First Name                     Initials


Mailing Address


Telephone Number
Note: Include copies of documents that provide your authority as a substitute decision-maker.
PART B: CORRECTION REQUEST
1.       List or attach the correction requested, with reasons for the correction.

          Requested Correction                           Reasons for Correction




2.       How do you wish to receive notice of the correction (in writing, by telephone)?




                                                                                                        101
Correcting Health Records

      3.    Would you like us to give notice of the correction, to the extent reasonably possible, to
            others to whom we have disclosed the incorrect information? (We will only do so if this
            notice will affect your health care or otherwise benefit you.)
            ¬Ü        Yes
            ¬Ü        No

             Signature                      Name (print)                   Title

             Date


      PART C: CORRECTION REQUEST RESPONSE (For Internal Use Only)
            ¬Ü        Correction made
            ¬Ü        Correction not made
            ¬Ü        Refusal letter (with reasons) sent
            ¬Ü        Statement of Disagreement attached to record
            ¬Ü        Date of Response ___________________
      1.    List names, contact information and comments of any individuals consulted




      2.    If correction was not made, provide reasons:




      3.    If an extension to the correction request response was required, please indicate:

            Date of Extension        Reason for Extension                 Date Patient Notified
                                                                          of Extension




      4.    Notice of correction provided to others to whom incorrect information was disclosed.
            List names:

      5.    Processed by:


             Signature                      Name (print)                   Title




102
Dealing with Health Information
                                 Dealing with Health Information

Table of Contents


Key Points........................................................................................................... 107

Storage and Retention ......................................................................................... 108
   The Rule.........................................................................................................108
       Storage
       Retention

What You Need To Do ....................................................................................... 108
      What You Should Do

Disposal............................................................................................................... 111
   The Rule.........................................................................................................111

What You Need To Do ....................................................................................... 111
      What You Should Do

Transfer ............................................................................................................... 112
   The Rule.........................................................................................................112

What You Need To Do ....................................................................................... 112
      Transfer to Another Facility
      Transfer to a Successor
      Transfer to Archives
      What You Should Do

Related Sections of the Act................................................................................. 113

Checklists, Templates and Tools ........................................................................ 113
       Summary of Retention Periods
       Supplementary Table A Retention Periods for Records Relating to
           Drugs Dispensed under the Ontario Drug Benefit Plan
       Supplementary Table B Retention Periods Required for Patient
           Records Relating to Dispensing of Drugs Under The Drugs
           and Pharmacies Regulations Act




                                                                                                                               105
Dealing with Health Information




106
                       Dealing with Health Information


Key Points
‚Ä¢   You must store your patients‚Äô personal health information in a reasonably
    secure manner and in accordance with the prescribed requirements, if any.

‚Ä¢   If your patients‚Äô personal health information is stolen, lost or accessed by
    unauthorized persons, you must notify your patients as soon as possible.

‚Ä¢   You must retain personal health records for their minimum retention periods,
    and as long as needed to allow patients to exhaust any legal recourse available
    to them involving an access request.

‚Ä¢   You must dispose of your patients‚Äô personal health records in a secure
    manner.

‚Ä¢   You may transfer your patients‚Äô personal health records as described in the
    Act so long as you do so securely.




                                                                                      107
Dealing with Health Information


      Storage and Retention

      The Rule

      Storage


      You must store your patients‚Äô personal health information in a reasonably secure
      manner and in accordance with the prescribed requirements, if any.

      If your patients‚Äô personal health information is stolen, lost or accessed by
      unauthorized persons, you must notify your patients as soon as possible. (There is
      an exception under the law for researchers. See the Research section for details.)


      Retention


      The Act does not establish specific retention periods, but does require records
      containing personal health information to be kept:

      ‚Ä¢   no longer than prescribed by law, and

      ‚Ä¢   for as long as needed to allow a patient to exhaust any legal recourse a patient
          has regarding a request for access.


      Note: The Regulations made under the Act do not currently contain any specific
            storage or retention rules; however, the Regulations made under the Public
            Hospitals Act that deal with record retention do continue to apply.




      What You Need To Do
      ‚Ä¢   Retain your patients‚Äô personal health information in a reasonably secure
          manner. See the Security section for guidelines on keeping information
          secure.




108
                        Dealing with Health Information

‚Ä¢   The steps you take must address your:

        ‚Äì physical security (for example, locked filing cabinets, restricted office
            access and alarm systems),

        ‚Äì technological security (for example, passwords, encryption and
            firewalls), and

        ‚Äì administrative controls (for example, security clearances, access
            restrictions, staff training and confidentiality agreements).


Note: If you are a physician and you keep electronic health information, make
      sure you:

‚Ä¢   can display and print your records for each patient in chronological order,

‚Ä¢   can retrieve your records by the patient‚Äôs name and health card number (where
    applicable),

‚Ä¢   keep an audit trail that:

    ‚Äì    records the date and time of each entry for each patient,

    ‚Äì    shows any changes in the record,

    ‚Äì    preserves the original record‚Äôs content when changed or updated, and can
         be printed separately from other patients‚Äô records,

‚Ä¢   use password protection or other features to secure the records from
    unauthorized access, and

‚Ä¢   install automatic back-up for file recovery to protect your records from loss
    and damage.


‚Ä¢   Retain personal health records for their minimum retention periods. This
    means in every instance you need to know the retention periods various laws
    prescribe and why the information you are dealing with was collected.

‚Ä¢   When patients have requested access to their own personal health records,
    retain those records as long as needed to allow the patients to exhaust any
    legal recourse involving the request.




                                                                                      109
Dealing with Health Information

      What You Should Do

      ‚Ä¢   Establish procedures for record storage. These procedures should be based on
          the guidelines provided in the Security section.

      ‚Ä¢   Establish procedures for record retention. These procedures should be based
          on the information in the Summary of Retention Periods.

      ‚Ä¢   Retain clinical records for the periods described in the Summary of Retention
          Periods. Retain educational, administrative, funding and research records for
          as long as you need them to serve their purpose, and then securely dispose of
          them.

      ‚Ä¢   In deciding how long you need to keep your patients‚Äô personal health
          information, follow three steps:

             ‚Äì First, consider why you collected the information (for example, for
                 research, funding or clinical purposes).

             ‚Äì Second, consider how long the law requires you to keep the records.
             ‚Äì Third, decide whether under the circumstances you should hold onto
                 the records longer than required to manage legal risks (for example,
                 where patients have sued in the past).

      ‚Ä¢   A general practice is to retain information for 10 years from the time the
          record was last updated or from when a minor attained the age of majority.

      ‚Ä¢   Consider your own risk management. What if a patient sues you after the
          minimum retention period has ended and you have destroyed records that
          could help defend you? Before disposing of files, ask yourself whether the
          patient‚Äôs case was one that might lead to a lawsuit. For example, certain kinds
          of treatment have a higher incidence of negligence claims (such as
          gynaecology and obstetrics). You may want to retain these records for longer
          periods than legally required.

      ‚Ä¢   See the Summary of Retention Periods for retention periods for:

             ‚Äì Health Records
             ‚Äì OHIP Records
             ‚Äì Research Records
             ‚Äì Education Records

110
                      Dealing with Health Information


Disposal

The Rule

You must dispose of your patients‚Äô personal health records in a secure manner.
The Act does not specify disposal methods.




What You Need To Do
‚Ä¢   Dispose of your patients‚Äô personal health records in a secure manner.

What You Should Do

‚Ä¢   Outline very clear procedures for securely disposing of personal health
    records and make sure your procedures are always followed.

‚Ä¢   For hard copy records, secure disposal means shredding or burning them.

‚Ä¢   For electronic records, secure disposal means either physically destroying the
    media they are stored on (such as a CD) or magnetically erasing or over-
    writing the information (in such a way that it cannot be recovered).
    Encrypting information is not a method of disposal even if the encryption keys
    are destroyed because it is possible that encrypted information may be
    recovered at some time in the future, even though keys have been destroyed.

‚Ä¢   For more information on the security aspects of record disposal, see Security ‚Äì
    People ‚Äì Personal Responsibilities for Security.

‚Ä¢   When you dispose of personal health records, record:
       ‚Äì the names of the patients whose records were disposed of,
       ‚Äì the dates the records were disposed of, and
       ‚Äì that you properly followed your hospital‚Äôs disposal procedures.
       Keep this record of disposal for as long as your hospital by-laws require.

       Note: This is a requirement of the Public Hospitals Act.


                                                                                      111
Dealing with Health Information


      Transfer

      The Rule

      You may transfer your patients‚Äô personal health records as described in the Act so
      long as you do so securely.




      What You Need To Do

      Transfer to Another Facility

      ‚Ä¢   When you transfer your patient‚Äôs personal health records to another facility or
          physician, you must keep the original record and only transfer a copy.

          Note: This is a requirement of the Public Hospitals Act and the Medicine Act.

      Transfer to a Successor

      ‚Ä¢   When you transfer your patients‚Äô personal health records to your successor,
          make reasonable efforts to notify your patients before transferring their
          records or, if that is not reasonably possible, as soon as possible afterwards.

      Transfer to Archives

      ‚Ä¢   When you transfer your patients‚Äô personal health records to:

             ‚Äì the Archives of Ontario, or
             ‚Äì in certain prescribed circumstances, a prescribed person whose duties
                 include collecting and preserving records of historical or archival
                 importance, if the transfer is made for that purpose,

          make sure you have the archivist‚Äôs agreement to the transfer.

          Note: In this context, the transfer is made for storage purposes.




112
                        Dealing with Health Information

What You Should Do

‚Ä¢   Follow the guidance provided in the Security section to make sure you are
    transferring information securely.




Related Sections of the Act
2, 3, 4, 10, 12, 16, 17, 42, 52, 72




Checklists, Templates and Tools

Summary of Retention Periods

See the Security section for additional checklists, templates and tools.




                                                                                113
Dealing with Health Information

                             SUMMARY OF RETENTION PERIODS

      Retention Periods for Health Records for Hospitals

      Patient Care Records:
      Adults:           10 years after the patient‚Äôs discharge or death (inpatient), or
                        10 years after the patient‚Äôs last visit or death (outpatient)

      Minors:           10 years after the day the patient turns or would have turned 18

      Diagnostic Imaging Records:
      Adults:           5 years after the day the record was created, except for diagnostic imaging
                        records of a breast examination, which must be retained for 10 years

      Minors:           5 years after the day the patient turned 18, except for diagnostic imaging records
                        of a breast examination, which must be retained for 10 years after the patient
                        turns 18

      Adults and Minors:

      Videotapes of diagnostic imaging examinations need not be kept unless the videotape constitutes
      the only record of the examination

      Investigations
      If a notice for an investigation or inspection under the Regulated Health Professions Act, Health
      Insurance Act or Coroners Act is received, the records must be retained until the investigation or
      inspection and any subsequent hearing is completed

      Patient Access Requests
      A personal health record cannot be disposed of if the patient the record relates to seeks access to
      those records and has not yet exhausted all avenues allowing for access

      Lawsuits
      Where a claim of negligence may arise:

      Adults:           A minimum of 15 years from the date on which the act or omission upon which
                        the claim of negligence could be based occurred

      Minors:           A minimum period of 15 years from the date the patient turned 18

      In both cases, if the patient cannot commence a claim because of a mental, physical or
      psychological condition and the individual has no litigation guardian, the records should be kept
      longer




114
                           Dealing with Health Information

The rules around discoverability of a negligence claim are complex and are dependent on the
specific facts of each case

For specific retention periods regarding individual cases, consult your lawyer

Retention Periods for OHIP Records for Hospitals
The Health Insurance Act requires that records be maintained to demonstrate that:

‚Ä¢   an insured service was provided

‚Ä¢   the hospital provided these services

‚Ä¢   the service was medically and therapeutically necessary

A minimum of 10 years, in line with statutory retention periods for clinical records, to assist in
proving billing was necessary

Retention Periods for Research Records for Hospitals

General Principle
Identifying data should be retained only as long as necessary to fulfill the research purpose;
however,

‚Ä¢   in a case where a claim of negligence may arise, records should be kept longer

‚Ä¢   due to the complexity of the discoverability rules in relation to claims of negligence, for
    record retention periods for specific research projects, consult your lawyer

If research is conducted without patient consent, the researcher receiving the information must
follow any return restrictions imposed by the originating health information custodian

Retention Periods for Health Records for Physicians (Private Office Records)

Patient Care Records:
Adults:           10 years after the last entry date, or until the physician stops practicing

Minors:           10 years after the day the patient turns or would have turned 18 or until the
                  physician stops practicing

Special Notes:
Family medicine and primary care physicians should refer to special transfer and disposition rules
if they plan to stop practicing

Dispensing physicians should refer to Supplementary Tables A and B for retention rules relating
to dispensing medications




                                                                                                     115
Dealing with Health Information

      Investigations
      If a notice for an investigation or inspection under the Regulated Health Professions Act, Health
      Insurance Act or Coroners Act is received, the records must be retained until the investigation or
      inspection and any subsequent hearing is completed

      Patient Access Requests
      A personal health record cannot be disposed of if the patient the record relates to seeks access to
      those records and has not yet exhausted all avenues allowing for access

      Lawsuits
      Where a claim of negligence may arise:

      Adults:           A minimum of 15 years from the date on which the act or omission upon which
                        the claim of negligence could be based occurred

      Minors:           A minimum period of 15 years from the date the patient turned 18

      In both cases, if the patient cannot commence a claim because of a mental, physical or
      psychological condition and the individual has no litigation guardian, the records should be kept
      longer

      The rules around discoverability of a negligence claim are complex and are dependent on the
      specific facts of each case

      For specific retention periods regarding individual cases, consult your lawyer

      Retention Periods for OHIP Records for Physicians
      The Health Insurance Act requires that records be maintained to demonstrate that:

      ‚Ä¢   an insured service was provided

      ‚Ä¢   the physician provided these services

      ‚Ä¢   the service was medically and therapeutically necessary

      A minimum of 10 years, in line with statutory retention periods for clinical records, to assist in
      proving billing was necessary

      Retention Periods for Research Records for Physicians

      General Principle
      Identifying data should be retained only as long as necessary to fulfill the research purpose;
      however,

      ‚Ä¢   in a case where a claim of negligence may arise, records should be kept longer



116
                           Dealing with Health Information

‚Ä¢   due to the complexity of the discoverability rules in relation to claims of negligence, for
    record retention periods for specific research projects, consult your lawyer

If research is conducted without patient consent, the researcher receiving the information must
follow any return restrictions imposed by the originating health information custodian




                                                                                                  117
Dealing with Health Information

                                     SUPPLEMENTARY TABLE A

            RETENTION PERIODS FOR RECORDS RELATING TO DRUGS
             DISPENSED UNDER THE ONTARIO DRUG BENEFIT PLAN



      Document                                                        Retention Period

      Statement of daily transaction totals                           2 years from the statement
                                                                      preparation date

      Summary remittance or reject statement from the                 2 years from the statement receipt
      Minister                                                        date

      Claim for payment or reversal submitted to the Ministry,        2 years from the claim submission
      with a record of the claim submission date                      date

      Monthly Ontario drug benefit eligibility card or copy of        2 years from the first drug
      the cards for each eligible person for whom a drug is           dispensing date
      dispensed

      Prescription with a no substitution direction and               2 years from the receipt date
      accompanying copy of the Health Canada adverse drug
      reaction form

      Ministry confirmation that drug is to be supplied if it         2 years from the confirmation
      meets the applicable clinical criteria set out in Part III of   receipt date
      the Formulary

      For each extemporaneous preparation supplied for an             2 years from the supply date
      eligible person, the formula, including the compounding
      time, all of the ingredients and the quantities and cost of
      those ingredients

      Where the acquisition cost of a drug is claimed, a copy         2 years from the receipt date
      of the supplier‚Äôs invoice and a detailed calculation in
      accordance with section 14 of the cost of purchasing the
      drug product




118
                         Dealing with Health Information

                              SUPPLEMENTARY TABLE B

      RETENTION PERIODS REQUIRED FOR PATIENT RECORDS
          RELATING TO DISPENSING OF DRUGS UNDER
        THE DRUGS AND PHARMACIES REGULATIONS ACT



Document                                      Retention Period

Required dispensing records                   6 years after the last entry date or
                                              until the physician stops practicing




                                                                                     119
120
Security ‚Äì Introduction
                                            Security ‚Äì Introduction


The Rule

You must implement reasonable physical, technical and administrative measures
to safeguard personal health information.

You must implement these measures to ensure the security and confidentiality of
personal health information. Specifically, you must:

‚Ä¢   prevent unauthorized use, copying or disclosure of the information,

‚Ä¢   protect the information during collection, storage, transfer and disposal, and

‚Ä¢   protect the integrity of the information by preventing unauthorized
    modification or disposal.

You must also notify your patients as soon as possible if their personal health
information is stolen, lost or accessed by unauthorized persons.


The Act does not prescribe specific security standards and safeguards; instead it
asks you to take reasonable steps. What is reasonable depends on the threats,
risks and vulnerabilities to which the information is exposed, on the sensitivity of
the information and on the extent to which it can be linked to an identifiable
individual. What is reasonable can also be judged by comparing the steps you
take with the best practices that other similar organizations with effective security
have implemented.

It is clear that taking reasonable steps includes implementing systems and controls
that safeguard how you collect, use, modify, disclose, retain, transfer and dispose
of personal health information. These steps cover:

‚Ä¢   physical security (for example, locked filing cabinets, restricted office access
    and alarms),

‚Ä¢   technical security (for example, passwords, encryption and firewalls), and

‚Ä¢   administrative controls (for example, security clearances, access restrictions,
    staff training and confidentiality agreements).

The four security sections that follow are designed to help you identify and
implement best practices for safeguarding personal health information. When you
read them, you will see statements such as ‚ÄúWhat you need to do‚Ä¶‚Äù You should
interpret this as ‚ÄúWhat you need to do to implement best practices for


                                                                                        123
Security ‚Äì Introduction

      safeguarding personal health information‚Äù as opposed to ‚ÄúWhat you must do to
      comply with the law‚Äù. Ultimately, it is up to you to decide what taking
      reasonable safeguard steps means for your organization, but we strongly suggest
      that adopting best practices is the approach to take. Clearly, if you take no steps,
      you are in violation of the Act.

      As you read the four security sections, you should keep in mind some simple
      principles of security that will help you make decisions and determine priorities:

      ‚Ä¢   Take a ‚Äúdefence in-depth‚Äù approach that assumes no single measure is
          perfect. So if it fails, you have other lines of defence to maintain protection.
          Consider mixing your measures between technical/physical measures and
          administrative measures (people and processes). Just having all technical
          measures could leave you exposed in situations such as power failures.

      ‚Ä¢   Make sure your security program is balanced and comprehensive. Security
          is only as strong as the weakest link. Paying too much attention to one
          vulnerability at the expense of others will leave you exposed and will not
          provide you with the best value for your security dollar. It is better to have a
          $50 lock on both front and back doors than a $1,000 lock on the front door
          and none on the back.

      ‚Ä¢   Don‚Äôt rely only on technology. Using technology will be important to help
          you protect personal health information but people and processes are often
          more important. Technology is useless if people don‚Äôt use it properly. And
          unfortunately, your people will often likely be the cause of security problems.
          So having informed, motivated staff using well designed processes is critical.

      ‚Ä¢   Security is not just the job of the security professionals ‚Äì security is
          everyone‚Äôs job. Once someone has ‚Äúsecurity‚Äù in their job title you may think
          you can relax and assume that they are taking care of everything. Staff must
          be made aware that they are all key to good security.

      ‚Ä¢   Keep personal health information in places you can best protect it. This
          sounds obvious but there are choices you can make to centralize the storage of
          both hardcopy and electronic information in areas that have good security
          controls such as supervised filing areas or servers managed by an IT
          department. Wherever possible, avoid letting the information be stored where
          you have less control, such as individual offices or personal computers.

      You should also apply your thinking to everyone who has access to personal
      health information within the institution, including employees, agents, contractors
      and volunteers. Throughout these sections we will collectively refer to these
      people either as ‚Äústaff‚Äù, or ‚Äúuser‚Äù if we are talking about electronic access to
      information. You must also think beyond your boundaries to how the security


124
                                             Security ‚Äì Introduction

protections will apply to third parties who handle personal health information on
your behalf.

In the next four sections, we‚Äôll deal with the critical aspects of an effective
security program:



‚Ä¢   Section 1: Security ‚Äì First Steps

           ‚Ä¢   Security Program and Policy

           ‚Ä¢   Roles and Responsibilities

           ‚Ä¢   Information Inventory and Classification

‚Ä¢   Section 2: Security ‚Äì People

           ‚Ä¢   Personal Responsibilities for Security

           ‚Ä¢   Authentication and Authorization

‚Ä¢   Section 3: Security ‚Äì Institutional Safeguards

           ‚Ä¢   Perimeter Security

           ‚Ä¢   Malicious Software

           ‚Ä¢   Wireless and Portable Devices

‚Ä¢   Section 4: Sustaining Security

           ‚Ä¢   Business Continuity

           ‚Ä¢   Development and Maintenance

           ‚Ä¢   Audit

           ‚Ä¢   Recommended Standards




                                                                                    125
126
Security ‚Äì First Steps
                                                                    Security ‚Äì First Steps

Table of Contents


Key Points............................................................................................................131

Security Program and Policy ...............................................................................132
   What You Should Do.....................................................................................132
       Small Office Applicability

Roles and Responsibilities ...................................................................................134
   What You Should Do.....................................................................................134
       Small Office Applicability

Information Inventory and Classification ............................................................135
    What You Should Do.....................................................................................135
       Small Office Applicability

Checklists, Templates and Tools .........................................................................136
       Appendix A ‚Äì Roles and Responsibilities
       Appendix B ‚Äì Information Inventory and Classification




                                                                                                                            129
Security ‚Äì First Steps




130
                                                Security ‚Äì First Steps


Key Points
To implement security effectively, you need a balanced approach that covers your
staff, your administrative processes and your technology. This section deals with
the fundamental first steps:

‚Ä¢   Set up a security program that takes a comprehensive approach to your
    physical, technological and administrative operations.

‚Ä¢   Assess your current security situation to determine your priorities and serve as
    a baseline for the program.

‚Ä¢   Develop a security policy that commits the organization to appropriate
    security measures and provides high-level direction on how this will happen.

‚Ä¢   Develop an appropriate set of security standards and procedures based on your
    policy.

‚Ä¢   Appoint a staff member with overall responsibility for security.

‚Ä¢   Define, document and communicate the responsibilities of this role and all the
    other roles required to support your security policy.

Documents you should create as a result of carrying out these steps include:

‚Ä¢   Security Policy, Standards and Procedures

‚Ä¢   Initial Security Review Results

‚Ä¢   Personal Health Information Inventory (Appendix B)




                                                                                       131
Security ‚Äì First Steps



      Security Program and Policy
      You must take reasonable steps to keep personal health information secure. What
      is reasonable may vary depending on your organization‚Äôs size and complexity,
      and the nature and extent of risks faced within the organization. Large hospitals
      dealing with significant amounts of sensitive personal health information that
      have internal networks, centrally managed IT and many staff members accessing
      information electronically will need different security than small offices. You
      must decide where your organization falls on the range between large institution
      and small office. Scale your measures to a reasonable level that fits your
      circumstances.

      What You Should Do

      Set up a security program that takes a comprehensive approach to your physical,
      technological and administrative operations.

      Assess your current security situation to determine your priorities and serve as a
      baseline for the program.

      Develop a security policy that commits the organization to appropriate security
      measures and provides high-level direction on how this will happen.

      Develop an appropriate set of security standards and procedures based on your
      policy.


      Your security program must be comprehensive because good security does not
      rely only on a strong lock on the front door. Good security relies on a series of
      measures in place just in case the lock gets broken or the back door is left open.
      Your program must also cover all security measures and not just technical ones.
      Installing anti-virus software to secure personal health information is important.
      But, it is not enough. Security is easily breached by simple mistakes such as
      sensitive information being left lying around or a wrong number being punched in
      when faxing a patient record.

      You need to make an initial assessment of your current security situation to
      determine the critical areas you must address first and also to set the baseline you
      will measure against to determine the effectiveness of your security program.
      This assessment should analyze both your current security risks and controls. If



132
                                                Security ‚Äì First Steps

you don‚Äôt have the skills within your institution to do this, seek outside help to do
the assessment properly since it will serve as the foundation for your program.

A written security policy is important as it will guide your staff on overall security
matters and provide a base for creating specific standards and procedures. Your
initial security assessment will provide input to help you build the policy that best
meets your security needs. You also need to have a good understanding of your
legal, regulatory, ethical and contractual security obligations in order to build
your policy.

At a minimum, the highest levels of your management should approve your
security policy. Your security policy should include:

‚Ä¢   what security means to your institution and why it is so important,

‚Ä¢   key security goals and principles,

‚Ä¢   individuals‚Äô basic security responsibilities and accountability,

‚Ä¢   how staff will be trained,

‚Ä¢   who will review and update your policy, and

‚Ä¢   how you will comply with your contractual and legal security obligations.

You should use your policy to help develop a fully documented set of security
standards (for example, password rules) and security procedures covering both:

‚Ä¢   how you protect your perimeter (such as the main entry point to your building
    or computer network), and

‚Ä¢   what occurs inside your building or network (because your employees are not
    free to see any information they want, nor are they immune from mistakes or
    bad judgement).

The security policy should integrate with other policies, particularly privacy.

Tell your staff and outside contractors about the policy.

Small Office Applicability

‚Ä¢   Give your staff a concise written set of security rules that also explain why the
    rules must be followed.

‚Ä¢   Remember that everyone must play their part in protecting your patients‚Äô
    personal health information and your facilities.


                                                                                         133
Security ‚Äì First Steps

      ‚Ä¢   The Small Office Applicability sub-sections in this section will help you
          customize rules that fit your office.




      Roles and Responsibilities

      What You Should Do

      Appoint a staff member with overall responsibility for security.

      Define, document and communicate the responsibilities of this role and all the
      other roles required to support your security policy.


      ‚Ä¢   Assign at least one staff member (for example, the Security Officer) overall
          responsibility for security. See Appendix A for a sample of this person‚Äôs
          responsibilities.

      ‚Ä¢   Establish a cross-functional team of senior managers to review security status,
          requirements and direction and ensure security issues are addressed with long-
          term solutions. The team should meet regularly.

      ‚Ä¢   Consider appointing a Data Steward to define the exact policies for access to
          stores of personal health information.

      ‚Ä¢   Appoint one staff member in each critical process (such as transferring health
          records) to be responsible for that security process. This role differs from the
          Data Steward role as the latter deals with disaster recovery requirements
          rather than specific data-handling requirements.

      ‚Ä¢   Define, document and communicate security responsibilities for personnel
          managers, general employees, and those with security roles for specific
          processes.

      ‚Ä¢   Define specific security duties for employees responsible for maintaining
          security controls and with special access for technical support.

      ‚Ä¢   Ensure security through ‚Äúseparation of duties.‚Äù Individuals should neither
          audit their own performance nor authorize their own access to a system.




134
                                                Security ‚Äì First Steps

‚Ä¢   If security responsibilities can be delegated, for example, where a Data
    Steward may appoint someone to perform day-to-day data access approvals,
    define the delegation precisely.

‚Ä¢   Do not delegate responsibility for determining security requirements to third
    parties. Define security requirements in any contract with third parties with
    access to personal health information. Build in appropriate monitoring
    measures to ensure they comply.

‚Ä¢   Your list of security responsibilities should cover specific security incident
    response procedures (such as responding to security breaches).

Small Office Applicability

‚Ä¢   The law makes physicians ultimately responsible for security.

‚Ä¢   While all staff play a role, the buck stops with the custodian of personal health
    information.

‚Ä¢   Make clear to your staff what are their security responsibilities (as described
    in other sections of this Toolkit). Train staff on these responsibilities when
    hired. Ask them to sign that they understand and will abide by their
    responsibilities.




Information Inventory and Classification

What You Should Do

Maintain a categorized inventory of all your stores of personal health information.


‚Ä¢   List all stores of personal health information. Capture the essential facts about
    the information and how it should be handled. (A sample Inventory Template
    is included in Appendix B).

‚Ä¢   The inventory should include all formats and media used to store personal
    health information including electronic, hardcopy, microfiche, audiovisual
    media, photographs and other images.




                                                                                        135
Security ‚Äì First Steps

      ‚Ä¢   Build your security policy around a data classification scheme. Categorize the
          types of information you collect and store and define security controls for
          each category. Typical categories include:

             ‚Äì public ‚Äì no restrictions,
             ‚Äì internal ‚Äì for use inside the institution only,
             ‚Äì confidential ‚Äì for use only on a ‚Äúneed to know‚Äù basis, and
             ‚Äì restricted ‚Äì controlled access to specified individuals.
      ‚Ä¢   All personal health information should be classified as at least confidential.

      Small Office Applicability

      ‚Ä¢   Keep accurate records of the personal health information you store. Include
          both hardcopy and electronic information and information that may be outside
          your office.

      ‚Ä¢   Implement appropriate security measures to protect the information. At a
          minimum, store hardcopy information under lock and key and protect
          electronic information by password.




      Checklists, Templates and Tools

      Appendix A ‚Äì ‚ÄúRoles and Responsibilities‚Äù provides a sample of a Security
      Officer‚Äôs Responsibilities

      Appendix B ‚Äì ‚ÄúInformation Inventory and Classification‚Äù provides a sample
      Information Inventory Template




136
                                                  Security ‚Äì First Steps

             APPENDIX A ‚Äì ROLES AND RESPONSIBILITIES

Sample Security Officer Responsibilities

‚Ä¢   Setting, reviewing and updating security policy.

‚Ä¢   Ensuring staff and contractors are aware of the policy and informed on how
    they should support it.

‚Ä¢   Driving implementation of supporting procedures and controls.

‚Ä¢   Ensuring security controls are audited.

‚Ä¢   Conducting Threat Risk Assessments (TRA) for any changes to processes or
    IT systems that could affect security. (See sample TRA form in Sustaining
    Security section.)

‚Ä¢   Investigating security incidents and ensuring long-term solutions are in place.
    (Security incidents occur whenever your security controls are compromised or
    challenged beyond any thresholds you have set. If the incident compromises
    personal health information, it also raises privacy issues; you should follow
    the guidelines in the Oversight section of this Toolkit to manage a privacy
    breach.)

‚Ä¢   Developing a close working relationship with the contact person or equivalent
    (since these roles are intimately connected).

‚Ä¢   Advising staff and contractors on security.




                                                                                      137
Security ‚Äì First Steps

        APPENDIX B ‚Äì INFORMATION INVENTORY AND CLASSIFICATION

      Sample Information Inventory Template

      Create and maintain a high-level inventory of all the computer and hardcopy
      stores of personal health information you have. Remove the sample information
      and customize the table according to your needs.

                 Information        Info                   Backup
       Name         Types         Steward    Location      Copies        Security        Retention

      Master    Patient info:    John Q.    UNIX         Offsite       Confidential:    Archive
      Patient   - Name           Deere      server at    storage       password-        after patient
      Health    - Contact info              College      facility in   protected via    inactive for
      Records   - Health #                  Street       Markham       EHR              h years
                - Family info               location                   application on
                - Health                                               encrypted in
                 history                                               storage

      MRI       MRI images       Jane P.    MRI          None          Confidential:    Destroy
      Records   with:            Doe        Cabinet,                   Locked           after patient
                -patient name               4th floor,                 cabinet (key     inactive h
                -health #                   University                 with Jane)       years
                -date                       Avenue                     Sign-out sheet
                                                                       for borrowing
                                                                       records




      Storage requirements should include any special measures needed for the type of
      media used, such as limits on heat, humidity, ultra-violet light and magnetic
      fields.




138
Security ‚Äì People
                                                                                 Security ‚Äì People

Table of Contents


Key Points............................................................................................................143

Personal Responsibilities for Security .................................................................144
   What You Should Do.....................................................................................144
   Physical Security............................................................................................144
       Small Office Applicability

Authentication and Authorization........................................................................146
   What You Should Do.....................................................................................146
      Small Office Applicability

Related Sections of the Act..................................................................................147

Checklists, Templates and Tools .........................................................................147
       Appendix A ‚Äì Personal Responsibilities for Security
       Appendix B ‚Äì Authentication and Authorization




                                                                                                                            141
Security ‚Äì People




142
                                                         Security ‚Äì People


Key Points
Your security is only as strong as your staff. Even the best technological security
is vulnerable if you do not have staff committed to safeguard confidential
information. This section deals with the steps needed to address the people aspect
of security:

‚Ä¢   Inform and motivate all staff and contractors. Give them the necessary tools
    to carry out their personal security responsibilities. Training should include
    awareness and commitment to:

       ‚Äì Security Policy
       ‚Äì Malicious Software Rules
       ‚Äì Responsibilities for Physical Security
       ‚Äì Acceptable Use Policy
       ‚Äì Rules for Fax Machines
       ‚Äì Confidentiality Agreement
       ‚Äì Password Policy
       ‚Äì Guidelines for Mobile Computing
       ‚Äì Incident Reporting Rules
‚Ä¢   Impose controls to ensure no one gains access to personal health information
    without proper authorization.

Documents you should create as a result of carrying out these steps include:

‚Ä¢   Staff Responsibilities for Physical Security (Appendix A)

‚Ä¢   Acceptable Use Policy and Rules for Fax Machines (Appendix A)

‚Ä¢   User ID and Access Management Procedures (Appendix B)

‚Ä¢   Password Policy (Appendix B)




                                                                                      143
Security ‚Äì People


      Personal Responsibilities for Security

      What You Should Do

      Inform and motivate all staff and contractors. Give them the necessary tools to
      carry out their personal security responsibilities.


      ‚Ä¢   Run appropriate background checks before hiring staff who:

             ‚Äì deal with personal health information,
             ‚Äì work on IT infrastructure, and
             ‚Äì have special security responsibilities.
      ‚Ä¢   Staff should sign confidentiality agreements.

             ‚Äì Have staff sign the agreement when they are hired and initial their
                 pledge annually as a reminder. Use this as an annual opportunity to
                 reinforce training on privacy and security and brief staff on recent
                 changes.

      ‚Ä¢   All third-party contractors (for instance, consultants) and key contractor staff
          who may have access to sensitive information should sign an agreement
          before they begin work.

      Physical Security
      ‚Ä¢   Your staff must:

             ‚Äì be aware of physical security responsibilities,
             ‚Äì lock up sensitive material,
             ‚Äì wear identity badges,
             ‚Äì secure information outside the normal work area, and
             ‚Äì report suspected incidents.



144
                                                        Security ‚Äì People

‚Ä¢   See Appendix A for more detailed guidance on physical security and use of
    fax machines.

‚Ä¢   Tell staff what they need to do for on-line security. Issue an ‚ÄúAcceptable
    Use‚Äù policy (see Appendix A for a sample).

‚Ä¢   Provide staff with necessary security tools (such as anti-virus software and
    laptop computer cable locks).

‚Ä¢   Create an ongoing security awareness and training program, reinforced with
    regular updates.

Small Office Applicability

‚Ä¢   Lock up hardcopy personal health information if left unattended. Assign
    someone the task of making sure the office, all personal health information
    and computers are locked at the end of each day.

‚Ä¢   Set power-on passwords on all computers. Install locking screen savers and
    instruct staff to use the screen saver whenever they leave their computer. Set
    the screen saver to automatically kick in when the computer is idle for a
    reasonable period of time.

‚Ä¢   Use your computers‚Äô built-in security features, such as power-on and hard-
    drive passwords.

‚Ä¢   Make sure staff understand that they should not install any unauthorized
    software or connect any unauthorized devices to their computers, or use their
    computers for unauthorized purposes.

‚Ä¢   Make sure staff understand that they may not copy or transmit any
    information from their computers unless authorized. This includes using
    email or instant messaging.

‚Ä¢   Avoid accidental exposure of personal health information, like the ‚Äúreader
    over the shoulder‚Äù or the neighbour who overhears loud conversations.

‚Ä¢   Give staff facilities (for example, shredding machines) to securely dispose of
    personal health information no longer required.




                                                                                     145
Security ‚Äì People


      Authentication and Authorization
      Although the following guidelines focus on computer systems, many apply
      equally to voicemail and hardcopy records.

      What You Should Do

      Impose controls to ensure no one gains access to personal health information
      without proper authorization.


      ‚Ä¢   Authentication establishes someone‚Äôs identity.

      ‚Ä¢   Authorization establishes what someone may do.

      Restrict access to all personal health information to only those who ‚Äúneed to
      know‚Äù. Give access only to people whose job requires access.

      Always provide only the minimum access needed to perform the job.

      ‚Ä¢   Where possible, use two-factor authentication to grant access to personal
          health information. This requires users to (1) know something, like a
          password, and (2) have something, like an ID badge. This is especially
          important when providing remote electronic access to personal health
          information.

      ‚Ä¢   Implement effective password policies and practices, including standards on
          password choice, protection and updating. (A sample password policy is
          found in Appendix B.)

      ‚Ä¢   Avoid shared User IDs so you can hold all individuals accountable for their
          use of computer and network resources.

      ‚Ä¢   Manage User IDs and associated access rights from issuance through deletion.
          Appendix B provides details on the practices you should adopt, including:

             ‚Äì periodically checking to see that User IDs are still needed, and
             ‚Äì creating special measures for User IDs with access overrides, such as
                 those technical support staff use.




146
                                                         Security ‚Äì People

Small Office Applicability

‚Ä¢   Provide staff with their own computers, User IDs and passwords, where
    possible. At a minimum, staff should have their own User IDs and passwords
    to access personal health information.

‚Ä¢   Remove or change access as soon as staff leave or change responsibilities.

‚Ä¢   Instruct your staff to create passwords that are hard to guess. Tell them not to
    write passwords down and, if they must, to store the paper someplace secure.
    (Recommend passwords at least 8 characters long containing both numbers
    and letters.) Ask them to change these passwords periodically and never re-
    use old ones.

‚Ä¢   Instruct your staff never to share their passwords except for temporary
    authorized backups. Passwords should be reset immediately afterwards.




Related Sections of the Act
2, 3, 4, 10, 12, 13, 14, 42, 53




Checklists, Templates and Tools

Appendix A ‚Äì ‚ÄúPersonal Responsibilities for Security‚Äù provides a sample List of
Staff Responsibilities for Physical Security, a sample Acceptable Use policy and
guidance on using fax machines.

Appendix B ‚Äì ‚ÄúAuthentication and Authorization‚Äù provides a sample password
policy.




                                                                                       147
Security ‚Äì People

          APPENDIX A ‚Äì PERSONAL RESPONSIBILITIES FOR SECURITY

      Sample List of Staff Responsibilities for Physical Security

      ‚Ä¢   Lock cabinets and secure removable computing devices such as laptops.

      ‚Ä¢   Follow ‚Äúclean desk‚Äù practices especially in unattended workspaces.

      ‚Ä¢   Wear identity badges, challenge ‚Äútail-gaters‚Äù entering restricted areas, and
          sign-in and escort visitors in restricted areas.

      ‚Ä¢   Secure information and computers used outside the normal work environment
          (for example, in a home office or while tele-commuting or travelling for
          business).

      ‚Ä¢   Avoid accidentally exposing information, for example, allowing a computer
          screen to be viewed or a conversation to be overheard.

      ‚Ä¢   Dispose of hardcopy personal health information properly (for example, using
          a shredding machine). Disposal methods such as shredding machines should
          meet security standards.

      ‚Ä¢   Properly dispose of digital media by either physical destruction (for example,
          shredding CDs) or by first erasing information in an approved manner.

      Sample Acceptable Use Policy

      Modify the following sample policy to suit your requirements. Review with your
      lawyers before publishing.

      ‚Ä¢   Applicability

             ‚Äì All users, including full- and part-time employees, contractors, temps,
                 affiliates, consultants, and interns, of [Insert Your Organization‚Äôs
                 Name] IT resources are subject to this Acceptable Use Policy.

      ‚Ä¢   Business Use

             ‚Äì Use [Insert Your Organization‚Äôs Name]‚Äôs resources exclusively to
                 conduct [Insert Your Organization‚Äôs Name] business or for other
                 uses management authorizes.

             ‚Äì The following statement (or equivalent statements) must be presented
                 to individuals logging onto [Insert Your Organization‚Äôs Name]


148
                                                       Security ‚Äì People

          systems during the identification and authentication process if the
          [Insert Your Organization‚Äôs Name] system is running an operating
          system that can provide notification (otherwise labels containing the
          text should be placed visibly at the user interface):

          ‚Ä¢   Use is subject to audit at any time by [Insert Your
              Organization‚Äôs Name] management. [Insert Your
              Organization‚Äôs Name] may from time to time monitor use of its
              IT resources, including email and the Internet, to ensure business
              use or to investigate suspected problems. Users are advised of this
              practice before being permitted access to [Insert Your
              Organization‚Äôs Name] IT resources.

‚Ä¢   Personal Use of Computing Equipment

       ‚Äì Only [Insert Your Organization‚Äôs Name] management may approve
          personal use of [Insert Your Organization‚Äôs Name] computing
          equipment (if the use is clearly insignificant and complies with [Insert
          Your Organization‚Äôs Name]‚Äôs Business Conduct Guidelines).
          Personal use may not be approved if it:

          ‚Ä¢   interferes or competes with [Insert Your Organization‚Äôs Name]
              business,

          ‚Ä¢   interferes with any employee‚Äôs job performance,

          ‚Ä¢   involves any additional costs to [Insert Your Organization‚Äôs
              Name],

          ‚Ä¢   involves commercial solicitation,

          ‚Ä¢   divulges company information to others, or

          ‚Ä¢   involves commercial or personal distribution lists.

       ‚Äì Questions concerning personal use of [Insert Your Organization‚Äôs
          Name] computing resources and Internet services should be discussed
          with the employee‚Äôs manager.

       ‚Äì Incidental or infrequent personal use of [Insert Your Organization‚Äôs
          Name]‚Äôs e-mail systems and access to the Internet for personal use
          may be allowed without management approval provided none of the
          above prohibitions are violated.




                                                                                     149
Security ‚Äì People

      ‚Ä¢   Instant Messaging

             ‚Äì Internal Instant Messaging is subject to all [Insert Your
                Organization‚Äôs Name] Acceptable Use standards and is subject to the
                same policies and standards as e-mail communications.

             ‚Äì External or public Instant Messaging software and services may not be
                used on any [Insert Your Organization‚Äôs Name] machines or from
                within the [Insert Your Organization‚Äôs Name] network.

             ‚Äì Only the authorized [Insert Your Organization‚Äôs Name] Instant
                Messaging solution may be used.

      ‚Ä¢   Cell Phones

             ‚Äì Cell Phones should not be used to store or transmit confidential
                information.

             ‚Äì Use of cell phone cameras is prohibited within [Insert Your
                Organization‚Äôs Name].

      ‚Ä¢   Wireless Network Access

             ‚Äì Management must approve in writing all Wireless Access Points. A
                Risk Assessment must be performed prior to their implementation.

             ‚Äì Only [Insert Your Organization‚Äôs Name] approved and installed
                Wireless technology is permitted on an [Insert Your Organization‚Äôs
                Name] machine or within [Insert Your Organization‚Äôs Name].

      ‚Ä¢   Chain Letters, Hoaxes and Virus Warnings

             ‚Äì Using [Insert Your Organization‚Äôs Name] computer systems to
                send, forward or reply to chain letters, free offers, hoaxes or virus
                warnings is prohibited.

             ‚Äì Should you receive an e-mail notice about a supposed virus or harmful
                code threat, notify the Help Desk or check the [Insert Your
                Organization‚Äôs Name] intranet site.




150
                                                       Security ‚Äì People

‚Ä¢   Offensive and Inappropriate Material

       ‚Äì [Insert Your Organization‚Äôs Name] employees may neither access
          nor distribute any material that could be considered inappropriate,
          offensive or disrespectful to others. Examples include material that:

          ‚Ä¢   contains sexually explicit images or descriptions,

          ‚Ä¢   advocates illegal activity, or

          ‚Ä¢   advocates intolerance for others.

       ‚Äì Employees should obtain company directives from their managers.
‚Ä¢   Internet Use

       ‚Äì These rules apply to both the Internet and internal [Insert Your
          Organization‚Äôs Name] intranet network.

       ‚Äì Unprotected information posted on the Internet is available to
          countless unknown people worldwide, not all of whom support [Insert
          Your Organization‚Äôs Name].

       ‚Äì [Insert Your Organization‚Äôs Name]‚Äôs information, computing assets,
          and corporate image on the Internet must be rigorously safeguarded
          from loss, modification or destruction.

       ‚Äì Using the [Insert Your Organization‚Äôs Name]‚Äôs Internet access is
          regulated by policy. Any infringement can result in disciplinary action
          up to and including termination. Prohibited uses can include:

          ‚Ä¢   viewing or posting any material considered inappropriate,
              offensive or disrespectful to others,

          ‚Ä¢   using unauthorized file exchange or sharing of services (such as
              Napster, Gnutella, WinMX, LimeWire, BearShare, Morpheus and
              Kazaa),

          ‚Ä¢   trading securities,

          ‚Ä¢   gambling on-line or entering prize competitions,

          ‚Ä¢   downloading entertainment software or games or playing games,
              and



                                                                                    151
Security ‚Äì People

                ‚Ä¢   uploading software or data owned by or licensed to [Insert Your
                    Organization‚Äôs Name] without appropriate authorization.

             ‚Äì [Insert Your Organization‚Äôs Name] may block access from within
                our networks to any sites we determine are inappropriate for any
                reason. If you find yourself connected incidentally to a site that
                contains inappropriate material, you must disconnect from the site
                immediately.

             ‚Äì Posting information about [Insert Your Organization‚Äôs Name]‚Äôs
                employees, vendors, suppliers or partners without a valid business
                justification is prohibited. Posting personal opinions about [Insert
                Your Organization‚Äôs Name]‚Äôs staff or affiliates is prohibited.

             ‚Äì Revealing confidential or personal health information, and any other
                material on chat rooms is prohibited.

             ‚Äì Downloading infected, unlicensed or unregistered software is
                prohibited.

      ‚Ä¢   Electronic Mail

             ‚Äì Electronic mail (e-mail) is a business tool that should be used with the
                same considerations for quality and appropriateness as any other
                business communication.

             ‚Äì E-mail is the property of [Insert Your Organization‚Äôs Name] and
                may be monitored or audited at any time to ensure compliance with
                Security Policies and Standards or for other reasons at management‚Äôs
                discretion.

             ‚Äì Employees may not represent themselves as someone or something
                they are not.

             ‚Äì Sending, creating or storing offensive or disruptive material using
                email is strictly prohibited. Violating [Insert Your Organization‚Äôs
                Name]‚Äôs business conduct guideline is also prohibited. Prohibited
                content includes, but is not limited to, racial or ethnic slurs, profanity,
                and messages containing sexually explicit language or graphics.

             ‚Äì Any communication using [Insert Your Organization‚Äôs Name]‚Äôs e-
                mail or Internet systems could be construed as representing [Insert
                Your Organization‚Äôs Name]‚Äôs corporate position. Only duly



152
                                                       Security ‚Äì People

          authorized individuals may speak or write on [Insert Your
          Organization‚Äôs Name]‚Äôs behalf.

       ‚Äì Users must never attempt to gain access to another user‚Äôs email
          messages without permission.

       ‚Äì Critical information should not be stored in email.
‚Ä¢   Representation of [Insert Your Organization‚Äôs Name] using IT facilities
    (such as email or the Internet)

       ‚Äì Employees representing [Insert Your Organization‚Äôs Name] must
          identify themselves accurately and completely (including their position
          and function where requested).

       ‚Äì Any false representation of authority or engagement in unauthorized
          business is strictly prohibited.

       ‚Äì Only duly authorized employees or officials may represent [Insert
          Your Organization‚Äôs Name] and its policies in speech or writing to
          the media, analysts, or at public gatherings. Other employees may
          participate in newsgroups or chats in the course of business, when
          relevant to their duties, as individuals speaking only for themselves.
          Individuals participating who are identified as an [Insert Your
          Organization‚Äôs Name] employee or agent must refrain from any
          unauthorized political advocacy, endorsement or apparent
          endorsement by [Insert Your Organization‚Äôs Name] of any
          commercial product or service.

       ‚Äì Electronic mail addresses may satisfy the requirement for a legal
          signature. Employees must avoid creating unwarranted contractual
          obligations. Where the possibility exists, a disclaimer must be
          included, indicating that official approval must be obtained before
          agreement.

       ‚Äì [Insert Your Organization‚Äôs Name] does not accept responsibility
          for the personal opinions its Internet users express. [Insert Your
          Organization‚Äôs Name] does not act as a publisher; it simply allows
          the means to distribute statements its employees make. Employees
          with Internet access must understand how Canadian intellectual
          property, defamation and criminal laws may expose [Insert Your
          Organization‚Äôs Name] to legal liability.




                                                                                    153
Security ‚Äì People

      ‚Ä¢   Compliance With Legal, Statutory and Regulatory Requirements

             ‚Äì [Insert Your Organization‚Äôs Name]‚Äôs facilities and computing
                resources must not be knowingly used to violate the laws of Canada
                or Ontario or of any other jurisdiction in any material way. Using
                institutional resources for illegal activity is grounds for immediate
                dismissal, and [Insert Your Organization‚Äôs Name] will cooperate
                with any legitimate law enforcement activity.

             ‚Äì Any software or files downloaded via the Internet into the [Insert
                Your Organization‚Äôs Name] network become the [Insert Your
                Organization‚Äôs Name]‚Äôs property. These files or software may be
                used only in ways consistent with their licences or copyrights.

             ‚Äì No employee may knowingly use [Insert Your Organization‚Äôs
                Name] facilities to download or distribute pirated software or data.

      ‚Ä¢   Malicious and Damaging Activities

             ‚Äì Employees must never use [Insert Your Organization‚Äôs Name]‚Äôs
                Internet facilities to propagate any virus, worm, Trojan Horse or trap
                door program code.

             ‚Äì Employees must never use [Insert Your Organization‚Äôs Name]‚Äôs
                Internet facilities to disable or overload any computer system or
                network, or circumvent any system intended to protect another user‚Äôs
                privacy or security. Employees also must never use [Insert Your
                Organization‚Äôs Name]‚Äôs computing resources to gain unauthorized
                access to remote systems. Any attempts to do so will be reported to the
                remote systems‚Äô administrators and law enforcement personnel where
                warranted. Deleting, examining, copying or modifying other users‚Äô or
                entities‚Äô data or files without their consent is prohibited.

             ‚Äì Employees must never run security-testing tools or programs against
                any Internet system or server.

             ‚Äì Employees must never knowingly write or run any computer program
                or process (including email) that would consume more computer
                resources than needed to perform [Insert Your Organization‚Äôs
                Name]‚Äôs work.

             ‚Äì Employees must not use [Insert Your Organization‚Äôs Name]‚Äôs
                network to perform an Internet transaction that may consume
                significant system resources and interrupt or delay system use.


154
                                                         Security ‚Äì People

           Information Technology management must approve any repetitive or
           large data transactions.

Rules for Transmitting Personal Health Information using Fax Machines

‚Ä¢   Transmit personal health information by fax only when no more secure
    practical alternative exists.

‚Ä¢   When you do need to send a fax, always remove common identifiers, such as
    a patient‚Äôs name and address (an ID number may be all that is required) from
    personal health information.

‚Ä¢   Because the fax machine receiving your transmission may not be located in a
    secure area, always let your recipients know that you are about to fax sensitive
    material to them and confirm their fax number.

‚Ä¢   Always include a cover page with your name, telephone number, date and
    number of pages sent.

‚Ä¢   Carefully enter the fax number to ensure you do not misdirect the message
    and seriously breach someone‚Äôs privacy.

‚Ä¢   To avoid manual keying errors to routine destinations, use the fax machine‚Äôs
    auto-dial features, if available.

‚Ä¢   Confirm that the intended recipient has received all pages you sent.

‚Ä¢   Immediately try to recapture any misdirected fax, and follow the guidelines in
    this Toolkit for handling a privacy breach if appropriate.




                                                                                       155
Security ‚Äì People

             APPENDIX B ‚Äì AUTHENTICATION AND AUTHORIZATION

      Sample User ID and Access Management Practices

      ‚Ä¢   Manage user IDs and access rights.

             ‚Äì Appoint one person to approve user ID set-up and periodically review
                 that access is required.

             ‚Äì Make sure users immediately change temporary passwords to a
                 password only they know when they first log on. Never issue
                 temporary passwords that are easy to guess (like ‚Äúwelcome‚Äù).

             ‚Äì Transmit passwords securely and separately from information that
                 identifies the user.

             ‚Äì Immediately remove access when people leave or change jobs.
             ‚Äì Never reuse old User IDs.
             ‚Äì Use Identity Management software to manage this process where
                 possible.

      ‚Ä¢   Implement ‚Äúneed to know‚Äù access.

             ‚Äì Give access only to those people whose jobs require access. (For
                 example, those caring for or treating a patient would need to know the
                 personal health information relating to their role in the patient‚Äôs
                 current primary care and treatment.)

             ‚Äì Restrict access by the specific fields and range of records each person
                 needs access to do the job. If the technology does not allow a fine
                 enough degree of control, modify or replace the technology as soon as
                 possible, using manual controls in the meantime. (This usually
                 requires spot-checking for correct use of information after the fact).

             ‚Äì Set default access to read-only (view), and add only where the job
                 function requires more.

             ‚Äì As the type of access needed increases, the level of approval should
                 increase.




156
                                                       Security ‚Äì People

       ‚Äì Always consider whether the job can be performed effectively with
           either anonymous rather than personal information.

       ‚Äì Build access rules in terms of defined job roles, such as ‚Äúemergency
           room physician‚Äù or ‚Äúacute care nurse,‚Äù instead of evaluating individual
           requests.

       ‚Äì Consider other methods to support ‚Äúneed to know‚Äù access, such as:

     Relationship Based          Location/ Method            Time Based
      Access Control              Based Access              Access Control
                                     Control




     User has a particular       Access may be denied           Access is given
     relationship with the       if the request comes           only for specific
     patient before access       from an insecure               times (e.g., no
     is granted, (e.g.,          location or method             after hours
     attending physician)        (e.g., no access from          access)
                                 home)


‚Ä¢   Users with special support User IDs that provide more powerful access
    capabilities should:

       ‚Äì have a regular User ID for when not performing support functions,
       ‚Äì not have special IDs in both production and test/development
           environments, and

       ‚Äì not have access to delete or modify audit logs.
‚Ä¢   Systems should lock User IDs automatically after three failed logon attempts
    in a row and these events should be logged.

Sample Password Policy

Modify the following Password Policy to suit your environment and
requirements, automating as many of the rules as possible.




                                                                                     157
Security ‚Äì People

      ‚Ä¢   Password Protection

             ‚Äì Do not transmit user passwords in clear text form over the Internet,
                public networks or wireless devices. Passwords generated for an
                individual user must be conveyed to the user in a secure manner, such
                as in a telephone or face-to-face conversation, password-protected
                voicemail box or encrypted electronic mail, or by separating the
                password from its context (user ID).

             ‚Äì A new password may be generated only after the system administrator
                or designated authority has positively identified the user.

             ‚Äì When temporary passwords are issued for new or reset User IDs, the
                user should immediately change to a password no one else knows
                when first logging on.

             ‚Äì Change default user IDs with default passwords shipped with
                operating systems and program products for use during system and
                product installation and set up as soon as possible during or following
                their initial use.

             ‚Äì If possible, user passwords must be encrypted when stored in files or
                databases on systems and servers. If passwords cannot be encrypted,
                access to the file or database element containing the passwords must
                be restricted to only authorized system security administrators. If
                passwords are not encrypted during storage and transmission, they
                may be hashed with a one-way function. Hashing with a one-way
                function is considered acceptable protection for passwords when
                transmitted across a network.

             ‚Äì Do not disclose to or share with anyone your password, including
                administrative assistants or secretaries, unless individual
                accountability can be maintained.

             ‚Äì User password management is a condition of your employment.
             ‚Äì Do not write your password down. If this is unavoidable and you must
                write it down, protect the writing containing your password as
                confidential information, keeping it in your possession or under lock
                and key at all times.

             ‚Äì Do not store passwords in a file on any computer system (including
                Palm Pilots or similar devices) without strong encryption.



158
                                                Security ‚Äì People

‚Äì Passwords must never be included in any automated log-on process
   (for example, stored in a macro or function key) or plain-text batch
   files residing on a user‚Äôs local workstation to hasten log on without
   having to key the user-ID/password combination in.

‚Äì Close or log out of password protected web sessions as soon as you
   have finished with them.

‚Äì Password files for a system must be separated from other system and
   application data, and should never be available to non-administrative
   users.

‚Äì Passwords for critical systems and network components must be
   sealed and stored in a physically secured location to permit emergency
   use by designated personnel when necessary.

‚Äì Do not possess nor use any tool that would obtain, disclose or capture
   another user‚Äôs password.

‚Äì Controls must be in place to prevent an unlimited number of invalid
   logon password attempts (like password hacking). This will be
   accomplished by either revoking or locking the user ID on the fourth
   consecutive invalid attempt or using a logon inductor to exponentially
   increase the amount of time between sign-on screens (to prevent
   automated tool password hacking).

‚Äì Passwords must be changed regularly (monthly or quarterly). Old
   passwords should not be re-used.

‚Äì If an account or password is suspected to have been compromised,
   report the incident and change all passwords.

‚Äì All unsuccessful authentication attempts will be logged.
‚Äì The Information Security Department or its delegates will perform
   password cracking or guessing periodically or randomly. If a
   password is compromised during one of these scans, the user will be
   required to change it.

‚Äì Password ‚ÄúDon‚Äôts‚Äù:
   ‚Ä¢   Never reveal your password to anyone including your manager,
       family members and co-workers, except as part of an authorized
       password management process.


                                                                            159
Security ‚Äì People

                ‚Ä¢   Never discuss your password in front of others or hint at your
                    password‚Äôs composition (for example, ‚Äúmy family name‚Äù).

                ‚Ä¢   Never reveal a password on questionnaires or security forms.

                ‚Ä¢   Never use the ‚ÄúRemember Password‚Äù feature on applications (for
                    example, Internet Explorer, Eudora, Outlook, Netscape
                    Messenger).

      ‚Ä¢   Password-cracking programs can break weak passwords in minutes. Observe
          these rules to build strong passwords.

             ‚Äì Passwords should be at least 8 characters long.
             ‚Äì Passwords should contain a mixture of letters, numbers and special
                characters (for example ! @ # $ %), if possible.

             ‚Äì Avoid dictionary words within the password.
             ‚Äì Avoid using passwords based on easily guessed ideas, such as birth
                dates, family or pet names, hobbies, sports or months.

             ‚Äì Avoid keyboard sequences (for example QWERTY) and logical
                sequences (for example, ABC, 1-2-3, zyx, 10-9-8).

             ‚Äì Avoid repetitive sequences (a number or letter used more than twice).
             ‚Äì Do not use the same password for work and non-work User IDs (for
                example, personal ISP account, option trading, benefits, etc.). Avoid
                using the same password for multiple work User IDs.

             ‚Äì Do not re-use old passwords.
             ‚Äì Temporary passwords for new or reset User IDs should not be re-used
                (generate a new one for each situation) and should not be easy to guess
                (for example ‚Äúwelcome‚Äù).




160
Security ‚Äì Institutional Safeguards
162
                      Security ‚Äì Institutional Safeguards

Table of Contents


Key Points............................................................................................................165

Perimeter Security................................................................................................166
    What You Should Do.....................................................................................166
       Physical Perimeter Security
       Electronic Access Points
       Small Office Applicability

Malicious Software ..............................................................................................168
   What You Should Do.....................................................................................168
       Small Office Applicability

Wireless and Portable Devices.............................................................................169
   What You Should Do.....................................................................................169
       Small Office Applicability

Related Sections of the Act..................................................................................170

Checklists, Templates and Tools .........................................................................170
       Appendix A ‚Äì Perimeter Security
       Appendix B ‚Äì Malicious Software
       Appendix C ‚Äì Wireless and Portable Devices




                                                                                                                            163
Security ‚Äì Institutional Safeguards




164
               Security ‚Äì Institutional Safeguards


Key Points
To implement security effectively, you need a balanced approach that covers your
staff, your administrative processes and your technology. This section deals with
the steps needed to secure the institution. These steps have some implications for
general users but would largely be carried out by IT professionals:

‚Ä¢   Secure the physical and electronic (computer) points that provide access to
    personal health information.

‚Ä¢   Use appropriate measures to protect information technology from malicious
    software.

‚Ä¢   Implement an incident response plan to address any incidents that do occur.

‚Ä¢   Put policies and procedures in place to reduce the security risks associated
    with wireless and portable devices.

Documents you should create as a result of carrying out these steps include:

‚Ä¢   Malicious Software Policy (Appendix B)

‚Ä¢   Malicious Software Guide (Appendix B)

‚Ä¢   User Guidelines for Mobile Computing (Appendix C)

‚Ä¢   An Incident Response Plan

‚Ä¢   A Plan to Manage Computer or Digital Media Theft or Loss (Appendix A)




                                                                                     165
Security ‚Äì Institutional Safeguards



      Perimeter Security

      What You Should Do

      Secure the physical and electronic (computer) points that provide access to
      personal health information.


      Physical Perimeter Security

      ‚Ä¢   Lock printed files and removable storage media (CDs etc.) containing
          personal health information in secure places.

      ‚Ä¢   Use physical entry barriers to restrict access to personal health information to
          identified and authorized personnel. Examples include:

             ‚Äì active supervision by a security guard or receptionist, or requiring
                 visitors to sign-in, and

             ‚Äì passive control through an automated security badge system,
                 surveillance cameras or alarms.

      ‚Ä¢   When printed health records are used outside the controlled area:

             ‚Äì use a sign-out log to record who borrows the record,
             ‚Äì tell them how the record is secured, and
             ‚Äì check the sign-out log at the end of each day to ensure all records due
                 back have been returned.

      ‚Ä¢   Supervise third-party access (for example, cleaners, building security and
          landlords) to areas where personal health information is kept.

      ‚Ä¢   Monitor printers and fax machines for documents containing personal health
          information or keep these devices in rooms with restricted access.

      ‚Ä¢   Remove or encrypt any personal health information from computer equipment
          sent off-site for maintenance. Ensure those who service your equipment sign
          an agreement as well (see the Managing Contracts and Agents section).


166
               Security ‚Äì Institutional Safeguards

‚Ä¢   Use secure medical courier delivery services wherever possible for
    transporting personal health information.

‚Ä¢   Use sealed containers to transport personal health information between secure
    facilities. Consider using packaging that reveals when it has been tampered
    with and check that quantities sent and received match.

‚Ä¢   Secure centrally managed computing equipment and network cabling to
    prevent tampering or unauthorized connection of other devices.

Electronic Access Points

‚Ä¢   Use network security measures, such as firewalls, to stop unauthorized traffic
    from entering your computer network.

‚Ä¢   Segregate the network into different security zones to provide more protection
    for information that needs it. See Appendix A for details on Security Zones of
    Control.

‚Ä¢   Use secure remote access methods, such as Virtual Private Networks (VPNs),
    to provide authorized users with secure remote access to your institution‚Äôs
    network.

    Note: VPNs refers to technology that allows authorized users to send
          information over a public network, like the Internet, in a protected
          manner with safeguards equivalent to that of using the organization‚Äôs
          private network.

‚Ä¢   Do not tolerate unmanaged network access points (for example user-
    connected dial-up modems or wireless hubs).

‚Ä¢   Erase or destroy personal health information stored on computer equipment
    when selling or disposing of the equipment. (Just using the ‚Äúdelete‚Äù or
    ‚Äúformat‚Äù function is often not good enough; you may need to destructively
    erase the storage devices, such as using a magnetic eraser, so the information
    cannot be recovered).

Small Office Applicability

‚Ä¢   Keep physical files containing personal health information in an area that is
    locked and supervised when not locked.

‚Ä¢   Watch printers and fax machines when in use or keep them in a locked area.




                                                                                     167
Security ‚Äì Institutional Safeguards

      ‚Ä¢   Run up-to-date Personal Firewall software on all computers in your office that
          store personal health information.

      ‚Ä¢   Destroy or destructively erase the storage devices on your computers when
          selling or disposing of the computer (just using the ‚Äúdelete‚Äù or ‚Äúformat‚Äù
          function is often not good enough).

      ‚Ä¢   Make sure personal health information is not exposed when your equipment is
          serviced. Remove or encrypt any information that is not protected by a hard
          drive password when equipment goes off-site and have those servicing your
          equipment sign an agreement (see the Managing Contracts and Agents
          section).




      Malicious Software

      What You Should Do

      Use appropriate measures to protect information technology from malicious
      software.

      Implement an incident response plan to address any incidents that do occur.


      Malicious software is designed to damage, break, overtake or steal information
      from your computers. It includes viruses, worms, Trojan Horses, logic bombs and
      spyware. Personal computers are especially vulnerable since most malicious
      software targets them directly. Protecting personal computers is vital.

      ‚Ä¢   Implement a malicious software policy and provide simple rules for your staff
          to follow (See Appendix B for a sample policy and user guide).

      ‚Ä¢   Develop an incident response plan to combat any malicious software attacks.
          Appoint one staff member to:

             ‚Äì co-ordinate your response,
             ‚Äì assess the incident‚Äôs severity and scope,
             ‚Äì identify the best way to address the incident,
             ‚Äì tell staff what to do,

168
               Security ‚Äì Institutional Safeguards

       ‚Äì implement a business continuity plan to restore lost facilities,
           programs and data, and

       ‚Äì implement long-term solutions.
‚Ä¢   Track and implement any applicable updates software vendors develop to fix
    known security problems with their software.

‚Ä¢   Larger institutions should consider workstation management software or
    services that automatically send vendors‚Äô fixes to all users.

Small Office Applicability

‚Ä¢   Run up-to-date Anti-Virus, Firewall and Spyware software on all computers
    that store personal health information.

‚Ä¢   Use the associated update service to ensure your software can catch the latest
    known viruses.




Wireless and Portable Devices

What You Should Do

Put policies and procedures in place to reduce the security risks associated with
wireless and portable devices.


Portable devices include Laptop Computers, Personal Digital Assistants and Cell
Phones.

‚Ä¢   Do not use wireless devices (such as email or cell phones) to transmit personal
    health information unless the system is designed to support wireless devices
    securely (for example, with data encryption).

‚Ä¢   Do not use portable devices to store personal health information, except
    systems designed to support portable devices securely.

‚Ä¢   Instruct staff using portable devices outside of the secure work area to take
    precautions against theft and unauthorized access. Remind them that the
    ‚Äúreader over the shoulder‚Äù is a real risk.



                                                                                      169
Security ‚Äì Institutional Safeguards

      ‚Ä¢   If you do not have the skills on staff, get an IT security professional to help
          design and implement your wireless systems securely.

      Small Office Applicability

      ‚Ä¢   Assume wireless and portable devices are not secure unless you had a security
          professional set them up to be secure.

      ‚Ä¢   Do not use wireless and portable devices to store or transmit personal health
          information. Cell phones are not secure and you need to be careful when
          deciding what information to discuss on a cell phone (use a regular phone
          whenever possible).




      Related Sections of the Act

      2, 3, 4, 10, 12, 13, 14, 42, 53




      Checklists, Templates and Tools

      Appendix A ‚Äì ‚ÄúPerimeter Security‚Äù provides guidance on Security Zones of
      Control, LAN Management and Managing Computer Theft

      Appendix B ‚Äì ‚ÄúMalicious Software‚Äù provides a sample policy on malicious
      software and a sample guide for users

      Appendix C ‚Äì ‚ÄúWireless and Portable Devices‚Äù provides a sample user guide and
      sample technical standards




170
               Security ‚Äì Institutional Safeguards

                  APPENDIX A ‚Äì PERIMETER SECURITY

Network Design ‚Äì Security Zones of Control

The concept of security zones is most applicable to large institutions with
complex networks serving many different types of user including the public and
external suppliers. A security zone is an area where a defined set of security
policies and measures combine to achieve a specific level of security. In a large
networked environment it is usually better to define multiple zones that group
computing resources with similar security requirements and cluster levels of risk
together. Specific security mechanisms and policies outline the criteria that should
be met to transit from one zone to another. Separating the zones ensures that
security failures in less secure zones do not compromise more secure zones.
Large organizations typically use the following zones.

A Highly Secure Zone is used to store or process sensitive, private or confidential
information. Access is strictly limited to identified entities. Very stringent
security mechanisms protect the Highly Secure Zone. User credential information
(password files) and highly sensitive personal health information typically belong
in the Highly Secure Zone. Any information requiring the highest confidentiality,
integrity or availability should be considered for this zone.

An Internal Zone is used for storing and processing sensitive, private or
confidential information. Security policies are strictly enforced, but access need
not be as restricted as in the Highly Secure Zone. The Internal Zone includes the
network, applications and facilities implemented to support internal systems. It
also includes internal systems outsourced to external partners, but does not
include systems that act as access points-of-entry for those partners.

The Internal Zone is subdivided into Security Sub-Domains. In general, a sub-
domain is a logical boundary separating a collection of resources that must be
separated from the rest of the security zones for some special purpose, but are still
considered part of the larger zone. These sub-domains should not weaken the
larger zone and may require additional security controls to isolate them from the
common infrastructure. Note that Sub-Domains should be exceptions and limited
in number.

A Buffer Zone (Internal Controlled) houses applications and services provided to
give entities in the Public or External Business Zone access to less-sensitive
resources. Security controls are applied at the Buffer Zone‚Äôs entry points and
between the Buffer and Internal Zones. No direct connection between the
Public/External Business Zones and the Internal Zone is allowed (meaning all
connections to the Internal Zone should be via the Buffer Zone).



                                                                                        171
Security ‚Äì Institutional Safeguards

      External Business Zone (External Controlled) is where information and systems
      are shared with other institutions, stakeholders and external users. Equivalent
      security policies and standards are mutually acceptable and verifiable. The
      External Business Zone generally is another organization‚Äôs infrastructure that
      your organization connects to for specified business activities. For instance, a
      regional hospital might link to local hospitals in this way.

      In the Public Zone (External Uncontrolled), information and systems are open and
      uncontrolled. Security policies and standards cannot be enforced. A hospital‚Äôs
      public website might represent such a zone.

      Guidance For Managing Computer and Digital Media Theft and Loss

      Most of the following steps for managing theft or loss should be taken before an
      incident occurs. Some of the steps address your options following a theft.

      ‚Ä¢   Conduct a regular inventory of all of your IT equipment and assets and ensure
          sure you have the following information:

             ‚Äì Owner Name
             ‚Äì Description (brand, model, and features)
             ‚Äì Location
             ‚Äì Serial Number
      ‚Ä¢   Larger institutions should consider using asset management software to
          inventory software assets on workstations.

      ‚Ä¢   Map assets:

             ‚Äì that support critical business functions within the Business Continuity
                 Plan, and

             ‚Äì containing the critical information identified in the Information
                 Inventory and Classification section.

      ‚Ä¢   Put Acceptable Use and other personal security policies in place that require
          staff to take protective measures, such as:

             ‚Äì physically locking portable IT assets (including computers, PDA‚Äôs,
                 and portable media such as hard drives, CDs and USB memory keys)
                 and keeping them hidden from view, and


172
                Security ‚Äì Institutional Safeguards

        ‚Äì using encryption for sensitive data and employing hard drive
            passwords.

‚Ä¢   Take additional steps for very valuable assets and those containing very
    sensitive data:

        ‚Äì Install GPS tracking devices.
        ‚Äì Use storage devices that over-write data when tampered with.
‚Ä¢   Establish a process to manage stolen or missing IT assets.

        ‚Äì Designate a contact point for reporting stolen assets (such as a Help
            Desk).

        ‚Äì Appoint a coordinator to:
‚Ä¢   establish how critical the lost or stolen asset is,

        ‚Äì contact law enforcement or tracking agencies as appropriate, and
        ‚Äì tell those who may be affected of the asset‚Äôs loss (see Section on
            Managing a Privacy Breach).

When an incident occurs, take the following steps:

‚Ä¢   Report the incident to police and internal security (providing serial numbers).

‚Ä¢   Lock any computer accounts associated with the asset.

‚Ä¢   Notify managers of affected departments to stem any wider implications the
    loss may have.

‚Ä¢   Notify your legal department and communications staff if you think any
    information leak may affect the public or clients so they can decide whether to
    alert regulators and draft a press statement.

‚Ä¢   Review the incident (consider any motive involved and look at similar
    incidents).

‚Ä¢   Consider prosecution if an individual is apprehended or identified.

‚Ä¢   Review security practices to determine whether the incident could have been
    prevented.



                                                                                      173
Security ‚Äì Institutional Safeguards

      Sample LAN Management Guidelines for Large Installations:

      LAN stands for Local Area Network

      Modify the following guidelines to suit your needs and own local area network.

      LAN Management Physical Security

      ‚Ä¢   Treat LAN devices like computers or servers and follow same physical and
          logical security rules for servers.

             ‚Äì LAN devices should be house in a secure enclosure accessible to only
                 authorized personnel.

             ‚Äì Grant access to a LAN device following the same rules and restrictions
                 for physical access to a server.

      ‚Ä¢   Do not expose LAN cables or leave them open to unauthorized access, except
          at their terminal end when connecting to the user workstation.

             ‚Äì Wiring closets and patch panels should be locked and accessible to
                 only authorized personnel.

             ‚Äì Cable runs should not be accessible from public-use areas. If cable
                 runs are accessible, consider using positive-pressure sealed conduits.

      ‚Ä¢   LAN connections should be available only inside internal areas with access
          restricted to authorized employees.

             ‚Äì If LAN connections are needed in semi-public areas (like meeting
                 rooms used for visitors or visitors‚Äô waiting areas) enable Media Access
                 Control (MAC) address filtering on those ports to accept only
                 connections from known machines. If policy-based networking has
                 been implemented, stronger protections may be deployed.

             ‚Äì If visitors need a LAN connection, it should be routed to a public
                 network zone in the network (Internet) and not to the internal LAN.

             ‚Äì Verify and audit physical port counts regularly.
      LAN Management Logical Security

      ‚Ä¢   Segment the LAN based on application and data usage. For example, Human
          Resources data should reside on servers on a different network than patient
          data.


174
               Security ‚Äì Institutional Safeguards

‚Ä¢   Intelligent LAN devices (for instance, large switches and routers) must be
    protected by passwords the same way and following the same rules as a
    server. Do not use default or generic passwords.

‚Ä¢   Consider using an out-of-band management network to manage networked
    devices. If direct console access is necessary, use a terminal server with
    strong authentication options.

‚Ä¢   Upgrade LAN devices using control software that can be maintained with
    patches as required.

       ‚Äì Treat these LAN devices the same way as a server and apply the same
           procedures for maintenance and security patches.

       ‚Äì Maintain strict controls on who may change these devices and keep
           backup copies of configurations in a separate backup location.

‚Ä¢   Do not enable remote access to network devices.

       ‚Äì If a valid administrative reason requires remote access, treat the access
           the same way as a server remote access, protecting it in the same way
           (using digital credentials, smartcard, or similar authentication
           mechanisms).

‚Ä¢   Only authorized computers or servers should connect to the LAN.

       ‚Äì At a minimum, if a computer or server not controlled by [Insert Your
           Organization‚Äôs Name] must be connected to the LAN, it should be
           inspected and verified by [Insert Your Organization‚Äôs Name]
           personnel to ensure no malicious elements (viruses, Trojan horses,
           etc.) are hidden in it.

       ‚Äì For stronger protections, consider policy-based networking, which
           typically combines authenticating before end-users gain access to the
           network, enforcing security policies and standards, and verifying that
           the end-client is patched and up to date.

‚Ä¢   No ‚Äúshares‚Äù or folders should be visible or available in the LAN without a
    proper login using a password that follows [Insert Your Organization‚Äôs
    Name] password rules.

       ‚Äì LAN passwords should follow the same rules as other critical
           passwords and have similar expiry times.




                                                                                     175
Security ‚Äì Institutional Safeguards

      ‚Ä¢   The protocols used in the LAN environment must be up-to-date and comply
          with the security policy. Avoid protocols that are obsolete or have known
          vulnerabilities. (For example, wherever possible, do not use basic SNMP,
          using common public and private community strings for access. SNMP
          versions 2 and 3 offer much more suitable authentication and encryption
          options).

      ‚Ä¢   Dual homing (connecting a computer to two different networks at the same
          time) is not allowed, unless [Insert Your Organization‚Äôs Name] controls
          both networks and the networks have identical security controls.

             ‚Äì Dual homing exists when a single computer is connected to the LAN
                 and another network, such as a wireless one, another LAN, or a dial-
                 up.

      ‚Ä¢   Control all outside access into the LAN from outside and deny access to
          unauthorized users.

             ‚Äì Protect all connections to other LANs and the Internet with Firewalls.
             ‚Äì Deploy Intrusion Detection Systems (IDS) in Internet connection
                 points to detect unauthorized attempts to access the LAN.

             ‚Äì Enable logging-in LAN devices whenever supported.
             ‚Äì Archive access logs in a protected server and review them regularly.
                 Review can be automated or manual, but should be at regular intervals.
                 Reports should be generated if any event flags intrusion attempts.

      ‚Ä¢   Disable all unused network ports.

      ‚Ä¢   Disable all unnecessary protocols.

      ‚Ä¢   Use anti-virus software, bandwidth throttling and other Quality-of-Service
          (QoS) mechanisms to help prevent and impede network-based malicious code
          from spreading.

      Sample LAN Management Guidelines for Small Installations

      Modify the following guidelines to suit your environment and your requirements.

      LAN Management Physical Security

      ‚Ä¢   Locate LAN hubs or switches out of the general public‚Äôs reach.



176
                Security ‚Äì Institutional Safeguards

‚Ä¢   LAN access ports should not be accessible to the general public.

‚Ä¢   Allow only authorized computers to connect to the LAN.

LAN Management Logical Security

‚Ä¢   No critical ‚Äúshares‚Äù or folders should be visible or available in the LAN
    without proper password protection.

‚Ä¢   If the LAN is used to access the Internet, use a router with built-in firewall as
    the main access point.

‚Ä¢   No computer should be connected to the LAN and another network (such as a
    wireless one or a dial-up connection) at the same time.

‚Ä¢   Do not use default or generic passwords on networking equipment.




                                                                                        177
Security ‚Äì Institutional Safeguards

                        APPENDIX B ‚Äì MALICIOUS SOFTWARE

      Sample Policy for Protecting Against Malicious Software

      Modify the following sample policy to suit your environment and your
      requirements.

      ‚Ä¢   To protect the integrity of software and information, implement precautions to
          prevent, detect, and cleanse the introduction of malicious software.

      ‚Ä¢   The following controls shall apply to all workstations (business or personal)
          used for [Insert Your Organization‚Äôs Name] business:

             ‚Äì A formal policy requiring compliance with software licenses and
                 prohibiting the use of unauthorized software.

             ‚Äì   [Insert Your Organization‚Äôs Name] supplied malicious software
                 detection and repair software must on any workstation used for [Insert
                 Your Organization‚Äôs Name] business or connected to the [Insert
                 Your Organization‚Äôs Name] network.

             ‚Äì Signature files routinely updated to within the two most recent
                 versions.

             ‚Äì Anti-virus software activated at boot-up time.
             ‚Äì If supported by anti-virus software, automatic scanning is enabled.
             ‚Äì Scanning any floppy or CD-ROM used for the first time.
             ‚Äì Scanning any files on electronic media of uncertain or unauthorized
                 origin, or files received over un-trusted networks, for viruses before
                 use.

             ‚Äì Scanning any electronic mail attachments and downloads for malicious
                 software before use. This check may be carried out at different places,
                 for example, at electronic mail servers, desk top computers or when
                 entering the [Insert Your Organization‚Äôs Name]‚Äôs network.

             ‚Äì Employees may download data from known and trusted suppliers after
                 following the virus-checking process and using the approved virus
                 detection package.




178
               Security ‚Äì Institutional Safeguards

       ‚Äì Staff are trained for security awareness, covering the virus protection
           on systems, training in their use, incident reporting procedures, and
           recovering from virus attacks

       ‚Äì Appropriate business continuity plans for recovering from virus
           attacks, including all necessary data and software back-up and
           recovery arrangements.

       ‚Äì Users are instructed not to intentionally write, generate, compile, copy,
           propagate, execute or attempt to introduce any computer code
           designed to self-replicate, damage or otherwise hinder the performance
           of any computer‚Äôs memory, file system or software.

       ‚Äì Computer users are instructed to not knowingly write or run any
           computer program/process that would consume more computer
           resources than necessary for performing [Insert Your Organization‚Äôs
           Name] work.

       ‚Äì Users are aware of the dangers of browser executable code and active
           scripts

       ‚Äì Browsers‚Äô ability to run ActiveX, JavaScript, Java and other forms of
           active scripts of code are disabled, where feasible.

       ‚Äì Users know to allow executable code and active scripts to be run from
           only trusted sites.

Sample Malicious Software Guide for Users

Modify the following sample to suit your environment and your requirements.

‚Ä¢   Do not remove or alter the anti-virus software installed on your computer and
    do not shut down its regular scanning activity. Contact the Help Desk if you
    suspect it is not running correctly.

‚Ä¢   If you suspect your computer has a virus, contact the Help Desk immediately,
    inform you manager and do not use your computer again until you get further
    instructions from the Help Desk

‚Ä¢   Do not respond to any instructions for dealing with viruses that are not from
    the Help Desk or not posted on our intranet site.




                                                                                     179
Security ‚Äì Institutional Safeguards

      ‚Ä¢   Do not send colleagues any instructions for dealing with viruses you receive.
          The instructions may not be appropriate for them. Our virus response team
          will handle all communications.

      ‚Ä¢   Beware of unexpected e-mail and e-mail from someone you do not know.
          Attachments may contain a virus.

      ‚Ä¢   Never use web-based e-mail at the office or download software from the
          Internet. These files are not scanned for viruses.

      ‚Ä¢   Always scan for viruses files downloaded from the Internet, e-mail
          attachments, CDs, DVDs, and diskettes.

      ‚Ä¢   Always scan new software for viruses before installing it on your computer.

      ‚Ä¢   As a precaution against a virus attack, make sure your critical files are backed
          up and retrievable.




180
              Security ‚Äì Institutional Safeguards

         APPENDIX C ‚Äì WIRELESS AND PORTABLE DEVICES

Sample User Guidelines for Mobile Computing

Modify the following guidelines to suit your environment and requirements.

‚Ä¢   Laptop Computer and Travel Security

       ‚Äì Always use a cable lock to secure your laptop to your workstation
       ‚Äì Keep your laptop in your possession as far as possible when travelling.
       ‚Äì When traveling by air, do not check laptops in as baggage, and be alert
          to the possibility of theft when going through airport security
          checkpoints.

       ‚Äì Never leave your laptop for an extended time in an unoccupied
          vehicle. If you must leave your laptop in an unoccupied vehicle,
          ensure it is not visible and secure the laptop to the body of the vehicle
          inside the trunk with a locking cable.

       ‚Äì Avoid leaving your laptop in a hotel room. If you must, lock it in the
          hotel safe or use a locking cable to secure it out of sight in your room.

       ‚Äì If you are traveling with [Insert Your organization‚Äôs Name]
          sensitive or confidential material on portable media such as paper,
          diskettes, notebooks, or other devices, protect this media following the
          same guidelines listed above for protecting your laptop.

       ‚Äì Do not work in open public areas where others may view user IDs,
          passwords, or [Insert Your organization‚Äôs Name] data. (Be
          especially cautious on airplanes).

       ‚Äì Never discuss [Insert Your organization‚Äôs Name] confidential
          information in public areas where you may be overheard, including in
          conversations on telephones or cell phones.

       ‚Äì Report the loss of any mobile device containing [Insert Your
          organization‚Äôs Name] information to the Help Desk immediately.




                                                                                      181
Security ‚Äì Institutional Safeguards

      ‚Ä¢   Security of Handheld Devices

             ‚Äì Hand-held devices (such as Personal Digital Assistants, RIM
                BlackBerry, and mobile phones with data access) storing or accessing
                [Insert Your organization‚Äôs Name] confidential or business-sensitive
                data require physical and electronic access controls. The following
                actions are required:

             ‚Äì Keep handheld devices in your possession whenever possible.
             ‚Äì Activate data encryption, power-on password and password-controlled
                time-out/lock-out features on all hand-held devices supporting these
                security features.

             ‚Äì Remote synchronization via modem to move the [Insert Your
                organization‚Äôs Name] data between the device and your workstation
                must go through a [Insert Your organization‚Äôs Name] authorized
                remote access gateway.

      ‚Ä¢   Wireless/Remote Network Access

             ‚Äì Use only [Insert Your organization‚Äôs Name]‚Äôs approved remote
                network access facilities to gain access to the network from outside the
                office.

             ‚Äì Ensure that you are not connected to any other network (for example
                via dial-up modem) at the same time you are connected to the [Insert
                Your organization‚Äôs Name] network.

             ‚Äì Never use publicly available wireless networks (often referred to as
                ‚Äúhotspots‚Äù), unless you (a) have adequate perimeter and antivirus
                controls and (b) are not transmitting sensitive [Insert Your
                organization‚Äôs Name] information.

             ‚Äì Only [Insert Your organization‚Äôs Name] approved and installed
                Wireless technology is permitted on any [Insert Your organization‚Äôs
                Name] machine or within [Insert Your organization‚Äôs Name]‚Äôs
                environment. Installation of personal wireless technology into the
                [Insert Your organization‚Äôs Name] environment is considered to
                be a serious breach of [Insert Your organization‚Äôs Name] security
                policy.




182
               Security ‚Äì Institutional Safeguards

Sample Technical Standards for Wireless Connections

Modify the following sample to suit your environment and requirements.

‚Ä¢   Cellular-based wireless is assumed to be public and untrustworthy. [Insert
    Your organization‚Äôs Name]‚Äôs secure remote access mechanisms must be
    used to access [Insert Your organization‚Äôs Name] resources over cellular-
    based networks.

‚Ä¢   The following standards must be observed for all wireless LANs:

       ‚Äì Unless explicitly for public-access use only, open wireless LAN
           networks must not be deployed. If public-access wireless is deployed,
           it must be adequately isolated from the other parts of the network.
       ‚Äì Simultaneous wireless and wired connections on the same machine are
           prohibited.
       ‚Äì Wireless Access Points must be isolated by firewalls and application
           proxies.
       ‚Äì Sensitive applications and data should not be available via wireless
           connections unless additional layers of security have been added at
           OSI Layer 2 (WPA, EAP, LEAP, PEAP) and OSI Layer 3 (IPSEC
           VPN Tunnel).
       ‚Äì Mitigate basic potential exposures created by the passive interception
           of wireless network traffic. Some exposures can be mitigated through
           good wireless network management, including:
           ‚Ä¢   MAC address filtering must be used to prevent unauthorized
               clients from connecting through wireless LAN access points.
               While these addresses can be imitated (spoofed), this does protect
               against casual attacks and unintentional client associations.

           ‚Ä¢   Service Set Identifier (SSID) broadcast must be disabled or the
               beacon interval must be increased to the maximum interval. As
               these broadcasts can be intercepted, naming conventions (for
               SSID, status fields, for instance) must not reveal any information (
               such as department or company name). Default (vendor-supplied)
               SSIDs are not permitted.

           ‚Ä¢   Connections from ‚Äúnull‚Äù or ‚Äúany‚Äù SSIDs must be denied.

           ‚Ä¢   Default administrative passwords must be changed to strong
               passwords. This should include SNMP community strings
               accessible from the wired side of any Access Point.


                                                                                      183
Security ‚Äì Institutional Safeguards

                 ‚Ä¢   All unnecessary protocols on the Wireless Access Point must be
                     disabled (including ad hoc networking capability). Adhoc
                     networking (peer-to-peer) must also be disabled on all [Insert
                     Your Organization‚Äôs Name] wireless LAN clients.

                 ‚Ä¢   Management of Wireless Access Points from any wireless
                     interface must be denied. Access Points may only be managed
                     from wired terminals.

                 ‚Ä¢   IP and MAC address filtering must be used to manage Wireless
                     Access Points.

                 ‚Ä¢   SNMP traps must be set on Wireless Access Point resets or
                     configuration reloads.

      ‚Ä¢   802.11i Wireless LANs must be deployed where possible to provide the
          highest level of confidentiality and integrity protections currently available. It
          combines:

             ‚Äì strong authentication and/or encrypted authentication channels for
               single-factor encryption, and
             ‚Äì robust cryptographic services based on the Advanced Encryption
               Standard (AES).

      ‚Ä¢   If 802.11i is not supported, WiFi Protected Access (WPA) may be used as it
          provides:

             ‚Äì the same cryptographic services as standard wireless LANs but with
               changing keys to help prevent compromise, and
             ‚Äì strong authentication and/or encrypted authentication channels for
               single-factor encryption.

      ‚Ä¢   Where 802.11i and WPA are not supported, an exception to this standard must
          be obtained. At a minimum, these exceptions must:

             ‚Äì support strong user-based authentication,
             ‚Äì enable 128-bit (or higher) WEP encryption, and
             ‚Äì change default WEP keys to a random value and rotate bi-weekly.
      ‚Ä¢   WEP-based protections must be used for home use. At a minimum, 802.11i
          and WPA Home Edition are strongly recommended (versions not requiring
          strong authentication are available).




184
Sustaining Security
                                                                             Sustaining Security

Table of Contents


Key Points............................................................................................................189

Business Continuity .............................................................................................190
   What You Should Do.....................................................................................190
       Small Office Applicability

Development and Maintenance............................................................................192
   What You Need To Do ..................................................................................192
      Small Office Applicability

Audit ....................................................................................................................193
   What You Need To Do ..................................................................................193
        Small Office Applicability

Recommended Standards.....................................................................................195
   What You Need To Know .............................................................................195

Related Sections of the Act..................................................................................197

Checklists, Templates and Tools .........................................................................197
       Appendix A ‚Äì Business Continuity
       Appendix B ‚Äì Development And Maintenance
       Appendix C ‚Äì Audit




                                                                                                                                187
Sustaining Security




188
                                                     Sustaining Security


Key Points
To implement security effectively, you need a balanced approach that covers your
staff, your administrative processes and your technology. Security is also an
ongoing program and not a one-time project. This section deals with the steps
needed to sustain security:

‚Ä¢   Ensure critical functions can operate and critical data is protected if local
    disasters or major technology failures cripple your systems.

‚Ä¢   Institute policies to ensure security is considered when developing, buying
    and maintaining IT resources.

‚Ä¢   Regularly audit your actual practices for compliance with your security
    policy.

Documents you should create as a result of carrying out these steps include:

‚Ä¢   Data Backup, Disaster Recovery and Business Recovery Plans (Appendix A)

‚Ä¢   Guidelines for Secure Applications (Appendix B)

‚Ä¢   Any relevant Threat Risk Assessments (Appendix B)

‚Ä¢   Security Review, Spot Check and Audit Results

‚Ä¢   A Strategy for Capturing and Using Audit Information (Appendix C)




                                                                                    189
Sustaining Security



      Business Continuity

      What You Should Do

      Ensure critical functions can operate and critical data is protected if local disasters
      or major technology failures cripple your systems.


      ‚Ä¢   Analyze the data and critical systems you need to maintain your operations if
          the technology fails.

             ‚Äì Identify key personnel for each critical process, such as health records
                 transfer, who can best identify the systems, staff and data needed to
                 run the process and give specific advice.

      ‚Ä¢   Build backups to support infrastructure, such as:

             ‚Äì computing facilities (for example third-party hot-site backup and
                 mutual arrangements with other institutions), and

             ‚Äì power (for example emergency generators, UPS systems, and dual-
                 power feeds).

      ‚Ä¢   Develop a Data Backup plan for critical and highly critical data that includes
          schedules for backups to be sent to a secure off-site storage facility.

      ‚Ä¢   Secure information being transferred to the off-site facility.

      ‚Ä¢   Ensure the data is secure while in off-site storage.

      ‚Ä¢   Develop a Disaster Recovery Plan describing how basic operations (for
          example, health, office, computing, power, and communications) will run and
          how data will be restored from backup. Ensure that key staff securely store
          copies of the plan at home or off site.

      ‚Ä¢   Develop a Business Recovery Plan describing how to restore critical business
          processes after the Disaster Recovery Plan.

      ‚Ä¢   Include specific recommendations from key personnel. Since you may not
          have computers available, develop temporary manual procedures. Designate
          deputies for key staff who may be unavailable.


190
                                                    Sustaining Security

‚Ä¢   In your plans, identify which activities depend on others and the order in
    which they should be performed.

‚Ä¢   Inventory your data (as suggested in the Inventory and Data Classification
    section) and measure how critical each set is based on the risk analysis.
    Critical data covers not only personal health information, such as patient
    records, but also the procedures that protect your institution.

For example:


      Not Critical: losing             Critical: losing data will
      data will be a mere              cause severe disruption
      inconvenience with no            of operations, potential
      significant impact               losses and legal liability




                        ‚Ä¢ CRITICALITY


                    Mildly Critical: losing                Highly critical: losing
                    data will inconvenience                data will mean
                    the institution, but will              institutional failure, risk
                    not cause significant                  of injury or loss of life,
                    loss or risk liability                 or criminal liability



‚Ä¢   Test your plans once or twice a year to make sure they still work and that
    backup data is useable.

‚Ä¢   The parts of your plans that cannot be tested in practice should be ‚Äúpaper‚Äù
    tested, with all staff involved walking through the plan together to identify
    potential problems.

Small Office Applicability

‚Ä¢   Regularly back up critical personal health information on your computers to
    removable media (for example CD) at least monthly.

‚Ä¢   Store these back-up copies in a secure off-site location. Secure the copies in
    transfer.


                                                                                         191
Sustaining Security

      ‚Ä¢   Periodically test back-up copies before storing them to ensure they are
          useable. You do not want to find useless copies when you need the back-up.




      Development and Maintenance

      What You Need To Do

      Institute policies to ensure security is considered when developing, buying and
      maintaining IT resources.


      ‚Ä¢   Spell out the security requirements that IT systems and software should meet
          to support your security policy. Specify the audit information that you need to
          check that specified requirements are met.

      ‚Ä¢   We recommend that you follow the HL7 (Health Level 7) standard for
          application interconnection in any application managing health records (see
          the Security Standards section).

      ‚Ä¢   Perform a Threat Risk Assessment (TRA) and a Privacy Impact Assessment
          (PIA) for any major change to systems or software to identify and address any
          security and privacy concerns (see Appendix B for a sample TRA form).
          Include appropriate security questions in your normal application
          development and IT purchasing processes.

      ‚Ä¢   Ensure that all built-in hardware and software security features in the IT
          products you buy are used properly (including changing administrator
          passwords from their default values).

      ‚Ä¢   Separate any development, test and production environments. Strictly control
          the transfer of code between these environments with a rigorous change
          control process to avoid introducing new security risks.

      ‚Ä¢   Do not use personal health information from your system to test changes in
          your test and development systems. Strip identifying details from information
          used outside your environment.

      ‚Ä¢   Regularly monitor software updates and implement code updates designed to
          address security vulnerabilities.




192
                                                       Sustaining Security

Small Office Applicability

‚Ä¢   Buy software from only reputable vendors. Professional journals and
    organizations like the Ontario Medical Association may suggest software for
    practices like yours.

‚Ä¢   Promptly install any updates vendors offer to close security vulnerabilities.




Audit

What You Need To Do

Regularly audit your actual practices for compliance with your security policy.


We interpret audit broadly in this section to mean any review of current practices
against policy or standards. This includes spot checks to make sure staff with
access to personal health information are protecting it appropriately.

‚Ä¢   If you have never audited security, conduct a ‚Äúgap analysis‚Äù review.

       ‚Äì A ‚Äúgap analysis‚Äù review compares a target of acceptable security
           practices with current practices, revealing a list of gaps between the
           two.

       ‚Äì The list of gaps identifies issues needing immediate attention and
           constitutes a baseline for future audits.

‚Ä¢   The Diagnostic Tool in this Toolkit can be used for a preliminary assessment
    of security gaps. However, we suggest hiring outside experts to conduct the
    initial audit if your staff lacks the skills. You can always require the experts
    to train staff if you wish to perform future audits yourself. Do periodic
    external audits to make sure you keep pace with technology, standards and
    what similar institutions are doing.

‚Ä¢   Conduct security audits at least annually. If internal staff will conduct the
    audit, choose individuals:

       ‚Äì without day-to-day responsibility for the practices, and



                                                                                       193
Sustaining Security

             ‚Äì with appropriate skills and current knowledge of security issues,
                 practices and threats.

      ‚Ä¢   A Security Audit should focus on how well actual security practices compare
          with the institution‚Äôs security policy, standards and procedures.

      ‚Ä¢   Use random spot checks to help ensure security policies are being followed
          and determine whether users are following Acceptable Use Policies, such as
          locking up sensitive information and locking computers.

      ‚Ä¢   Audit the performance of any third parties handling personal health
          information on your behalf to make sure they meet the security requirements
          your agreement specifies.

      ‚Ä¢   Perform Threat Risk Assessments (TRA) and Privacy Impact Assessments
          (PIA) whenever you make major changes to systems or software to make sure
          you have addressed any security and privacy concerns (a sample TRA form is
          included with this Toolkit). Follow up and resolve any problems.

      ‚Ä¢   Your audit results and action plans to correct security risks should be:

             ‚Äì regularly reviewed within the institution‚Äôs management system (for
                 example monthly senior management operations review), and

             ‚Äì built into individual managers‚Äô performance measures and evaluation
                 systems.

      ‚Ä¢   Make sure you consider problems that will take time to resolve within the
          institution‚Äôs risk management process and get the appropriate senior manager
          to sign off on the risks.

      ‚Ä¢   Design your systems to give you all the information you will need for audits.
          (Appendix C contains suggestions.)

      ‚Ä¢   Perform both external and internal security penetration tests (sometimes called
          ethical hacking) at least annually. (You may need to hire outside professionals
          to provide the necessary expertise and objectivity for the tests).

      ‚Ä¢   Evaluate your security audit program‚Äôs effectiveness by reviewing:

             ‚Äì actual security incidents and determining whether your audit program
                 should have caught them, and

             ‚Äì staffing levels compared with those at similar institutions having
                 effective security programs to ensure your resources are in line.


194
                                                        Sustaining Security

  Appendix C ‚Äì ‚ÄúAudit‚Äù provides more detailed guidance on capturing and using
  audit information.

  Small Office Applicability

  ‚Ä¢    Perform spot checks on a regular basis to ensure all staff are following the
       security rules and take appropriate action.




  Recommended Standards

  What You Need To Know
  The emerging international cross-industry standard for security is ISO/IEC 17799.
  Details of the standard can be found at www.iso.org. The 10 core areas of the
  framework are summarized below. The topics covered in the above security
  sections fit into this framework.

      Policy       Organization          Asset            Personnel          Physical
                                     Classification        Security          Security
                                      and Control



Security rules    Management                             Rules to
the               framework                              reduce the       Measures to
organization      supporting the     System to           risk of          prevent
follows           policy and         inventory and       individuals      unauthorized
                  ensuring           classify assets     compromising     access and
                  compliance         so they may be      security         damage to
                                     more effectively                     systems
                                     managed and
                                     protected




                                                                                         195
Sustaining Security

       Computer            System            System            Business         Compliance
      and Network          Access          Development         Continuity
      Management           Control             and
                                           Maintenance


                                                                               Program to
                                                                               ensure that
      Disciplines to   Mechanisms to                          Plan to          security
      manage IT        ensure that                            ensure that      measures
      resources to     only authorized    Disciplines to      critical         meet policy
      support          personnel          ensure the IT       business         and legal and
      security         have access        environment         activities can   contractual
      objectives       and for only       sustains the        be performed     security
                       authorized         required security   if technology    requirements
                       purposes           levels              fails or local
                                                              disasters
                                                              happen



        Other recommended standards and guidelines include:

        ‚Ä¢   ITIL (IT Infrastructure Library). This widely accepted approach to IT
            Service Management provides comprehensive guidance on best practices for
            the processes required to support IT, including those critical to security such
            as Change Management. More details can be found at
            http://www.ogc.gov.uk/index.asp?id=2261

        ‚Ä¢   COBIT (Control Objectives for Information Technology). This widely
            accepted standard for good Information Technology (IT) security and control
            practices provides a reference framework for management, users, and IS audit,
            control and security practitioners. More details can be found at
            http://www.isaca.org/

        ‚Ä¢   COACH (Canada‚Äôs Health Informatics Association). The COACH-
            published ‚ÄúGuidelines for the Protection of Health Information,‚Äù which
            supplements this Toolkit, provides more detail on how to implement security
            in a healthcare setting. More details can be found at
            http://www.coachorg.com/

        ‚Ä¢   RCMP Guidelines for Threat Risk Analysis (TRA). Details on this tool for
            identifying and mitigating security risks can be found at http://www.rcmp-
            grc.gc.ca (Also see Appendix B for a sample TRA form.)




196
                                                   Sustaining Security

‚Ä¢   HL7 (Health Level 7). We recommend any applications managing health
    records meet the HL7 (Health Level 7) standard for application
    interconnection. Based on the ISO OSI (Open Systems Interconnection)
    Level 7 application interconnection standard, this standard is gaining wide
    acceptance and includes security checks. More details can be found at
    http://www.hl7.org.




Related Sections of the Act

2, 3, 4, 10, 12, 13, 14, 42, 53




Checklists, Templates and Tools

Appendix A ‚Äì ‚ÄúBusiness Continuity‚Äù provides a sample Business Recovery and
Disaster Recovery Management Guide

Appendix B ‚Äì ‚ÄúDevelopment and Maintenance‚Äù provides more detailed guidance
on Secure Applications and a sample TRA form

Appendix C ‚Äì ‚ÄúAudit‚Äù provides more detailed guidance on capturing and using
audit information




                                                                                  197
Sustaining Security

                        APPENDIX A ‚Äì BUSINESS CONTINUITY

      Business Recovery and Disaster Recovery Management Guide

      1. Establish a Business Recovery and Disaster Recovery Management Process.
      A Business Recovery and Disaster Recovery management process must be
      implemented across the organization to reduce an interruption‚Äôs effects to a level
      acceptable by service unit management and regulatory authorities.

      ‚Ä¢   Assign a staff member responsibility for Business Recovery planning
          (Business) for the institution. Also appoint individuals within each service
          unit performing critical functions and sub-functions.

      ‚Ä¢   Assign a staff member responsibility for Disaster Recovery planning (IT).

      ‚Ä¢   Identify service units critical to the organization, based on criteria
          management agrees to.

      ‚Ä¢   Define a Business Recovery group with members from each service unit to
          ensure that adequate recovery plans are instituted and kept current as the
          business changes.

      ‚Ä¢   Make the Business Recovery group responsible for ensuring that staff from
          each service unit are assigned and trained on Business Recovery and Disaster
          Recovery processes and procedures.

      ‚Ä¢   Involve assigned staff in testing the recovery plan (at least annually)

      ‚Ä¢   Review the institution‚Äôs insurance coverage annually for opportunities to use
          coverage as an alternative way to manage the risk of an interruption or the
          organization‚Äôs response to an interruption

      2. Perform a Business Recovery and Disaster Recovery Impact Analysis.
      Business Recovery and Disaster Recovery planning must be based on the results
      of a Recovery Requirements Analysis that assesses the impacts of interruptions to
      critical service. Determine the resources on which those critical functions rely and
      the maximum period of interruption each critical function can withstand.

      ‚Ä¢   All service units must participate in the Recovery Requirements Analysis.

      ‚Ä¢   The threshold for a critical service unit is based on an analysis of the impact
          of interruptions to its services and the maximum time those services can
          withstand interruption.




198
                                                    Sustaining Security

‚Ä¢   Develop strategies to determine the overall approach to Business Recovery
    and Disaster Recovery and have management endorse them.

‚Ä¢   Establish a budget for contingency planning.

3. Develop and Maintain the Business Recovery and Disaster Recovery Plans.
Business Recovery and Disaster Recovery Plans will be documented for each
service unit identified in the Recovery Requirements Analysis. The plans should:

‚Ä¢   include the requirements for alternate sites, hardware, software and service
    unit specific items,

‚Ä¢   make saving human life top priority,

‚Ä¢   be supported by documented Crisis Management and Emergency Response
    procedures and responsibilities,

‚Ä¢   include detailed procedures on the tasks the management team must perform
    when declaring a disaster,

‚Ä¢   include detailed procedures on what each service unit must do when a disaster
    is declared,

‚Ä¢   specify authority for activating recovery strategies, such as an alternative
    location (hot-site) and assign authorities for other specific functions,

‚Ä¢   identify the budget that will be made available when declaring a disaster and
    how the expenditures will be managed during the interruption,

‚Ä¢   identify how staff will be informed of the disaster and given instructions on
    what they should do,

‚Ä¢   include the types and frequency of testing required, and

‚Ä¢   include maintenance to ensure that currency with business processes and
    priorities and resource changes.

4. Test the plans. Regularly test Business Recovery and Disaster Recovery Plans
to ensure they are up to date and effective.

‚Ä¢   Test the plans at least annually or when major changes occur in the
    organization, priorities or infrastructure

‚Ä¢   Testing should range from the following:

       ‚Äì Table-top dry runs.

                                                                                    199
Sustaining Security

             ‚Äì Simulation testing.
             ‚Äì Parallel system running.
             ‚Äì Partial cut-over testing.
             ‚Äì Full contingency testing.
      ‚Ä¢   Testing may be either planned or unplanned.

      ‚Ä¢   Testing should occur with different staff to ensure cross-training.

      5. Maintain and re-assess the plans. Business Recovery and Disaster Recovery
      plans must be kept current with the business. Use a change management and
      control process to make sure the plans reflect the organization‚Äôs current needs.
      The following changes could affect the plans:

      ‚Ä¢   People with responsibility within specific plans changing functions within the
          organization

      ‚Ä¢   New business strategies

      ‚Ä¢   New location, facilities or resources

      ‚Ä¢   Amendments to applicable legislation

      ‚Ä¢   New key vendors, personnel, supplies, management or customers

      ‚Ä¢   New service unit processes or priorities

      ‚Ä¢   A service unit providing different services (which could alter whether the unit
          is designated as a critical service unit).




200
                                                    Sustaining Security

          APPENDIX B ‚Äì DEVELOPMENT AND MAINTENANCE

General Guiding Principles for Secure Applications

These principles are written for application designers but can also be used to
evaluate an application that is being purchased.

‚Ä¢   Avoid Security Through Obscurity. Assume hackers will inspect source
    code and design applications accordingly. Secrets, such as hidden fields and
    path names, may slow an attacker down, but secrets do not stay secret forever.
    Application security based on ‚Äúsecurity by obscurity‚Äù is doomed to fail.

‚Ä¢   Principle of Least Privilege. Do not require the application to run on the
    administrator account. Use coding discipline to determine what privileges
    actually needed and explicitly grant only the privileges to the non-
    administrator account on which the application runs.

‚Ä¢   Principle of Least Trust. Do not trust input external users provide. Assume
    that external systems are insecure. Vet data and inspect every return code
    from every function call, including system library routines.

‚Ä¢   Separate Services. Dedicate a single service to a single platform. Though it
    may cost less to run multiple services on the same platform, doing so
    increases the system‚Äôs overall complexity and increases security
    vulnerabilities.

‚Ä¢   Principle of Simplicity. Ensure that security subsystems are manageable and
    not overly complicated for users and administrators. Large interfaces and
    complex solutions run a higher risk security vulnerabilities than small
    interfaces and simple code. The more entrance points available to an
    application and the more intricate the application‚Äôs functionality, the higher
    the potential for bugs, some of which will be security-related. Security
    functionality added to an application must be easy to install, configure, and
    use. Otherwise, it will be disabled or bypassed.

‚Ä¢   Avoid a Single Point of Failure. Applications should never be designed so
    that a single mechanism failure (such as a firewall or authentication service
    Failure) jeopardizes the entire application. Also called Defence in Depth, this
    principle recommends a design where a second mechanism that must be
    defeated before one mechanism‚Äôs failure can compromise the application, a
    third must be defeated if a second fails, and so on. A Buffer Zone is an
    example of more than single-point failure. If the outer firewall fails, though
    the web server may be vulnerable and compromised, the rest of the
    application and data remains safe behind a second firewall.


                                                                                      201
Sustaining Security

      ‚Ä¢   Secure Defaults. Do not enable services by default unless absolutely
          necessary. Services should be disabled by default and enabled only by an
          explicit decision. The default settings should be the secure mode of operation.
          Security should not have to be turned on; it should always be present unless
          explicitly disabled.

      ‚Ä¢   Fail in a secure mode. Ensure that applications, systems, and subsystems fail
          in a secure manner. This is called failing closed (as opposed to failing open).
          Write code to explicitly verify what is allowed before allowing it. Do not
          attempt to check for the cases where things are disallowed; an event may be
          missed and the application could fail in a non-secure mode because the failure
          was not recognized.

             ‚Äì Examples that translate this principles into design features include:
             ‚Äì Systems and applications should not store passwords in unencrypted
                 form.

             ‚Äì Systems and applications should centrally manage time-outs of
                 inactive sessions used to access health information.

             ‚Äì Applications should have input controls to ensure valid data and
                 prevent data input from causing error conditions.

             ‚Äì Applications should avoid writing personal information to client
                 devices.

             ‚Äì Applications should be able to capture and honour any patient consents
                 such as lock-box provisions.

             ‚Äì Applications should minimize the possibilities for unauthorized
                 transmission, print or cut-and-paste copying.

      Sample Threat Risk Assessment Form

      Use this sample Threat Risk Assessment (TRA) Form based on RCMP/CSE
      Methodology. Although a row of sample data is provided, refer to the RCMP
      website for more guidance. If you have never performed a TRA, consider using
      outside professional help for your first one.




202
                                                                      Sustaining Security


                            1. Asset Characteristics



                                  Statement of Sensitivity                              Effect if Compromised




                                        Confidentiality



                                                          Integrity



                                                                       Availability
#          Description
                                                                                      Privacy
                          Value                                                                  Potential Loss
                                                                                      Impact




1   EHR Database Server   $35K         High               High        High             High     Service,
                                                                                                financial, legal,
                                                                                                trust




2




3




                                                                                                                    203
Sustaining Security


                                                2. Threat Assessment



                         Threat Details                                        Exposure
      #
                                                                                                   Exposure
                    Nature                    Class       Likelihood       Loss           Impact
                                                                                                    Rating


      1   *Server Failure                 Disclosure      Medium       Confidentiality     High      High
          *Computer Component Failure     Interruption                 Integrity
          *Criminal Activity              Modification                 Availability
          *Hacker Activity                Destruction                  Privacy
          *Staff Termination              Removal /
          *Tampering                      Loss
          *IT Malfunctions
          *Penetration
          *Theft of Equipment
          *Theft of Data
          *Modification of Data
          *Cryptographic Failure




      2




      3




204
                                                               Sustaining Security



                                   3. Risk Assessment (Current State)



                                                                                             Risk
#                          Safeguard                         Effectiveness   Vulnerability
                                                                                             Level


1   *Identification                                             Medium         Medium        Medium
    *Authentication
    *Authorization
    *Training
    *Structural access protection
    *Climate control
    *Fire detection/sprinklers
    *Encryption
    *Auditing
    *Testing procedures
    *Change control procedures
    *Virus scan
    *Emergency and contingency planning
    *A secure physical environment
    *Limitation of physical access to the server
    *Strong database management skills
    *Data reconstruction procedures
    *Personnel security screening


2




3




                                                                                                      205
Sustaining Security


                                                   4. Residual Risk


                                   Mitigation                                     Outcome

      #
                                                           Mitigated
                      Recommendations                        Risk      Decision             Implemented
                                                            Level


      1   *Applications that draw on information stored      Low       Accepted                Yes
          within a database, will use the authentication
          and access controls for authorization
          provided by the security framework.
          *Ensure that a Threats and Risks
          Assessment has been approved for each
          associated application.
          *The Database Server should be protected
          by a firewall for security




      2




      3




206
                                                   Sustaining Security

                            APPENDIX C ‚Äì AUDIT

Guidance on Capturing and Using Audit Information

‚Ä¢   Design your processes and systems to capture information from the sources
    you will need to audit against, such as:

       ‚Äì application logs (for applications that process personal health
           information),

       ‚Äì system logs,
       ‚Äì network logs,
       ‚Äì exception logs (for example, failed logon attempts),
       ‚Äì operator logs,
       ‚Äì badge access or sign-in logs for restricted areas, and
       ‚Äì sign-out logs for hardcopy personal health information.
‚Ä¢   Make sure these logs contain the information required for your audit analysis
    and keep them secure.

       ‚Äì Collect identifying information, such as User ID, the time and date,
           terminal location, resource being accessed, application being used, and
           the action being taken on personal health information (for example,
           view, update or delete).

       ‚Äì Collect event information, such as failed access attempts, intrusion
           alerts and other management alarms, policy violations (for example,
           firewalls) and system failures.

       ‚Äì Do not store logs on the same server being monitored.
       ‚Äì Protect logs against destruction or unauthorized changes (users with
           privileged support IDs should not be able to alter or erase logs).

‚Ä¢   Determine a strategy for using the audit information you collect:

       ‚Äì Start with exceptional incidents where the record captures potentially
           threatening events, such as excessive failed access attempts.



                                                                                     207
Sustaining Security

         ‚Äì Adopt a spot-checking approach, such as reviewing the activity of a
            user ID (especially privileged IDs) over a period of time to check for
            regularity. review access to particular records to ensure
            appropriateness. Access to some records may be worth watching more
            closely than others (for example those of a well-know politician,
            celebrity or senior manager).

         ‚Äì If the spot-checks reveal many potential problems or miss problems
            discovered through other means, increase the frequency.

         ‚Äì Whenever possible, use automated tools to scan the logs and find
            correlations that may indicate possible intrusion attempts,
            unauthorized or illegal activities, or suspicious events, such as activity
            beyond normal bounds of a job role, that require further investigation.




208
Research
                                                                                                              Research

Table of Contents


Key Points............................................................................................................213

The Rule...............................................................................................................214

What You Need To Do ........................................................................................215
  Collection.......................................................................................................215
  Use .................................................................................................................215
  Disclosure ......................................................................................................216
  Research Plan.................................................................................................216
  Research Ethics Board Composition .............................................................216
  Research Ethics Board Duties........................................................................217
  Researcher Duties ..........................................................................................218
  Disclosure Under Other Acts .........................................................................219
  Transition Rules .............................................................................................219
  Express Consent.............................................................................................219

Related Sections of the Act..................................................................................220

Checklist, Templates and Tools...........................................................................220
       Research Approval Checklist
       Sample Application to Research Ethics Board
       Sample Information Sharing Agreement
       Sample Consent Form for Study Participant




                                                                                                                             211
Research




212
                                                                             Research


Key Points
‚Ä¢   The Act sets out specific requirements for the collection, use and disclosure of
    personal health information for research that is conducted without consent.

‚Ä¢   The research provisions of the Act apply to:

       ‚Äì researchers, who collect and use personal health information for
           research purposes,

       ‚Äì health information custodians (such as hospitals and physicians), who
           disclose personal health information for research purposes, and

       ‚Äì research ethics boards, who review and approve research plans.
‚Ä¢   This Toolkit uses the term ‚Äúyou‚Äù for the sake of brevity. In this section, the
    term ‚Äúyou‚Äù refers to the health information custodians and researchers reading
    this section, who must observe the Act‚Äôs requirements concerning research in
    their respective roles.

‚Ä¢   You may collect personal health information for research purposes without
    consent from non-health information custodians who are not prohibited by law
    from disclosing the information to you, and from health information
    custodians who are permitted or required by law to disclose the information to
    you. Before collecting the information without consent, you must comply
    with the Act‚Äôs requirements concerning research plans and research ethics
    board approval.

‚Ä¢   You may use personal health information for research purposes without
    consent if a law permits you to do so. To use personal health information for
    research purposes without consent, you must comply with the Act‚Äôs
    requirements concerning research plans and research ethics board approval.
    When conducting research under the approved research plan, you must follow
    the research duties that are listed in the Act.

‚Ä¢   You may disclose personal health information to a researcher without consent;
    however, before disclosing the information to a researcher, you must enter
    into a written agreement with the researcher to protect the information, and
    receive from the researcher a written application, a written research plan and a
    copy of the research ethics board‚Äôs approval of the research plan from the
    researcher. You may also impose any other obligations on the researcher that
    you feel are appropriate.



                                                                                       213
Research

      The Rule

      You may collect, use and disclose personal health information for research
      purposes without consent, but only if you meet the strict conditions described
      below.


      ‚Ä¢   The research provisions of the Act apply to:

             ‚Äì researchers, who collect and use personal health information for
                 research purposes,

             ‚Äì health information custodians (such as hospitals and physicians), who
                 disclose personal health information for research purposes, and

             ‚Äì research ethics boards, who review and approve research plans.
      ‚Ä¢   This Toolkit uses the term ‚Äúyou‚Äù for the sake of brevity. In this section, the
          term ‚Äúyou‚Äù refers to the health information custodians and researchers reading
          this section, who must observe the Act‚Äôs requirements concerning research in
          their respective roles.

      ‚Ä¢   The Act has rules on dealing with personal health information, but does not
          have specific rules on dealing with specimens. You should follow standard
          best practices when dealing with specimens in research, in addition to the
          rules in the Act.

      ‚Ä¢   This section of the Toolkit deals with retrospective research for which consent
          is not obtained. It does not discuss clinical trials, genetic testing or other
          studies, for which express consent is required.

      ‚Ä¢   This Toolkit only deals with privacy issues. A discussion on other ethical
          issues relating to research, including risks and harms, is beyond the scope of
          this Toolkit.

      ‚Ä¢   Nothing in the Act prevents you from collecting, using or disclosing personal
          health information for research purposes with express consent. You may not
          rely on implied consent to collect, use or disclose personal health information
          for research purposes.




214
                                                                             Research


What You Need To Do

Collection
You may collect personal health information for research purposes indirectly and
without consent:

‚Ä¢   from non-health information custodians (such as family members) who are not
    prohibited by law from disclosing the information to you,

‚Ä¢   from health information custodians who are permitted by law to disclose the
    information to you, and

‚Ä¢   if you are permitted or required by law to collect the information indirectly
    and without consent.

However, before doing so, you must comply with the Act‚Äôs requirements
concerning research plans and research ethics board approval (which are
described below).

The person disclosing the information will ask you to enter into a written
agreement concerning protection of the information (which might address the
information‚Äôs use, security, disclosure, return or disposal).

Note: If someone within your hospital provides the information to you, you will
      likely be asked to sign a Confidentiality Agreement. If someone external
      to the hospital discloses the information to you, you will likely be asked to
      sign an Information Sharing Agreement.

Use
You may use personal health information for research purposes without consent if
a law permits you to do so.

However, when doing so, you must:

‚Ä¢   conduct your research under a research plan that a research ethics board has
    approved (as described below), or

‚Ä¢   follow the rules for research approved outside Ontario (as described below) if
    the personal health information originates outside Ontario.




                                                                                      215
Research

          When using personal health information for research purposes without consent,
          you must submit a research plan to a research ethics board outlining your
          anticipated use of the information.
          Once a research ethics board approves your plan (or, in the case of information
          originating outside of Ontario, a similar research approval body), you must follow
          the Researcher Duties described below.
          If you do not have a research ethics board at your facility, you must have some
          other research ethics board approve your research plan.

          Disclosure
          You may disclose personal health information to a researcher without consent if
          you enter into a written agreement with the researcher to protect the information
          (which may address its use, security, disclosure, return or disposal) (for disclosure
          outside the hospital), and receive from the researcher:
          ‚Ä¢     a written application,
          ‚Ä¢     a written research plan (meeting the requirements described below), and
          ‚Ä¢     a copy of a research ethics board‚Äôs approval of the research plan.
          The discretion to grant requests for disclosure remains with you and you may
          impose additional obligations on the researcher.

          Research Plan
          The research plan must describe all of the matters
          listed on the Research Approval Checklist.

          Research Ethics Board Composition

              A research ethics board must be composed of at least five members,
              including:


                                                                  &
              an independent    a member with            two members with             a member with
              community         experience in           relevant expertise in           expertise in
              member            research ethics               research                health privacy

      No one member can fulfill more than one of these roles.‚Ä†
      ‚Ä†
        The information set out in bold italicized text is based on draft Regulations that have not yet been
      finalized. We will send you a replacement page with updated information once the Regulations are
      finalized.
216
                                                                            Research

Research Ethics Board Duties
A research ethics board reviews research plans prepared by researchers. Members
of a research ethics board must not review proposals with which they have a
conflict of interest.

If the research ethics board is deciding whether to approve a research plan for
research to be conducted without consent, it must consider everything relevant,
including:

‚Ä¢   whether the researcher could reasonably achieve the research goals without
    using personal health information,

‚Ä¢   whether the researcher will put adequate safeguards in place to protect privacy
    and preserve the information‚Äôs confidentiality,

‚Ä¢   the public interest in conducting the research and the public interest in
    protecting the privacy of the individuals whose information is being disclosed,
    and

‚Ä¢   whether it would be impractical to require the researcher to get the affected
    individuals‚Äô consent.

A research ethics board should ensure that it considers all matters listed on the
Research Approval Checklist. Once it makes a decision, a research ethics board
must explain to the researcher in writing:

‚Ä¢   whether the research plan has been approved,

‚Ä¢   why it has been approved or denied, and

‚Ä¢   if approved, whether the approval is subject to any conditions.




                                                                                      217
Research

      Researcher Duties
      As a researcher, you must:

      ‚Ä¢   comply with the approved research plan, including any conditions imposed by
          the research ethics board,

      ‚Ä¢   implement the safeguards for protecting the information (see security and
          storage guidelines) outlined in your plan,

      ‚Ä¢   use personal health information only for the purposes described in your
          research plan,

      ‚Ä¢   not publish personal health information in a way that could reasonably allow
          others to identify the individual whose information was used in the research,

      ‚Ä¢   only disclose the information when required by law,

      ‚Ä¢   not contact individuals whose information is used in the research unless they
          have given consent to the disclosing health information custodian to be
          contacted by you (even if personal health information you hold is stolen, lost
          or accessed by unauthorized individuals),

          Note: The health information custodian who collects the information would
                obtain this consent.

      ‚Ä¢   comply with the terms and conditions of the written agreement that you enter
          into with the disclosing health information custodians, and

          Note: Hospitals should develop a clear policy on signing these agreements,
                which includes the designation of a person responsible for them.

      ‚Ä¢   provide written notice of any breach of the written agreement or any of these
          duties to the disclosing health information custodians.

      If you work under a research plan approved outside Ontario, you must follow the
      same requirements that researchers whose plans are approved in Ontario must
      follow.

      You may use or disclose personal health information originating outside Ontario
      for research purposes without consent if a research ethics board (or other body
      outside Ontario responsible for approving research) has approved the research.




218
                                                                            Research

Disclosure Under Other Acts
You must still follow all of the above disclosure rules, where another law allows
you to disclose personal health information to a researcher without consent (for
example, to Cancer Care Ontario as a researcher).

Transition Rules
The Act provides a three-year transition period for all research projects approved
before November 1, 2004.

This means that if you have been using or disclosing personal health information
for research purposes without consent within the three years before November 1,
2004 and your research will not continue past November 1, 2007, you do not need
to take any additional steps.

If your research will continue past November 1, 2007, however, you must take
steps to comply with the Act.

Express Consent
Nothing in the Act prevents you from collecting, using or disclosing personal
health information for research purposes with express consent. You may not rely
on implied consent to collect, use or disclose personal health information for
research purposes.

The strict conditions described above apply to research that is conducted without
consent. As a best practice, you should still comply with these conditions when
you collect, use and disclose personal health information for research purposes
with express consent.

The research rules in the Act relate to retrospective research for which consent is
not obtained. You must obtain express consent when you conduct clinical trials,
genetic testing or other research that is not simply retrospective.

For additional guidelines on researching with express consent, see the Tri-Council
Policy Statement on Ethical Conduct for Research Involving Humans.




                                                                                      219
Research



      Related Sections of the Act
      2, 3, 4, 12(3), 36(1)(a), 36(1)(d), 36(1)(g), 36(1)(h), 37(1)a), 37(1)b, 37(1)(j),
      36(1)(k), 37(3), 44




      Checklist, Templates and Tools

      ‚Ä¢   Research Approval Checklist

      ‚Ä¢   Sample Research Application

      ‚Ä¢   Sample Information Sharing Agreement

      ‚Ä¢   Sample Consent Form




220
                                                                                            Research

                       RESEARCH APPROVAL CHECKLIST‚Ä†


The research plan must describe:
    the name, affiliation, roles and qualifications of everyone working on the research
    and accessing personal health information
    adequate justification for disclosing personal health information to these persons
    the nature of the research
    the particular research objectives and related research questions
    the duration of the research
    the anticipated public and scientific benefit of the research
    the required personal health information
    the sources of the personal health information
    the use of personal health information, including details on information linkage (if
    any)
    adequate justification for using the personal health information
    adequate justification for linking the personal health information
    adequate consent process/form OR adequate justification for proceeding without
    consent
    the reasonably foreseeable harms and benefits of the information use
    adequate explanation of how foreseeable harms will be addressed
    adequate privacy and security safeguards
    how long the information will remain identifiable and why
    how and when the information will be destroyed or returned to source
    details on research funding
    details on whether the researcher has applied for other research ethics board
    approval and its response or the status of the application
    details on the researcher‚Äôs conflicts of interest
    details on any additional matters the law or ethical guidelines and conventions may
    require

‚Ä†
  The information set out in bold italicized text is based on draft Regulations that have not yet been
finalized. We will send you a replacement page with updated information once the Regulations
are finalized.




                                                                                                         221
Research
                                                                REB File No.:
                                                                Date:

               SAMPLE APPLICATION TO RESEARCH ETHICS BOARD

      A.      GENERAL INFORMATION

      PRIMARY INVESTIGATOR

      Name                                 Signature

      Dept./Div.                           Position

      Qualification:                       Email Address

      CO-INVESTIGATOR

      Name                                 Signature

      Dept./Div.                           Position

      Qualification:                       Email Address

      CO-INVESTIGATOR

      Name                                 Signature

      Dept./Div.                           Position

      Qualification:                       Email Address




      OTHER RESEARCH TEAM MEMBERS WHO ARE NOT CO-
      INVESTIGATORS Please type names, roles and qualifications (signatures
      not needed)




222
                                                                                        Research

B.   DETAILS OF PROJECT

1.   Project Title _________________________________________________

2.   Is this project funded? _________________________________________

              NO

              YES

3.   Sponsor ____________________________________________________

4.   Duration of Funding: from __/__/__ to __/__/__

5.   Conflict of Interest Declaration ‚Äì Do you have any conflicts of interest
     (actual, apparent, perceived or potential) relating to this project?*

              NO

              YES

     Description of conflict of interest

     Mandatory Signature

     (Application will be returned without signature)
     *Conflicts of interest include but are not limited to the following situations and also must
     be disclosed under institutional policy for review: If you or any of the involved research
     team members or your/their dependants have:

     (1) employment or consulting arrangements and/or a financial interest in the sponsor of
     the study, or with proposed subcontractors, vendors or collaborators; or

     (2) a financial interest in the subject of the study.

6.   Protocol:

     a)       Nature



     b)       Objectives



     c)       Methods



                                                                                                    223
Research


           d)     Statistical Analysis



           e)     Anticipated Public or Scientific Benefit



           f)     Duration of Research



           g)     Foreseeable Harms and Benefits of Research (describe how harms
                  will be addressed)

      C.   INFORMATION REQUESTED

      1.   What patient information do you require?




      2.   What patient information source are you accessing?

                Health Records           Specify which
                Clinic/Office Files

                Electronic Database      Specify which

                Outside Institution      Specify which

                Other                    Specify which


      3.   Proposed number of research subjects _____________________________




224
                                                                          Research

4.   Are you requesting information that identifies or potentially identifies
     individuals?

            NO
            YES

     If yes, explain why you cannot use anonymized or aggregate information:




5.   Have you obtained consent from the individuals to collect and use the
     identifying information on this project?

         YES       Attach a sample consent form

         NO

      If no, explain why




6.   What security measures will be in place to protect the information during
     transmission?




D.   INFORMATION LINKAGE

1.   Does your project involve linking any information from this request to
     other information?

            NO
            YES




                                                                                 225
Research

           If yes,

           Describe what information is to be linked:




           Describe what type of linkage is required:




           Describe the rationale for this linkage:




      2.   Have you received approval from the other information sources including
           division/department head authorization to conduct this linkage?

               NO

               YES      Please attach the approval(s)


      3.   Will the information be retained in linked form?

               NO

               YES


      4.   When will the information be de-identified after the linkage?




226
                                                                           Research

E.   DISSEMINATION OF ANALYSIS AND/OR REPORTS:

1.   How do you plan to disseminate and/or publish the results of your
     analyses?




2.   What is the expected date of dissemination and/or publication?



F.   DISCLOSURE AVOIDANCE PRACTICES

1.   How will you ensure that information will be aggregated prior to
     disclosure, to the level required in the information sharing agreement,
     confidentiality agreement, privacy policy and other applicable policies and
     procedures?

     __________________________________________________________

     __________________________________________________________

      __________________________________________________________

2.   Attach information collection form or list of fields (mandatory: application
     will be returned if this information has not been included)
     Please note that the content of the form should be adequate to answer
     the research questions.

3.   Are any sensitive issues raised in this study or its publication (e.g. HIV
     status, mental health status, subjects identifiable, pedigrees, other)

         NO

         YES       Please specify




                                                                                    227
Research

      G.   SECURITY AND ACCESS

      1.   The information obtained from the records described above will be used
           for the outlined research purposes only:

               NO       If no, a separate request must be submitted
               YES

      2.   List all of the persons who will have access to the records in an
           individually recognized form for the research purpose described and why
           they need this access: (name and role in research)

           __________________________________________________________

           __________________________________________________________

      3.   Have all these persons signed confidentiality agreements?

               NO

               YES        If no, please indicate when agreement will be signed

            _________________________________________________________

            _________________________________________________________


      H.   SECURITY MEASURES

      1.   Describe how you will keep information secure

                           Premises will be locked except when one or more of the
                           individuals named in E1(b) are present

                           Access to the premises will be controlled (passcards,
                           security clearances etc.)

                           Access to the information will be restricted to the
                           research team by:

                              Patient code    Files/Folders password   Computer password

                           Please provide name of password software




228
                                                                               Research

                     Other computer security methods that prevent
                     unauthorized access will be used

                         Encryption      Firewalls   Identifying Information
                                                     Scrambled/De-linked

                     Other (Describe):

                     Staff will be trained regarding privacy

                     Staff will sign a confidentiality agreement

                     Other, explain

      __________________________________________________________

      __________________________________________________________



2.   Will information remain within the institution?

          NO

          YES        If no, please indicate why and how information will be
                     exported outside

       _________________________________________________________

       _________________________________________________________


3.   Will system files be backed up automatically?

            NO
            YES

     If yes, please specify provisions that would be made for a private drive
     that cannot be accessed by anyone other than your research team, or that
     could not be backed up by computer support staff within your
     organization:

     __________________________________________________________

     __________________________________________________________




                                                                                     229
Research

      All original personal health records received from must be returned to
      and all copies of personal health records that were made or received must be
      destroyed in accordance with the information sharing agreement.

      I certify that the information reported here is accurate and that the personal
      health information will not be used for future projects without prior
      approval of a research ethics board.


      Primary Investigator Signature            Date



      Division/Department Head                  Date
      Signature of Approval


      Do you plan on accessing information from another division/department?

             NO
             YES      If yes, authorization from the division/department head is
                      requested

       Signature of Approval                     Date


      In making this request, I acknowledge that failure to comply with the terms
      and conditions of the information sharing agreement is cause for termination
      of the agreement and, where applicable, a complaint to the Information and
      Privacy Commissioner/Ontario.

      Date                                      Signature of Requestor


                     Research Ethics Board for Retrospective Research


      Signature of REB Chair

      Date of Approval                          Approval Expires

      Level of Continuing Review:




230
                                                                                   Research

NOTE TO USER: This Sample Information Sharing Agreement is intended to be used
for retrospective research. You should consider using a material transfer agreement if
your research involves the transfer of specimens. A customized version of this Sample
Information Sharing Agreement could be used when a hospital discloses personal health
information to a health data institute for research purposes. It would not typically be used
when a health information custodian shares personal health information with a researcher
from the same hospital; however your confidentiality agreement with your internal
researcher should include many of the same standards for confidentiality contained here.
Hospitals should develop a clear policy on drafting, negotiating and signing these
agreements, which includes the designation of a person responsible for them.



             SAMPLE INFORMATION SHARING AGREEMENT

This Information Sharing Agreement is effective as of [insert date] between
[insert name of Hospital] (as the disclosing party) and [insert name of
Researcher] (as the collecting party)

BACKGROUND:

A.      [Identify the Hospital.]

B.      [Identify the Researcher.]

C.      [Outline the details of the proposed disclosure of personal health
        information and the research.]

D.      The parties now wish to set out terms and conditions about the collection,
        transmission, use, retention, disclosure and disposal of certain personal
        health information provided by the Hospital to the Researcher.

E.      Section 44 of the Act gives the Hospital and the Researcher statutory
        authority to engage in this collection, use and disclosure.

F.      [Identify any other legislative authority for the collection, use and
        disclosure of the personal health information, such as legislation that
        governs the parties or the research.]

FOR VALUE RECEIVED, the parties agree as follows:

SECTION 1 - INTERPRETATION

1.1     Definitions

In this Agreement:


                                                                                               231
Research

      (1)    Act means the Personal Health Information Protection Act, 2004
             (Ontario) and where the context requires includes the regulations under
             that Act, including any amendments;

      (2)    Hospital means [insert name of Hospital];

      (3)    Research has the meaning set out in Section 2.2(1);

      (4)    Research Plan means the research plan attached as Schedule A;

      (5)    Researcher means [insert name of Researcher];

      (6)    Shared Information has the meaning set out in Section 2.1;

      (7)    [Include definitions of any terms or acronyms that may be unique to
             the subject matter of the Agreement.]

      1.2    Purpose of Agreement

      The purpose of this Agreement is to set out terms and conditions about the
      collection, transmission, use, retention, disclosure and disposal of the Shared
      Information that is provided by the Hospital to the Researcher for the purposes
      described in Section 2.2(1) and Schedule A.

      SECTION 2 - INFORMATION SHARING

      2.1    Shared Information

      The Hospital will provide to the Researcher the personal health information
      described in Schedule B (the ‚ÄúShared Information‚Äù).

      2.2    Use of Personal Health Information

      (1)    The Researcher shall use the Shared Information, and shall ensure that the
             Shared Information is used, only as necessary to fulfill the specific
             research objectives and related research questions described in the
             Research Plan (the ‚ÄúResearch‚Äù).

      (2)    The Researcher shall not collect or use the Shared Information for any
             purposes other than those purposes described in Section 2.2(1) and
             Schedule A or specifically authorized by legislation.

      (3)    The Researcher shall require the prior approval of the Hospital for any
             proposed changes to the Research described in Section 2.2(1) and
             Schedule A.



232
                                                                            Research

(4)    The Researcher confirms that the Research Plan has been approved by the
       [insert name of research ethics board] on [insert date] and that a copy
       of the [insert name of research ethics board]‚Äôs written approval of the
       Research Plan is attached as Schedule C.

(5)    The Researcher agrees that the Researcher shall comply with the
       conditions imposed by the [insert name of research ethics board]
       concerning the Research Plan, if any.

2.3    Method of Sharing Information

(1)    The Hospital will provide to the Researcher the Shared Information via
       [specify manner of information sharing such as provision of a
       computer tape, computer disk, hard copy, electronic data interchange,
       etc.)].

(2)    Such transmission of Shared Information will take place on [specify
       frequency of sharing].

(3)    Such transmission of Shared Information will [describe the measures
       that will be taken to ensure that the Shared Information will be
       protected against loss and unauthorized access during transfer].

2.4    Accuracy, Completeness and Currency of Shared Information

The Hospital may conduct audits in accordance with an information quality
framework and may follow up with the Researcher concerning the maintenance of
appropriate technical standards for information quality, integrity and security in
order to seek to ensure the accuracy, completeness and currency of the Shared
Information. The Researcher shall fully cooperate with the Hospital in this
regard. [If this provision is not appropriate in the circumstances, describe
what steps will be taken to ensure the accuracy, completeness and currency
of the Shared Information before it is disclosed to the Researcher].

2.5    Access to the Shared Information

The Researcher shall refer individuals seeking access to their own personal health
information that forms part of the Shared Information to the Hospital. The
Researcher will cooperate with the Hospital and these individuals in providing
access to this information.

2.6    Security of Shared Information

(1)    The Researcher shall comply with the Act and all statutes, regulations,
       rulings and orders relating to the collection, use and disclosure of personal


                                                                                       233
Research

             health information in respect of the Shared Information, as the same may
             apply or be amended from time to time.

      (2)    The Researcher shall implement the following security safeguards in
             handling the Shared Information:

      [Insert appropriate provisions. The following provisions are examples of
      best practices. You should try to include all of these provisions in your
      agreements; however, it is likely that the Researcher will resist some of these
      high standards. Consult your lawyers when negotiating these agreements.]

             (a)    The Researcher shall not disclose the Shared Information, except
                    as permitted by this Agreement or required by law;

             (b)    The Researcher shall give access to the Shared Information, in a
                    form in which the individual to whom it relates can be identified,
                    only to the Researcher‚Äôs staff members listed in Schedule D (the
                    ‚ÄúNamed Staff‚Äù). The Named Staff are responsible for encrypting
                    identifying numbers, linking files, storing and retrieving files from
                    safes that are located in secure rooms and for destroying
                    information in accordance with Section 2.6(2)(m);

             (c)    Identifying information, including names and numbers, forming
                    part of the Shared Information will be encrypted immediately after
                    the applicable computer program first links the Shared
                    Information;

             (d)    The Researcher‚Äôs working files will not contain any identifying
                    information about an individual but will contain the encrypted
                    number;

             (e)    Other than the Named Staff, staff members of the Researcher will
                    access the working files only in an anonymized form and will
                    produce analyses required for reports from such files;

             (f)    All of the Researcher‚Äôs staff members, including the Named Staff,
                    shall sign a confidentiality agreement in a form acceptable to the
                    Hospital;

             (g)    The Researcher shall take appropriate disciplinary action against
                    any Named Staff member who breaches the terms of his or her
                    Confidentiality Agreement in relation to the Shared Information
                    and shall deny such individual any further access to the Shared
                    Information;



234
                                                                          Research

      (h)    The Researcher shall keep the Shared Information in a physically
             secure manner at all times [Consider adding details of physical
             security.];

      (i)    The Researcher shall monitor access to the Shared Information to
             ensure security;

      (j)    The Researcher shall allow the Hospital to inspect the Researcher‚Äôs
             security arrangements at any time upon reasonable notice and shall
             notify the Hospital of any material changes to these practices;

      (k)    The Researcher shall present only aggregated information in its
             reports so as to prevent any identification of individuals, whether
             direct or indirect. The Researcher shall ensure that each cell has at
             least five observations, unless express written authorization for
             fewer cells is provided by the Hospital;

      (l)    The Researcher shall not contact any individual to whom the
             Shared Information relates without the Hospital‚Äôs prior approval
             [The Hospital must ensure that it has the individual‚Äôs consent
             to being contacted by the Researcher before giving its
             approval.];

      (m)    The Researcher shall retain the Shared Information for as long as
             necessary to fulfill the purposes identified in Section 2.2. The
             parties will review the Researcher‚Äôs information retention practices
             as required;

      (n)    The Researcher shall destroy, in a secure manner, information that
             is no longer required to fulfill the purposes identified in Section
             2.2. Such information shall be permanently erased, rendered
             anonymous or destroyed in such a way that it cannot be
             reconstructed or retrieved. The Researcher shall provide the
             Hospital with confirmation in writing of the destruction and
             manner of destruction of the Shared Information; and

      (o)    The Researcher shall notify the Hospital in writing immediately
             upon becoming aware that any of the terms or conditions of this
             Agreement has been breached.

(3)   If a Named Staff member no longer has access to the Shared Information
      in a form in which the individual to whom it relates can be identified, the
      Researcher shall so notify the Hospital in writing. The Researcher shall be
      entitled to substitute another individual for that Named Staff member by
      notice in writing to the Hospital. After the Hospital has been given such


                                                                                     235
Research

            written notice, the substituted individual shall be deemed to be a Named
            Staff member under this Agreement.

      (4)   In the event that the Hospital has concerns about the Researcher‚Äôs
            compliance with the provisions of this Agreement, the Hospital shall
            provide the Researcher with written notice of such concerns and its
            reasons for them. The Researcher shall, within five days of receipt of the
            notice, investigate the matter and provide the Hospital with a report stating
            the cause of the deficiency, if any, and the steps taken to prevent a
            recurrence, if required.

      (5)   If the Researcher becomes aware that a person has obtained access to the
            Shared Information other than in accordance with this Agreement through
            the Researcher‚Äôs breach of this Agreement, or the Researcher or the
            Researcher‚Äôs staff members have used or disclosed the Shared Information
            other than in accordance with this Agreement, the Researcher shall
            immediately notify the Hospital in writing and meet any requirements
            prescribed by law.

      (6)   The Researcher shall keep the Hospital appraised of any changes to its
            policies and procedures followed in respect of the Shared Information.

      (7)   Each party undertakes to give the other written notice of any changes in
            legislation, regulations or policies governing or regulating it that are likely
            to affect the parties‚Äô rights and obligations under this Agreement.

      (8)   Upon expiration or termination of this Agreement, the Researcher shall
            continue to protect the Shared Information in accordance with Section 2.6.

      2.7   Term and Termination

      (1)   This Agreement shall start on [insert start date] and shall end on
            [termination date], unless ended earlier in accordance with Section 2.7(2)
            or if the parties agree in writing to extend it. [State whether the
            proposed information sharing will be a one-time occurrence, time-
            limited, or ongoing.]

      (2)   This Agreement may be terminated in any of the following ways:

            (a)    by written agreement of the parties;

            (b)    by either party, if written notice of termination is given to the other
                   party, because:




236
                                                                              Research

               (i)     the other party fails to perform or comply with any term or
                       condition of Section 2.6 and such failure to perform or
                       comply is not remedied within [five] days of written notice
                       to do so; or

               (ii)    the other party fails to perform or comply with any material
                       term or condition of this Agreement (other than a term or
                       condition contained in Section 2.6) and such failure to
                       perform or comply is not remedied within [ten] days of
                       written notice to do so.

(3)    Notwithstanding the termination of this Agreement for any reason, a
       research project of the type referred to in Section 2.2 may be completed
       provided that the Researcher has obtained written authorization from the
       Hospital to do so and the Researcher continues to comply with the terms
       and conditions contained in this Agreement in respect of that research
       project.

2.8    Indemnity

In addition to any other protections from liability available to the Hospital at law,
the Researcher shall indemnify and hold harmless the Hospital and its directors,
officers, employees, agents and medical staff members against any and all third
party civil or administrative actions, claims or proceedings, including reasonable
legal fees, incurred by the Hospital in connection with any failure by the
Researcher or any person for whom it is responsible at law to perform its
obligations under this Agreement.

SECTION 3 - GENERAL

3.1    Entire Agreement

This Agreement constitutes the entire agreement between the parties concerning
the information sharing described in this Agreement.

3.2    Amendments

This Agreement may be amended only by written agreement of the parties.

3.3    Severability

Each provision contained in this Agreement is distinct and severable. Any
declaration by a court of competent jurisdiction of the invalidity or
unenforceability of any provision or part of a provision will not affect the validity
or enforceability of any other provision of this Agreement.


                                                                                        237
Research

      3.4    Waiver

      The failure of either party to insist upon strict performance of any terms and
      conditions or to exercise any of its rights set out in this Agreement shall not
      constitute a waiver of these rights, and these rights shall continue in full force and
      effect.

      3.5    Assignment and Enurement

      The Researcher may not assign the Researcher‚Äôs rights or obligations under this
      Agreement without the prior written consent of the Hospital. This Agreement
      enures to the benefit of and binds the parties and their respective successors and
      permitted assigns.

      3.6    Survival

      Sections 2.5, 2.6 and 2.7(3) shall survive the expiration or termination of this
      Agreement indefinitely.

      3.7    Delivery by Fax and in Counterparts

      This Agreement may be executed and delivered by fax and in any number of
      counterparts, each of which when executed and delivered is deemed an original
      but all of which when taken together constitute one and the same instrument.

      3.8    Governing Law

      This Agreement is governed by, and is to be construed and interpreted in
      accordance with, the laws of the Province of Ontario and the laws of Canada
      applicable in the Province of Ontario. Each of the parties irrevocably submits to
      the non-exclusive jurisdiction of the courts of the Province of Ontario.

      The parties have executed this Agreement.



                                        [INSERT NAME OF HOSPITAL]

                                        By:
                                              Name:
                                              Title:


            [If the Researcher is an individual, use the following signing lines]



238
                                                              Research

SIGNED, SEALED AND   )
DELIVERED in the     )
presence of:         )
                     )
                     )   By:
                     )         Name: [Insert name of Researcher]
Witness              )         Title:
                     )




                         [INSERT NAME OF CORPORATION]

                         By:
                               Name:
                               Title:




                                                                    239
Research

                                   Schedule A ‚Äì Research Plan‚Ä†

      [Attach the Research Plan, which has been approved by a research ethics
      board and which describes in detail:

      ‚Ä¢   the name, affiliation, roles and qualifications of everyone working on the
          research and accessing personal health information,

      ‚Ä¢   adequate justification for disclosing personal health information to these
          persons,

      ‚Ä¢   the nature of the research,

      ‚Ä¢   the particular research objectives and related research questions,

      ‚Ä¢   the duration of the research,

      ‚Ä¢   the anticipated public and scientific benefit of the research,

      ‚Ä¢   the required personal health information,

      ‚Ä¢   the sources of the personal health information,

      ‚Ä¢   the use of personal health information, including details on information
          linkage (if any),

      ‚Ä¢   adequate justification for using the personal health information,

      ‚Ä¢   adequate justification for linking the personal health information,

      ‚Ä¢   adequate consent process/form OR adequate justification for proceeding
          without consent,

      ‚Ä¢   the reasonably foreseeable harms and benefits of the information use,

      ‚Ä¢   adequate explanation of how foreseeable harms will be addressed,

      ‚Ä¢   adequate privacy and security safeguards,

      ‚Ä¢   how long the information will remain identifiable and why,



      ‚Ä†
        The information set out in bold italicized text is based on draft Regulations that have not yet been
      finalized. We will send you a replacement page with updated information once the Regulations
      are finalized.



240
                                                                          Research

‚Ä¢   how and when the information will be destroyed or returned to source,

‚Ä¢   details on research funding,

‚Ä¢   details on whether the researcher has applied for other research ethics
    board approval and its response or the status of the application,

‚Ä¢   details on the researcher‚Äôs conflicts of interest, and

‚Ä¢   details on any additional matters the law or ethical guidelines and
    conventions may require.]




                                                                                241
Research

                           Schedule B ‚Äì Shared Information

      [List all elements of personal health information that will be disclosed to the
      Researcher for the specified research objectives and related research
      questions. Only those components of personal health information that are
      absolutely necessary to achieve the research objectives and to answer the
      related research questions should be disclosed.]




242
                                                                     Research

            Schedule C ‚Äì Research Ethics Board Approval

[Attach a copy of the written decision of the Research Ethics Board. The
entire decision, with conditions, if any, must be attached.]




                                                                           243
Research

                              Schedule D ‚Äì Named Staff

      [List appropriate individuals. Limit these individuals to a small number.]




244
                                                                             Research

          SAMPLE CONSENT FORM FOR STUDY PARTICIPANT

Note to User: This Sample Consent Form is intended to be used for
retrospective research. It is not appropriate to use it for clinical trials.


Study Title:
Primary Investigator:
Sponsor:
Background:                     [Insert details about the study.]
What is involved?
Benefits and Risks:

Voluntary: Participation in this study is completely voluntary. You can refuse
to sign this consent form and to participate in the study. You can also withdraw
your consent any time by writing to . Your care at will not be affected by
your decision.

Confidentiality:                 [Insert details about collection, use, disclosure,
                                 storage and disposal of personal health
                                 information.]

You have had this study explained to you and had an opportunity to ask questions.
You have been given a copy of this Consent Form. If you have any additional
questions about the study, please contact at . If you have any additional
questions about your privacy, please contact at .

Participant Consent

I agree to participate in this study.
I also permit to collect, use and disclose health information about me for the
purposes of this study.



Name of Participant (print)                Signature of Participant


Investigator‚Äôs Signature                   Signature of Substitution Decision-
                                           Maker (if required)

Date


                                                                                      245
246
Fundraising
248
                                                                                                     Fundraising

Table of Contents


Key Points............................................................................................................251

The Rule...............................................................................................................252

What You Need To Do ........................................................................................252
  Relying on Implied Consent ..........................................................................252
  Obtaining Express Consent............................................................................253
  Disclosing Information to the Hospital Foundation.......................................253
  Providing Information to Hired Fundraisers..................................................254

Related Sections of the Act..................................................................................254

Checklists, Templates and Tools .........................................................................254
       Fundraising Decision Tree
       Sample Consent Form for Fundraising
       Sample Withdrawal of Consent Form for Fundraising




                                                                                                                             249
Fundraising




250
                                                                                     Fundraising


Key Points‚Ä†
‚Ä¢   A hospital may collect, use and disclose information about its patients for
    fundraising so long as the hospital has its patients‚Äô implied consent and the
    information collected, used or disclosed for fundraising purposes is limited to
    the patient‚Äôs name and mailing address.

‚Ä¢   You must obtain express consent when you collect, use or disclose more than
    the patient‚Äôs name and mailing address for fundraising.

‚Ä¢   You must not ask a patient to make a donation until at least 60 days after
    the patient has been discharged from the hospital.

‚Ä¢   You must not discuss a patient‚Äôs state of health or health care when you
    contact them for fundraising purposes.

‚Ä¢   You should require your hospital foundation to follow an appropriate privacy
    policy.

‚Ä¢   You should follow the guidelines on Managing Contracts and Agents if you
    transfer patient information to others who raise funds for you.




‚Ä†
  The information set out in bold italicized text is based on draft Regulations that have not yet been
finalized. We will send you a replacement page with updated information once the Regulations
are finalized.



                                                                                                         251
Fundraising


      The Rule‚Ä†

      A hospital may collect, use and disclose information about its patients for
      fundraising so long as the hospital has its patients‚Äô implied consent and the
      information collected, used or disclosed for fundraising purposes is limited to the
      patient‚Äôs name and mailing address.

      You must obtain express consent when you collect, use or disclose more than the
      patient‚Äôs name and mailing address for fundraising.

      You must not ask a patient to make a donation until at least 60 days after the
      patient has been discharged from the hospital.

      You must not discuss a patient‚Äôs state of health or health care when you contact
      them for fundraising purposes.


      Note: ‚ÄúPatient‚Äù refers to an in-patient or out-patient of the hospital facility,
            service or program.




      What You Need To Do

      Relying on Implied Consent
      You can rely on implied consent when you collect, use and disclose only patient
      names and mailing addresses for fundraising purposes.

      You can rely on implied consent by:

      ‚Ä¢   telling patients that, unless they request otherwise, you will use and disclose
          their names and mailing addresses for fundraising purposes, and




      ‚Ä†
        The information set out in bold italicized text is based on draft Regulations that have not yet been
      finalized. We will send you a replacement page with updated information once the Regulations
      are finalized.



252
                                                                                     Fundraising

‚Ä¢   telling patients how they can opt-out of receiving fundraising solicitations in
    your notices, signs and brochures, as well as in every fundraising
    solicitation.

If a patient opts-out, you must treat this as a withdrawal of consent.‚Ä†

See the Consent section for additional guidelines on relying on implied consent.

Obtaining Express Consent
If you obtain express consent, you can collect, use and disclose whatever
information the patient has given you permission to collect, use and disclose for
fundraising purposes (such as the patient‚Äôs name, mailing address and place of
employment).

However, you should only collect, use and disclose information that is necessary
for fundraising. Do not collect more information than you need.

You cannot disclose a list of patients with a common medical condition for
fundraising purposes (for example, lung cancer patients for lung cancer research
fundraising) without express consent. Such a list contains more than just names
and mailing addresses ‚Äì it also contains specific information about a patient‚Äôs
medical condition. However, specialty hospitals (such as cancer, paediatric and
rehabilitation hospitals) do not need express consent to use and disclose patient
names and mailing addresses for fundraising purposes.

See the Consent section for guidelines on obtaining express consent.

Disclosing Information to the Hospital Foundation
Hospitals who have obtained consent (express or implied) may disclose
information to their own hospital foundation for fundraising.

Hospitals should tell patients (either in person, or through notices, signs or
brochures) that this information may be disclosed to the hospital foundation for
fundraising purposes as permitted under the Act.

You should require your hospital foundation to follow an appropriate privacy
policy.


‚Ä†
  The information set out in bold italicized text is based on draft Regulations that have not yet been
finalized. We will send you a replacement page with updated information once the Regulations
are finalized.



                                                                                                         253
Fundraising

      You should obtain the hospital foundation‚Äôs commitment that information you
      have provided will only be used for fundraising.

      Providing Information to Hired Fundraisers
      If you transfer patient information to others who raise funds for you, you should
      follow the guidelines on Managing Contracts and Agents.




      Related Sections of the Act
      Sections 2, 3, 4, 17, 18(4)(b), 32, 49




      Checklists, Templates and Tools

      ‚Ä¢   Fundraising Decision Tree

      ‚Ä¢   Consent Form for Fundraising

      ‚Ä¢   Withdrawal of Consent Form for Fundraising




254
                                                                               Fundraising

                         FUNDRAISING DECISION TREE




Note: ‚ÄúPatient‚Äù refers to an in-patient or out-patient of the hospital facility, service or
      program.
‚Ä†
  The references to name and address is based on draft Regulations that have not yet been
finalized. We will send you a replacement page with updated information once the Regulations
are finalized.



                                                                                               255
Fundraising

                      SAMPLE CONSENT FORM FOR FUNDRAISING

                      Consent to the Collection, Use and Disclosure
                    of Personal Information for Fundraising Purposes


      We may use personal [demographic] information about you, including your name and mailing
      address, in order to contact you to support our fundraising programs. We may also share this
      information with [our hospital foundation], which will contact you on our behalf. By signing
      below, you permit us to contact you with information on our fundraising campaigns at a later date.

      You can refuse to sign this consent form. You can also withdraw your consent any time by
      writing to z. Your refusal or withdrawal of consent will in no way affect the care or treatment
      that you receive at [Hospital].
      Patient Consent

      I, _________________________________ authorize [Hospital] to collect, use and disclose:
               (First and Last Name)

                        just my name and mailing address
                        my name, mailing address and the following personal information:
                        _____________________________________________________
      [Note to User: Consider inserting the personal information you would like to use for
      fundraising purposes, such as email address, work telephone number, etc.]

      for use in fundraising relating to the [Hospital]‚Äôs charitable activities. I understand that you might
      share information about me with the hospital foundation.


      Name:

      Address:



      Tel. Home:                                      Tel. Work:

      Signature:                                      Date:




256
                                                                          Fundraising

                                                           FOR INTERNAL OFFICE USE ONLY

                                                                    Patient ID. No. _________




  SAMPLE WITHDRAWAL OF CONSENT FORM FOR FUNDRAISING

    Withdrawal of Consent to the Collection, Use and Disclosure of
           Personal Information for Fundraising Purposes


I, ___________________________________ no longer wish [Hospital] to use the
        (First and Last Name)
following personal information about me for fundraising purposes:

           ¬Ü   my name and mailing address
           ¬Ü   the following personal information:
               _____________________________________________________
               (check all that apply)




Name:

Address:



Tel. Home:                                 Tel. Work:

Signature:                                 Date:




                                                                                                257
258
Managing Contracts and Agents
                              Managing Contracts and Agents

Table of Contents


Key Points............................................................................................................263

The Rule...............................................................................................................264

What You Need To Do ........................................................................................265
  Due Diligence ................................................................................................266
  Contracts ........................................................................................................267
  Enforcement...................................................................................................267
  Information Sharing Agreements...................................................................268
  Dealing with Agents Operating Outside of Ontario ......................................268

Related Sections of the Act..................................................................................269

Checklists, Templates and Tools .........................................................................269
       Checklist for Agents Agreements
       Checklist for Information Sharing Agreements




                                                                                                                             261
Managing Contracts and Agents




262
                    Managing Contracts and Agents


Key Points
‚Ä¢   Your service providers, suppliers, employees, volunteers and others who help
    you carry out your duties are considered ‚Äúagents‚Äù under the Act. Your agents
    can be internal or external to your organization.

‚Ä¢   You may allow your agents to handle your patients‚Äô personal health
    information if you follow the rules in this Section.

‚Ä¢   Your agents must comply with the rules in the Act that apply to them.

‚Ä¢   You should ensure that your agents are informed of these rules.

       ‚Äì For example, by providing staff training to internal agents, and by
           entering into Information Sharing Agreements with external agents.

‚Ä¢   You must take reasonable steps to protect personal health information that you
    provide to your agents. For example, you should:

       ‚Äì conduct effective due diligence before hiring agents,
       ‚Äì include privacy protection clauses in your contracts with agents, and
       ‚Äì enforce the privacy protection clauses in your contracts with agents.




                                                                                     263
Managing Contracts and Agents


      The Rule
      You may hire service providers, suppliers, employees and others to help you carry
      out your duties. When working with these individuals and organizations you may
      find you need to provide patients‚Äô personal health information to them. These
      individuals and organizations are considered to be your ‚Äúagents‚Äù (as defined in the
      Act), and they can be internal or external to your organization. Regardless of
      their internal or external status, your agents must comply with the rules in the Act
      that apply to them.


      You may allow your agents to collect, use, disclose, retain or dispose of your
      patients‚Äô personal health information if:

      ‚Ä¢   you yourself may collect, use, disclose, retain or dispose of the information,

      ‚Ä¢   your agents will collect, use, disclose, retain or dispose of the information as
          part of their obligations to you (through a service or supply contract, for
          example) and their actions are not contrary to the limits imposed by you, the
          Act or another law.

      You should ensure that your agents are informed of their duties under the Act.


      Your agents must:

      ‚Ä¢   have your permission to collect, use, disclose, retain and dispose of personal
          health information on your behalf,

      ‚Ä¢   use the information only for the stated purpose and for no other purpose
          except as permitted or required by any law, and

      ‚Ä¢   alert you if the information they handle for you is stolen, lost, accessed by
          unauthorized persons, or used, disclosed or disposed of in an unauthorized
          manner.

      You may share personal health information with your agents for a number of
      reasons. In addition to the reasons you describe to your patients when you collect
      their personal health information, additional reasons may include:

      ‚Ä¢   planning or delivering programs or services, allocating resources to them,
          evaluating or monitoring their success, and preventing fraud or unauthorized
          receipt of services or benefits,



264
                        Managing Contracts and Agents

‚Ä¢   managing risk and error,

‚Ä¢   improving or maintaining the quality of care, programs and services,

‚Ä¢   teaching your agents how to provide health care,

‚Ä¢   disposing of or removing personal information from a document,

‚Ä¢   obtaining consent (only name and contact information may be used for this
    reason),

‚Ä¢   testifying in a court or other proceeding (only applies to related information),

‚Ä¢   collecting payment or processing claims for payment for health care services,

‚Ä¢   conducting research (if the requirements for research are followed), and

‚Ä¢   reasons permitted or required by any Act.




What You Need To Do

You must take reasonable steps to protect personal health information that you
provide to your agents.


You should:

     Due Diligence                     Contracts                      Enforcement

‚Ä¢   Investigate potential      ‚Ä¢   Include privacy              ‚Ä¢   Enforce the privacy
    agents for their privacy       protection clauses in your       protection clauses in your
    compliance before hiring       contracts with agents            contracts with agents
    them (for organizations)
                                   (e.g., in your third-party
‚Ä¢   Interview potential            supply contracts with
    agents with privacy            external agents or in your
    compliance in mind             confidentiality
    before hiring them (for        agreements with internal
    individuals)                   agents)




                                                                                                 265
Managing Contracts and Agents

      Note: The remainder of this section describes best practices for dealing with
            external institutional agents. The Security sections of this Toolkit describe
            best practices for dealing with individual agents (whether internal or
            external).


      Due Diligence

      You should investigate whether potential agents have taken steps to comply with
      applicable privacy laws before you hire them.


      You should check whether they have:

      ‚Ä¢   appointed a privacy officer,

      ‚Ä¢   developed a written privacy policy,

      ‚Ä¢   assessed security risks, understood legal requirements, and taken steps to
          address any risks,

      ‚Ä¢   adopted a reasonable security standard,

      ‚Ä¢   demonstrated a commitment to privacy, and

      ‚Ä¢   effectively trained and sensitized staff to privacy and security related issues.

      When using a Request for Proposal, you should:

      ‚Ä¢   outline privacy protection requirements,

      ‚Ä¢   confirm that your agents build the full cost of privacy compliance into their
          proposals,

             ‚Äì If not, you could end up paying if a supplier needs to change
                 technology, security, or data handling or storage practices

      ‚Ä¢   ask potential agents how they plan to meet your privacy requirements and
          evaluate them on this plan,

             ‚Äì Ask how they propose to protect personal health information from
                 theft, loss and unauthorized access, copying, modification, use,
                 disclosure and disposal, and how they will make sure information is
                 accurate



266
                     Managing Contracts and Agents

‚Ä¢   get enough information to assess and differentiate between privacy
    compliance plans and validate the capability and resources to comply with the
    plan,

       ‚Äì Get as much information as you can, but do not make your questions
           too specific. You want potential agents‚Äô honest and accurate answers,
           not the answers they think you want to hear. Ask open-ended
           questions that do not suggest answers. For example, instead of asking
           whether potential agents have appointed a privacy officer, ask them to
           fully describe their current privacy practices. Their descriptions will
           let you decide whether they fully understand and are committed to
           privacy.

Contracts

You should make sure your agents put in place effective privacy protection
practices.


When you share personal health information with your agents, you place sensitive
information under your control into their hands. You have an obligation to take
reasonable steps to ensure that this information continues to be protected.

Do this by writing effective contracts, and follow the Checklist for Agents
Agreements.

You should review your existing contracts and, where necessary, amend them to
contain appropriate privacy protection clauses. If your agents resist these
amendments, call your lawyers for advice.

Enforcement

Your responsibilities do not end after you sign a contract. You should monitor
whether your agents are meeting the privacy requirements in their contracts.


You should do this by:

‚Ä¢   setting dates for your agents to report on their compliance,

‚Ä¢   conducting on-site evaluations of your agents‚Äô privacy protections,




                                                                                     267
Managing Contracts and Agents

      ‚Ä¢   holding regular meetings to discuss how current procedures are working and
          develop ways to remedy issues, and

      ‚Ä¢   notifying agents of any changes in your own information practices and asking
          them to put applicable changes in place.

      You should enforce the privacy requirements in your contracts. If your agents do
      not resolve existing problems, you may need to go to court or end the contract.

      Information Sharing Agreements

      If you share personal health information with others, such as external researchers
      or health data institutes, you should have a proper one or two-way information
      sharing agreement.


      To ensure that your information sharing agreements provide sufficient privacy
      protection, follow the Checklist for Information Sharing Agreements.

      Dealing with Agents Operating Outside of Ontario

      The best way to make your agents comply with Ontario‚Äôs law is to put Ontario‚Äôs
      privacy requirements into your contract.


      It may be difficult to ensure that your agents who operate outside of Ontario
      comply with Ontario privacy requirements.

      ‚Ä¢   They are less likely to be familiar with Ontario‚Äôs privacy requirements.

      ‚Ä¢   They may claim that their compliance with other privacy requirements should
          suffice.

      ‚Ä¢   They may not feel compelled to respect privacy laws beyond their own
          jurisdiction.

      ‚Ä¢   The Commissioner will hold you responsible for the activities of your agents
          outside of Ontario.

      Your contracts should bind your agents to Ontario‚Äôs privacy requirements, and
      you can enforce the contracts against these agents outside of Ontario if need be.




268
                      Managing Contracts and Agents


Related Sections of the Act
2, 3, 4, 6(1), 7(1)(b)(ii), 10(4), 17, 37




Checklists, Templates and Tools

‚Ä¢   Checklist for Agents Agreements

‚Ä¢   Checklist for Information Sharing Agreements




                                                   269
Managing Contracts and Agents

                      CHECKLIST FOR AGENTS AGREEMENTS

      Bind your agents to:

      ‚Ä¢   name someone to be responsible for privacy compliance,

      ‚Ä¢   only use the information you share with them as needed to fulfill the contract,

      ‚Ä¢   only disclose information you or the law allows,

      ‚Ä¢   put effective administrative, technological and physical safeguards in place to
          stop theft, loss and unauthorized access, copying, modification, use, disclosure
          or disposal of information that are at least as rigorous as your own and those
          offered to the agents‚Äô other clients,

      ‚Ä¢   only give access to subcontractors that you have approved, and only enter into
          subcontracts that have all of the security provisions contained in your contract
          with them,

      ‚Ä¢   educate their employees on privacy laws and policies and take reasonable
          steps to ensure employee compliance through staff training, confidentiality
          agreements and employee sanctions,

      ‚Ä¢   ensure that employees who are fired or resign return all information and
          cannot access applications, hardware, software, network and facilities
          belonging to either you or the agents,

      ‚Ä¢   remind exiting employees of their continued responsibility to maintain the
          confidentiality of the information,

      ‚Ä¢   use reasonable efforts, including virus protection software, to avoid viruses,
          worms, back doors, trap doors, time bombs and other malicious software,

      ‚Ä¢   maintain backup security and acceptable business recovery plans (including
          disaster recovery, data backup and alternate power),

      ‚Ä¢   follow all applicable privacy laws, including the Act,

      ‚Ä¢   comply with their own privacy policies,

      ‚Ä¢   share their privacy policy with you and send you any updates or changes made
          during the term of the contract,

      ‚Ä¢   refer anyone trying to access, correct or complain about their personal health
          information to your contact person,



270
                     Managing Contracts and Agents

‚Ä¢   let you inspect their premises and security practices to ensure they are
    following the law, your contract and privacy policies,

‚Ä¢   let you review their internal practices, books and records relating to their use
    and disclosure of your patients‚Äô information so you can ensure compliance,

‚Ä¢   review security regularly and address any threats revealed,

‚Ä¢   regularly report on compliance,

‚Ä¢   report any security breaches or incidents to you within an agreed time,

‚Ä¢   revoke any user‚Äôs access if security is breached and on your reasonable
    request,

‚Ä¢   give you a copy of your data when you ask for it,

‚Ä¢   securely discard or return any personal health information on your request,

‚Ä¢   comply with any sanctions for breaching the contract, including ending the
    contract or compensating you,

‚Ä¢   end the contract for not following it in a significant way,

‚Ä¢   return or destroy all information received or created in any form when the
    contract ends, and where this is not possible, keep the contract‚Äôs privacy
    measures in place to protect the remaining information, and

‚Ä¢   never deny you access to information you request because of your late or
    disputed payment for services.

Your contracts should also include:

‚Ä¢   your right to go to court for an order stopping an agent from violating privacy
    sections of the contract and an acknowledgement that you have been
    irreparably harmed,

‚Ä¢   your remedies for an agent‚Äôs breach of the contract, and

‚Ä¢   a clause making your agent responsible to you for any costs you pay because
    of your agent‚Äôs failure to sufficiently protect your patients‚Äô information, with
    insurance to back the clause up.




                                                                                       271
Managing Contracts and Agents

      When sharing personal health information with health information network
      providers, you must make sure your contract requires them to give you:

      ‚Ä¢   an electronic record of all accesses, uses and disclosures of the information,
          including the time and source of access, and

      ‚Ä¢   a written assessment of how the services they offer may threaten, make
          vulnerable or risk the security and integrity of the information, and how
          they impact privacy.‚Ä†




      ‚Ä†
        The information set out in bold italicized text is based on draft Regulations that have not yet been
      finalized. We will send you a replacement page with updated information once the Regulations
      are finalized.



272
                     Managing Contracts and Agents

       CHECKLIST FOR INFORMATION SHARING AGREEMENTS

To ensure that your information sharing agreements provide sufficient privacy
protection:

‚Ä¢   define all terms that may be unique to your agreement,

‚Ä¢   define ‚Äúpersonal health information‚Äù to include all of the information you will
    share,

‚Ä¢   describe the purpose for sharing the information,

‚Ä¢   refer to the law that allows you to collect and share the information,

‚Ä¢   list the kind of personal health information each party will share with the
    other,

‚Ä¢   identify the allowed uses for the shared information and require the other
    party to use the information for only those purposes,

‚Ä¢   describe exactly how you will share the information,

‚Ä¢   identify whether any information may be linked to or matched with other
    information,

‚Ä¢   agree to verify the information received to ensure it is accurate before using it,

‚Ä¢   set out the administrative, technological and physical safeguards needed to
    protect the security of the information (see the Checklist for Agent
    Agreements for examples of appropriate safeguards clauses),

‚Ä¢   place necessary restrictions on disclosure,

‚Ä¢   limit the agreement‚Äôs term to ensure information will be shared for only as
    long as necessary,

‚Ä¢   state how long personal health information will be kept and how it should be
    disposed of when the time comes, and

‚Ä¢   describe any process for ending the agreement before the agreed upon date.




                                                                                         273
274
Oversight
276
                                                                                                            Oversight

Table of Contents


Key Points............................................................................................................279

Privacy Breaches..................................................................................................280
    What is a Privacy Breach? .............................................................................280
    Avoiding a Privacy Breach ............................................................................280
    Addressing a Privacy Breach .........................................................................281
       Containment
       Notification
       Additional Steps
    Reviewing a Privacy Complaint ....................................................................283

The Commissioner‚Äôs Role....................................................................................284

The Commissioner‚Äôs Powers ...............................................................................284
   Responding to Privacy Complaints................................................................284
   Initiating Privacy Reviews.............................................................................285
   Conducting Privacy Reviews.........................................................................285
   Result of the Review......................................................................................286
   Offence and Sanctions ...................................................................................287

Related Sections of the Act..................................................................................288

Checklists, Templates and Tools .........................................................................289
       Sample Inventory of Personal Health Information
       Commissioner Contact Information
       Decision Tree - Responding to Complaints About Privacy Breaches




                                                                                                                            277
Oversight




278
                                                                          Oversight


Key Points
‚Ä¢   The Information and Privacy Commissioner/Ontario (Commissioner) oversees
    compliance with the Act.

‚Ä¢   Patients have the right to file a complaint against you with the Commissioner
    (for example, if they believe you have breached their privacy, or if they are
    not satisfied with your response to their access or correction request).

‚Ä¢   The Commissioner responds to complaints that it receives about you, may
    review a complaint and may initiate a review on its own behalf.

‚Ä¢   The Commissioner has the power to make a range of orders, where
    appropriate.

‚Ä¢   A breach of privacy may entitle affected individuals to sue you for damages.

‚Ä¢   If you are found guilty of an offence, you could be fined.

‚Ä¢   You should implement procedures to avoid privacy breaches.

‚Ä¢   You should implement procedures to manage any privacy breaches. These
    procedures must cover, at the very least, containment and notification
    procedures.

‚Ä¢   You should implement procedures and make best efforts to resolve privacy
    complaints internally.

‚Ä¢   The procedures that you implement, as well as your response to a privacy
    breach or complaint, may positively affect the final outcome of any
    Commissioner‚Äôs review.




                                                                                    279
Oversight



      Privacy Breaches

      What is a Privacy Breach?
      A privacy breach happens when personal health information is collected, used,
      disclosed or disposed of in a way that does not comply with the Act.

      The most common privacy breaches are:

      ‚Ä¢   unauthorized collection of personal health information (information is
          collected without consent or legal authority),

      ‚Ä¢   unauthorized disclosure of personal health information through:

             ‚Äì loss (a file is misplaced),
             ‚Äì theft (a laptop is stolen), or
             ‚Äì mistake (a letter addressed to one person gets faxed to the wrong
                 person), and

      ‚Ä¢   unauthorized or unsecured disposal of personal health information (an
          unshredded file is left in the garbage).

      Individuals with reason to believe that you have breached or are about to breach
      or compromise privacy may complain to the Commissioner. And, if the
      Commissioner learns of a possible privacy breach on its own, it may initiate a
      privacy review.

      Because a complaint review is future-oriented, if a privacy breach is established,
      the Commissioner will help you to take the steps necessary to prevent another
      privacy breach from happening.

      Avoiding a Privacy Breach
      Adopt some or all of the following pro-active measures to prevent a privacy
      breach from occurring:

      Requirements

      ‚Ä¢   Designate a contact person.


280
                                                                              Oversight

‚Ä¢   Inform management and staff about privacy compliance.

‚Ä¢   Put policies and procedures in place that address privacy compliance.

Best Practices

‚Ä¢   Identify and document roles and responsibilities needed to prevent, and when
    necessary manage, a privacy breach as part of a general incident management
    plan.

‚Ä¢   Document information management and protection procedures and audit
    results so that you can avoid a privacy breach (and if necessary quickly
    determine the cause of a breach) and be able to demonstrate your good
    practices to the Commissioner.

‚Ä¢   Determine whether your new technologies, information systems and proposed
    programs or policies meet basic privacy requirements.

‚Ä¢   When in doubt, obtain advice from your lawyers.

‚Ä¢   Consult the Commissioner‚Äôs Policy and Compliance Department.

Addressing a Privacy Breach

If you learn of a privacy breach, you should take immediate action. Your first
two priorities are to contain the breach and notify anyone affected.


Containment

Requirement

‚Ä¢   Identify the extent of the privacy breach and take steps to contain it.

Best Practices

‚Ä¢   Retrieve the hard copies of any personal health information that has been
    disclosed.

‚Ä¢   Ensure that the person who was not authorized to receive the information did
    not make or keep copies of the information and get that person‚Äôs contact
    information in case you need to follow up.



                                                                                     281
Oversight

      ‚Ä¢   Determine whether the privacy breach allows unauthorized access to any other
          personal health information (for example, through an electronic information
          system). Take all appropriate steps (for example, change passwords) to stop
          any further breaches.

      Notification

      Requirement

      ‚Ä¢   Identify the people whose privacy has been breached.

      ‚Ä¢   Notify (by telephone or in writing) anyone whose privacy was breached
          (except for any of those who do not have the right to see or obtain their own
          information).

      ‚Ä¢   Specify what and how much personal health information was affected.

      ‚Ä¢   Explain immediate and long-term steps you and others have taken to rectify
          the breach.

      ‚Ä¢   Note down the unauthorized uses and disclosures in or linked to the affected
          personal health records.

      Additional Steps

      Best Practices

      ‚Ä¢   Identify the person who will manage the breach and the person who will
          communicate with the public about the breach. Tell staff who will play these
          roles. (The same person might play both roles in a smaller organization.)

      ‚Ä¢   Tell staff not to communicate with the public ‚Äî that is the communicator‚Äôs
          role. This is especially important when it comes to communicating with the
          media.

      ‚Ä¢   Make an assessment of the personal health information you hold and how you
          use and disclose it so that you can see what information might be involved in
          a breach.

          See the Sample Inventory of Personal Health Information for an example.

      ‚Ä¢   Notify management, staff, your contact person and your lawyers (if
          appropriate) of the privacy breach.



282
                                                                             Oversight

‚Ä¢   If the breach is a serious one, or raises questions of public concern, investigate
    what happened to:

       ‚Äì ensure immediate containment and notification requirements are
           addressed,

       ‚Äì identify the circumstances surrounding the breach, and
       ‚Äì review whether your policies and procedures adequately protect
           personal health information.

‚Ä¢   Address the situation systemically. In some cases, program- or institution-
    wide procedures may warrant review (for example, a misdirected fax may
    prompt review of your faxing process).

‚Ä¢   Inform the Commissioner‚Äôs registrar of the privacy breach and work
    constructively with the Commissioner‚Äôs staff.

‚Ä¢   Let the Commissioner know your findings and work together to make needed
    changes.

‚Ä¢   Train management and staff appropriately on privacy compliance.

Reviewing a Privacy Complaint
Depending on the circumstances, when the Commissioner reviews a complaint it
may:

‚Ä¢   ensure all containment and notification issues have been addressed,

‚Ä¢   discuss the complaint with the parties and obtain any relevant information,

‚Ä¢   interview individuals involved with the privacy breach and others who can
    provide relevant information,

‚Ä¢   seek and consider your representations on the issues raised by the privacy
    complaint,

‚Ä¢   ask you to report any actions you have taken,

‚Ä¢   review a copy of the personal health information involved,

‚Ä¢   research its precedents,

‚Ä¢   discuss options for resolution,


                                                                                         283
Oversight

      ‚Ä¢   recommend changes to current privacy policies, procedures and relevant
          documents,

      ‚Ä¢   issue a report or order at the end of the review, and

      ‚Ä¢   make recommendations on the privacy implications of any matter that is the
          subject of the review.




      The Commissioner‚Äôs Role
      The Information and Privacy Commissioner/Ontario (‚ÄúCommissioner‚Äù) oversees
      privacy and freedom of information legislation in Ontario, including the Act.
      Under the Act, the Commissioner:

      ‚Ä¢   responds to privacy complaints,

      ‚Ä¢   initiates privacy reviews,

      ‚Ä¢   authorizes certain information collection practices, where appropriate,

      ‚Ä¢   educates and communicates with the public about health privacy,

      ‚Ä¢   researches issues affecting health privacy, and

      ‚Ä¢   offers comments advice on current or proposed information practices, on
          request.




      The Commissioner‚Äôs Powers

      Responding to Privacy Complaints
      Generally, the Commissioner will inform you of any complaints it receives about
      you, although you may not receive notice where, for example, the Commissioner
      dismisses the complaint at an early stage of the process.

      If the Commissioner decides a formal review is warranted, the Commissioner may
      ask:




284
                                                                            Oversight

‚Ä¢   those who complained what other steps they have taken to resolve their
    concerns,

‚Ä¢   those who complained to try to resolve their complaint directly with you by a
    certain date, and

‚Ä¢   a mediator to facilitate resolving the complaint by a certain date.

If the complaint is not resolved, the Commissioner may decide to review the
complaint. The Commissioner will let you know if it plans to review a complaint.

The Commissioner may decide not to review the complaint if:

‚Ä¢   you have dealt with the complaint adequately,

‚Ä¢   the complaint should be dealt with in another manner,

‚Ä¢   the complaint was made too late,

‚Ä¢   the person who complained had insufficient personal interest in the complaint,

‚Ä¢   the complaint was frivolous, vexatious or made in bad faith, or

‚Ä¢   for any other reason the Commissioner considers proper.

If the Commissioner decides not to review the complaint, it will tell the
complainant why.

Initiating Privacy Reviews
The Commissioner may (on its own initiative) conduct a privacy review of any
matter where there are reasons to believe you may not be complying with the Act.
The Commissioner will tell you if it intends to conduct a review that affects you
or your operations.

Conducting Privacy Reviews
When conducting a review, the Commissioner will follow its own established
rules of procedure and receive any information it considers appropriate and
relevant to the complaint.

In certain circumstances, the Commissioner may also:

‚Ä¢   inspect your business (without a warrant or court order),



                                                                                     285
Oversight

      ‚Ä¢   inspect your residence (with a warrant or your consent),

      ‚Ä¢   review your books, records, information, information practices and other
          materials, and

      ‚Ä¢   compel others to provide information.

      The Commissioner needs your patients‚Äô consent before reviewing their personal
      health information that you hold. However, if:

      ‚Ä¢   the Commissioner believes that it must review the information to complete the
          review,

      ‚Ä¢   the public interest justifies reviewing the information without consent,

      ‚Ä¢   the Commissioner places conditions or restrictions on the review that the
          Commissioner determines is warranted, and

      ‚Ä¢   the Commissioner tells you in writing of its decision to review information
          without consent, and its reasons for and conditions and restrictions on, this
          review,

      then, consent is not necessary.

      Result of the Review
      Depending upon the nature of the complaint and the result of its review, the
      Commissioner may order you to:

      ‚Ä¢   give access to the requested record to the person who complained,

      ‚Ä¢   make the correction that the person who complained asked for,

      ‚Ä¢   perform a duty imposed by the Act,

      ‚Ä¢   stop collecting, using or disclosing personal health information in breach of
          the Act or a contract,

      ‚Ä¢   dispose of records of personal health information collected, used or disclosed
          in breach of the Act or a contract (so long as doing so would not negatively
          affect any individual‚Äôs health care),

      ‚Ä¢   change, stop or never start a particular information practice that breaches the
          law, or




286
                                                                          Oversight

‚Ä¢   put an information practice in place to comply with the law.

The Commissioner may also order those acting on your behalf or with you (your
agents) to take action to ensure you comply with its order against you.

The Commissioner may make recommendations on how anything subject to the
review affects privacy.

The Commissioner will provide a copy of any comments, recommendations or an
order made, together with the terms and reasons, to:

‚Ä¢   you,

‚Ä¢   the complainant (if there is one),

‚Ä¢   other persons to whom the order is directed,

‚Ä¢   the body or bodies that regulate you,

‚Ä¢   any person whom the Commissioner considers appropriate.

If the Commissioner makes no order after its review, the Commissioner will tell
both you and the person who complained why.

You may appeal the Commissioner‚Äôs orders (except access and correction orders)
to the court on questions of law. The court may order the Commissioner to take
actions that the Commissioner is authorized to take under the Act, confirm the
Commissioner‚Äôs order or, if necessary, vary or set the order aside. Once all
avenues of appeal have been exhausted, or if no appeal is made, an order may be
filed with the court, making it an enforceable judgment of the court.

If new facts come to light following the Commissioner‚Äôs order, the Commissioner
may cancel or change its order or make a further order, even if the original order
has been filed with the court. All affected parties will receive notice of the new
order. You or other parties may appeal the new order.

You may ask the court to review an order concerning access or correction by way
of an application for judicial review.

Offence and Sanctions
If you violate the law, you may face:

‚Ä¢   a Commissioner‚Äôs order,



                                                                                     287
Oversight

      ‚Ä¢   a fine for an offence, and/or

      ‚Ä¢   a lawsuit for damages.

      Note: Not all privacy breaches are subject to fines.

      It is an offence for you to:

      ‚Ä¢   wilfully collect, use or disclose personal health information in breach of the
          law,

      ‚Ä¢   dispose of a personal health record to evade an access request,

      ‚Ä¢   dispose of a personal health record in a manner that is not secure,

      ‚Ä¢   wilfully obstruct or mislead the Commissioner in performing its duties under
          the Act,

      ‚Ä¢   wilfully fail to comply with a Commissioner‚Äôs order, or

      ‚Ä¢   discipline or disadvantage an employee for trying to comply with the Act.

      If prosecuted and convicted of an offence:

      ‚Ä¢   a hospital or physician could be fined up to $250,000 or $50,000 respectively,
          and
      ‚Ä¢   the hospital‚Äôs officers, members, employees or other agents who authorized or
          could have prevented the offence may be fined $50,000, whether or not the
          hospital itself is prosecuted or convicted.

      A breach of privacy may entitle affected individuals to sue you for damages for:

      ‚Ä¢   actual harm a privacy breach caused, or

      ‚Ä¢   mental anguish (up to $10,000) where wilful or reckless behaviour caused the
          breach.




      Related Sections of the Act
      2, 3, 4, 12(2), 12(3), 16(2), 17(3), 54(8), 56-72




288
                                                                      Oversight




Checklists, Templates and Tools

‚Ä¢   Sample Inventory of Personal Health Information

‚Ä¢   Commissioner Contact Information

‚Ä¢   Decision Tree ‚Äì Responding to Complaints about Privacy Breaches




                                                                             289
Oversight

         SAMPLE INVENTORY OF PERSONAL HEALTH INFORMATION

                                    Info                   Backup
      Name       Information                  Location                Security     Retention     Notes
                                  Steward                  Copies

      Master    Patient info:     John Q.   UNIX           Offsite   Password      Archive     Accessed
      Patient   - Name            Deere     server at      storage   protected     after z     via EHR
      Health    - Contact info              College                  via EHR       years       application
      Records   - Health #                  Street                   application
                - Family info
                - Health                                             Encrypted
                history                                              in storage




                        COMMISSIONER CONTACT INFORMATION


                                 The Commissioner can be reached at:


                                     Telephone: (416) 326-3333

                                     Email:             info@ipc.on.ca

                                     Website:           www.ipc.on.ca




290
                                           Oversight

DECISION TREE - RESPONDING TO COMPLAINTS
        ABOUT PRIVACY BREACHES




                                                  291
292
Glossary
                                                                             Glossary

This glossary explains the meaning the following terms have when used in this
Toolkit.




A
‚ÄúAccess‚Äù means patients‚Äô ability to examine or obtain information about
themselves.

‚ÄúAct‚Äù means the Personal Health Information Protection Act, 2004 and the
regulations made under it.

‚ÄúAgent‚Äù means any person that a health information custodian authorizes to act
for it to deal with other people‚Äôs personal health information, whether or not the
agent has authority to bind the custodian, is employed by the custodian, or is
being paid.

‚ÄúAttorney for personal care‚Äù means someone who can legally act for
another person because they have a power of attorney for personal care under the
Substitute Decisions Act.

‚ÄúAttorney for property‚Äù means someone who can legally act for another
person because they have a continuing power of attorney for property under the
Substitute Decisions Act.

‚ÄúAudit‚Äù means to conduct an independent review and examination of system
records, activities and practices to test the adequacy and effectiveness of data
security and data integrity procedures, to ensure compliance with health records
policy and operational procedures.

‚ÄúAuthentication‚Äù means the procedure for establishing the identity of a user.

‚ÄúAuthorization‚Äù means the procedure to establish which resources and
information a user may access and what actions they are allowed to perform on
those resources such as reading, updating, creating or deleting information.




                                                                                     295
Glossary


      B
      ‚ÄúBiometrics‚Äù means the use of methods of authenticating or verifying an
      individual‚Äôs identity based upon a physical or behavioural characteristic, such as
      retina patterns or skin characteristics.

      ‚ÄúBoard‚Äù means the Consent and Capacity Board created under the Health Care
      Consent Act.



      C
      ‚ÄúCapable‚Äù means mentally able to make decisions for oneself.

      ‚ÄúCapacity‚Äù means the mental ability to make decisions for oneself.

      ‚ÄúCollect‚Äù means to gather, receive or obtain personal health information in any
      way and from anyone.

      ‚ÄúCollection‚Äù means the gathering, receiving or obtaining of personal health
      information in any way and from anyone.

      ‚ÄúCommissioner‚Äù means the Information and Privacy Commissioner/Ontario
      appointed by statute.

      ‚ÄúConditional consent‚Äù means a patient‚Äôs consent to the collection, use or
      disclosure of personal health information on which the patient has placed a
      restriction. See the Consent section for additional information on conditional
      consent.

      ‚ÄúContact Person‚Äù means the person designated to assist you in meeting your
      privacy obligations and to respond to patient inquiries about privacy-related
      matters.




296
                                                                             Glossary


D
‚ÄúDe-identified information‚Äù means health information from which
personally identifying information has been removed, and for which no means
exists to re-identify patients.

‚ÄúDisclose‚Äù means to release or make personal health information available to
another person, organization or health information custodian; it does not mean to
use the information.

‚ÄúDisclosure‚Äù means the release or making available of personal health
information to another person, organization or health information custodian; it
does not mean the use of the information.




E
‚ÄúEncryption‚Äù means using recognized techniques to transform plain electronic
information into an unintelligible form that requires a special key in order to
transform it back into the intelligible format.




F
‚ÄúFirewall‚Äù means a combination of hardware and software, used to protect a
network from unauthorized access, intrusion or traffic.




G
‚ÄúGuardian of property‚Äù means a person appointed to act as guardian of
another person‚Äôs property, or a statutory guardian of property under the Substitute
Decisions Act.

‚ÄúGuardian of the person‚Äù means a person appointed to act as guardian of
another person under the Substitute Decisions Act.



                                                                                      297
Glossary



      H
      ‚ÄúHealth card‚Äù means a card provided to a person by the Ontario Health
      Insurance Plan to identify the card holder as entitled to health care benefits in
      Ontario.

      ‚ÄúHealth care‚Äù means treating, observing, examining, assessing or caring for a
      person for a health-related purpose and includes to:

      ‚Ä¢   diagnose, treat or maintain the person‚Äôs physical or mental condition,

      ‚Ä¢   prevent disease or injury,

      ‚Ä¢   promote health, or

      ‚Ä¢   provide a service as part of palliative care.

      Health care includes compounding, dispensing or selling drugs, devices,
      equipment or any other item prescribed to an individual. It also includes any
      community service a service provider performs ‚Äì see the Long-Term Care Act.

      ‚ÄúHealth care practitioner‚Äù means:
      ‚Ä¢   a member of any profession that the Regulated Health Professions Act covers
          who provides health care,

      ‚Ä¢   anyone registered as a drugless practitioner under the Drugless Practitioners
          Act who provides health care,

      ‚Ä¢   a member of the Ontario College of Social Workers and Social Service
          Workers who provides health care, and

      ‚Ä¢   anyone else who primarily provides health care for which they get paid.

      ‚ÄúHealth information custodian‚Äù means any person or organization who
      controls other people‚Äôs personal health information as part of their role as:

      ‚Ä¢   a health care practitioner or operator of a group practice of health care
          practitioners,

      ‚Ä¢   a service provider who provides a community service under the Long-Term
          Care Act,


298
                                                                             Glossary

‚Ä¢   a community care access corporation under the Community Care Access
    Corporations Act,

‚Ä¢   someone who operates one of the following facilities, programs or services:

       ‚Äì a hospital under the Public Hospitals Act, a private hospital under the
           Private Hospitals Act, a psychiatric facility under the Mental Health
           Act, an institution under the Mental Hospitals Act or an independent
           health facility under the Independent Health Facilities Act,

       ‚Äì an approved charitable home for the aged under the Charitable
           Institutions Act, a placement co-ordinator under the Charitable
           Institutions Act, a home or joint home under the Homes for the Aged
           and Rest Homes Act, a placement co-ordinator under the Homes for the
           Aged and Rest Homes Act, a nursing home under the Nursing Homes
           Act, a placement co-ordinator under the Nursing Homes Act or a care
           home under the Tenant Protection Act,

       ‚Äì a pharmacy under the Drug and Pharmacies Regulation Act,
       ‚Äì a laboratory or specimen collection centre under the Laboratory and
           Specimen Collection Centre Licensing Act,

       ‚Äì an ambulance service under the Ambulance Act,
       ‚Äì a home for special care under the Homes for Special Care Act, or
       ‚Äì a centre, program or service for community health or mental health
           whose primary purpose is to provide health care,

‚Ä¢   an evaluator under the Health Care Consent Act or an assessor under the
    Substitute Decisions Act,

‚Ä¢   a medical officer of health or a board of health under the Health Protection
    and Promotion Act,

‚Ä¢   the Minister or Ministry of Health and Long-Term Care, and

‚Ä¢   any other person described as a health information custodian under the
    regulations to the Act with custody or control of personal health information
    as part of performing powers, duties or work.

‚ÄúHealth number‚Äù means the number that OHIP assigns to an insured person
under the Health Insurance Act.



                                                                                    299
Glossary


      I
      ‚ÄúIdentifying information‚Äù means any information that identifies an individual
      or that one could reasonably foresee might be used either on its own or with other
      information to identify an individual.

      ‚ÄúIncapable‚Äù means mentally unable to make decisions for oneself.

      ‚ÄúIncapacity‚Äù means the mental inability to make decisions for oneself.

      ‚ÄúIndividual‚Äù means any living or deceased person about whom personal health
      information was or will be collected or created.

      ‚ÄúInformation practices‚Äù means a health information custodian‚Äôs policies
      concerning when, how and why the health information custodian routinely
      collects, uses, modifies, discloses, retains or disposes of personal health
      information, and the administrative, technological and physical safeguards and
      practices maintained to protect personal health information.

      ‚ÄúIT‚Äù means information technology and refers to any electronic computing or
      network technology, including both hardware and software.

      ‚ÄúIT infrastructure‚Äù means the collective set of information technology tools
      and technologies, especially networks and servers, that an organization uses to
      support its operations.




      L
      ‚ÄúLock box‚Äù is not defined in the Act. Lock box describes the limits that
      patients can place on the disclosure of their personal health information. See the
      Managing Health Information section for additional information on lock boxes.

      ‚ÄúLogical security‚Äù means software and procedural measures designed to
      provide protection of IT resources and electronically stored information against
      deliberate and accidental logical threats, including measures such as passwords
      and firewalls.




300
                                                                            Glossary


M
‚ÄúMalicious software‚Äù means software designed to corrupt, destroy, take over
or deny availability to information technology resources. Malicious software
includes viruses, worms, Trojan Horses, logic bombs and other types of harmful
code.

‚ÄúMinister‚Äù means the Minister of Health and Long-Term Care.



N
‚ÄúNeed to know‚Äù means the principle that a staff member should have access
only to the personal health information needed to perform a particular function.




P
‚ÄúPartner‚Äù means one of two persons who have lived together for at least one
year in a close personal relationship of primary importance to both people in the
relationship.

‚ÄúPerson‚Äù means an individual person, a partnership, an association and any
other entity.

‚ÄúPersonal health information‚Äù means oral or recorded identifying
information about someone that relates to:

‚Ä¢   a person‚Äôs physical or mental health or family health history,

‚Ä¢   health care an individual receives, including who provided the health care,

‚Ä¢   a plan of service for an individual under the Long-Term Care Act,

‚Ä¢   an individual‚Äôs eligibility for health care payments or the payments made for
    an individual‚Äôs health care, and

‚Ä¢   an individual‚Äôs donation of any body part or bodily substance or anything
    derived from testing or examining a donated body part or bodily substance.


                                                                                    301
Glossary

      Personal health information also includes:

      ‚Ä¢   an individual‚Äôs health number,

      ‚Ä¢   anything that identifies an individual‚Äôs substitute decision-maker, and

      ‚Ä¢   anything that identifies an individual and that is contained in a personal health
          record.

      Personal health information does not include records maintained for human
      resources purposes.

      ‚ÄúPhysical security‚Äù means physical measures designed to provide physical
      protection of resources against deliberate and accidental threats, to prevent
      unauthorized access to equipment, installations, documents, files and records,
      including measures, such as door locks, camera, passcard readers, safes and
      security guards taken to control access to restricted areas.

      ‚ÄúPrivacy‚Äù means the right of individuals to determine for themselves when, how
      and to what extent personal information about the individuals is communicated,
      and to be secure from unauthorized use or disclosure of their personal
      information.

      ‚ÄúProceeding‚Äù means any proceeding before a court, tribunal or committee of a
      professional regulatory body.

      ‚ÄúProvincially funded health resource‚Äù means any health-related or
      prescribed service, subsidy or other benefit that the Ontario government wholly or
      partially funds.




      R
      ‚ÄúRecord‚Äù means an information record in any form or media, including written,
      printed, photographic or electronic form, but excluding computer programs and
      other mechanisms that produce a record.

      ‚ÄúRelative‚Äù means one of two people related to each other by blood, marriage or
      adoption.

      ‚ÄúResearch‚Äù means a systematic investigation to develop or establish principles,
      facts or knowledge including developing, testing and evaluating research.


302
                                                                                Glossary

‚ÄúResearcher‚Äù means anyone who conducts research.

‚ÄúResearch Ethics Board‚Äù means a board established to approve research
plans under the Act.




S
‚ÄúSecurity‚Äù means the physical, technological and administrative protective
measures and techniques that are designed to ensure that personal health
information remains confidential, available and uncompromised. This includes
measures such as encryption, passwords, and firewalls designed to prevent
unauthorized access to information, to protect the integrity of computing
resources, and to limit the potential damage that can be caused by unauthorized
access.

‚ÄúSpouse‚Äù means one of two people whose relationship is recognized under
Ontario law and includes married people and people living together in common
law relationships.

‚ÄúSubstitute decision-maker‚Äù means a person that the Act authorizes to give
consent on another‚Äôs behalf to the collection, use or disclosure of the other
person‚Äôs personal health information.




T
‚ÄúThreat risk assessment‚Äù means a structured process to evaluate security
threats, vulnerabilities, probabilities and business impact to an organization‚Äôs
critical assets.




U
‚ÄúUse‚Äù means to handle or deal with personal health information but does not
mean to disclose personal health information.




                                                                                      303
Glossary

      ‚ÄúUser‚Äù means a person authorized to use specific resources of an organization‚Äôs
      IT infrastructure. A user usually accesses these resources through a User ID and a
      password.




      V
      ‚ÄúVirus‚Äù means a piece of malicious computer software. Anti-virus software is
      software designed to detect and prevent computer viruses from causing harm.




304
Appendix of Forms
Contents

Sample Written Statement of Information Practices

Sample Consent Form

Sample Withdrawal of Consent Form

Sample Confidentiality Agreement

Sample Consent to Disclose Personal Health Information Form

Sample Form to Request Access to Personal Health Record

Sample Letter for Extension to Comply with Request

Sample Refusal of Access Letter

Sample Request Form for Correction to Personal Health Record

Sample Application to Research Ethics Board

Sample Consent Form for Study Participant

Sample Consent Form for Fundraising

Sample Withdrawal of Consent Form for Fundraising
NOTE TO USER: When you use this Statement, you must ensure that you have included all
of your proposed uses and disclosures. If you use or disclose a patient‚Äôs personal health
information, without the patient‚Äôs consent, in a manner that is not described on the
Statement, you must: (a) inform the patient of this as soon as possible unless the patient
does not have a right of access to their personal health record, and (b) make and keep a note
of the use or disclosure in or linked to the affected patient‚Äôs personal health record.



    SAMPLE WRITTEN STATEMENT OF INFORMATION PRACTICES

Collection of Personal Health Information
We collect personal health information about you directly from you or from the person acting on
your behalf. The personal health information that we collect may include, for example, your
name, date of birth, address, health history, records of your visits to [the Hospital] and the care
that you received during those visits. Occasionally, we collect personal health information about
you from other sources if we have obtained your consent to do so or if the law permits.

Uses and Disclosures of Personal Health Information
We use and disclose your personal health information to:
‚Ä¢ treat and care for you,
‚Ä¢ get payment for your treatment and care (from OHIP, WSIB, your private insurer or others),
‚Ä¢ plan, administer and manage our internal operations,
‚Ä¢ conduct risk management activities,
‚Ä¢ conduct quality improvement activities (such as sending patients satisfaction surveys),
‚Ä¢ teach,
‚Ä¢ compile statistics,
‚Ä¢ fundraise to improve our healthcare services and programs,
‚Ä¢ comply with legal and regulatory requirements, and
‚Ä¢ fulfil other purposes permitted or required by law.


Your Choices                                       How to Contact Us
You may access and correct your personal           Our privacy contact person is .
health records, or withdraw your consent for       For more information about our privacy
some of the above uses and disclosures by          protection practices, or to raise a concern you
contacting us (subject to legal exceptions).       have with our practices, contact us at:
Important Information                              [Address, fax, email, telephone number and
                                                   website]
‚Ä¢   We take steps to protect your personal
    health information from theft, loss and        You have the right to complain to the
    unauthorized access, copying,                  Information and Privacy
    modification, use, disclosure and disposal.    Commissioner/Ontario if you think we have
‚Ä¢   We conduct audits and complete                 violated your rights. The Commissioner can
    investigations to monitor and manage our       be reached at:
    privacy compliance.                            [Address, fax, email, telephone number and
‚Ä¢   We take steps to ensure that everyone who      website]
    performs services for us protect your
    privacy and only use your personal health
    information for the purposes you have
    consented to.
NOTE TO USER: This Sample Consent Form provides a sample list of purposes for the collection, use and
disclosure of personal health information where express consent is required under the Act. You should consider
whether your intended purpose for the collection, use and disclosure of personal health information requires express
consent and amend this form to include all such purposes. If you choose to rely on express oral consent, no such
form is needed.




                                   SAMPLE CONSENT FORM

             Consent to the Collection, Use and Disclosure
                   of Personal Health Information
I, _________________________________, have reviewed the [Hospital]‚Äôs written statement
concerning the collection, use and disclosure of personal health information.
I understand that the [Hospital] is seeking my consent for it to collect, use and/or disclose my
personal health information from me or from the person acting on my behalf to:
          conduct patient satisfaction surveys,
          teach outside the [Hospital],
          fundraise for the [Hospital]‚Äôs charitable activities, and
          compile statistics (that are not otherwise required by law to be compiled).

I understand that the [Hospital] will only collect, use and disclose my personal health information
with my consent [as set out in its privacy policy] unless a particular collection, use or disclosure
is permitted or required by law without my consent.
I also understand that I can refuse to sign this consent form. I can also withdraw my consent any
time by writing to .
I hereby authorize [Hospital] to collect, use and disclose my personal health information for the
purposes that I have checked-off above.


Name:

Address:

Tel. Home:                                                         Tel. Work:

Signature:                                                               Date:
                                                                        FOR INTERNAL OFFICE USE ONLY
                                                                        Patient ID. No. _________



                 SAMPLE WITHDRAWAL OF CONSENT FORM

                              Withdrawal of Consent
I, ___________________________________, wish to withdraw my consent to any further use or
disclosure by [Hospital/Physician name] of my personal health information for: (Please check
all that apply)

         Conducting patient satisfaction surveys

         Teaching outside the [Hospital]

         Compiling statistics (other than as required by law)

         Fundraising

I wish to place the following conditions on any further use or disclosure of my personal health
information:




(Please specify conditions)



This withdrawal of consent does not have retroactive effect nor does it affect the uses and
disclosures of personal health information collected by [Hospital/Physician Name] where the
uses and disclosures are permitted or required by law without consent.


Name:

Address:

Tel. Home:                                                Tel. Work:

Signature:                                                      Date:
                     SAMPLE CONFIDENTIALITY AGREEMENT



NOTE TO USER: Modify this sample agreement to suit your institution and your needs.
Review with your lawyers before releasing.


I acknowledge that I have read and understood the [ ] policies and procedures on privacy,
confidentiality and security.

I understand that:

‚Ä¢   all confidential and/or personal health information that I have access to or learn through my
    employment or affiliation with [ ] is confidential,

‚Ä¢   as a condition of my employment or affiliation with [ ], I must comply with these policies
    and procedures, and

‚Ä¢   my failure to comply may result in the termination of my employment or affiliation with [ ]
    and may also result in legal action being taken against me by [ ] and others.

I agree that I will not access, use or disclose any confidential and/or personal health information
that I learn of or possess because of my affiliation with [ ], unless it is necessary for me to do so
in order to perform my job responsibilities. I also understand that under no circumstances may
confidential and/or personal health information be communicated either within or outside of [ ],
except to other persons who are authorized by [ ] to receive such information.

I agree that I will not alter, destroy, copy or interfere with this information, except with
authorization and in accordance with the policies and procedures.

I agree to keep any computer access codes (for example, passwords) confidential and secure. I
will protect physical access devices (for example, keys and badges) and the confidentiality of any
information being accessed.

I will not lend my access codes or devices to anyone, nor will I attempt to use those of others. I
understand that access codes come with legal responsibilities and that I am accountable for all
work done under these codes. If I have reason to believe that my access codes or devices have
been compromised or stolen, I will immediately contact the [ ].



      Name (Please Print)                     Signature                             Date
           SAMPLE CONSENT TO DISCLOSE PERSONAL HEALTH
                       INFORMATION FORM


I                                         hereby authorize
                                                                    (Name of hospital/physician‚Äôs office)

to disclose the following personal health information:




     (Description of personal health information to be disclosed and dates of contact/hospitalization)

to


                      (Name and address of person/agency requesting information)

from the records of
                                       (Name of Patient)                               (Birth date)

Mailing Address of Patient:



I understand that this personal health information is to be used only by the
recipient for the purposes of:




Date:

I hereby waive any and all claims against [insert name of hospital/physician‚Äôs
office] in connection with the disclosure of this personal health information.

Witness:                                          Signed by:
                                                                (Patient or Substitute Decision-Maker)

Date:
                                                                      (Relationship to the Patient)
                     SAMPLE FORM TO REQUEST ACCESS
                       TO PERSONAL HEALTH RECORD


Information and Instructions

We will provide you with access to your personal health record, unless a legal exception applies.
We will review all health record access requests, and will make every effort to respond to your
request in a timely fashion. Please complete Parts A and B of this Form. Part C is for our internal
use. For information about our privacy protection practices, contact at: [Address, fax, email
and telephone number.]


PART A: REQUESTOR INFORMATION

Patient Contact Information:


Last Name                                     First Name                                   Initials

Mailing Address

Telephone Number                              Date of Birth

Hospital ID Number

If you are a substitute decision-maker, your contact information:


Last Name                                     First Name                                   Initials

Mailing Address

Telephone Number

Note:    Include copies of documents that provide your authority as a substitute decision-maker.

PART B: ACCESS REQUEST

1.       Please describe what you need and include details that will help us locate the record (e.g.,
         dates, name of healthcare provider, etc.).




2.       How would you prefer to access this information? Please check off:
                  Receive hard copies of originals
                  Receive electronic copies of originals (please supply storage medium)
                  Examine originals in the facility


Signature                         Name (print)                       Date
PART C: RESPONSE TO ACCESS REQUEST (For Internal Use Only)

1.       Information Regarding Receipt and Initial Review of Request


         Date Request Received

2.       Information Regarding Response


         Date Response Issued

                  Access request granted
                  Access request not granted
                  Access request granted in part

         If complete access request was not granted, reason for refusing the request/part of the
         request.




3.       Information Regarding Extension

If an extension to the access request response was required, please indicate:


     Date of Extension                Reason for Extension               Date Patient Notified




4.       Processed by:


Signature                         Name (print)                       Title
   SAMPLE LETTER FOR EXTENSION TO COMPLY WITH REQUEST



[Name and Address of Health Care Facility]
XXX Street
City/Town, Ontario
ABC 123

Date

Dear Sir/Madam,

RE:     Request for Access to Personal Health Record of [Patient‚Äôs Name]

Health Record #:

An extension of _____ days is required to address your request to access the personal health
record of the individual named above. While every effort is made to retrieve the information
requested, this extension is required for the following reason:

[Reason for extension]

If you have any concerns or questions please contact _________________ (Contact Person). If
they are unable to resolve your concerns, you may file a complaint with the Information and
Privacy Commissioner/Ontario, who may be contacted at:

[Contact information for the Information and Privacy Commissioner/Ontario]


Sincerely,




[Name, Title]
                    SAMPLE REFUSAL OF ACCESS LETTER



[Name and Address of Health Care Facility]
XXX Street
City/Town, Ontario
ABC 123

Date

Dear Sir/Madam,

RE:      Request for Access to Personal Health Record of [Patient‚Äôs Name]

Your request for access to the personal health record has been declined for the following reason:

[Reason for declining request]

If you have any questions or concerns please contact __________________ (Contact Person). If
we are unable to resolve your concerns, you may contact the Information and Privacy
Commissioner/Ontario, who may be contacted at:

[Contact information for the Information and Privacy Commissioner/Ontario]

Sincerely,



[Name, Title]
                 SAMPLE REQUEST FORM FOR CORRECTION
                     TO PERSONAL HEALTH RECORD


Information and Instructions

We will correct health record information if it is demonstrated, to our satisfaction, that the record
is not correct or complete for the purpose for which we collect, use or disclose the information.
We will make every effort to respond to your request in a timely fashion. Please complete Parts A
and B of this Form. Part C is for our internal use. For information about our privacy protection
practices, contact [ ] at: [Address, fax, email and telephone no.]


PART A: REQUESTOR INFORMATION

Patient Contact Information:


Last Name                                             First Name                     Initials


Mailing Address


Telephone Number                            Date of Birth             Hospital ID Number

If you are a substitute decision-maker, your contact information:


Last Name                                             First Name                     Initials


Mailing Address


Telephone Number
Note: Include copies of documents that provide your authority as a substitute decision-maker.

PART B: CORRECTION REQUEST

1.       List or attach the correction requested, with reasons for the correction.


          Requested Correction                           Reasons for Correction




2.       How do you wish to receive notice of the correction (in writing, by telephone)?
3.    Would you like us to give notice of the correction, to the extent reasonably possible, to
      others to whom we have disclosed the incorrect information? (We will only do so if this
      notice will affect your health care or otherwise benefit you.)

               Yes
               No

       Signature                      Name (print)                   Title

       Date



PART C: CORRECTION REQUEST RESPONSE (For Internal Use Only)

               Correction made
               Correction not made
               Refusal letter (with reasons) sent
               Statement of Disagreement attached to record
               Date of Response ___________________

1.    List names, contact information and comments of any individuals consulted




2.    If correction was not made, provide reasons:




3.    If an extension to the correction request response was required, please indicate:


      Date of Extension        Reason for Extension                 Date Patient Notified
                                                                    of Extension




4.    Notice of correction provided to others to whom incorrect information was disclosed.
      List names:

5.    Processed by:


       Signature                      Name (print)                   Title
                                                      REB File No.:
                                                      Date:

         SAMPLE APPLICATION TO RESEARCH ETHICS BOARD

A.      GENERAL INFORMATION

PRIMARY INVESTIGATOR

Name                                 Signature

Dept./Div.                           Position

Qualification:                       Email Address

CO-INVESTIGATOR

Name                                 Signature

Dept./Div.                           Position

Qualification:                       Email Address

CO-INVESTIGATOR

Name                                 Signature

Dept./Div.                           Position

Qualification:                       Email Address




OTHER RESEARCH TEAM MEMBERS WHO ARE NOT CO-
INVESTIGATORS Please type names, roles and qualifications (signatures
not needed)
B.   DETAILS OF PROJECT

1.   Project Title _________________________________________________

2.   Is this project funded? _________________________________________

              NO

              YES

3.   Sponsor ____________________________________________________

4.   Duration of Funding: from __/__/__ to __/__/__

5.   Conflict of Interest Declaration ‚Äì Do you have any conflicts of interest
     (actual, apparent, perceived or potential) relating to this project?*

              NO

              YES

     Description of conflict of interest

     Mandatory Signature

     (Application will be returned without signature)
     *Conflicts of interest include but are not limited to the following situations and also must
     be disclosed under institutional policy for review: If you or any of the involved research
     team members or your/their dependants have:

     (1) employment or consulting arrangements and/or a financial interest in the sponsor of
     the study, or with proposed subcontractors, vendors or collaborators; or

     (2) a financial interest in the subject of the study.

6.   Protocol:

     a)       Nature



     b)       Objectives



     c)       Methods
     d)     Statistical Analysis



     e)     Anticipated Public or Scientific Benefit



     f)     Duration of Research



     g)     Foreseeable Harms and Benefits of Research (describe how harms
            will be addressed)

C.   INFORMATION REQUESTED

1.   What patient information do you require?




2.   What patient information source are you accessing?

          Health Records           Specify which
          Clinic/Office Files

          Electronic Database      Specify which

          Outside Institution      Specify which

          Other                    Specify which




3.   Proposed number of research subjects _____________________________
4.   Are you requesting information that identifies or potentially identifies
     individuals?

               NO
               YES

     If yes, explain why you cannot use anonymized or aggregate information:




5.   Have you obtained consent from the individuals to collect and use the
     identifying information on this project?

          YES        Attach a sample consent form

          NO

      If no, explain why




6.   What security measures will be in place to protect the information during
     transmission?




D.   INFORMATION LINKAGE

1.   Does your project involve linking any information from this request to
     other information?

               NO
               YES

     If yes,

     Describe what information is to be linked:
     Describe what type of linkage is required:




     Describe the rationale for this linkage:




2.   Have you received approval from the other information sources including
     division/department head authorization to conduct this linkage?

        NO

        YES       Please attach the approval(s)


3.   Will the information be retained in linked form?

        NO

        YES


4.   When will the information be de-identified after the linkage?




E.   DISSEMINATION OF ANALYSIS AND/OR REPORTS:

1.   How do you plan to disseminate and/or publish the results of your
     analyses?




2.   What is the expected date of dissemination and/or publication?
F.   DISCLOSURE AVOIDANCE PRACTICES

1.   How will you ensure that information will be aggregated prior to
     disclosure, to the level required in the information sharing agreement,
     confidentiality agreement, privacy policy and other applicable policies and
     procedures?

     __________________________________________________________

     __________________________________________________________

      __________________________________________________________

2.   Attach information collection form or list of fields (mandatory: application
     will be returned if this information has not been included)
     Please note that the content of the form should be adequate to answer
     the research questions.

3.   Are any sensitive issues raised in this study or its publication (e.g. HIV
     status, mental health status, subjects identifiable, pedigrees, other)

         NO

         YES       Please specify



G.   SECURITY AND ACCESS

1.   The information obtained from the records described above will be used
     for the outlined research purposes only:

         NO        If no, a separate request must be submitted
         YES

2.   List all of the persons who will have access to the records in an
     individually recognized form for the research purpose described and why
     they need this access: (name and role in research)

     __________________________________________________________

     __________________________________________________________
3.   Have all these persons signed confidentiality agreements?

         NO

         YES        If no, please indicate when agreement will be signed

      _________________________________________________________

      _________________________________________________________


H.   SECURITY MEASURES

1.   Describe how you will keep information secure

                     Premises will be locked except when one or more of the
                     individuals named in E1(b) are present

                     Access to the premises will be controlled (passcards,
                     security clearances etc.)

                     Access to the information will be restricted to the
                     research team by:

                        Patient code      Files/Folders password    Computer password

                     Please provide name of password software

                     Other computer security methods that prevent
                     unauthorized access will be used

                         Encryption      Firewalls   Identifying Information
                                                     Scrambled/De-linked

                     Other (Describe):

                     Staff will be trained regarding privacy

                     Staff will sign a confidentiality agreement

                     Other, explain

      __________________________________________________________

      __________________________________________________________
2.   Will information remain within the institution?

          NO

          YES        If no, please indicate why and how information will be
                     exported outside

       _________________________________________________________

       _________________________________________________________

3.   Will system files be backed up automatically?

            NO
            YES

     If yes, please specify provisions that would be made for a private drive
     that cannot be accessed by anyone other than your research team, or that
     could not be backed up by computer support staff within your
     organization:

     __________________________________________________________

     __________________________________________________________
All original personal health records received from must be returned to
and all copies of personal health records that were made or received must be
destroyed in accordance with the information sharing agreement.

I certify that the information reported here is accurate and that the personal
health information will not be used for future projects without prior
approval of a research ethics board.


Primary Investigator Signature            Date



Division/Department Head                  Date
Signature of Approval


Do you plan on accessing information from another division/department?

       NO
       YES      If yes, authorization from the division/department head is
                requested

 Signature of Approval                     Date


In making this request, I acknowledge that failure to comply with the terms
and conditions of the information sharing agreement is cause for termination
of the agreement and, where applicable, a complaint to the Information and
Privacy Commissioner/Ontario.

Date                                      Signature of Requestor


               Research Ethics Board for Retrospective Research


Signature of REB Chair

Date of Approval                          Approval Expires

Level of Continuing Review:
          SAMPLE CONSENT FORM FOR STUDY PARTICIPANT

Note to User: This Sample Consent Form is intended to be used for retrospective
research. It is not appropriate to use it for clinical trials.


Study Title:
Primary Investigator:
Sponsor:
Background:                     [Insert details about the study.]
What is involved?
Benefits and Risks:

Voluntary: Participation in this study is completely voluntary. You can refuse
to sign this consent form and to participate in the study. You can also withdraw
your consent any time by writing to . Your care at will not be affected by
your decision.

Confidentiality:                 [Insert details about collection, use, disclosure,
                                 storage and disposal of personal health
                                 information.]

You have had this study explained to you and had an opportunity to ask questions.
You have been given a copy of this Consent Form. If you have any additional
questions about the study, please contact at . If you have any additional
questions about your privacy, please contact at .

Participant Consent

I agree to participate in this study.
I also permit to collect, use and disclose health information about me for the
purposes of this study.



Name of Participant (print)                Signature of Participant



Investigator‚Äôs Signature                  Signature of Substitution Decision-Maker
                                          (if required)

Date
                SAMPLE CONSENT FORM FOR FUNDRAISING

                Consent to the Collection, Use and Disclosure
              of Personal Information for Fundraising Purposes


We may use personal [demographic] information about you, including your name and mailing
address, in order to contact you to support our fundraising programs. We may also share this
information with [our hospital foundation], which will contact you on our behalf. By signing
below, you permit us to contact you with information on our fundraising campaigns at a later date.

You can refuse to sign this consent form. You can also withdraw your consent any time by
writing to . Your refusal or withdrawal of consent will in no way affect the care or treatment
that you receive at [Hospital].
Patient Consent

I, _________________________________ authorize [Hospital] to collect, use and disclose:
         (First and Last Name)

                  just my name and mailing address
                  my name, mailing address and the following personal information:
                  _____________________________________________________
[Note to User: Consider inserting the personal information you would like to use for
fundraising purposes, such as email address, work telephone number, etc.]

for use in fundraising relating to the [Hospital]‚Äôs charitable activities. I understand that you might
share information about me with the hospital foundation.


Name:

Address:



Tel. Home:                                      Tel. Work:

Signature:                                      Date:
                                                           FOR INTERNAL OFFICE USE ONLY

                                                                    Patient ID. No. _________



  SAMPLE WITHDRAWAL OF CONSENT FORM FOR FUNDRAISING

    Withdrawal of Consent to the Collection, Use and Disclosure of
           Personal Information for Fundraising Purposes


I, ___________________________________ no longer wish [Hospital] to use the
        (First and Last Name)
following personal information about me for fundraising purposes:

               my name and mailing address
               the following personal information:
               _____________________________________________________
               (check all that apply)




Name:

Address:



Tel. Home:                                 Tel. Work:

Signature:                                 Date:
Diagnostic Tool
                                                                                     Diagnostic Tool

Table of Contents


   Overview........................................................................................................335
   Purpose .........................................................................................................335
   Instructions.....................................................................................................335

   Survey Questions ...........................................................................................336
   1 ‚Äì General Privacy Compliance ...................................................................336
   2 ‚Äì Contact Person .........................................................................................337
   3 ‚Äì Consent ....................................................................................................338
   4 ‚Äì Managing Health Information..................................................................339
   5 ‚Äì Accessing Health Records .......................................................................341
   6 ‚Äì Correcting Health Records.......................................................................342
   7 ‚Äì Dealing With Health Information ............................................................343
   8 ‚Äì Security, First Steps .................................................................................344
   9 ‚Äì Security, People .......................................................................................345
   10 ‚Äì Security, Institutional.............................................................................347
   11 ‚Äì Security, Sustaining Security .................................................................350
   12 ‚Äì Research.................................................................................................352
   13 ‚Äì Fundraising ............................................................................................353
   14 ‚Äì Managing Contracts & Agents...............................................................354
   15 ‚Äì Oversight................................................................................................355

   Score Sheet and Analysis...............................................................................356
   Score Evaluation ............................................................................................358




                                                                                                                          333
Diagnostic Tool




334
                                                              Diagnostic Tool



Overview

Purpose
This Diagnostic Tool is designed to help you identify areas you may need to work
on to meet your privacy obligations. It is intended as a companion to the OHA
Privacy Toolkit and you will still need the Toolkit to address any gaps you
identify.

This version of the Diagnostic Tool is designed for larger healthcare institutions
and not for smaller medical offices.

Instructions
For each question, use the Score Sheet to circle the answer that best describes
your current situation. The number corresponding to each answer translates into
the following achievement levels:

‚Ä¢   0 ‚Äì Not adequate (don't know/haven't started)

‚Ä¢   1 ‚Äì Partially adequate (have made a start, have some key pieces in place)

‚Ä¢   2 ‚Äì Adequate (have covered the essentials)

‚Ä¢   3 ‚Äì Best practice (have achieved best practice status)

Some questions may not be applicable for your situation but if you find you are
marking off many as ‚Äúnot applicable‚Äù, re-examine your thought process as most
questions should be applicable under normal circumstances.

Consider adding handwritten notes for each question to explain your answer as
this will help you or others to make sense of it when it is reviewed at a later date.
This is especially important if you answer "Not Applicable".

Once you have completed the questions, follow the Score Evaluation below the
Score Sheet.

Note that an electronic version of the Diagnostic Tool is available on the OHA
web site.




                                                                                        335
Diagnostic Tool


      Survey Questions

      1 ‚Äì General Privacy Compliance
      a)       Do you have a written statement?

           0   No, we haven't had time to write one

           1   Yes, but it‚Äôs very basic

           2   Yes, we have a written statement that we make available to the public, which describes
               our information practices (in general terms), provides our contact person's contact
               information and describes our access, correction, inquiry and complaints procedures

           3   Above, plus our written statement describes our other privacy-related procedures



      b)       Have you developed procedures to support your written statement?

           0   No or don't know

           1   No, but we generally know what to do based on the written statement

           2   Yes, we have developed procedures to ensure that our information practices are
               followed and to help us deal with requests for access and corrections, as well as
               privacy-related inquiries and complaints

           3   Above, plus we have trained our internal agents (e.g., staff, volunteers and others) to
               follow our procedures, and we track the effectiveness of these procedures and modify
               them where necessary




336
                                                                  Diagnostic Tool


2 ‚Äì Contact Person
a)       Have you designated a contact person to assist you in meeting your privacy
         obligations?

     0   No or don't know

     1   No, but staff would know which senior manager to consult

     2   Yes, we have a designated a contact person, and we have made his/her contact
         information available to the public

     3   Above, plus we have provided the contact person with the appropriate resources to the
         do the job and we monitor his/her performance



b)       What are your contact person‚Äôs responsibilities?

     0   Not sure, we haven't developed a job description

     1   Our contact person handles all privacy-related matters

     2   Our contact person helps us to comply with the Act, responds to requests for access
         and corrections, deals with questions and complaints, and ensures our agents are
         informed of their legal duties under the Act

     3   Above, plus our contact person conducts privacy impact assessments, privacy audits
         and develops privacy-related policies, procedures and tools




                                                                                                 337
Diagnostic Tool


      3 ‚Äì Consent
      a)       What do you do to ensure your patients' implied consent is valid?

           0   Nothing in particular, or don't know

           1   We tell our patients that we need their personal health information in order to provide
               services to them

           2   We give our patients the information they need to understand why we collect their
               personal health information, how we may use it or disclose it, and what they must do to
               withhold or withdraw their consent

           3   Above, plus we post notices or place brochures about these matters in high traffic
               areas and waiting rooms



      b)       What do you do to ensure your patients' express consent is valid?

           0   Nothing in particular, or don't know

           1   We make sure the patient has the capacity to consent or has a substitute decision-
               maker who has the capacity and authority to consent on the patient's behalf

           2   We make sure the patient has the capacity to consent, or has a substitute decision-
               maker who has the capacity and authority to consent on the patient's behalf, and we
               ensure that the person giving consent know why we are collecting the information and
               how it will be used and/or disclosed

           3   Above, plus we also document the consent, whether it was given in writing or orally




338
                                                                    Diagnostic Tool


4 ‚Äì Managing Health Information
a)       Have you documented the purposes for which you collect your patient‚Äôs
         personal health information?

     0   No or don't know

     1   Purposes are documented but this information is not shared with patients

         A written statement outlining purposes for collection is available to patients upon
     2
         request

     3   A written statement outlining purposes for collection is posted in a public area



b)       Are there policies in place outlining who is authorized to use your patients‚Äô
         personal health information and for which purpose?

     0   No or don't know

     1   Policies are in place and they include consent requirements

     2   Above, plus staff receive training on the use of personal health information

         Above, plus compliance with the policies are built into agreements with third party
     3
         agents who use personal health information



c)       Is there a policy for disclosing information to family members and friends?

     0   No or don't know

         There is a policy in place but it is not communicated to patients, family members and
     1
         friends unless the situation warrants it

     2   A written policy is posted in a public area and communicated to patients

     3   Above, plus staff receive training on the policy




                                                                                                 339
Diagnostic Tool

      d)       Do your volunteers understand their obligations related to protection of privacy?

           0   No or don't know

               We don‚Äôt do anything formal, but our volunteers know they need to protect our patients‚Äô
           1
               privacy

           2   All volunteers receive training on privacy

           3   Above, plus volunteers are required to sign a non-disclosure agreement



      e)       Does your organization have an individual who is responsible for disclosure
               requests from third parties?

           0   No or don't know

               No, but the requester is directed to the supervisor of the clinical area the patient was
           1
               being treated in

               Yes, we have an individual with defined responsibilities to address third party requests
           2
               for disclosure and staff know who to direct requests to

               Above, plus this individual has the resources and training to address disclosure
           3
               requests



      f)       Do you have procedures and policies in place to address a patient request that
               part of their personal health record not be disclosed to others involved in their
               care? ("Lock Box")

           0   No or don't know

               We have procedures to flag information a patient wishes not to be disclosed, but we do
           1   not have policies in place to respond to a situation where we receive notice that
               information has been placed in a lock box

           2   Yes, we have procedures and policies in place and staff are aware of them

           3   Above, plus the patient is informed of our procedures and policies




340
                                                                     Diagnostic Tool


5 ‚Äì Accessing Health Records
a)        Is there a procedure in place for handling patient requests for access to their
          personal health information?

     0 No or don't know

     1 There is a procedure but it is not documented or it is not being followed

     2 There is a documented procedure that is easy to locate and used when needed

     3 Above plus relevant staff received training on the procedure



b)        Are patients informed who to contact if they want to see their personal health
          record?

     0 No or don't know

     1 Patients are informed orally if they ask

     2 A written statement is available telling patients who to contact and what do if they want to
       see their personal health record

     3 Above, plus the written statement is posted in a public area



c)        Is there a procedure for patients to complain about a decision to refuse access
          to a personal health record?

     0 No or don't know

     1 There is a procedure but it is not communicated to patients

     2 Patients receive a written statement outlining the complaint procedure when access is
       refused

     3 Above plus patients are told that they may contact the Commissioner if they are still not
       satisfied with the outcome




                                                                                                      341
Diagnostic Tool

      6 ‚Äì Correcting Health Records
      a)       Is there a procedure in place for handling patient requests for corrections to
               their personal health information?

           0   No or don't know

           1   There is a procedure but it is not documented or it is not being followed

           2   There is a documented procedure that is easy to locate and use when needed

           3   Above plus relevant staff received training on the procedure



      b)       Once a correction has been made, is there a procedure in place to ensure that
               anyone who uses the record or to whom the record has been disclosed is
               informed of the correction?

           0   No or don't know

           1   The patient must make a request and provide names and contact information for the
               people who he/she wants to be informed

           2   A notice is provided to anyone who currently uses the record

           3   Above, plus a notice is provided to anyone to whom the record has been disclosed



      c)       Is there a procedure for patients to include a description of their correction
               request to be included in their health record if the request was refused?

           0   No or don't know

           1   There is a procedure but it is not communicated to patients

           2   Patients are told that they can request a written statement outlining the correction
               request be included in their health record

           3   Above plus reasonable efforts are made to disclose the statement to anyone to whom
               the health record had been disclosed




342
                                                                   Diagnostic Tool

d)       Do you have a procedure for patients to complain about a refusal to correct a
         personal health record?

     0   No or don't know

     1   There is a procedure but it is not communicated to patients

     2   Patients receive a written statement outlining the complaint procedure when a request
         for correction is refused

     3   Above plus patients are told that they may contact the Commissioner if they are still not
         satisfied with the outcome




7 ‚Äì Dealing With Health Information
a)       Do you store patient personal health information in a secure manner?

     0   No or don't know

     1   Each physician stores records his/her patients‚Äô own records in the manner that best
         suits them

     2   We have documented standards for storage of personal health information and these
         standards are communicated to all staff

     3   Above, plus we have spot audits to confirm adherence to the storage standards



b)       Do you dispose of patient personal health information in a secure manner?

     0   No or don't know

     1   Each physician disposes of his/her patients‚Äô own records in the manner that best suits
         them

     2   We have documented standards for the disposal of both hard copies and electronic
         records in a manner that records cannot be recovered in any way

     3   Above, plus individuals disposing of records must confirm that they followed hospital
         procedures when completing the record of disposal nagement and has the resources
         and training to do their job well




                                                                                                     343
Diagnostic Tool

      8 ‚Äì Security, First Steps
      a)       Does your organization have a security policy?

           0   No or don't know

           1   We have some documented security practices but no overall policy

           2   We have a documented security policy approved by management

           3   Above, plus the policy is regularly reviewed and updated when necessary



      b)       Does your organization have someone with overall responsibility for security?

           0   No or don't know

           1   No but each senior manager would be responsible for security in their area

           2   We have a security officer with defined responsibilities and our staff know who this
               person is

           3   Above, plus our security officer has access to senior management and has the
               resources and training to do their job well



      c)       Have you identified all of the personal health information (electronic & hardcopy)
               that you need to protect?

           0   No or don't know

           1   We don't have an inventory but we know where most of the information is

           2   We have a complete inventory where information is stored and the information is
               classified as to its handling requirements

           3   Above, plus we assign staff members to keep the inventory current and make sure
               handling requirements are being followed




344
                                                                  Diagnostic Tool

9 ‚Äì Security, People
a)       Does your organization have security procedures for staff to follow?

     0   No or don't know

     1   We have documented security procedures in some but not all areas where we need
         them

     2   We have documented procedures for both physical and computer security that line up
         with our security policy

     3   Above, plus each procedure has a designated staff member who regularly reviews and
         updates it when necessary



b)       Do you equip your staff to carry out their security responsibilities?

     0   No, don't know or we don't have defined security responsibilities

     1   We make security information and tools available but we don't promote them

     2   We educate staff on their security responsibilities and provide them tools such as anti-
         virus software to be able to meet them

     3   Above, plus we have orientation and ongoing awareness training for security and we
         teach them how to use the tools provided



c)       Do your staff understand their security obligations and know what to do if they
         suspect a security problem?

     0   No or don't know

     1   We don't formally check but most people know and they know who to call if they
         suspect a problem

     2   All staff and contractors sign a confidentiality agreement and we have a published
         incident response process

     3   Above, plus we reinforce this by requiring that the agreement is signed again annually




                                                                                                    345
Diagnostic Tool

      d)       Do you control access to personal health information on a "Need to Know"
               basis?

           0   No or don't know

           1   We give general access only to appropriate staff and trust that they will only use the
               information they need

           2   We limit access as far as possible to what staff need for their jobs and we remove
               access as soon as it is no longer required

           3   Above, plus where we can't strictly limit access we spot-check to ensure no improper
               use




346
                                                                  Diagnostic Tool


10 ‚Äì Security, Institutional


a)       Do you have physical security controls in place to protect personal health
         information?

     0   No or don't know

     1   The information is locked up or supervised, we have after hours security and our IT
         facilities are protected

     2   Above, plus we have controlled access to the areas with the information at all times
         (supervision or a badge access system)

     3   Above, plus we have a clean-desk policy that is enforced and all staff must wear ID
         badges and challenge those without them



b)       Do you have Information Technology security controls in place to protect
         electronic personal health information?

     0   No or don't know

     1   All applications used to access the information require passwords

     2   Above, plus we have mandatory standards for passwords and do not allow user IDs to
         be shared

     3   Above, plus we use automation to enforce our password standards



c)       Do you have security controls in place to protect the computers used to access
         personal information?

     0   No or don't know

     1   Our users must use operating system passwords (for Windows, Mac, Linux etc.) on
         their computers

     2   Above, plus they must use power-on passwords and use locking screen savers when
         leaving their computers unattended

     3   Above, plus we perform spot-checks to make sure our users are following these rules




                                                                                                347
Diagnostic Tool

      d)       Do you have security controls in place to protect your computers from viruses

           0   No or don't know

           1   We supply anti-virus software to our users and we have a mechanism to communicate
               urgent security issues to our staff

           2   Above, plus we automate updates to the software and have an incident management
               process to respond to attacks which is know to all staff

           3   Above, plus we have a process to make sure long term solutions for problems are
               implemented where appropriate



      e)       If you provide staff with remote network access to your systems, do you provide
               a secure way for them to do so?

           0   No or don't know

           1   We provide a virtual private network, or equivalent, for remote staff access

           2   Above, plus we only allow access from computers with our standard software image
               that includes a personal firewall

           3   Above, plus we have mechanisms to make sure that our computers are running the
               correct software



      f)       Do you protect personal health information that you transport outside of your
               office environment?

           0   No or don't know

           1   We use reputable companies to transport the information

           2   Above, plus we use locked boxes or equivalent for hard copy information

           3   Above, plus we use tamper-evident packaging and check that the right quantities
               arrive at the destination




348
                                                                Diagnostic Tool


g)       Do you protect personal health information on computers being repaired, sold or
         disposed of?

     0   No or don't know

     1   We only use reputable companies for servicing, purchasing or disposing of our
         computers

     2   Above, plus we ensure personal health information is properly erased before these
         computers are released

     3   Above, plus we design our systems so that the minimum amount of personal health
         information is stored on personal computers




                                                                                             349
Diagnostic Tool


      11 ‚Äì Security, Sustaining Security
      a)       Do you make sure that all affiliates and third party providers meet your security
               requirements?

           0   No or don't know

           1   We don't check on this but we only deal with those who have good reputations and no
               past history of problems

           2   We make sure all our contracts spell out the security requirements we expect them to
               meet

           3   Above, plus we provide them with any information that may help them comply and we
               periodically check that they are complying



      b)       Do you have a plan to be able to restore critical personal health information if the
               primary copies are not available?

           0   No or don't know

           1   We make backup copies of all critical personal health information and test that the
               copies are usable

           2   Above, plus we have tested disaster and business recovery plans that will restore this
               information in crisis situations

           3   Above, plus we regularly re-test our disaster and business recovery plans to make
               sure they still work effectively



      c)       Do you have controls to make sure that changes to your IT environment don't
               introduce new security problems?

           0   No or don't know

           1   We have separate development/test/production IT environments and our
               developers/testers cannot change the production environment

           2   Above, plus we have a formal change control process which looks at the security
               implications of any proposed changes

           3   Above, plus we have security checks built into our IT development and procurement
               processes and we use a Threat Risk Assessment process




350
                                                                 Diagnostic Tool


d)       Do you regularly check that you are in compliance with your security policy?

     0   No, don't know or we don't have a security policy

     1   We don't have a formal review program but we do security spot checks

     2   We collect all necessary audit information, conduct regular reviews by independent
         staff and track issues found until they are resolved

     3   Above, plus we also have external assessments/audits and review the results with
         senior management




                                                                                              351
Diagnostic Tool


      12 ‚Äì Research
      a)       When and how do you collect personal health information for research purposes
               without consent?

           0 Not sure or don't know

           1 We have a written policy on the collection of personal health information for research
             purposes, but it's very basic
           2 Our procedures require us to comply with the Act's requirements concerning research
             plans and research ethics board approval, and we only collect personal health
             information from those parties who are permitted by law to give us such information
           3 Above, plus we enter into written agreements concerning the protection of the
             information with health information custodians who give us the information



      b)       When and how do you use personal health information for research purposes
               without consent?

           0 Not sure or don't know
           1 We have a written policy on the use of personal health information for research
             purposes, but it's very basic
           2 Our procedures require us to use personal health information for research purposes
             without consent only if the law permits us to do so, and only if conducted under a
             research plan that a research ethics board has approved (or we follow the rules for
             research approved outside Ontario if the personal health information originates outside of
             Ontario)
           3 Above, plus we have developed procedures to safeguard the information



      c)       When and to whom do you disclose personal health information for research
               purposes without consent?

           0 Not sure or don't know
           1 We have a written policy on the disclosure of personal health information for research
             purposes, but it's very basic
           2 Our procedures require us to disclose personal health information to researchers without
             consent only if we enter into a written agreement with the researcher to protect the
             information and we receive from the researcher a written application, a written research
             plan, and a copy of a research ethics boards' approval of the research plan
           3 Above, plus our procedures require us to impose additional obligations on the researcher
             in certain circumstances




352
                                                                     Diagnostic Tool


13 ‚Äì Fundraising
a)       Do you ensure that you have your patients' implied consent before using their
         names and mailing addresses for fundraising purposes?

     0   No or don't know

     1   Not sure, but someone usually tells our patients that our foundation will contact them for
         fundraising purposes

     2   Yes, we tell patients that we will use and disclose their names and mailing address for
         fundraising purposes and we tell them how they can opt-out of receiving fundraising
         solicitations

     3   Above, plus we tell patients how to opt-out of receiving fundraising solicitations in our
         notices, signs and brochures, as well as in every fundraising solicitation



b)       Have your fundraisers been properly trained on privacy matters?

     0   No, fundraisers are not given any particular rules or guidelines to follow

     1   Yes, fundraisers are asked to be respectful of patients' privacy, and we give them
         limited guidance

     2   Yes, we inform our fundraisers on privacy compliance ‚Äì for example, we tell them not
         to solicit donations from a patient until at least 60 days' after the patient has been
         discharged from the hospital and we tell them not to discuss a patient's state of health
         or health care when they contact the patient for fundraising purposes

     3   Above, plus we audit our fundraiser's practices from time to time to ensure compliance




                                                                                                      353
Diagnostic Tool


      14 ‚Äì Managing Contracts & Agents
      a)        Have you taken steps to protect the personal health information that you provide
                to your external agents?

           0 No or don't know

           1 Yes, we only use reputable companies who have privacy policies but we don't have any
             written policies on this

           2 Yes, we investigate potential agents for their privacy compliance before hiring them and
             we include privacy protection clauses in our contracts with our agents

           3 Above, plus we monitor our contracts regularly and when necessary, we enforce them




354
                                                                    Diagnostic Tool


15 ‚Äì Oversight
a)        What should you do if a privacy breach occurs?

     0 Not sure or don't know

     1 Our contact person would deal with any breach on an ad hoc basis

     2 Our procedures require us to contain the breach and to notify anyone affected by it

     3 Above, plus we review whether our policies and procedures adequately protect personal
       health information



b)        How do you contain a privacy breach?

     0 Not sure or don't know

     1 Our contact person would direct us

     2 Our procedures require us to retrieve any personal health information that has been
       disclosed, ensure that the person who was not authorized to receive the information did
       not make or keep copies of the information, and get that person's contact information in
       case there's a need to follow-up

     3 Above, plus we determine whether the breach allows unauthorized access to any other
       personal health information and, if so, take all appropriate steps to stop further breaches



c)        What do you tell the individual(s) whose personal health information was
          disclosed without authorization?

     0 Not sure

     1 We would apologize and take responsibility for the disclosure

     2 Our procedures requires us to explain what and how much personal health information
       was affected, explain immediate and long-term steps we and others have taken to rectify
       the breach, and make note of the unauthorized uses/disclosures in (or linked to) the
       affected personal health records

     3 Above, plus we designate one person who would communicate with the public about the
       breach




                                                                                                     355
Diagnostic Tool



      Score Sheet and Analysis
      Please go through each of the Survey Questions and circle the number that best
      corresponds to the level of compliance that you have achieved to date.

      Note, you may wish to make photo copies of this Score Sheet so that you can
      repeat the survey at a future date or complete separate surveys for different areas
      of your institution‚Äôs operations.

       Questions                             Answers (circle one per question)

       General Privacy Compliance
       1.a)                                  0          1           2          3
       1.b)                                  0          1           2          3
       Contact Person
       2.a)                                  0          1           2          3
       2.b)                                  0          1           2          3
       Consent
       3.a)                                  0          1           2          3
       3.b)                                  0          1           2          3
       Managing Health Information
       4.a)                                  0          1           2          3
       4.b)                                  0          1           2          3
       4.c)                                  0          1           2          3
       4.d)                                  0          1           2          3
       4.e)                                  0          1           2          3
       4.f)                                  0          1           2          3
       Accessing Health Records
       5.a)                                  0          1           2          3
       5.b)                                  0          1           2          3
       5.c)                                  0          1           2          3
       Correcting Health Records
       6.a)                                  0          1           2          3
       6.b)                                  0          1           2          3
       6.c)                                  0          1           2          3
       6.d)                                  0          1           2          3
       Dealing With Health Information
       7.a)                                  0          1           2          3
       7.b)                                  0          1           2          3




356
                                                          Diagnostic Tool

Questions                            Answers (circle one per question)
Security, First Steps
8.a)                                 0         1          2          3
8.b)                                 0         1          2          3
8.c)                                 0         1          2          3
Security, People
9.a)                                 0         1          2          3
9.b)                                 0         1          2          3
9.c)                                 0         1          2          3
9.d)                                 0         1          2          3
Security, Institutional Safeguards
10.a)                                0         1          2          3
10.b)                                0         1          2          3
10.c)                                0         1          2          3
10.d)                                0         1          2          3
10.e)                                0         1          2          3
10.f)                                0         1          2          3
10.g)                                0         1          2          3
Security, Sustaining Security
11.a)                                0         1          2          3
11.b)                                0         1          2          3
11.c)                                0         1          2          3
11.d)                                0         1          2          3
Research
12.a)                                0         1          2          3
12.b)                                0         1          2          3
12.c)                                0         1          2          3
Fundraising
13.a)                                0         1          2          3
13.b)                                0         1          2          3
Managing Contracts & Agents
14.a)                                0         1          2          3
Oversight
15.a)                                0         1          2          3
15.b)                                0         1          2          3
15.c)                                0         1          2          3




                                                                         357
Diagnostic Tool

      Score Evaluation
      If you have a 0 or 1 in any of the above answers, you should review that particular
      section in the Privacy Toolkit and take additional steps to achieve compliance
      with the Act.

      If you have a 2 in any of the above answers, you have achieved compliance with
      the Act. You might consider reviewing that particular section in the Privacy
      Toolkit and take additional steps to achieve best practices.

      If you have a 3 in any of the above answers, you have achieved compliance with
      the Act and have also implemented the proposed best practices. Well done! Keep
      in mind that you are responsible for on-going training, maintenance and
      monitoring of your Privacy program.




358
Index
                                                                                            Index

ACCEPTABLE USE POLICIES                        AGENTS
sample policy, 148-155                         application of Act, 7
                                               contracts
ACCESS TO HEALTH RECORDS                            checklist, 270-272
see also INFORMATION PRACTICES                      privacy compliance, 267
complaints re, see PRIVACY                     employees, see EMPLOYEES AND STAFF
  COMPLAINTS                                   information sharing agreements
contact person, 17-19                               checklist, 273
disposal of record to evade, 81, 288                privacy protections, 268
electronic access points, 167                  managing, 263-273
fees, 78                                            contracts, 267
key points, 75                                      due diligence, 266-267
management of, sample practices, 156-157            enforcement, 267-268
passwords, sample policy, 157-160                   information sharing agreements, 268
process checklist, 86-87                            key points, 263
process map, 83                                     operating outside Ontario, 268
refusal of access, 79-81                            the rule, 264-265
     complaint to Commissioner, 79                  what you need to do, 265-266
     guidelines for refusal, 79-81             privacy breach, liability, 288
     sample letter, 89                         service providers, see CONTRACTORS
request to access                                AND SUPPLIERS
     failure to respond to, 81                 unauthorized use of personal health
     sample form, 84-85                          information, 50-51
     timeframe for responding, 78-79           volunteers, see VOLUNTEERS
     urgent requests, 79
retention periods                              AGREEMENTS
     hospitals, 114                            confidentiality/non-disclosure agreements,
     physicians, 116                             see CONFIDENTIALITY AGREEMENTS
security concerns, see SECURITY                contractors and suppliers, see
the rule, 76                                     CONTRACTORS AND SUPPLIERS
timeframe for responding to request, 78-79     information sharing agreements, see
     extension to comply, 78                     INFORMATION SHARING
     sample extension letter, 88                 AGREEMENTS
     urgent requests, 79
unauthorized access, notice requirement, 108   APPEALS
user ID, sample practices, 156-157             privacy reviews, 287
what you need to do, 76-77
what you should do, 77-78                      AUDIOTAPING OF PERSONAL HEALTH
written statement, 9, 77                        INFORMATION
     sample written statement, 11              medical education, 53-54
                                               patient care, 53
ACT
see PERSONAL HEALTH INFORMATION                AUTHENTICATION AND
  PROTECTION ACT, 2004                           AUTHORIZATION
ADJUSTERS                                      see also SECURITY
disclosure requests, 64-65                     passwords, sample policy, 157-160
                                               small offices, 147
ADMINISTRATIVE CONTROLS                        user ID and access management, sample
see also INSTITUTIONAL SAFEGUARDS;               practices, 156-157
  SECURITY                                     what you should do, 146
recommended standards, 195
storage of records, 109                        BACKUPS
the rule, 123                                  what you should do, 190-191


                                                                                               361
Index

            small businesses, 191-192                   correction to personal health record
                                                             process map, 100
        BOOKS AND RECORDS                                    request form, 101-102
        inspection by Commissioner, 286                 disaster recovery management guide,
                                                          198-200
        BUSINESS PREMISES                               disclosure of personal health information
        inspection by Commissioner, 285                      consent form, 41, 69
                                                             disclosure tables, 60-65
        BUSINESS RECOVERY PLANS                              process map, 67
        management guide, 198-200                       fax machines, rules for transmitting
        recommended standards, 196                        information, 155
        what you should do, 190-191                     fundraising
                                                             consent form, 256
        CAPACITY                                             decision tree, 255
        patient consent, 36-39                               withdrawal of consent form, 257
             children and teenagers, 38                 information inventory template, 138
             substitute-decision makers, 37-38, 38-39   information sharing agreements
                                                             contracts and agents, 273
        CELLULAR PHONES                                      research, 231-244
        see also WIRELESS AND PORTABLE                  inventory of personal health information, 290
          DEVICES                                       LAN management guidelines
        institutional safeguards, 169-170                    large institutions, 174-176
             sample guidelines for mobile computing,         small institutions, 176-177
                182                                     malicious software
             sample technical standards for wireless         guide for users, 179-180
                connections, 183-184                         policy for protecting against, 178-179
             small offices, 170                         mobile computing, user guidelines, 181-182
             what you should do, 169-170                network design - security zones of control,
        sample acceptable use policy, 150                 171-172
                                                        password policy, 157-160
        CHAPLAINS                                       privacy breaches, complaint decision tree,
        see SPIRITUAL CARE                                291
                                                        record retention periods, 114-119
        CHECKLISTS, TEMPLATES AND TOOLS                      drug dispensing records, 118-119
        acceptable use policy, 148-155                  research plans
        access to personal health record                     application to ethics board, 222-230
             extension to comply letter, 88                  approval checklist, 221
             management practices, 156-157              retrospective research
             process checklist, 86-87                        consent form for study participation, 245
             process map, 83                                 information sharing agreement, 231-244
             refusal of access guidelines, 79-81        secure applications, guiding principles,
             refusal of access letter, 89                 201-202
             request form, 84-85                        security audit information, capturing and
        agents agreements, 270-272                        using, 207-208
        business recovery management guide,             security officer responsibilities, 137
          198-200                                       staff responsibilities for physical security,
        Commissioner contact information, 290             148
        confidentiality/non-disclosure agreement, 68    technical security threat risk assessment
        consent to collection, use and disclosure of      form, 202-206
          information                                   theft and loss of computer and digital media,
             decision tree for consent, 40                managing, 172-173
             form, 41, 69                               user ID and access management practices,
             withdrawal of consent form, 42               156-157


362
                                                                                               Index

wireless connections, technical standards,        COMPLAINTS TO COMMISSIONER
 183-184                                          see PRIVACY COMPLAINTS; PRIVACY
written statement of information practices, 11      REVIEWS

CHILDREN                                          COMPUTING EQUIPMENT
capacity to consent, 38                           see also ELECTRONIC HEALTH
                                                    RECORDS; TECHNICAL SECURITY
CIRCLE OF CARE                                    acceptable use, sample policy, 149-155
definition, 52                                    business continuity, 190-192
lock boxes, effect, 59                            development and maintenance, 192-193
use of personal health information, 51-53         electronic access points, 167
                                                  malicious software, 168-169
COLLECTION OF DEBTS                                    protecting against, sample policy,
personal health information, use, 32                      178-179
                                                       sample guide for users, 179-180
COLLECTION OF PERSONAL HEALTH                          small offices, 169
  INFORMATION                                          what you should do, 168-169
see also INFORMATION PRACTICES                    mobile computing, sample guidelines,
agents, by, see AGENTS                              181-182
complaints re, see PRIVACY                        passwords, sample policy, 157-160
  COMPLAINTS                                      recommended security standards, 196-197
consent requirements, see CONSENT                 secure applications, guiding principles,
fundraising purposes, see FUNDRAISING               201-202
indirect collection, 31                           theft and loss, managing, 172-173
key points, 47                                    threat risk assessment form, 202-206
privacy breach, see PRIVACY BREACH                wireless and portable devices, 169-170
psychiatric facilities, 35-36
religious or other organizational affiliations,   CONFIDENTIALITY AGREEMENTS
  58                                              contractors, 144
researchers, 215                                  sample agreement, 68
sample consent form, 41                           staff, 144
     sample withdrawal of consent form, 42        use, 51
the rule, 48                                      volunteers, 57
videotaping, audiotaping and photographing,
  53-54                                           CONSENT
what you need to do, 48-49                        fundraising
what you should do, 49                                personal information, 27, 29
written statement, 9, 49                              sample consent form, 256
     sample written statement, 11                     sample withdrawal of consent form, 257
                                                      the rule, 252
COMMISSIONER                                          what you need to do, 252-253
see INFORMATION AND PRIVACY                       personal health information, 25-42
  COMMISSIONER                                        Commissioner, review by, 286
                                                      conditional consent, 30
COMPLAINTS PROCEDURES                                 deceased patients, 57
see also PRIVACY COMPLAINTS                           decision tree, 40
contact person, 17-19                                 express consent, 28-30
decision tree, 291                                    fundraising purposes, 252-253
refusal of access, 79                                 implied consent, 27-28
written statement, 9                                  key points, 25
     sample written statement, 11                     locked information, 30, 59
                                                      patient capacity, 36-39
                                                      psychiatric facilities, 35-36


                                                                                                  363
Index

             researchers, 214-216, 219                  CORRECTIONS TO HEALTH RECORDS
             sample collection, use, disclosure form,   see also INFORMATION PRACTICES
                41                                      complaints re, see PRIVACY
             sample disclosure form, 69                   COMPLAINTS
             sample withdrawal form, 42                 contact person, 17-19
             substitute decision-makers, 37-38, 38-39   key points, 95
             the rule, 26, 48                           refusal of request
             valid consent, 26                               circumstances, 98
             what you need to do, 27-35                      complaint to Commissioner, 99
             when not required, 30-34, 60-62, 63-64          conflict resolution, 99
             withdrawal of consent, 30                       written description of refused correction,
        research study participation, sample form,              99
          245                                           request for correction, sample form, 101-102
        treatment, to, 26                               responding to requests, 96-97
        videotaping, audiotaping and photographing      the rule, 96
          of procedures, 53-54                          timeframe for responding, 98-99
                                                             extension to comply, 98-99
        CONTACT PERSON                                  what you need to do, 96-97
        duties and responsibilities, 18, 19             what you should do, 97-98
        key points, 17                                  where you do not have to make corrections,
        the rule, 18                                      98
        what you need to do, 18-19                      written statement, 9, 95
        what you should do, 19                               sample written statement, 11
        written statement, 9, 17
             sample written statement, 11               COURT-ORDERED ASSESSMENTS
                                                        express consent, 29
        CONTRACTORS AND SUPPLIERS
        see also AGENTS                                 DAMAGES
        acceptable use policy, sample policy,           privacy breach, 288
          148-155
        confidentiality/non-disclosure agreements,      DATA STEWARD
          144                                           see also SECURITY
        contracts                                       appointment, 134
             checklist for agreements, 270-272          delegation of duties, 135
             what you need to do, 267
        hired fundraisers, providing information to,    DE-IDENTIFYING INFORMATION
          254                                           formal educational programs, 54
        information sharing agreements                  non-disclosure agreements requiring, 51
             checklist, 273
             necessity, 268                             DECEASED PATIENTS
        security                                        disclosure of personal health information, 57
             policies, 133                              DIAGNOSTIC IMAGING RECORDS
             personal responsibilities, 144             retention of records, 114
        unauthorized use of personal health
          information, 50-51                            DIGITAL MEDIA
        what you need to do                             see COMPUTER EQUIPMENT
             contracts, 267
             due diligence, 266-270                     DISASTER RECOVERY PLANS
             enforcement, 267-268                       management guide, 198-200
             information sharing agreements, 268        recommended standards, 196
             operations outside Ontario, 268            what you should do, 190-191




364
                                                                                            Index

DISCLOSURE OF PERSONAL HEALTH                   volunteers, 57
  INFORMATION                                   what you need to do, 55
see also INFORMATION PRACTICES                  what you should do, 55-56
adjuster requests, 64-65                        written statement, 9, 55
agents, by, see AGENTS                               sample written statement, 11
complaints re, see PRIVACY
  COMPLAINTS                                    DISPOSAL OF PERSONAL HEALTH
confidentiality agreements, see                   INFORMATION
  CONFIDENTIALITY AGREEMENTS                    see also INFORMATION PRACTICES
consent form, sample form, 69                   electronic records, 111
consent requirements, see CONSENT               security concerns, see SECURITY
de-identifying information                      staff responsibilities, sample list, 148
     formal educational programs, 54            the rule, 111
     non-disclosure agreements requiring, 51    unlawful disposal, 288
deceased patients, 57                                to avoid access request, 81, 288
family members or friends, 56-57                     unsecured manner, 288
fundraising purposes, see FUNDRAISING           what you need to do, 111
health related programs and legislation,        what you should do, 111
  63-64
in-patient transfers, 57                        DOCTORS
insurance company requests, 64-65               see PHYSICIANS
investigator requests
     law enforcement officials, 65              DRUG DISPENSING RECORDS
     lawyers and insurance companies, 64-65     retention of records, 118-119
     regulated health professionals, 61, 63
key points, 47                                  E-MAIL
law enforcement officials, 65                   see ELECTRONIC MAIL (E-MAIL)
lawyer requests, 64-65
lock boxes, see LOCK BOXES                      EDUCATIONAL PURPOSES
mandatory disclosures, 60-62                    see also RESEARCH
media requests, 58-59                           use of personal health information, 31
medical education, 54                                videotaping, audiotaping and
     grand rounds, 31, 54                              photographing of procedures, 53-54
non-disclosure agreements, see
  CONFIDENTIALITY AGREEMENTS                    ELECTRONIC HEALTH RECORDS
permitted disclosures, 63-64                    see also COMPUTING EQUIPMENT;
privacy breach, see PRIVACY BREACH                PERSONAL HEALTH RECORDS
process map, 67                                 disposal of records, 111
psychiatric facilities, 35-36                   security
researchers, 216                                     physical security, see PHYSICAL
     disclosure under other Acts, 219                   SECURITY
     sample information sharing agreement,           technical security, see TECHNICAL
        231-244                                         SECURITY
safeguards, see INSTITUTIONAL                   storage and retention, 109
  SAFEGUARDS
spiritual care, 58                              ELECTRONIC MAIL (E-MAIL)
tables, 60-65                                   acceptable use, sample policy, 152-153
     lawyers, insurance companies, adjusters,   passwords, sample policy, 157-160
        investigators, 64-65
     legal and law enforcement officials, 65    EMPLOYEES AND STAFF
     mandatory disclosures, 60-62               see also AGENTS
     permitted disclosures, 63-64               acceptable use policy, sample policy,
the rule, 48                                      148-155


                                                                                               365
Index

        application of Act, 7                           the rule, 252
        confidentiality/non-disclosure agreements,      what you need to do, 252-254
          144                                                disclosing information to hospital
        data steward, see DATA STEWARD                          foundation, 253-254
        fax machines, rules for using, 155                   obtaining express consent, 253
        privacy breach by hospital, liability, 288           providing information to hired
        privacy law, discipline for complying with,             fundraisers, 254
          288                                                relying on implied consent, 252-253
        security
             officer, see SECURITY OFFICER              HANDHELD DEVICES
             personal responsibilities, 144-145         see CELLULAR PHONES; WIRELESS
             policies, 133                                AND PORTABLE DEVICES
             roles and responsibilities, 134-135
             sample list of responsibilities, 148       HEALTH CARE
        unauthorized use of personal health             see PATIENT CARE
          information, 50-51
                                                        HEALTH CARE PRACTITIONERS
        FAMILY MEMBERS                                  see also HEALTH INFORMATION
        disclosure of personal health information to,     CUSTODIANS
          56-57                                         application of Act, 7
                                                        circle of care, see CIRCLE OF CARE
        FAX MACHINES                                    disclosure requests, 60-63
        perimeter security, 166                         physicians, see PHYSICIANS
             small offices, 167
        rules for transmitting information using, 155   HEALTH INFORMATION
                                                        see PERSONAL HEALTH INFORMATION
        FEES
        access to health records, 78                    HEALTH INFORMATION CUSTODIANS
                                                        agents of, see AGENTS
        FINANCIAL REIMBURSEMENT                         application of Act, 7
        see COLLECTION OF DEBTS                         health care practitioners, see HEALTH
                                                          CARE PRACTITIONERS
        FORMS                                           hospitals, see HOSPITALS
        see SAMPLE STATEMENTS AND FORMS                 information practices, see INFORMATION
                                                          PRACTICES
        FOUNDATIONS                                     physicians, see PHYSICIANS
        see HOSPITAL FOUNDATIONS
                                                        HEALTH INFORMATION NETWORK
        FRIENDS                                           PROVIDERS
        disclosure of personal health information to,   checklist for agreements, 272
          56-57
                                                        HEALTH RECORDS
        FUNDRAISING                                     see PERSONAL HEALTH RECORDS
        consent
            names and mailing addresses, implied        HEALTH RELATED PROGRAMS
               consent, 27, 252-253                     disclosure requests, 63-64
            other personal information, express
               consent, 29, 253                         HOSPITAL FOUNDATIONS
            sample consent form, 256                    see also FUNDRAISING
            sample withdrawal of consent form, 257      disclosure of information for fundraising,
            the rule, 252                                 253-254
        decision tree, 255
        key points, 251


366
                                                                                            Index

HOSPITALS                                       privacy breach, see PRIVACY BREACH
see also HEALTH INFORMATION                     retention, see STORAGE AND
  CUSTODIANS                                      RETENTION OF HEALTH
agents, see AGENTS                                INFORMATION
application of Act, 7                           security, see SECURITY
circle of care, 52-53                           storage, see STORAGE AND RETENTION
employees, see EMPLOYEES AND STAFF                OF HEALTH INFORMATION
foundation, see HOSPITAL                        use, see USE OF PERSONAL HEALTH
  FOUNDATIONS                                     INFORMATION
information practices, see INFORMATION          written statement, 9
  PRACTICES                                          sample written statement, 11
privacy breach, sanctions, 287-288
record retention periods                        INFORMATION SHARING
     health records, 114-115                    see DISCLOSURE OF PERSONAL
     OHIP records, 115                            HEALTH INFORMATION;
     research records, 115                        INFORMATION SHARING
                                                  AGREEMENTS
IN-PATIENT TRANSFERS
disclosure of personal health information, 57   INFORMATION SHARING
                                                  AGREEMENTS
INFORMATION AND PRIVACY                         checklist, 273
  COMMISSIONER                                  retrospective research, sample agreement,
complaints to, see PRIVACY                        231-244
  COMPLAINTS                                    what you need to do, 268
contact information, 290
obstructing, 288                                INFORMATION TECHNOLOGY (IT)
orders                                          see COMPUTING EQUIPMENT;
     appeals, 287                                 ELECTRONIC HEALTH RECORDS;
     copies, 287                                  TECHNICAL SECURITY
     enforcement, 287
     failure to comply, 288                     INSPECTION
     powers, 286-287                            privacy reviews, 285-286
powers, 284-287
     privacy reviews, 283-284, 285-287          INSTITUTIONAL SAFEGUARDS
     responding to privacy complaints,          see also SECURITY
        284-285                                 administrative controls
     sanctions, 287                                  recommended standards, 195
review of complaints, see PRIVACY                    storage of records, 109
  REVIEWS                                            the rule, 123
role, 284                                       key points, 165
                                                malicious software, 168-169
INFORMATION PRACTICES                                protecting against, sample policy,
access, see ACCESS TO HEALTH                            178-179
  RECORDS                                            sample guide for users, 179-180
collection, see COLLECTION OF                        small offices, 169
  PERSONAL HEALTH INFORMATION                        what you should do, 168-169
complaints re, see PRIVACY                      perimeter security, 166-168
  COMPLAINTS                                         computer and digital media theft and
contact person, 17-19                                   loss, managing, 172-173
disclosure, see DISCLOSURE OF                        electronic access points, 167
  PERSONAL HEALTH INFORMATION                        LAN management guidelines for large
disposal, see DISPOSAL OF PERSONAL                      institutions, 174-176
  HEALTH INFORMATION


                                                                                               367
Index

            LAN management guidelines for small       LAW SUITS
               institutions, 176-177                  see also LEGAL PROCEEDINGS
            network design - zones of control,        privacy breach, 288
               171-172                                record retention
            physical perimeter security, 166-167           hospitals, 114-115
            small offices, 167-168                         physicians, 116
            what you should do, 166
        wireless and portable devices, 169-170        LAWYERS
            sample guidelines for mobile computing,   disclosure requests, 64-65
               181-182
            sample technical standards for wireless   LEGAL PROCEEDINGS
               connections, 183-184                   see also LAW SUITS
            small offices, 170                        court-ordered assessments, see
            what you should do, 169-170                 COURT-ORDERED ASSESSMENTS
                                                      personal health information, use, 31, 32
        INSURANCE COMPANIES
        disclosure requests, 64-65                    LIABILITY
                                                      privacy breach, 288
        INTERNET USE
        see also ELECTRONIC MAIL (E-MAIL);            LOCK BOXES
          TECHNICAL SECURITY                          concept described, 59
        acceptable use, sample policy, 149-155        conditional consent, 30
        malicious software, see MALICIOUS             flagging, 59
          SOFTWARE                                    override by Act, 59
        passwords, sample policy, 157-160
                                                      LOST INFORMATION OR EQUIPMENT
        INVENTORY                                     see THEFT AND LOSS
        personal health information
            sample inventory, 290                     MALICIOUS SOFTWARE
            small offices, 136                        see also COMPUTING EQUIPMENT;
            template, 138                               TECHNICAL SECURITY
            what you should do, 135                   institutional safeguards, 168-169
        security inventory, see SECURITY                   protecting against, sample policy,
                                                              178-179
        INVESTIGATIONS                                     sample guide for users, 179-180
        disclosure requests                                small offices, 169
             law enforcement officials, 65                 what you should do, 168-169
             lawyers and insurance companies, 64-65
             regulated health professionals, 61, 63   MANAGING HEALTH INFORMATION
        record retention                              see INFORMATION PRACTICES
             hospitals, 114
             physicians, 116                          MEDIA
                                                      digital media, see COMPUTING
        JUDICIAL REVIEW                                 EQUIPMENT
        Commissioner‚Äôs order, 287                     information requests, 58-59

        LAPTOP COMPUTERS                              MEDICAL PROCEDURES
        see COMPUTING EQUIPMENT;                      videotaping, audiotaping and photographing,
          WIRELESS AND PORTABLE DEVICES                 53-54

        LAW ENFORCEMENT OFFICIALS                     MEMBERS
        disclosure requests, 65                       privacy breach by hospital, liability, 288



368
                                                                                              Index

MENTAL HEALTH ACT                             OVERSIGHT
disclosure of personal health information,    see also PRIVACY COMPLIANCE
  consent requirements, 35-36                 key points, 279
                                              offence and sanctions, 287-288
MOBILE DEVICES                                privacy breaches, see PRIVACY BREACH
see CELLULAR PHONES; WIRELESS                 privacy complaints, see PRIVACY
  AND PORTABLE DEVICES                          COMPLAINTS
                                              privacy reviews, see PRIVACY REVIEWS
NON-DISCLOSURE AGREEMENTS                     role of Commissioner, 284
see CONFIDENTIALITY AGREEMENTS
                                              PASSWORDS
NOTICE                                        see also AUTHENTICATION AND
locked information, 59                          AUTHORIZATION; SECURITY
lost or stolen personal health information,   sample policy, 157-160
  108
privacy breach, 282                           PATIENT CARE
privacy complaint, 284                        see also PATIENTS
privacy review, 285                           circle of care, 51-53
request for access                            collection of personal health information, 31
     extension to comply, 78                  disclosure of personal health information,
     refusal of request, 79                     32-34
     sample letter for extension, 88          videotaping, audiotaping and photographing
request for correction                          of procedures, 53
     extension to comply, 98                  records, see PERSONAL HEALTH
     refusal of request, 99                     RECORDS
unauthorized access to information, 108       retention of records
withdrawal of consent, 30                          hospitals, 114
                                                   physicians, 115
OFFENCES AND PENALTIES
Commissioner, obstruction, 288                PATIENTS
discipline for compliance with privacy law,   access to records, see ACCESS TO
  288                                           HEALTH RECORDS
personal health records, unlawful disposal,   care of, see PATIENT CARE
  288                                         consent capacity, see CAPACITY
privacy breach, 288                           consent, see CONSENT
                                              deceased patients, see DECEASED
OFFICERS AND DIRECTORS                          PATIENTS
privacy breach by hospital, liability, 288    personal health information, see PERSONAL
                                                HEALTH INFORMATION
OHIP RECORDS                                  requests, contact person, 18
retention period                              transfers, see IN-PATIENT TRANSFERS
     hospitals, 115
     physicians, 116                          PERIMETER SECURITY
                                              generally, see under SECURITY
ORDERS                                        physical security, see PHYSICAL
Commissioner, privacy review, 286-287           SECURITY
    appeals, 287                              technical security, see TECHNICAL
    copies, 287                                 SECURITY
    enforcement, 287
    failure to comply, 288                    PERSONAL DIGITAL ASSISTANTS
    powers, 286-290                           see WIRELESS AND PORTABLE
court-ordered assessments, see                  DEVICES
  COURT-ORDERED ASSESSMENTS


                                                                                                 369
Index

        PERSONAL HEALTH INFORMATION                    access, see ACCESS TO HEALTH
        see also PERSONAL HEALTH RECORDS                 RECORDS
        access, see ACCESS TO HEALTH                   contact person, 18
          RECORDS                                      correction, see CORRECTIONS TO
        collection, see COLLECTION OF                    HEALTH RECORDS
          PERSONAL HEALTH INFORMATION                  disclosure, see DISCLOSURE OF
        confidential classification, 136                 PERSONAL HEALTH INFORMATION
        consent to collect, use or disclose, see       disposal, see DISPOSAL OF PERSONAL
          CONSENT                                        HEALTH INFORMATION
        dealing with, 107-119                          electronic records, see ELECTRONIC
             key points, 107                             HEALTH RECORDS
        disclosure, see DISCLOSURE OF                  in-patient transfers, 57
          PERSONAL HEALTH INFORMATION                  security, see SECURITY
        disposal, see DISPOSAL OF PERSONAL             storage and retention, see STORAGE AND
          HEALTH INFORMATION                             RETENTION OF HEALTH
        electronic health information, 109               INFORMATION
        fax machines, rules for using, 155             transfer of records, see TRANSFER OF
        information practices, see INFORMATION           HEALTH RECORDS
          PRACTICES                                    written description of refused correction,
        lock boxes, see LOCK BOXES                       attachment, 99
        lost or stolen, notice requirement, 108
        privacy breach, see PRIVACY BREACH             PHOTOGRAPHING OF PERSONAL
        review by Commissioner, 286                     HEALTH INFORMATION
        sample inventory, 290                          medical education, 53-54
        security, see SECURITY                         patient care, 53
        storage and retention, see STORAGE AND
          RETENTION OF HEALTH                          PHYSICAL SECURITY
          INFORMATION                                  see also INSTITUTIONAL SAFEGUARDS;
        transfer of records, see TRANSFER OF             SECURITY
          HEALTH RECORDS                               perimeter security, 166-167
        unauthorized access, notice requirement, 108        computer and digital media theft and
        use, see USE OF PERSONAL HEALTH                        loss, managing, 172-173
          INFORMATION                                       small offices, 167
        videotaping, audiotaping and photographing,    recommended standards, 195
          53-54                                        staff responsibilities, 144-145
        written statement of information practices,         sample list, 148
          see WRITTEN STATEMENT OF                          small offices, 145
          INFORMATION PRACTICES                        storage of records, 109
                                                       the rule, 123
        PERSONAL HEALTH INFORMATION
          PROTECTION ACT, 2004                         PHYSICIANS
        application and scope, 7                       see also HEALTH INFORMATION
        compliance, see PRIVACY COMPLIANCE               CUSTODIANS
        lock box override, 59                          application of Act, 7
        offence and sanctions, 287-288                 circle of care, 53
        oversight, see OVERSIGHT                       information practices, see INFORMATION
        overview, 6                                      PRACTICES
        transition rules for research projects, 219    patient records, see PERSONAL HEALTH
                                                         RECORDS
        PERSONAL HEALTH RECORDS                        privacy breach, sanctions, 288
        see also PERSONAL HEALTH                       record retention periods
          INFORMATION                                       health records, 115-116
                                                            OHIP records, 116


370
                                                                                         Index

    research records, 116-117             PRIVACY REVIEWS
small office security, see SMALL OFFICE   see also INFORMATION AND PRIVACY
 SECURITY                                   COMMISSIONER
                                          appeals, 287
POLICE                                    conducting, 285-286
see LAW ENFORCEMENT OFFICIALS             copy of recommendations or order, 287
                                          enforcement of orders, 287
PORTABLE DEVICES                          grounds, 285
see WIRELESS AND PORTABLE                 initiating, 285
  DEVICES                                 powers of Commissioner, 283-284, 285-287
                                          result of review, 286-287
PRIVACY BREACH                            rules of procedure, 285
addressing a breach, 281-283
    additional steps, 282-283             PSYCHIATRIC FACILITIES
    containment, 281-282                  personal health information, disclosure,
    notification, 282                      35-36
avoiding a breach, 280-281
    best practices, 281                   RECORDS
    requirements, 280-281                 books and records, see BOOKS AND
complaints re, see PRIVACY                  RECORDS
  COMPLAINTS                              diagnostic imaging records, see
sanctions, 287-288                          DIAGNOSTIC IMAGING RECORDS
    civil damages, 288                    drug dispensing, see DRUG DISPENSING
    Commissioner‚Äôs order, 287               RECORDS
    offences and penalties, 288           health records, see PERSONAL HEALTH
what is a breach, 280                       RECORDS
                                          OHIP records, see OHIP RECORDS
PRIVACY COMPLAINTS                        research records, see RESEARCH
contact person, 17-19
refusal of access, 79                     REGULATED HEALTH
refusal to correct record, 99, 100          PROFESSIONALS
responding to, decision tree, 291         see also HEALTH CARE
response by Commissioner, 284-285           PRACTITIONERS
review by Commissioner, see PRIVACY       disclosure requests, 60-63
  REVIEWS
written statement of procedures, 9        RELIGIOUS ORGANIZATIONS AND
     sample written statement, 11           PROGRAMS
                                          see SPIRITUAL CARE
PRIVACY COMPLIANCE
agents                                    REMOTE NETWORK ACCESS
    contracts, 267, 270-272               see also WIRELESS AND PORTABLE
    due diligence, 266-267                  DEVICES
    enforcement, 267-268                  sample guidelines, 182
    operating outside Ontario, 268
breaches, see PRIVACY BREACH              RESEARCH
contact person, 17-19                     consent
generally, 5-11                                express consent, 29, 219
oversight, see OVERSIGHT                       study participation, sample form, 245
toolkit, using, 5-6                            when not required, 32, 33, 214
what you need to do, 9-10                 ethics board, see RESEARCH ETHICS
                                            BOARD
PRIVACY PRINCIPLES                        information originating outside Ontario, 218
described, 7-9


                                                                                            371
Index

        information sharing of retrospective research,        request form, 84-85
          sample agreement, 231-244                      confidentiality/non-disclosure agreement, 68
        key points, 213                                  consent forms
        record retention                                      collection, use and disclosure, 41
             hospitals, 115                                   disclosure of personal health
             physicians, 116-117                                 information, 69
        research plans                                        fundraising, 256
             approval checklist, 221                          fundraising withdrawal of consent, 257
             compliance by researcher, 218                    research study participation, 245
             requirements, 216                                withdrawal of consent, 42
             review by ethics board, 217                 correction to personal health record, request
             sample application to ethics board,           form, 101-102
                222-230                                  fundraising
        study participation, sample consent form, 245         consent form, 256
        the rule, 214                                         withdrawal of consent form, 257
        what you need to do, 215-219                     research ethics board, application to, 222-230
             collection of information, 215              threat risk assessment form, 202-206
             disclosure of information, 216              retrospective research
             disclosure under other Acts, 219                 consent form for study participation, 245
             research plan, 216-217                           information sharing agreement, 231-244
             researcher duties, 218                      written statement of information practices, 11
             transition rules, 219
             use of information, 215-216                 SANCTIONS
                                                         see also OFFENCES AND PENALTIES;
        RESEARCH ETHICS BOARD                              OVERSIGHT
        see also RESEARCH                                privacy breach, 287-288
        composition, 216
        duties, 217                                      SECURITY
        sample application to, 222-230                   administrative controls
        RESIDENCES                                            recommended standards, 195
        inspection by Commissioner, 286                       storage of records, 109
                                                              the rule, 123
        RETENTION OF HEALTH                              audits, see SECURITY AUDITS
          INFORMATION                                    authentication and authorization, 146-147
        see STORAGE AND RETENTION OF                          passwords, sample policy, 157-160
          HEALTH INFORMATION                                  small office applicability, 147
                                                              user ID and access management, sample
        REVIEWS                                                  practices, 156-157
        see PRIVACY REVIEWS                                   what you should do, 146
                                                         business continuity, 190-192
        ROUNDS                                                business recovery and disaster recovery
        personal health information, use, 31, 54                 guide, 198-200
                                                              small office applicability, 191-192
        SAFEGUARDS                                            what you should do, 190-191
        see INSTITUTIONAL SAFEGUARDS;                    development and maintenance, 192-193
          SECURITY                                            what you need to do, 192
                                                              small office applicability, 193
        SAMPLE STATEMENTS AND FORMS                           secure applications, guiding principles,
        see also CHECKLISTS, TEMPLATES AND                       201-202
          TOOLS                                               threat risk assessment form, 202-206
        access to personal health records                information inventory and classification,
             letter for extension to comply, 88            135-136
             refusal of access letter, 89                     recommended standards, 195


372
                                                                                                Index

     sample inventory template, 138                  what you should do, 134-135
     small office applicability, 136            small offices, see SMALL OFFICE
     what you should do, 135-136                  SECURITY
institutional safeguards, see                   standards
  INSTITUTIONAL SAFEGUARDS                           development, 132, 133
key points, 131                                      recommended standards, 195-197
malicious software, 168-169                     stored records, 108-109
     sample guide for users, 179-180            sustaining security, 189-208
     sample policy for protecting against,           audits, 193-195, 207-208
        178-179                                      business continuity, 190-192
     small office applicability, 169                 development and maintenance, 192-193
     what you should do, 168-169                     key points, 189
people aspect, 143-160                               recommended standards, 195-197
     acceptable use, sample policy, 148-155     technical security, see TECHNICAL
     authentication and authorization,            SECURITY
        146-147                                 the rule, 123-125
     fax machines, rules for using, 155         wireless and portable devices, 169-170
     key points, 143                                 sample guidelines for mobile computing,
     physical security, 144-145                         181-182
     recommended standards, 195                      sample technical standards for wireless
     small office applicability, 145                    connections, 183-184
     staff responsibilities, sample list, 148        small office applicability, 170
     what you should do, 144-145                     what you should do, 169-170
perimeter security, 166-168
     computer and digital media theft and       SECURITY AUDITS
        loss, managing, 172-173                 see also SECURITY
     electronic access points, 167              capturing and using audit information,
     LAN management guidelines for large          207-208
        institutions, 174-176                   small office applicability, 195
     LAN management guidelines for small        what you need to do, 193-195
        institutions, 176-177
     network design - zones of control,         SECURITY OFFICER
        171-172                                 see also SECURITY
     physical perimeter security, 166-167       appointment, 134
     small office applicability, 167-168        sample responsibilities, 137
     what you should do, 166
physical security, see PHYSICAL                 SERVICE PROVIDERS
  SECURITY                                      see AGENTS; CONTRACTORS AND
procedures, development, 132, 133                 SUPPLIERS; HEALTH CARE
program and policy, 132-134                       PRACTITIONERS
     contents, 133
     management approval, 133                   SMALL OFFICE SECURITY
     recommended standards, 195                 see also SECURITY
     small office applicability, 133-134        audit of practices, 195
     what you should do, 132-133                authentication and authorization, 147
roles and responsibilities, 134-135             backups, 191-192
     data steward, see DATA STEWARD             information inventory and classification, 136
     delegation of responsibilities, 135        malicious software, 169
     personal responsibilities, 144-145,        perimeter security, 167-168
        148-155                                 personal responsibilities for security, 145
     security officer, see SECURITY             program and policy, 133-134
        OFFICER                                 roles and responsibilities, 135
     small office applicability, 135            sustaining security


                                                                                                   373
Index

            audits, 195                                 dealing with substitute decision-makers,
            business continuity, 191-192                  38-39
            development and maintenance, 193            requests to correct a health record, 96
        wireless and portable devices, 170
                                                        SUPPLIERS
        SOFTWARE VIRUSES                                see CONTRACTORS AND SUPPLIERS
        see MALICIOUS SOFTWARE
                                                        TECHNICAL SECURITY
        SPIRITUAL CARE                                  see also INSTITUTIONAL SAFEGUARDS;
        collection of information of religious            SECURITY
          affiliation, 58                               business continuity, 190-192
        disclosure of personal health information, 58        small offices, 191-192
             name and location, implied consent, 28          what you should do, 190-192
                                                        development and maintenance, 192-193
        STAFF                                                secure applications, guiding principles,
        see EMPLOYEES AND STAFF                                 201-202
                                                             small offices, 193
        STATEMENTS                                           threat risk assessment form, 202-206
        see SAMPLE STATEMENTS AND                            what you need to do, 192
          FORMS; WRITTEN STATEMENT OF                   malicious software
          INFORMATION PRACTICES                              protecting against, sample policy,
                                                                178-179
        STOLEN INFORMATION OR                                sample guide for users, 179-180
          EQUIPMENT                                          small offices, 169
        see THEFT AND LOSS                                   what you should do, 168-169
                                                        perimeter security
        STORAGE AND RETENTION OF                             electronic access points, 167
          HEALTH INFORMATION                                 LAN management guidelines for large
        see also INFORMATION PRACTICES                          institutions, 174-176
        disposal of records, see DISPOSAL OF                 LAN management guidelines for small
          PERSONAL HEALTH INFORMATION                           institutions, 176-177
        drug dispensing records, 118-119                     network design - control zones, 171-172
        electronic health information, 109                   passwords, sample policy, 157-160
        health records                                       small offices, 168
             hospitals, 114-115                         recommended standards, 196-197
             physicians, 115-116                        storage of records, 109
        OHIP records                                    the rule, 123
             hospitals, 115                             wireless and portable devices
             physicians, 116                                 mobile computing, sample guidelines,
        research records                                        181-182
             hospitals, 115                                  small offices, 170
             physicians, 116-117                             what you should do, 169-170
        security concerns, see SECURITY                      wireless connections, sample technical
        summary of retention periods, 114-119                   standards, 183-184
        the rule, 108
             retention, 108                             TEENAGERS
             storage, 108                               capacity to consent, 38
        what you need to do, 108-109
        what you should do, 110                         THEFT AND LOSS
                                                        computer and digital media, guidance for
        SUBSTITUTE DECISION-MAKERS                        managing, 172-173
        access to health records, 76                    health information, patient notification, 108
        consent re personal health information, 37-39


374
                                                                                              Index

TRANSFER OF HEALTH RECORDS                    VIDEOTAPING OF PERSONAL HEALTH
see also DISCLOSURE OF PERSONAL                INFORMATION
  HEALTH INFORMATION                          medical education, 53-54
another facility or physician, 112            patient care, 53
archiving, 112
fax machines, rules for using, 155            VOLUNTEERS
in-patient transfers, 57                      see also AGENTS
physical perimeter security, 166-167          application of Act, 7
successor, to, 112                            confidentiality/non-disclosure agreements, 57
the rule, 112                                 disclosure of personal health information, 57
what you need to do, 112
what you should do, 113                       WIRELESS AND PORTABLE DEVICES
                                              cellular phones, see CELLULAR PHONES
TRANSFER OF PATIENTS                          computing equipment, see COMPUTING
see IN-PATIENT TRANSFERS                        EQUIPMENT
                                              e-mail, see ELECTRONIC MAIL (E-MAIL)
TRANSFER OF SPECIMENS                         institutional safeguards, 169-170
see also DISCLOSURE OF PERSONAL                    mobile computing, sample guidelines,
  HEALTH INFORMATION                                  181-182
material transfer agreement, use, 231              small offices, 170
                                                   what you should do, 169-170
TRAVEL SECURITY                                    wireless connections, sample technical
mobile computing, sample guidelines, 181              standards, 183-184

USE OF PERSONAL HEALTH                        WRITTEN STATEMENT OF
  INFORMATION                                   INFORMATION PRACTICES
see also INFORMATION PRACTICES                see also INFORMATION PRACTICES
acceptable use, sample policy, 148-155        access to health records, 77
agents, by, see AGENTS                        collection of information, 49
circle of care, 51-53                         contact person, 17
complaints re, see PRIVACY                    correction of health records, 95
  COMPLAINTS                                  disclosure of information, 55
consent forms                                 requirement, 9
     sample form, 41                          sample statement, 11
     sample withdrawal of consent form, 42    use of information, 49
consent requirements, see CONSENT
fundraising purposes, see FUNDRAISING
grand rounds, 31, 54
key points, 47
lock boxes, see LOCK BOXES
privacy breach, see PRIVACY BREACH
psychiatric facilities, 35-36
religious programs, 58
researchers, 215-216
the rule, 48
unauthorized use by authorized users,
  preventing, 50-51
videotaping, audiotaping and photographing,
  53-54
what you need to do, 49-50
what you should do, 50
written statement, 9, 49
     sample written statement, 11


                                                                                                 375
Circle of Care
Sharing Personal Health
Information for Health-Care
Purposes
August 2015
The Information and Privacy Commissioner of Ontario, can-
ada would like to thank the following organizations for their
participation in this brochure:

College of Physicians and Surgeons of Ontario

Ontario Association of Community Care Access Centres

Ontario Association of Non-Profit Homes and Services for Seniors

Ontario Hospital Association

Ontario Long Term Care Association

Ontario Medical Association

Ontario Ministry of Health and Long-Term Care
CONTENTS
  Introduction.......................................................................................................1

  1. The health information custodian must fall within a
     category of health information custodians that are entitled to
     rely on assumed implied consent. ...................................................................3

  2. The personal health information to be collected, used
     or disclosed by the health information custodian must have
     been received from the individual, his or her substitute
     decision-maker or another health information custodian.................................4

  3. The health information custodian must have received the
     personal health information that is being collected, used or
     disclosed for the purpose of providing or assisting in the
     provision of health care to the individual.........................................................5

  4. The purpose of the collection, use or disclosure of personal
     health information by the health information custodian must
     be for the provision of health care or assisting in the
     provision of health care to the individual.........................................................7

  5. In the context of disclosure, the disclosure of personal health
     information by the health information custodian must be to
     another health information custodian. ............................................................9

  6. The health information custodian that receives the personal
     health information must not be aware that the individual has
     expressly withheld or withdrawn his or her consent to the
     collection, use or disclosure. ........................................................................10

Limiting principles and options when consent cannot be
assumed to be implied. .......................................................................................11




CIRCLE OF CARE: Sharing Personal Health Information for Health-Care Purposes
INTRODUCTION
The term ‚Äúcircle of care‚Äù is not a defined term in the Personal Health Informa-
tion Protection Act, 2004 (PHIPA). It is a term commonly used to describe the
ability of certain health information custodians to assume an individual‚Äôs implied
consent to collect, use or disclose personal health information for the purpose of
providing health care, in circumstances defined in PHIPA.

The purpose of this brochure is to clarify the circumstances in which a health
information custodian may assume implied consent and the options available to
a health information custodian where consent cannot be assumed to be implied.
Throughout the brochure, appropriate application of the assumed implied con-
sent provisions of PHIPA will be illustrated using a variety of health-care sce-
narios involving a fictional 61-year-old gentleman named David Mann. It should
be noted that the assumed implied consent provisions of PHIPA apply equally to
paper-based and electronic records of personal health information.




     In an appointment with his family physician, David Mann complains of memory loss,
     disorientation, speech problems and mood swings.

     The family physician examines David and asks him a series of questions relating to
     his medications, his health history and the health history of his family. The family
     physician also conducts a mini-mental state examination and provides David with a
     requisition for blood and urine testing and for magnetic resonance imaging. The fam-
     ily physician indicates that she will refer David to both a neurologist and geriatrician
     for further assessments.




Circumstances When you may Assume Consent to be
Implied
A health information custodian may only assume an individual‚Äôs implied
consent to collect, use or disclose personal health information if all of the
following six (6) conditions are satisfied.




CIRCLE OF CARE: Sharing Personal Health Information for Health-Care Purposes                    1
    1. THE HEALTH INFORMATION CUSTODIAN MUST FALL WITHIN
       A CATEGORY OF HEALTH INFORMATION CUSTODIANS THAT
       ARE ENTITLED TO RELY ON ASSUMED IMPLIED CONSENT.
    Most health information custodians may rely on assumed implied consent to col-
    lect, use and disclose personal health information for the purpose of providing
    health care or assisting in the provision of health care to an individual.

    A health information custodian is a person or organization described in PHIPA
    with custody or control of personal health information as a result of, or in con-
    nection with, the performance of its powers, duties or work. For example, health
    information custodians include:

       ‚Ä¢ health care practitioners

       ‚Ä¢ long-term care homes

       ‚Ä¢ community care access centres

       ‚Ä¢ hospitals, including psychiatric facilities

       ‚Ä¢ specimen collection centres, laboratories, independent health facilities

       ‚Ä¢ pharmacies

       ‚Ä¢ ambulance services

       ‚Ä¢ Ontario Agency for Health Protection and Promotion

    However, it is important to note that some health information custodians are not
    entitled to rely on assumed implied consent. For example, these include:

       ‚Ä¢ an evaluator within the meaning of the Health Care Consent Act, 1996

       ‚Ä¢ an assessor within the meaning of the Substitute Decisions Act, 1992

       ‚Ä¢ the Minister or Ministry of Health and Long-Term Care

       ‚Ä¢ the Minister or Ministry of Health Promotion

       ‚Ä¢ the Canadian Blood Services




2                             CIRCLE OF CARE: Sharing Personal Health Information for Health-Care Purposes
2. THE PERSONAL HEALTH INFORMATION TO BE COLLECTED,
   USED OR DISCLOSED BY THE HEALTH INFORMATION
   CUSTODIAN MUST HAVE BEEN RECEIVED FROM THE
   INDIVIDUAL, HIS OR HER SUBSTITUTE DECISION-MAKER OR
   ANOTHER HEALTH INFORMATION CUSTODIAN.
The personal health information to be collected, used or disclosed must have
been received from the individual to whom the personal health information
relates, from his or her substitute decision-maker or from another health informa-
tion custodian.

Personal health information is defined in PHIPA as identifying information relating
to the physical or mental health of an individual, the provision of health care to
an individual, the identification of the substitute decision-maker for the individual
and the payments or eligibility of an individual for health care or coverage for
health care, including the individual‚Äôs health number.

A substitute decision-maker is a person authorized under PHIPA to consent on
behalf of an individual to the collection, use or disclosure of personal health
information.

If the personal health information to be collected, used or disclosed was
received from a third party, other than the substitute decision-maker for the
individual or another health information custodian, consent cannot be assumed
to be implied. For example, a health information custodian may not rely on
assumed implied consent if the personal health information was received from an
employer, insurer or educational institution.




     David‚Äôs family physician provides the neurologist and geriatrician with a referral let-
     ter summarizing David‚Äôs symptoms, health history, and family health history, along
     with the results of his examination.

     Can the family physician disclose and can the neurologist and geriatrician col-
     lect this personal health information based on assumed implied consent?

     Yes. The family physician, neurologist and geriatrician may assume implied consent.
     The family physician received the personal health information directly from David
     and the neurologist and geriatrician received the information directly from another
     health information custodian, the family physician, for the purpose of providing
     health care to David.




CIRCLE OF CARE: Sharing Personal Health Information for Health-Care Purposes                   3
           3. THE HEALTH INFORMATION CUSTODIAN MUST HAVE
              RECEIVED THE PERSONAL HEALTH INFORMATION THAT IS
              BEING COLLECTED, USED OR DISCLOSED FOR THE PURPOSE
              OF PROVIDING OR ASSISTING IN THE PROVISION OF HEALTH
              CARE TO THE INDIVIDUAL.
           The personal health information to be collected, used or disclosed must have
           been received for the purpose of providing health care or assisting in the pro-
           vision of health care to the individual to whom it relates. A health information
           custodian may not rely on assumed implied consent if the personal health infor-
           mation was received for other purposes, such as research, fundraising, mar-
           keting or providing health care or assisting in providing health care to another
           individual or group of individuals.




    The geriatrician to whom the referral is made is a co-investigator in a research study
    involving familial predisposition to Alzheimer‚Äôs disease. In the course of the research
    study, while reviewing the list of study participants, the geriatrician notices the name
    ‚ÄúDavid Mann.‚Äù The geriatrician reviews the research file of David Mann and deter-
    mines, based on a comparison with the information contained in the referral letter,
    that it is the same David Mann.

    The geriatrician photocopies the records of personal health information contained
    in the research file and places them in the clinical file for use at an appointment with
    David scheduled for November 13.

    Can the geriatrician use the personal health information in this way based on
    assumed implied consent?

    No. The geriatrician may not assume implied consent because the personal health
    information in the research file was not received for the purpose of providing health
    care or assisting in the provision of health care to David, but rather, for research
    purposes.




4                                      CIRCLE OF CARE: Sharing Personal Health Information for Health-Care Purposes
     Following the appointment with David on November 13, the geriatrician would like
     to contact the laboratory for the results of the blood and urine testing ordered by
     David‚Äôs family physician. The geriatrician would also like to contact the pharmacy
     where David indicated he routinely fills his prescriptions in order to obtain a list of all
     current medications.

     Can the laboratory and pharmacy disclose and can the geriatrician collect this
     personal health information based on assumed implied consent?

     Yes. The laboratory, pharmacy and geriatrician may assume implied consent. The
     personal health information was received by the laboratory and pharmacy, and will
     be received by the geriatrician, for the purpose of providing health care to David.




CIRCLE OF CARE: Sharing Personal Health Information for Health-Care Purposes                       5
           4. THE PURPOSE OF THE COLLECTION, USE OR DISCLOSURE
              OF PERSONAL HEALTH INFORMATION BY THE HEALTH
              INFORMATION CUSTODIAN MUST BE FOR THE PROVISION OF
              HEALTH CARE OR ASSISTING IN THE PROVISION OF HEALTH
              CARE TO THE INDIVIDUAL.
           The collection, use or disclosure must be for the purposes of providing health care
           or assisting in the provision of health care to the individual to whom the per-
           sonal health information relates. A health information custodian may not rely on
           assumed implied consent if the collection, use or disclosure is for other purposes,
           such as research, fundraising, marketing or providing health care or assisting in
           the provision of health care to another individual or group of individuals.


    Several years pass and David‚Äôs cognitive abilities continue to decline. Based on a
    diagnosis of probable Alzheimer‚Äôs disease and the growing loss of David‚Äôs functional
    abilities, David‚Äôs geriatrician makes a referral to the local Community Care Access
    Centre. For purposes of assessing David‚Äôs eligibility and service levels, the care co-
    ordinator at the local Community Care Access Centre contacts David‚Äôs family physi-
    cian to obtain further information about David‚Äôs health history, current medications
    and treatment.

    Can the Community Care Access Centre collect and can the family physician
    disclose this personal health information based on assumed implied consent?

    Yes. The Community Care Access Centre is collecting this personal health informa-
    tion and the family physician is disclosing this personal health information for the
    purpose of providing health care or assisting in the provision of health care to David.

    Ultimately, the local Community Care Access Centre facilitates the placement of
    David into a long-term care home.

    One morning, following breakfast at the long-term care home, David falls and is
    transferred to the hospital by ambulance with a suspected hip fracture.

    The next day David‚Äôs former spouse, a nurse in the labour and delivery unit of the
    hospital, is advised by their son that David was admitted. The nurse looks at David‚Äôs
    electronic health record to determine the reason for admission. The nurse signed a
    confidentiality agreement with the hospital.

    Can the nurse use the personal health information in this way based on
    assumed implied consent?

    No. The nurse may not assume implied consent to use the personal health informa-
    tion because she is not providing health care or assisting in the provision of health
    care to David.



6                                     CIRCLE OF CARE: Sharing Personal Health Information for Health-Care Purposes
     Following a physical examination and X-ray, it is confirmed that David has a hip frac-
     ture and David undergoes a surgical procedure. A week later, David is discharged
     from hospital and returns to the long-term care home.

     Two days following discharge, a nurse at the long-term care home notices small red,
     swollen and pus-filled bumps on David‚Äôs skin. David also complains of fever, chills
     and shortness of breath. Following laboratory testing, David is diagnosed with MRSA
     infection. Since the infection may have been acquired at the hospital, the nurse
     would like to disclose the fact that David has MRSA to the hospital to prevent or
     reduce the risk of a possible outbreak.

     Can this personal health information be disclosed to the hospital by the nurse
     at the long-term care home?

     Yes. PHIPA permits a health information custodian to disclose personal health infor-
     mation without consent if there are reasonable grounds to believe that it is neces-
     sary to eliminate or reduce a significant risk of serious bodily harm to a person or
     group of persons. The nurse, however, may not rely on assumed implied consent
     because the disclosure is not for the purposes of providing health care or assisting
     in providing health care to David.




CIRCLE OF CARE: Sharing Personal Health Information for Health-Care Purposes                  7
           5. IN THE CONTEXT OF DISCLOSURE, THE DISCLOSURE
              OF PERSONAL HEALTH INFORMATION BY THE HEALTH
              INFORMATION CUSTODIAN MUST BE TO ANOTHER HEALTH
              INFORMATION CUSTODIAN.
           A health information custodian may not assume an individual‚Äôs implied consent
           in disclosing personal health information to a person or organization that is not a
           health information custodian, regardless of the purpose of the disclosure.




    David is planning to attend an outing away from the long-term care home and will be
    accompanied by his cousin and the spouse of his cousin.

    On the Wednesday prior to the outing, the spouse of David‚Äôs cousin contacts the
    long-term care home. She would like information about the medications David is
    currently taking, including the frequency and dose, and ‚Äúany other information about
    his condition‚Äù that will assist her in ‚Äúhelping David.‚Äù

    Can the long-term care home disclose this personal health information based
    on assumed implied consent?

    No. The long-term care home may not assume implied consent because the spouse
    of David‚Äôs cousin is not a health information custodian within the meaning of PHIPA.




8                                    CIRCLE OF CARE: Sharing Personal Health Information for Health-Care Purposes
6. THE HEALTH INFORMATION CUSTODIAN THAT RECEIVES THE
   PERSONAL HEALTH INFORMATION MUST NOT BE AWARE
   THAT THE INDIVIDUAL HAS EXPRESSLY WITHHELD OR
   WITHDRAWN HIS OR HER CONSENT TO THE COLLECTION,
   USE OR DISCLOSURE.
PHIPA permits an individual to expressly withhold or withdraw consent to the
collection, use or disclosure of his or her personal health information, unless
the collection, use or disclosure is permitted or required by PHIPA to be made
without consent. In most circumstances, if an individual decides to withhold or
withdraw consent, PHIPA requires the receiving health information custodians or
their agents to be notified if the disclosing health information custodian is pre-
vented from disclosing all of the information that is considered to be reasonably
necessary for the provision of health care.

For further information about the ability of an individual to expressly withhold or
withdraw consent to the collection, use or disclosure of personal health informa-
tion for health-care purposes, and the obligations on health information custodians
in this context, please refer to the Lock-box Fact Sheet produced by the Informa-
tion and Privacy Commissioner of Ontario, which is available at www.ipc.on.ca.


     David must visit the orthopedic clinic of the hospital for follow up related to his hip
     fracture. The orthopedic clinic is staffed by physiotherapists, occupational thera-
     pists, physicians and nurses.

     David‚Äôs current spouse, who is his substitute decision-maker, learns that his former
     spouse, who was a nurse in the labour and delivery unit of the hospital, now works
     as a nurse in the orthopedic clinic. David‚Äôs current spouse wants to ensure that
     the former spouse and her colleagues do not view David‚Äôs electronic health record.
     David‚Äôs current spouse requests the hospital to ensure that only the orthopedic
     surgeon and the physiotherapist providing health care to David are permitted to view
     his electronic health record.

     Can David‚Äôs current spouse make this request?

     Yes. David has been determined to be incapable of consenting to the collection, use
     and disclosure of personal health information and his current spouse is hi substitute
     decision-maker for these purposes. As the substitute decision-maker, David‚Äôs cur-
     rent spouse may expressly withhold or withdraw consent to the collection, use and
     disclosure of David‚Äôs personal health information. The hospital, as a health informa-
     tion custodian, must comply with this decision unless the collection, use or disclo-
     sure is required or permitted by PHIPA to be made without consent.



CIRCLE OF CARE: Sharing Personal Health Information for Health-Care Purposes                   9
     LIMITING PRINCIPLES AND OPTIONS WHEN CONSENT CANNOT
     BE ASSUMED TO BE IMPLIED.

     FACTORS TO BE CONSIDERED IN RELYING ON ASSUMED IMPLIED
     CONSENT

     In general, a health information custodian must not collect, use or disclose
     personal health information if other information will serve the purpose and must
     not collect, use or disclose more personal health information than is reasonably
     necessary for that purpose. These general limiting principles apply even where
     a health information custodian is entitled to rely on an individual‚Äôs assumed
     implied consent.


     OPTIONS AVAILABLE WHEN YOU CANNOT ASSUME CONSENT TO BE
     IMPLIED

     When consent cannot be assumed to be implied, health information custodians
     should consider other options. Depending on the circumstances, a health infor-
     mation custodian may be permitted to collect, use or disclose personal health
     information without consent, with the implied consent of the individual to whom
     the personal health information relates or with the express consent of that indi-
     vidual. PHIPA distinguishes between implied consent and assumed implied con-
     sent. In the case of implied consent, health information custodians must ensure
     that all of the elements of consent are fulfilled; whereas in the case of assumed
     implied consent, health information custodians may assume that all of the ele-
     ments of consent are fulfilled, unless it is not reasonable to do so in the circum-
     stances.


     WITHOUT CONSENT

     Health information custodians may collect, use or disclose personal health
     information without consent if the collection, use or disclosure is permitted or
     required by PHIPA to be made without consent.1 For example, health information
     custodians are permitted to disclose personal health information without con-
     sent to a medical officer of health if the disclosure is made for purposes of the
     Health Protection and Promotion Act. In addition, in certain circumstances set
     out in sections 37(1)(a), 38(1)(a) and 50(1)(e) of PHIPA, health information custo-
     1   Sections 36 and 37 of PHIPA, respectively, set out the circumstances in which personal health infor-
         mation may be collected and used without consent and sections 38 - 48 and section 50 set out the
         circumstances in which personal health information is permitted or required to be disclosed without
         consent.




10                                   CIRCLE OF CARE: Sharing Personal Health Information for Health-Care Purposes
dians may use or disclose personal health information without consent where it
is reasonably necessary for the provision of health care and the individual has
not expressly instructed otherwise.


IMPLIED CONSENT

Health information custodians may imply an individual‚Äôs consent to collect and
use personal health information for most purposes. They may also imply consent
to disclose personal health information to another health information custodian
for the purpose of providing or assisting in the provision of health care to the
individual. However, subject to limited exceptions, health information custodians
cannot rely on implied consent when disclosing personal health information to a
person or organization that is not a health information custodian. This exception
applies regardless of the purpose of the disclosure.

In order to rely on implied consent, health information custodians must be satis-
fied that all the required elements of consent are fulfilled.


EXPRESS CONSENT

In all other circumstances, health information custodians may only collect, use
or disclose personal health information with the express consent, (i.e., verbal or
written consent) of the individual to whom the personal health information relates
or his or her substitute decision-maker.

In order to rely on express consent, health information custodians must be satis-
fied that all of the required elements of consent are fulfilled.




CIRCLE OF CARE: Sharing Personal Health Information for Health-Care Purposes         11
     ELEMENTS OF CONSENT

     The consent of an individual for the collection, use or disclosure of personal health
     information by a health information custodian:

      ‚Ä¢ Must be a consent of the individual or his or her substitute decision-maker;

      ‚Ä¢ Must be knowledgeable;

      ‚Ä¢ Must relate to the information that will be collected, used or disclosed; and

      ‚Ä¢ Must not be obtained through deception or coercion.

     For consent to be knowledgeable, it must be reasonable to believe that the individ-
     ual knows the purpose of the collection, use or disclosure and knows that he or she
     may give or withhold consent.

     It is reasonable to believe that an individual knows the purpose of the collection,
     use or disclosure if the health information custodian posts or makes readily avail-
     able a notice describing these purposes where it is likely to come to the individual‚Äôs
     attention or provides the individual with such a notice. Although health information
     custodians are not required to provide notice in those circumstances where consent
     may be assumed to be implied, health information custodians are encouraged to do
     so as a best practice.




12                                     CIRCLE OF CARE: Sharing Personal Health Information for Health-Care Purposes
ABOUT THE INFORMATION AND PRIVACY COMMISSIONER OF ONTARIO

The role of the Information and Privacy Commissioner of Ontario is set out
in three statutes: the Freedom of Information and Protection of Privacy Act,
the Municipal Freedom of Information and Protection of Privacy Act and
the Personal Health Information Protection Act. The Commissioner acts
independently of government to uphold and promote open government and
the protection of personal privacy.

Under the three Acts, the Commissioner:

   ‚Ä¢ Resolves access to information appeals and complaints when
     government or health care practitioners and organizations refuse to
     grant requests for access or correction;

   ‚Ä¢ Investigates complaints with respect to personal information held by
     government or health care practitioners and organizations;

   ‚Ä¢ Conducts research into access and privacy issues;

   ‚Ä¢ Comments on proposed government legislation and programs; and

   ‚Ä¢ Educates the public about Ontario‚Äôs access and privacy laws.
Information and Privacy Commissioner of Ontario
2 Bloor Street East, Suite 1400
Toronto, Ontario
Canada M4W 1A8

Web site: www.ipc.on.ca
Telephone: 416-326-3333
Email: info@ipc.on.ca

August 2015
Frequently Asked Questions
Personal Health Information
Protection Act

September 2015
This Frequently Asked Questions (FAQ) provides a general
overview of the Personal Health Information Protection Act
and Regulation 329/04. The information contained in this
document is for general reference purposes only and should
not be considered as legal advice. Legal counsel should be
consulted for all purposes of interpretation. This FAQ is not
binding on the Information and Privacy Commissioner of
Ontario (IPC) and should not be construed to interfere with
the IPC‚Äôs ability to discharge its duties under the Personal
Health Information Protection Act.

This publication is also available on the IPC website.

Cette publication est √©galement disponible en fran√ßais.
CONTENTS
Introduction.............................................................................................. 1
     What is the Personal Health Information Protection Act
     and why is it necessary?........................................................................ 1

Overview................................................................................................... 3
     What is the purpose of PHIPA?............................................................... 3
     What rights do individuals have?............................................................ 3
     What is the relationship between PHIPA and the federal Personal
     Information Protection and Electronic Documents Act (PIPEDA)?.............. 4
     What is the relationship between PHIPA, the Freedom of Information
     and Protection of Privacy Act (FIPPA) and the Municipal Freedom of
     Information and Protection of Privacy Act (MFIPPA)?............................... 5

Interpretation and Application of PHIPA..................................................... 6
     To whom does PHIPA apply?.................................................................. 6
     What is personal health information?...................................................... 6
     What does ‚Äúhealth care‚Äù mean?.............................................................. 7
     What is a custodian?.............................................................................. 7
     Is a health care practitioner working for a non-custodian
     considered to be a custodian? ............................................................... 8
     What is an agent?................................................................................. 9
     Does PHIPA apply to insurance companies or employers?....................... 10
     What is an electronic service provider?................................................. 10
     What is a health information network provider?..................................... 10
     Who is a prescribed person?................................................................. 11
     What is a prescribed entity?................................................................. 12

Practices to Protect Personal Health Information...................................... 13
     How does PHIPA protect personal health information?........................... 13
     What are the notification requirements in PHIPA in the event
     of a breach?....................................................................................... 14
     Do custodians have responsibilities with respect to accountability
     and openness?.................................................................................... 14
     What are the requirements for the treatment of personal
     health records in the event of a change in practice?............................... 15


Frequently Asked Questions: Personal Health Information Protection Act
Consent Concerning Personal Health Information...................................... 16
    What are the requirements for consent?............................................... 16
    What is the difference between express and implied consent?................. 16
    When is express consent required?....................................................... 17
    When is implied consent sufficient?...................................................... 17
    What is the ‚Äòcircle of care‚Äô?.................................................................. 18
    When can custodians assume implied consent?...................................... 19
    Are pharmacists required to obtain express consent from an
    individual to disclose personal health information to a third party
    benefits payor?................................................................................... 20
    Can individuals control what personal health information is recorded
    in their file?........................................................................................ 20
    Can individuals withdraw their consent?................................................ 20
    What is a ‚Äòlock-box‚Äô?........................................................................... 21
    What are the restrictions and limitations on the lock-box?..................... 21
    What happens when an individual is incapable of providing consent?....... 22
    Can a child under 16 years old provide consent?.................................... 23
    Can another person, such as a family member, provide consent
    on an individual‚Äôs behalf when picking up or dropping off
    a prescription?.................................................................................... 23

Collection, Use and Disclosure of Personal Health Information................... 24
    What are the general limitations on the collection, use and disclosure
    of personal health information?............................................................ 24
    Collection........................................................................................... 24
       What is a collection of personal health information
       under PHIPA?................................................................................. 24
       What are the rules regarding the collection of personal health
       information?................................................................................... 25
       When can custodians indirectly collect personal
       health information?......................................................................... 25
    Use.................................................................................................... 26
       What is a use of personal health information under PHIPA? ............... 26
       What are the rules regarding the use of personal
       health information?......................................................................... 26
       When can personal health information be used without consent?........ 26


                                          Frequently Asked Questions: Personal Health Information Protection Act
     Disclosure.......................................................................................... 27
         What is a disclosure of personal health information
         under PHIPA?................................................................................. 27
         What are the rules regarding the disclosure of personal
         health information?......................................................................... 27
         When can personal health information be disclosed without
         consent?........................................................................................ 28
         Can personal health information be disclosed in the event of
         an emergency?............................................................................... 29
         Does PHIPA permit disclosure of personal health information
         about a deceased individual?........................................................... 30
         Can a custodian disclose personal health information to the
         Workplace Safety and Insurance Board (WSIB) about an injured
         worker without the individual‚Äôs consent?........................................... 31
         Can a custodian store, access or disclose personal health
         information outside of Ontario?........................................................ 31

Fundraising and Marketing....................................................................... 33
     Can custodians collect, use or disclose personal health information
     for fundraising activities?.................................................................... 33
     Can personal health information be collected, used or disclosed for
     marketing purposes?........................................................................... 34

Research................................................................................................. 35
     What are the requirements for the collection, use and disclosure
     of personal health information for research?......................................... 35
     Are there any requirements for research ethics boards and
     research plans?................................................................................... 36

Ontario Health Cards and Health Numbers................................................ 37
     Who can collect, use or disclose Ontario health numbers and
     under what circumstances?.................................................................. 37
     Are other organizations permitted to request the production
     of a health card?................................................................................. 37

Access to Records of personal Health Information and Correction.............. 39
     Access............................................................................................... 39
         Are individuals permitted to access their own personal health
         information?................................................................................... 39




Frequently Asked Questions: Personal Health Information Protection Act
       How do an individual obtain access to their personal
       health information?......................................................................... 40
       How long does a custodian have to respond to an individual‚Äôs
       request for access to personal health information?............................ 40
       Can a custodian refuse to provide access to an individual‚Äôs
       personal health information?........................................................... 41
       Is there a fee associated with an access request?............................... 41
       What if the custodian works for a non-custodian that is covered
       under public sector access and privacy legislation, such as a
       school board or municipality?.......................................................... 42
    Correction.......................................................................................... 42
       Can individuals correct errors in their personal
       health information?......................................................................... 42
       How does an individual correct errors?............................................. 43
       Can a custodian refuse to correct an individual‚Äôs personal health
       information?................................................................................... 43

Administration and Enforcement.............................................................. 44
    How is PHIPA enforced?...................................................................... 44
    How does an individual initiate a complaint?.......................................... 44
    Is there a time limit within which an individual may complain?................. 45
    If a person is not satisfied with an IPC order, what can be done?.............. 45
    Can a person seek compensation for damages? .................................... 45
    What is an offence under PHIPA?......................................................... 45
    What are the consequences for committing an offence
    under PHIPA?..................................................................................... 46
    Who is responsible for prosecuting offences under PHIPA?..................... 46




                                         Frequently Asked Questions: Personal Health Information Protection Act
INTRODUCTION
WHAT IS THE PERSONAL HEALTH INFORMATION
PROTECTION ACT AND WHY IS IT NECESSARY?
The Personal Health Information Protection Act (PHIPA) is Ontario‚Äôs health-
specific privacy legislation which came into force on November 1, 2004. PHIPA
governs the manner in which personal health information may be collected, used
and disclosed within the health sector. It regulates health information custodians
(custodians), as well as individuals and organizations that receive personal health
information from custodians.

Personal health information is among the most sensitive of personal
information. People are understandably protective about sharing personal
details relating to their medical conditions. At the same time, personal health
information must flow freely between health care practitioners in order to
ensure the best care for patients.

The nature of our health system is that personal health information passes
through many links in the health care chain: from a doctor‚Äôs office, to a referral
to a specialist, to a medical lab, to a hospital or to an insurance company for
reimbursement of claims. There are many circumstances in which personal health
information must be readily, as well as expeditiously shared, such as in the case
of a medical emergency. Beyond patient care, personal health information is
needed for important activities, such as health research which is vital to develop
new treatments and cures.

PHIPA creates a consistent approach to protecting personal health information
across the health sector. The legislation was designed to give individuals
greater control over how their personal health information is collected, used or
disclosed. PHIPA balances the privacy rights of individuals with the legitimate
need of custodians to collect, use and disclose personal health information in
order to deliver effective and timely health care and to plan and manage our
publicly funded health system.

With limited exceptions, PHIPA requires custodians to obtain consent before
personal health information is collected, used or disclosed. In addition, PHIPA
provides individuals with a right to access and request correction of their
personal health information. PHIPA also provides a means for redress through
the Office of the Information and Privacy Commissioner of Ontario (IPC) when
privacy rights relating to personal health information have been violated.




Frequently Asked Questions: Personal Health Information Protection Act                1
     What is the   The IPC is the designated oversight body responsible for administering
Personal Health    and enforcing these health sector privacy rules. As such, we have prepared
    Information    the following questions and answers to guide Ontarians and custodians in
 Protection Act    understanding their respective privacy rights and obligations.
   and why is it
     necessary?




     2                                           Frequently Asked Questions: Personal Health Information Protection Act
OVERVIEW
WHAT IS THE PURPOSE OF PHIPA?
PHIPA establishes rules for the collection, use and disclosure of personal health
information and includes provisions that:

     ‚Ä¢ require consent for the collection, use and disclosure of personal health
       information, with necessary but limited exceptions,

     ‚Ä¢ require that custodians treat all personal health information as confidential
       and keep it secure,

     ‚Ä¢ provide individuals with a right of access to their personal health
       information, as well as the right to correct errors,

     ‚Ä¢ give individuals the right to withhold or withdraw consent to the collection,
       use or disclosure of personal health information or to expressly instruct
       custodians not to use or disclose their personal health information for
       health care purposes,

     ‚Ä¢ establish clear rules for the collection, use and disclosure of personal
       health information for fundraising and marketing purposes,

     ‚Ä¢ set guidelines for the collection, use and disclosure of personal health
       information for research purposes,

     ‚Ä¢ ensure accountability by granting individuals the right to complain to the
       IPC about the practices of custodians and

     ‚Ä¢ establish remedies for breaches of the legislation.



WHAT RIGHTS DO INDIVIDUALS HAVE?
PHIPA gives individuals the right to:

     ‚Ä¢ be informed of the purposes for the collection, use and disclosure of
       personal health information,

     ‚Ä¢ be notified by a custodian if personal health information has been stolen,
       lost or accessed by unauthorized persons,

     ‚Ä¢ refuse or give consent to the collection, use or disclosure of personal
       health information, except in circumstances specified in PHIPA,




Frequently Asked Questions: Personal Health Information Protection Act                 3
   What rights do      ‚Ä¢ withdraw consent by providing notice to the custodian,
individuals have?
                       ‚Ä¢ expressly instruct a custodian not to use or disclose personal health
                         information for health care purposes without consent,

                       ‚Ä¢ access a copy of their own personal health information, except in limited
                         circumstances specified in PHIPA,

                       ‚Ä¢ request corrections to be made to their personal health information,

                       ‚Ä¢ complain to the IPC about a custodian‚Äôs refusal to give access to all or
                         part of a record of personal health information,

                       ‚Ä¢ complain to the IPC about a custodian‚Äôs refusal to grant a correction
                         request,

                       ‚Ä¢ complain to the IPC about any breach or potential breach of PHIPA or its
                         regulations and

                       ‚Ä¢ begin a proceeding in court for damages for actual harm suffered, if
                         affected by a final order or conduct leading to a final conviction for an
                         offence under PHIPA.

                    PHIPA establishes a formal process for individuals to access and correct
                    their personal health information, within specified time frames and the right to
                    complain if an access or correction request is denied.



                    WHAT IS THE RELATIONSHIP BETWEEN PHIPA AND THE
                    FEDERAL PERSONAL INFORMATION PROTECTION AND
                    ELECTRONIC DOCUMENTS ACT (PIPEDA)?
                    The collection, use and disclosure of personal information within the commercial
                    sector is regulated by federal privacy legislation‚Äîthe Personal Information
                    Protection and Electronic Documents Act. PIPEDA was enacted to regulate the
                    collection, use or disclosure of personal information in the hands of private sector
                    organizations. PIPEDA does not apply to personal information in provinces and
                    territories that have ‚Äúsubstantially similar‚Äù privacy legislation in place.

                    The federal government has deemed PHIPA to be ‚Äúsubstantially similar‚Äù to
                    PIPEDA. Custodians and their agents are exempted from having to comply with
                    the provisions of PIPEDA to the extent that they collect, use and disclose personal
                    health information within Ontario. PIPEDA continues to apply to all commercial
                    activities relating to the exchange of personal health information between
                    provinces and territories and to information transfers outside of Canada.




      4                                             Frequently Asked Questions: Personal Health Information Protection Act
WHAT IS THE RELATIONSHIP BETWEEN PHIPA, THE
FREEDOM OF INFORMATION AND PROTECTION OF
PRIVACY ACT (FIPPA) AND THE MUNICIPAL FREEDOM
OF INFORMATION AND PROTECTION OF PRIVACY ACT
(MFIPPA)?
Organizations that are both custodians under PHIPA and institutions under
public sector privacy and access to information legislation, namely the provincial
FIPPA or its municipal counterpart MFIPPA, include hospitals, the Ontario
Agency for Health Protection and Promotion, the Ministry of Health and Long-
Term Care, medical officers of health and municipally operated long-term care
homes and ambulance services.

The general rule is that, subject to certain exceptions, a custodian that is also an
institution or a part of an institution is governed by PHIPA, not FIPPA or MFIPPA,
with respect to personal health information in its custody or under its control.
All other recorded information about an individual that is not personal health
information and that is in the custody or under the control of an organization that
is both a custodian and an institution or part of an institution is subject to FIPPA
or MFIPPA, as the case may be.

PHIPA also contains provisions that are specific to custodians that are
institutions. For example, PHIPA provides a number of exceptions to the general
rule that custodians are only permitted to collect personal health information
directly from the individual to whom the information relates. In addition to the
exceptions available to all custodians, a custodian that is also an institution
under FIPPA or MFIPPA may collect personal health information indirectly for a
purpose related to investigating a breach of an agreement or a contravention or
alleged contravention of laws of Ontario or Canada, the conduct of a proceeding
or possible proceeding or the statutory function of the custodian.

For further information, please see the IPC documents Applying PHIPA and
FIPPA/MFIPPA to Personal Health Information, Freedom of Information at Ontario
Hospitals: Frequently Asked Questions, and Applying PHIPA and FIPPA to
Personal Health Information: Guidance for Hospitals.




Frequently Asked Questions: Personal Health Information Protection Act                 5
    INTERPRETATION AND
    APPLICATION OF PHIPA
    TO WHOM DOES PHIPA APPLY?
    PHIPA applies to a wide variety of persons and organizations defined as health
    information custodians. PHIPA also applies to agents who are authorized to
    act for or on behalf of custodians. Additionally, PHIPA applies to the use and
    disclosure of personal health information by those who receive personal health
    information from custodians (recipients) and to electronic service providers,
    including health information network providers.



    WHAT IS PERSONAL HEALTH INFORMATION?
    Personal health information is ‚Äúidentifying information‚Äù about an individual,
    whether oral or recorded if the information:

       ‚Ä¢ relates to the individual‚Äôs physical or mental condition, including family
         medical history,

       ‚Ä¢ relates to the provision of health care to the individual,

       ‚Ä¢ is a plan of service for the individual,

       ‚Ä¢ relates to payments, or eligibility for health care or for coverage for health
         care,

       ‚Ä¢ relates to the donation of any body part or bodily substance or is derived
         from the testing or examination of any such body part or bodily substance,

       ‚Ä¢ is the individual‚Äôs health number or

       ‚Ä¢ identifies a health care provider or a substitute decision-maker for the
         individual.

    ‚ÄúIdentifying information‚Äù includes information that identifies an individual or for
    which it is reasonably foreseeable that it could be used, either alone or with
    other information, to identify an individual.

    Personal health information includes identifying information that is not personal
    health information but that is contained in a record that contains personal health
    information. Personal health information does not include identifying information




6                                    Frequently Asked Questions: Personal Health Information Protection Act
about an employee or agent of the custodian that is not maintained primarily for    What is
the provision of health care. For example, a doctor‚Äôs note to support an absence    personal health
from work in the personnel file of a secretary employed by a custodian is not       information?
personal health information.



WHAT DOES ‚ÄúHEALTH CARE‚Äù MEAN?
‚ÄúHealth care‚Äù means any observation, examination, assessment, care,
service or procedure that is done for a health-related purpose and that is
carried out or provided:

     ‚Ä¢ for diagnosis, treatment or maintenance of an individual‚Äôs physical or
       mental condition,

     ‚Ä¢ for prevention of disease or injury or the promotion of health or

     ‚Ä¢ as part of palliative care.

It also includes:

     ‚Ä¢ the compounding, dispensing or selling of a drug, device or equipment
       pursuant to a prescription,

     ‚Ä¢ a community service that is described in the Home Care and Community
       Services Act and

     ‚Ä¢ taking blood or a blood product donation from an individual.



WHAT IS A CUSTODIAN?
A custodian is a person or organization listed in PHIPA that, as a result of his,
her or its power or duties or work set out in PHIPA, has custody or control of
personal health information. Examples of custodians include:

     ‚Ä¢ health care practitioners, (including doctors, nurses, speech-language
       pathologists, chiropractors, dental professionals, dieticians, medical
       laboratory technologists, massage therapists, midwives, occupational
       therapists, opticians and physiotherapists),

     ‚Ä¢ community care access corporations,

     ‚Ä¢ hospitals,

     ‚Ä¢ psychiatric facilities,

     ‚Ä¢ long-term care homes,



Frequently Asked Questions: Personal Health Information Protection Act                       7
  What is a      ‚Ä¢ pharmacies,
custodian?
                 ‚Ä¢ laboratories,

                 ‚Ä¢ ambulance services,

                 ‚Ä¢ retirement homes and homes for special care,

                 ‚Ä¢ medical officers of health of boards of health,

                 ‚Ä¢ the Minister of Health and Long-Term Care and

                 ‚Ä¢ Canadian Blood Services.

              A custodian does not include:

                 ‚Ä¢ a health care practitioner, service provider, evaluator or assessor who is an
                   agent of a custodian,

                 ‚Ä¢ a person authorized to act for or on behalf of a person that is not a
                   custodian, if the scope of duties of the authorized person does not include
                   the provision of health care,

                 ‚Ä¢ an aboriginal healer who provides traditional healing services to aboriginal
                   persons or members of an aboriginal community,

                 ‚Ä¢ an aboriginal midwife who provides traditional midwifery services to
                   aboriginal persons or members of an aboriginal community and

                 ‚Ä¢ a person who provides treatment solely by spiritual means or by prayer.



              IS A HEALTH CARE PRACTITIONER WORKING FOR A
              NON-CUSTODIAN CONSIDERED TO BE A CUSTODIAN?
              A health care practitioner, who provides health care, but who contracts with, is
              employed by or volunteers for an organization that is not defined as a custodian
              under PHIPA, would fall within the definition of a custodian under PHIPA and
              must comply with all requirements for custodians.

              Examples of custodians who work for non-custodians include:

                 ‚Ä¢ a nurse employed by a school board to provide health care services to
                   students,

                 ‚Ä¢ a doctor employed by a professional sports team in order to diagnose
                   sporting injuries,




8                                             Frequently Asked Questions: Personal Health Information Protection Act
     ‚Ä¢ a registered massage therapist providing health care services to clients of Is a health care
       a spa and                                                                   practitioner
                                                                                   working for a
    ‚Ä¢ a nurse employed in-house by a manufacturing firm in a health care capacity.
                                                                                   non-custodian
A custodian cannot disclose personal health information to a non-custodian,        considered to be a
including the non-custodian for whom the individual is working, unless the         custodian?
individual whose personal health information is at issue has given express
consent or the disclosure is permitted or required by PHIPA or another law.

For further information, please see the IPC fact sheet, Health Information
Custodians Working for Non-Health Information Custodians.



WHAT IS AN AGENT?
PHIPA defines an agent to include any person who is authorized by a custodian
to perform services or activities in respect of personal health information on the
custodian‚Äôs behalf and for the purposes of that custodian.

An agent may include a person or company that contracts with, is employed
by or volunteers for a custodian and, as a result, may have access to personal
health information. PHIPA permits custodians to provide personal health
information to their agents only if the custodian is permitted to collect, use,
disclose, retain or dispose of the information.

For example, an agency relationship under PHIPA includes a nurse who is
employed by, or a student who volunteers at, a hospital. An agency relationship
may also include a physician who is not employed by a hospital, but has
admitting privileges to use the hospital‚Äôs equipment or facilities. In such cases,
the custodian hospital is permitted to authorize the agent to handle or deal with
personal health information on its behalf, as long as the agent complies with
PHIPA and adopts the information practices of the custodian. An agent must
notify the custodian if the personal health information the agent is handling is
stolen, lost or accessed by unauthorized persons.

The custodian remains accountable for the personal health information in its
custody or under its control, even where the agent is authorized to act on its
behalf with respect to that personal health information. The custodian also
remains accountable for the personal health information in its custody or under
its control where the agent acted beyond what was authorized by the custodian.
For example, in Order HO-013, employees were found to be agents when they
used and/or disclosed personal health information in the custody or under the
control of a hospital for the purpose of selling or marketing Registered Education
Saving Plans. The custodian hospital was accountable for the contravention of
PHIPA, even though the agents may have acted beyond the authority delegated
by the hospital.



Frequently Asked Questions: Personal Health Information Protection Act                       9
     DOES PHIPA APPLY TO INSURANCE COMPANIES OR
     EMPLOYERS?
     Certain organizations, such as insurance companies and employers, who may
     hold personal health information in their files, are not governed by PHIPA, unless
     they receive personal health information from a custodian. When an insurance
     company or employer receives personal health information from a custodian,
     the receiving entity may, in general, only use or disclose the information for the
     authorized purpose for which the information was disclosed or for the purpose
     of carrying out a statutory or legal duty. This rule is colloquially referred to as the
     ‚Äúrecipient rule.‚Äù

     However, an exception to the recipient rule applies to insurance providers that
     receive personal health information from a pharmacist. In that situation, PHIPA
     permits the insurance provider to disclose personal health information to the
     pharmacist to assist the pharmacist in advising the individual or providing the
     individual with health care. For example, the insurance provider may disclose to
     a pharmacist the types of medications an individual has purchased from different
     pharmacies so that the pharmacist may advise of any incompatible prescriptions.



     WHAT IS AN ELECTRONIC SERVICE PROVIDER?
     An electronic service provider is a person who supplies services that enable a
     custodian to collect, use, modify, disclose, retain or dispose of personal health
     information electronically. If the electronic service provider is not an agent of the
     custodian, then it shall not use any personal health information to which it has
     access in the course of providing services to the custodian, except as necessary
     in the course of providing the service and it cannot disclose the information.
     Electronic service providers must also ensure their employees or any other
     persons acting on their behalf agree to comply with these restrictions.



     WHAT IS A HEALTH INFORMATION NETWORK PROVIDER?
     PHIPA contains requirements that apply to a specific type of electronic
     service provider, referred to as a health information network provider. A health
     information network provider is a person who provides services to two or more
     custodians, where the services are provided primarily to enable the custodians
     to use electronic means to disclose personal health information to one another,
     whether or not the person is an agent of any of the custodians. Among other
     requirements, health information network providers must:




10                                    Frequently Asked Questions: Personal Health Information Protection Act
     ‚Ä¢ notify the custodian of any breaches,                                          What is a health
                                                                                      information
     ‚Ä¢ perform threat risk assessments and privacy impact assessments,
                                                                                      network provider?
     ‚Ä¢ upon request, provide an electronic record to the custodian of all accesses
       and transfers of the personal health information,

     ‚Ä¢ ensure that retained third parties comply with necessary restrictions and
       conditions,

     ‚Ä¢ enter into a written agreement with the custodian and

     ‚Ä¢ make publicly available information about its services to the custodian.



WHO IS A PRESCRIBED PERSON?
The regulations prescribe a list of persons who compile and maintain registries
of personal health information for the purpose of facilitating or improving the
provision of health care or that relates to the storage or donation of bodily parts
or substances. Custodians are permitted to disclose personal health information
without consent to these listed persons. They consist of the following:

     ‚Ä¢ Cardiac Care Network of Ontario in respect of its registry of cardiac
       services,

     ‚Ä¢ INSCYTE (Information System for Cytology etc.) Corporation in respect of
       CytoBase,

     ‚Ä¢ Hamilton Health Sciences Corporation in respect of the Critical Care
       Information System,

     ‚Ä¢ Cancer Care Ontario in respect of the Ontario Cancer Screening Registry,

     ‚Ä¢ Children‚Äôs Hospital of Eastern Ontario in respect of the Better Outcomes
       Registry and Network and

     ‚Ä¢ Ontario Institute for Cancer Research in respect of the Ontario Tumour
       Bank.

The above-noted prescribed persons may use and disclose personal health
information for the purpose of facilitating or improving the provision of health
care or for the storage or donation of bodily parts or substances. They are
also permitted to use and disclose personal health information for research
purposes, with a research plan approved by a research ethics board (REB) in
certain circumstances. These prescribed persons are also permitted to disclose
personal health information to prescribed entities for the planning, management
or analysis of the health system.



Frequently Asked Questions: Personal Health Information Protection Act                       11
Who is a prescribed The regulations also require that prescribed persons make publicly available:
            person?
                      ‚Ä¢ a plain language description of the functions of the registry and

                      ‚Ä¢ a summary of the practices and procedures to protect the privacy of the
                        individuals whose personal health information they receive and to maintain
                        the confidentiality of the information.

                   The prescribed person must have its practices and procedures approved by the
                   IPC every three years.

                   For further information, please see the IPC‚Äôs Manual for the Review and Approval
                   of Prescribed Persons and Prescribed Entities.



                   WHAT IS A PRESCRIBED ENTITY?
                   The regulations prescribe a list of entities, including any registries maintained
                   within these listed entities, that custodians are permitted to disclose personal
                   health information to without consent for purposes of planning, management and
                   analysis of the health system. Prescribed entities consist of the following:

                      ‚Ä¢ Cancer Care Ontario,

                      ‚Ä¢ Canadian Institute for Health Information,

                      ‚Ä¢ Institute for Clinical Evaluative Sciences and

                      ‚Ä¢ Pediatric Oncology Group of Ontario.

                   In certain circumstances, with a research plan approved by a research ethics
                   board, these prescribed entities are permitted to use and disclose personal
                   health information for research purposes as if they were custodians. A
                   prescribed entity is permitted to disclose personal health information to a
                   prescribed person who compiles or maintains a registry of personal health
                   information, and to another prescribed entity for purposes related to the
                   planning, management and analysis of the health system.

                   The regulations also require that prescribed entities make publicly available:

                      ‚Ä¢ a plain language description of the functions of the entity and

                      ‚Ä¢ a summary of the practices and procedures to protect the privacy of the
                        individuals whose personal health information they receive and to maintain
                        the confidentiality of the information.

                   The prescribed entity must have its practices and procedures approved by the
                   IPC every three years.

                   For further information, please see the IPC‚Äôs Manual for the Review and Approval
                   of Prescribed Persons and Prescribed Entities.


       12                                          Frequently Asked Questions: Personal Health Information Protection Act
PRACTICES TO PROTECT
PERSONAL HEALTH
INFORMATION
HOW DOES PHIPA PROTECT PERSONAL HEALTH
INFORMATION?
PHIPA establishes certain privacy rights for individuals and imposes specific
obligations on custodians in protecting personal health information. Custodians
who have custody or control of personal health information must develop and
implement information practices that comply with the requirements of PHIPA.
Custodians must also ensure that personal health information is as accurate,
up-to-date and complete as is necessary for the purposes for which they use or
disclose personal health information.

PHIPA requires custodians to take steps that are reasonable in the
circumstances to ensure personal health information in their custody or under
their control is protected against theft, loss and unauthorized use and disclosure,
and to ensure that records of personal health information are protected against
unauthorized copying, modification or disposal.

PHIPA also requires custodians to ensure that records of personal health
information are retained, transferred and disposed of in a secure manner.
According to the definition of ‚Äúdisposed of in a secure manner‚Äù in the
regulations, records of personal health information must be destroyed in such a
manner that their reconstruction is not reasonably foreseeable.

PHIPA requires records of personal health information to be kept for as long as
needed to allow an individual to exhaust any legal recourse regarding an access
request. As PHIPA does not establish specific retention periods for personal
health information, custodians should refer to their governing legislation to
determine applicable record retention requirements. For example, regulations
under the Public Hospitals Act specify how long hospitals must retain records of
personal health information.

For further information, please see the IPC fact sheets, Safeguarding Personal
Health Information, Secure Destruction of Personal Health Information, Encrypting
Personal Health Information on Mobile Devices, Health-Care Requirement for
Strong Encryption and The Secure Transfer of Personal Health Information, and
the IPC discussion papers Get Rid of it Securely to keep it Private ‚Äì Best Practices
for the Secure Destruction of Personal Health Information and Detecting and
Deterring Unauthorized Access to Personal Health Information.



Frequently Asked Questions: Personal Health Information Protection Act                 13
     WHAT ARE THE NOTIFICATION REQUIREMENTS IN PHIPA
     IN THE EVENT OF A BREACH?
     PHIPA contains notification requirements for both agents and custodians.
     If personal health information handled by an agent on behalf of a custodian
     is stolen, lost or accessed by unauthorized persons, the agent must notify
     the custodian of the breach at the first reasonable opportunity. PHIPA also
     requires custodians to notify individuals at the first reasonable opportunity
     if personal health information is stolen, lost or accessed by an unauthorized
     person. However, a custodian who is a researcher and received personal health
     information for research purposes from another custodian must not notify an
     individual, unless the researcher is informed that the individual has given consent
     to being contacted.

     For further information please see the IPC guidelines, What to do When Faced
     With a Privacy Breach: Guidelines for the Health Sector.



     DO CUSTODIANS HAVE RESPONSIBILITIES WITH RESPECT
     TO ACCOUNTABILITY AND OPENNESS?
     In order to enhance transparency, PHIPA contains specific requirements for
     custodians that relate to accountability and openness. For example, a custodian
     must designate a contact person who is authorized on behalf of the custodian
     to facilitate compliance with PHIPA, ensure agents are appropriately informed
     of their duties, respond to inquiries about the custodian‚Äôs information practices,
     respond to access and correction requests and receive complaints from the
     public. A custodian that is a natural person may designate a contact person, or
     else perform the functions on their own.

     A custodian must also provide a written statement that is readily available to the
     public and describes the custodian‚Äôs information practices, how to reach the
     contact person, how to obtain access to or request a correction of a record of
     personal health information and how to make a complaint to the custodian and
     to the IPC.

     Unless an individual does not have a right of access, a custodian must notify the
     individual of any uses and disclosures of personal health information that occur
     without the individual‚Äôs consent in a manner that is outside of the scope of the
     custodian‚Äôs description of its information practices. The custodian must also
     make a note of these uses and disclosures and keep the note as a part of, or
     linked to, the records of personal health information.




14                                   Frequently Asked Questions: Personal Health Information Protection Act
WHAT ARE THE REQUIREMENTS FOR THE TREATMENT
OF PERSONAL HEALTH RECORDS IN THE EVENT OF A
CHANGE IN PRACTICE?
A change in practice occurs in a variety of circumstances, for example, due to
death, bankruptcy, retirement or relocation. It is important to identify who the
custodian of records of personal health information is in the event of a planned
or unforeseen change in practice. Generally, a custodian remains a custodian
with respect to a record of personal health information until complete custody
and control of the record passes to another person who is legally authorized to
hold it.

Upon the death of a custodian, the estate trustee or the person who assumed
responsibility for the administration of the estate becomes the custodian,
until custody and control passes to another person who is legally authorized
to hold the records. If another person, for example a trustee in bankruptcy,
obtains complete custody or control of the records as a result of the bankruptcy
or insolvency of the custodian, then that person becomes the custodian. A
custodian may also divest itself of responsibility for records by transferring them
to an archive.

When complete custody or control of the records is transferred to a successor,
then the successor becomes the custodian. The original custodian must
make reasonable efforts to notify the individual to whom the personal health
information relates before the transfer to the successor, or, if that is not
reasonably possible, as soon as possible after transferring the records.

If none of the above conditions apply, then the existing custodian of the records
remains the custodian. PHIPA requires custodians to protect personal health
information and to ensure that records of personal health information are
retained, transferred and disposed of in a secure manner. A custodian remains
responsible for records of personal health information even where the records
are being retained by an agent of the custodian, such as a record storage
company.

For further information, please see the IPC documents How to Avoid Abandoned
Records: Guidelines on the Treatment of Personal Health Information, in the
Event of a Change in Practice and Checklist for Health Information Custodians in
the Event of a Planned or Unforeseen Change in Practice.




Frequently Asked Questions: Personal Health Information Protection Act                15
     CONSENT CONCERNING
     PERSONAL HEALTH
     INFORMATION
     WHAT ARE THE REQUIREMENTS FOR CONSENT?
     The general rule is that a custodian needs to obtain an individual‚Äôs consent to
     collect, use and disclose personal health information, unless PHIPA allows the
     collection, use or disclosure without consent. An individual‚Äôs consent may be
     express or implied. Under PHIPA, regardless of whether it is express or implied,
     consent must be:

        ‚Ä¢ knowledgeable,

        ‚Ä¢ voluntary (not obtained through deception or coercion),

        ‚Ä¢ related to the information in question and

        ‚Ä¢ given by the individual.

     Knowledgeable consent means that it is reasonable in the circumstances to
     believe that an individual knows why a custodian collects, uses or their personal
     health information and that they may give or withhold this consent.

     A custodian may ensure that consent is knowledgeable by posting or making
     readily available a notice that is likely to come to the individual‚Äôs attention,
     describing the purposes for the collection, use and disclosure of personal
     health information.



     WHAT IS THE DIFFERENCE BETWEEN EXPRESS AND
     IMPLIED CONSENT?
     Express consent to the collection, use or disclosure of personal health
     information by a custodian is consent that has been clearly and unmistakably
     given. Express consent may be explicitly provided, either orally or in writing.

     Implied consent to the collection, use or disclosure of personal health
     information is consent that a custodian concludes has been given based on an
     individual‚Äôs action or inaction in particular factual circumstances.




16                                   Frequently Asked Questions: Personal Health Information Protection Act
For example, when an individual discloses their personal health information          What is the
for the purposes of filling out a prescription, a pharmacist can reasonably infer    difference between
consent to the collection of that information.                                       express and
                                                                                     implied consent?

WHEN IS EXPRESS CONSENT REQUIRED?
Subject to very limited exceptions, express consent is required:

     ‚Ä¢ where personal health information is disclosed to a person or an
       organization, such as an insurance company, that is not a custodian and

     ‚Ä¢ where information is disclosed by one custodian to another for a purpose
       other than providing or assisting in providing health care.

     ‚Ä¢ Express consent is also required where a custodian:

     ‚Ä¢ collects, uses or discloses personal health information other than an
       individual‚Äôs name and mailing address for fundraising purposes,

     ‚Ä¢ collects, uses or discloses personal health information for marketing or
       marketing research and

     ‚Ä¢ collects, uses or discloses personal information for research purposes,
       unless certain conditions and restrictions are met.



WHEN IS IMPLIED CONSENT SUFFICIENT?
In practice, a custodian is not required to obtain an individual‚Äôs written or
verbal consent every time personal health information is collected, used or
disclosed. Custodians may rely on the implied consent of an individual to collect
and use personal health information for most purposes. They may also infer
consent to disclose personal health information to another custodian for the
purposes of providing or assisting in providing health care. Subject to limited
exceptions, custodians cannot rely on implied consent when disclosing personal
health information to a person or organization that is not a custodian, or when
disclosing personal health information for a purpose other than providing or
assisting in providing health care.

Subject to additional requirements and restrictions, implied consent is permitted
if a custodian collects, uses or discloses names or mailing addresses for the
purposes of fundraising. In addition, if individuals have provided information
about their religious affiliation to a health care facility, the facility may rely
on implied consent to provide the individual‚Äôs name and location within the
facility to a person representing their religious organization. Before making this



Frequently Asked Questions: Personal Health Information Protection Act                      17
   When is implied disclosure, the facility must provide the individual with an opportunity to withhold
consent sufficient? or withdraw consent. A health care facility may also disclose to a person the fact
                   that an individual is a patient or resident in the facility, the individual‚Äôs general
                   health status, and the location of the individual, if the individual is offered the
                   option, at the first reasonable opportunity after admission to the facility, to object
                   to such disclosures and does not do so.

                   PHIPA distinguishes between implied consent and assumed implied consent.
                   In the case of implied consent, custodians must ensure that all the required
                   elements of consent are fulfilled; whereas in the case of assumed implied
                   consent, custodians may assume that all the elements of consent are fulfilled,
                   unless it is not reasonable to do so in the circumstances.



                   WHAT IS THE ‚ÄòCIRCLE OF CARE‚Äô?
                   The ‚Äúcircle of care‚Äù is not a defined term under PHIPA. It is a term of reference
                   used to describe the provisions of PHIPA that enable custodians to rely on
                   an individual‚Äôs assumed implied consent when collecting, using or disclosing
                   personal health information for the purpose of providing or assisting in providing
                   health care. For example:

                      ‚Ä¢ With respect to a physician‚Äôs office, the circle of care may include: the
                        physician, a nurse, a specialist or other health care practitioner referred
                        by the physician and any other health care practitioner selected by the
                        patient, such as a pharmacist or physiotherapist.

                      ‚Ä¢ In the context of a hospital, the circle of care may include: the attending
                        physician and the health care team, for example residents, nurses, clinical
                        clerks and employees assigned to the patient, who have the responsibility
                        of providing care to the individual or assisting with that care. The circle
                        of care could also include, among others, custodians, external to the
                        hospital, who will be involved in providing health care to the patient upon
                        discharge from the hospital.

                   The circle of care does not include:

                      ‚Ä¢ custodians who are not part of the direct or follow-up treatment of an
                        individual and

                      ‚Ä¢ non-custodians, for example, insurance companies.




       18                                           Frequently Asked Questions: Personal Health Information Protection Act
WHEN CAN CUSTODIANS ASSUME IMPLIED CONSENT?
In order to rely on assumed implied consent to collect, use or disclose
personal health information, the custodian must first fall within a category
of custodians that are entitled to rely on assumed implied consent. Most
custodians, such as health care practitioners, hospitals, pharmacies, long-
term care homes and community care access centres, can rely on assumed
implied consent. However, some custodians are not entitled to assume implied
consent. For example, these include:

     ‚Ä¢ an evaluator under the Health Care Consent Act,

     ‚Ä¢ an assessor under the Substitute Decisions Act, 1992,

     ‚Ä¢ the Minister or Ministry of Health and Long-Term Care and

     ‚Ä¢ Canadian Blood Services.

In order for a custodian to rely on assumed implied consent a number of other
requirements must also be fulfilled, including:

     ‚Ä¢ The personal health information to be collected, used or disclosed by the
       custodian must be received from the individual, the substitute-decision
       maker or another custodian.

               o    For example, a custodian may not rely on assumed implied
                    consent if the personal health information was received from an
                    employer, insurance provider or educational institution.

     ‚Ä¢ The custodian must have received the personal health information that is
       being collected, used or disclosed for the purpose of providing or assisting
       in the provision of health care to the individual.

               o    A custodian may not rely on assumed implied consent if the
                    personal health information was received for other purposes such
                    as research, fundraising, marketing or providing or assisting in the
                    provision health care to another individual or group of individuals.

     ‚Ä¢ The purpose of the collection, use or disclosure of personal health
       information by the custodian must be for the provision of health care or
       assisting in the provision of health care to the individual.

               o    A custodian may not rely on assumed implied consent if the
                    collection, use or disclosure is for other purposes, such as
                    research, fundraising, marketing or providing or assisting in
                    the provision of health care to another individual or group of
                    individuals.



Frequently Asked Questions: Personal Health Information Protection Act                     19
         When can       ‚Ä¢ In the context of disclosure, the disclosure of personal health information
custodians assume         by the custodian must be to another custodian.
  implied consent?
                        ‚Ä¢ The custodian that receives the personal health information must not be
                          aware that the individual have expressly withheld or withdrawn consent to
                          the collection, use or disclosure.

                     For further information, please see the IPC guidance document Circle of Care:
                     Sharing Personal Health Information for Health-Care Purposes.



                     ARE PHARMACISTS REQUIRED TO OBTAIN EXPRESS
                     CONSENT FROM AN INDIVIDUAL TO DISCLOSE PERSONAL
                     HEALTH INFORMATION TO A THIRD PARTY BENEFITS
                     PAYOR?
                     No. The regulations provide an exception to the express consent requirement
                     where a pharmacist discloses personal health information to a third party who is
                     not a custodian and who is being asked to provide payment for a medication or
                     related goods or services provided to an individual. Pharmacists are permitted
                     to rely on an individual‚Äôs implied consent as long as they are satisfied that all the
                     required elements of consent are fulfilled.



                     CAN INDIVIDUALS CONTROL WHAT PERSONAL HEALTH
                     INFORMATION IS RECORDED IN THEIR FILE?
                     Yes, but any condition placed on the collection, use or disclosure of personal
                     health information cannot prohibit or restrict the recording of personal health
                     information that is required by law or by established standards of professional or
                     institutional practice.



                     CAN INDIVIDUALS WITHDRAW THEIR CONSENT?
                     Yes. An individual may, with limited exceptions, withdraw consent at any time
                     for the collection, use or disclosure of personal health information by providing
                     notice to the custodian. This applies to implied, as well as express consent.

                     A withdrawal of consent is not retroactive. For example, this means that where
                     a disclosure has been made on the basis of consent, the custodian is not
                     required to retrieve the information that has already been disclosed. However,
                     the custodian must stop disclosing the personal health information as soon as


        20                                            Frequently Asked Questions: Personal Health Information Protection Act
the notice of withdrawal is received. A withdrawal of consent would not apply        Can individuals
to a collection or use that had already occurred prior to receiving the notice of    withdraw their
withdrawal. It would only apply to new collections of personal health information    consent?
and future uses for the purpose of which the consent was initially obtained.



WHAT IS A ‚ÄòLOCK-BOX‚Äô?
The ‚Äúlock-box‚Äù is not a defined term under PHIPA. It is a term commonly used
to describe the right of individuals to withhold or withdraw their consent to the
collection, use or disclosure of their personal health information for health care
purposes. Individuals may expressly instruct custodians not to use or disclose
their personal health information for health care purposes without consent,
where PHIPA would otherwise permit a use or disclosure for such a purpose.

The withholding or withdrawal of consent or the express instructions may take
various forms, including communications from individuals to custodians: not to
collect, use or disclose a particular item of personal health information, such
as a specific diagnosis, for health care purposes, not to collect, use or disclose
the contents of their entire record of personal health information for health care
purposes and/or not to use and/or disclose their personal health information
to a particular custodian, a particular agent of a custodian or to a class of
custodians or agents, for example, physicians, nurses or social workers, for
health care purposes.



WHAT ARE THE RESTRICTIONS AND LIMITATIONS ON THE
LOCK-BOX?
Once an individual locks personal health information, a custodian subject to
the withdrawal or withholding of consent or express instruction cannot collect,
use or disclose, as the case may be, that personal health information for health
care purposes, unless the individual provides express consent and informs the
custodian accordingly or unless PHIPA otherwise permits the collection, use or
disclosure to be made without consent. For example, the custodian is permitted
to disclose the locked personal health information where the custodian believes,
on reasonable grounds, that the disclosure is necessary for the purpose of
eliminating or reducing a significant risk of serious bodily harm to an individual
or a group of persons.

Further, an individual‚Äôs restriction may not impede a custodian from recording
personal health information about an individual that is required by law or by
established standards of professional or institutional practice.




Frequently Asked Questions: Personal Health Information Protection Act                       21
     What are the    If a custodian discloses an individual‚Äôs personal health information to another
  restrictions and   custodian for the provision of health care and the disclosing custodian does not
limitations on the   have consent to disclose all the personal health information that it considers
        lock-box?    reasonably necessary for that purpose, the disclosing custodian must notify the
                     receiving custodian of that fact. The receiving custodian would then be able to
                     explore the matter of the locked personal health information with the individual
                     and seek their express consent to access the locked information.

                     For further information, please see the IPC fact sheet, Lock-Box Fact Sheet.



                     WHAT HAPPENS WHEN AN INDIVIDUAL IS INCAPABLE OF
                     PROVIDING CONSENT?
                     Under PHIPA, individuals are presumed to be capable of making their own
                     decisions regarding the collection, use or disclosure of their personal health
                     information. Individuals are capable of consent if they are able to understand
                     information relevant to deciding whether to consent to the collection, use
                     or disclosure of their personal health information, and to appreciate the
                     reasonably foreseeable consequences of giving, not giving, withholding or
                     withdrawing their consent.

                     If a custodian believes that an individual is incapable of providing consent, PHIPA
                     permits a substitute decision-maker, such as a relative, spouse, child‚Äôs parent, or
                     the Public Guardian and Trustee, to make a decision on an individual‚Äôs behalf.

                     PHIPA lists, in order of priority, the following substitute decision-makers who
                     may consent on behalf of an individual when consent is required, including:

                        ‚Ä¢ a substitute decision-maker within the meaning of section 9, section
                          39 and section 56 of the Health Care Consent Act, if the purpose of the
                          collection, use or disclosure is necessary for, or ancillary to, a decision
                          about a treatment under Part II, a decision about admission to a care
                          facility under Part III or a decision about a personal assistance service
                          under Part IV of the Health Care Consent Act respectively,,

                        ‚Ä¢ the attorney for personal care or for property,

                        ‚Ä¢ the representative appointed by the Consent and Capacity Board,

                        ‚Ä¢ the spouse or partner,

                        ‚Ä¢ a child or parent, including a children‚Äôs aid society,

                        ‚Ä¢ a parent who has a right of access,




       22                                            Frequently Asked Questions: Personal Health Information Protection Act
     ‚Ä¢ a sibling,

     ‚Ä¢ a relative and if no other person meets the requirements

     ‚Ä¢ the Public Guardian and Trustee.



CAN A CHILD UNDER 16 YEARS OLD PROVIDE CONSENT?
A custodian may obtain consent for the collection, use and disclosure of
personal health information from a capable child, regardless of age. As
discussed above, individuals are capable of consent if they are able to
understand information relevant to deciding whether to consent to the collection,
use or disclosure of their personal health information, and to appreciate the
reasonably foreseeable consequences of giving, not giving, withholding or
withdrawing their consent.

If the child is less than 16 years old, a parent of the child or a children‚Äôs aid
society or other person who is lawfully entitled to give or refuse consent in the
place of the parent may also give, withhold or withdraw consent. However, this
does not apply in the context of information that relates to treatment within
the meaning of the Health Care Consent Act, about which children have made
a decision on their own, or counselling in which children have participated on
their own under the Child and Family Services Act. A parent does not include a
parent who has only a right of access to the child. If there is a conflict between
a capable child who is less than 16 years old, and the person who is entitled to
act as the child‚Äôs substitute decision-maker, the decision of the capable child
regarding giving, withholding or withdrawing consent prevails.



CAN ANOTHER PERSON, SUCH AS A FAMILY MEMBER,
PROVIDE CONSENT ON AN INDIVIDUAL‚ÄôS BEHALF WHEN
PICKING UP OR DROPPING OFF A PRESCRIPTION?
Yes. The regulations permit a pharmacist to provide a prescription to another
person. This is also permitted under the Drug and Pharmacies Regulation Act.




Frequently Asked Questions: Personal Health Information Protection Act               23
     COLLECTION, USE AND
     DISCLOSURE OF PERSONAL
     HEALTH INFORMATION
     WHAT ARE THE GENERAL LIMITATIONS ON THE
     COLLECTION, USE AND DISCLOSURE OF PERSONAL
     HEALTH INFORMATION?
     A custodian is prohibited from collecting, using or disclosing personal health
     information unless consent has been obtained and the collection, use and
     disclosure is, to the best of the custodian‚Äôs knowledge, necessary for a lawful
     purpose or is permitted or required by PHIPA.

     According to PHIPA, a custodian must not collect, use or disclose personal
     health information if other information will serve the purpose of the collection,
     use or disclosure. For example, a custodian may be able to provide a researcher
     conducting a study with de-identified information, rather than disclosing
     personal health information. A custodian must not collect, use or disclose more
     personal health information than is reasonably necessary to meet the purpose
     of the collection, use or disclosure. For example, if a patient requests a doctor‚Äôs
     note to give to the employer, the doctor should only include the minimum
     information necessary, rather than the patient‚Äôs entire health history.



     COLLECTION

     WHAT IS A COLLECTION OF PERSONAL HEALTH INFORMATION
     UNDER PHIPA?

     PHIPA defines the term ‚Äúcollect‚Äù as the gathering, acquiring, receiving or
     obtaining of personal health information by any means from any source. This
     means that personal health information can be collected by a custodian or an
     authorized agent under PHIPA in several ways, such as when a doctor makes
     notes about a patient or when a pharmacist receives a prescription to be filled.




24                                   Frequently Asked Questions: Personal Health Information Protection Act
WHAT ARE THE RULES REGARDING THE COLLECTION OF PERSONAL
HEALTH INFORMATION?

As a general rule, consent is required for any collection of an individual‚Äôs
personal health information, unless PHIPA allows the collection without consent.
Custodians within the ‚Äúcircle of care‚Äù may rely on an individual‚Äôs implied
consent, or assumed implied consent if the requirements are fulfilled, to collect
personal health information for the purpose of providing health care.

With limited exceptions, custodians must collect personal health information
directly from the individual involved. Custodians must not collect personal health
information if other information will serve the purpose of the collection and may
only collect as much information as is necessary to meet the purpose of collection.


WHEN CAN CUSTODIANS INDIRECTLY COLLECT PERSONAL HEALTH
INFORMATION?

Custodians may collect personal health information indirectly where, for
example:

     ‚Ä¢ the individual consents,

     ‚Ä¢ the collection is necessary for providing health care and it is not possible
       to collect personal health information directly from the individual that can
       be relied on as accurate and complete,

     ‚Ä¢ the collection is necessary for providing health care and it is not possible
       to collect personal health information directly from the individual in a timely
       manner,

     ‚Ä¢ the custodian collects personal health information for the purposes of
       research from a person who is not a custodian, provided that certain
       conditions are met,

     ‚Ä¢ the indirect collection is required or permitted by law,

     ‚Ä¢ the custodian is a prescribed entity and is collecting personal health
       information from a person who is not a custodian for the purposes of the
       planning and management of the health system or

     ‚Ä¢ the IPC authorizes the indirect collection.




Frequently Asked Questions: Personal Health Information Protection Act                   25
     USE

     WHAT IS A USE OF PERSONAL HEALTH INFORMATION UNDER PHIPA?

     The term ‚Äúuse‚Äù in relation to personal health information in the custody or under
     the control of a custodian or a person, is defined under PHIPA as meaning to
     handle or deal with personal health information, but does not include to disclose
     the information. Where a custodian is authorized to use the information, the
     custodian may provide the information to an agent of the custodian to use it for
     that purpose on behalf of the custodian. The sharing of information between
     a custodian and its agent is considered to be a use and not a disclosure or a
     collection for the purposes of PHIPA. Order HO-013 found that handling and
     dealing with personal health information includes accessing/viewing personal
     health information.


     WHAT ARE THE RULES REGARDING THE USE OF PERSONAL HEALTH
     INFORMATION?

     As a general rule, consent is required for any use of an individual‚Äôs personal
     health information, unless PHIPA allows the use without consent. Custodians
     must not use personal health information if other information will serve the
     purpose of the use, and may only use as much information as is necessary to
     meet the purpose of the use of personal health information.

     When using personal health information, a custodian must take reasonable
     steps to ensure that the individual‚Äôs personal health information is as accurate,
     complete and up-to-date as is necessary for the purposes for which the
     custodian uses the information.


     WHEN CAN PERSONAL HEALTH INFORMATION BE USED WITHOUT
     CONSENT?

     PHIPA sets out a limited set of acceptable uses of personal health information
     without consent, including, for example, the following purposes:

        ‚Ä¢ planning or delivering programs or services,

        ‚Ä¢ risk management, error management or activities to improve or maintain
          the quality of care or any related program or service,

        ‚Ä¢ educating agents to provide health care,



26                                   Frequently Asked Questions: Personal Health Information Protection Act
     ‚Ä¢ obtaining payment or processing, monitoring, verifying or reimbursing           When can personal
       health care claims,                                                             health information
                                                                                       be used without
     ‚Ä¢ research, provided that specific requirements and conditions are met and
                                                                                       consent?
     ‚Ä¢ If permitted or required by law.

A custodian may provide personal health information to an agent of the
custodian for any of these purposes.



DISCLOSURE

WHAT IS A DISCLOSURE OF PERSONAL HEALTH INFORMATION UNDER
PHIPA?

The term ‚Äúdisclose‚Äù in relation to personal health information in the custody or
under the control of a custodian or a person, is defined under PHIPA as meaning
to make the personal health information available or to release it to another
custodian or person. It does not include providing personal health information
back to the person who provided it or disclosed it in the first place, whether or
not the personal health information has been manipulated or altered, as long as
it does not include additional identifying information.


WHAT ARE THE RULES REGARDING THE DISCLOSURE OF PERSONAL
HEALTH INFORMATION?

As a general rule, consent is required to disclose an individual‚Äôs personal health
information, unless PHIPA allows the disclosure without consent. Custodians
must not disclose personal health information if other information will serve the
purpose of the disclosure, and may only disclose as much information as is
necessary to meet the purpose.

A custodian and its authorized agents may rely on implied consent, or assumed
implied consent if the requirements are fulfilled, for the disclosure of personal
health information within the ‚Äúcircle of care‚Äù while providing health care, as long
as the disclosure is reasonably necessary for the provision of health care, and
the individual has not expressly withheld or withdrawn consent.

PHIPA permits custodians to disclose personal health information in certain
limited situations. However, simply because a disclosure is permitted does not
mean it is mandatory, unless it is necessary to carry out a statutory or legal duty.




Frequently Asked Questions: Personal Health Information Protection Act                         27
    What are the    Unless permitted or required by law, express consent is generally required when
 rules regarding    personal health information is disclosed by a custodian to a non-custodian,
the disclosure of   where a custodian discloses to another custodian for a purpose other than
 personal health    for health care or for market research, unless specific conditions are met and
    information?    fundraising, if more than contact information is provided.

                    When disclosing personal health information, the custodian should take care to
                    ensure that no information is inadvertently disclosed to third parties.


                    WHEN CAN PERSONAL HEALTH INFORMATION BE DISCLOSED WITHOUT
                    CONSENT?

                    PHIPA recognizes the need for a flexible approach to regulating information
                    exchanges between custodians in order to ensure the effective and efficient
                    operation of the health system. Consequently, custodians may disclose personal
                    health information without an individual‚Äôs consent in certain circumstances.
                    While these disclosures without consent are permitted by PHIPA, they are not
                    mandatory, unless they are necessary to carry out a statutory or legal duty.
                    Examples of permitted disclosures of personal health information without
                    consent include:

                       ‚Ä¢ if the disclosure is reasonably necessary for providing health care and
                         consent cannot be obtained in a timely manner, unless there is an express
                         request from the individual instructing otherwise,

                       ‚Ä¢ in order for the Minister of Health and Long-Term Care to provide funding
                         to the custodian for the provision of health care,

                       ‚Ä¢ for the purpose of contacting a relative or friend or potential substitute
                         decision-maker of an individual who is injured, incapacitated or ill and
                         unable to give consent personally,

                       ‚Ä¢ to inform any person that an individual is a patient or resident in a facility,
                         the individual‚Äôs general health status, and the location of the individual
                         in the facility, unless there is an express request from the individual
                         instructing otherwise,

                       ‚Ä¢ to eliminate or reduce a significant risk of serious bodily harm to a person
                         or group of persons,

                       ‚Ä¢ when transferring records to the archives for conservation,

                       ‚Ä¢ for the purpose of carrying out an inspection, investigation or similar
                         procedure that is authorized by a warrant, PHIPA or another Act,




      28                                            Frequently Asked Questions: Personal Health Information Protection Act
     ‚Ä¢ for determining or verifying eligibility for publicly funded health care or       When can personal
       related goods, services or benefits,                                              health information
                                                                                         be disclosed
     ‚Ä¢ for the purpose of administration and enforcement of various Acts by the
                                                                                         without consent?
       professional Colleges and other regulatory bodies,

     ‚Ä¢ to a prescribed person, listed in the regulations, that compiles and
       maintains a registry of personal health information for the purposes of
       facilitating or improving the provision of health care or that relates to the
       storage or donation of body parts or bodily substances,

     ‚Ä¢ to a prescribed entity, listed in the regulations, for the purpose of analysis
       or compiling information with respect to the management, evaluation or
       monitoring of the health system,

     ‚Ä¢ to the Public Guardian and Trustee, a children‚Äôs aid society and the
       Children‚Äôs Lawyer for the purpose of carrying out their statutory functions,

     ‚Ä¢ to a person conducting an audit or reviewing an accreditation or
       application for accreditation related to the services of a custodian,

     ‚Ä¢ for the purpose of legal proceedings, in specific circumstances,

     ‚Ä¢ for the purpose of research, subject to restrictions and conditions and

     ‚Ä¢ for any purpose as required or permitted by law.


CAN PERSONAL HEALTH INFORMATION BE DISCLOSED IN THE EVENT
OF AN EMERGENCY?

PHIPA does not prevent the rapid sharing of personal health information in
certain situations. PHIPA is not intended to stand in the way of the disclosure
of vital‚Äîand in some cases, life-saving‚Äîinformation in emergency or critical
situations affecting individuals or public health and safety, as well as in situations
that call for compassion.

Personal health information may be disclosed without consent if the custodian
believes on reasonable grounds that the disclosure is necessary for eliminating
or reducing a significant risk of serious bodily harm to a person or group
of persons. For example, a psychologist at a university could disclose a
student‚Äôs personal health information to the student‚Äôs family or physician, if the
psychologist believes it is necessary to reduce the risk of suicide. PHIPA also
allows a custodian to disclose personal health information without consent in
order to contact a relative, friend or potential substitute decision-maker if an
individual is injured, incapacitated or ill and unable to consent.




Frequently Asked Questions: Personal Health Information Protection Act                           29
     Can personal    Custodians may also disclose personal health information if permitted or
health information   required by another law. For example, the Health Protection and Promotion Act
   be disclosed in   (HPPA) requires certain custodians to report diseases defined as reportable,
   the event of an   communicable or virulent to the Medical Officer of Health. PHIPA also provides
      emergency?     that custodians may disclose personal health information without consent to
                     the Chief Medical Officer of Health or to a local medical officer of health for
                     the purposes of the HPPA and to the Ontario Agency for Health Protection and
                     Promotion for the purposes of the Ontario Agency for Health Protection and
                     Promotion Act. For example, a custodian may report an outbreak of a suspicious
                     condition that is not identified as a reportable, communicable or virulent disease,
                     but which the custodian believes could be dangerous.

                     For further information, please see the IPC fact sheet, Disclosure of Information
                     Permitted in Emergency or other Urgent Circumstances.


                     DOES PHIPA PERMIT DISCLOSURE OF PERSONAL HEALTH INFORMATION
                     ABOUT A DECEASED INDIVIDUAL?

                     PHIPA permits the disclosure of personal health information about a deceased
                     individual in certain circumstances. A custodian may only disclose personal
                     health information to a person who is not a custodian, such as a relative of a
                     deceased individual, if the individual whose personal health information is at
                     issue, has given express consent or the disclosure is permitted or required by
                     PHIPA or another law. In the case of a deceased individual, the consent may be
                     given by the deceased individual‚Äôs substitute decision-maker. This means that if
                     the substitute decision-maker consents, the personal health information may be
                     disclosed to a relative of the deceased individual.

                     PHIPA permits, but does not require, a custodian to disclose personal health
                     information without consent for the purposes of identifying the individual or
                     for informing people whom it is reasonable to inform, that the individual is
                     deceased or reasonably suspected to be deceased and the circumstances of
                     death, where appropriate. PHIPA also permits disclosure to the spouse, partner,
                     sibling or child of the deceased individual if the recipients reasonably require the
                     information to make decisions about their own, or their children‚Äôs health care.

                     For further information, please see the IPC fact sheet, Obtaining Personal Health
                     Information About a Deceased Relative.




       30                                            Frequently Asked Questions: Personal Health Information Protection Act
CAN A CUSTODIAN DISCLOSE PERSONAL HEALTH INFORMATION TO
THE WORKPLACE SAFETY AND INSURANCE BOARD (WSIB) ABOUT AN
INJURED WORKER WITHOUT THE INDIVIDUAL‚ÄôS CONSENT?

Yes. PHIPA permits the disclosure of personal health information without
consent, if permitted or required by another law. For example, this means that
PHIPA does not interfere with the Workplace Safety and Insurance Act (Act),
where that Act requires a hospital or health facility, which provides health care
to a worker claiming benefits under the insurance plan, to give the WSIB such
information relating to the worker as the WSIB may require. This requirement also
applies to a health care practitioner who provides health care to a worker or is
consulted with respect to a worker‚Äôs health care. When requested to do so by an
injured worker or the employer, the Act requires a health care practitioner treating
the worker to give the WSIB, the worker and the employer prescribed information
concerning the worker‚Äôs functional abilities.

PHIPA also does not interfere with the Occupational Health and Safety Act,
which sets out an employer‚Äôs and supervisor‚Äôs duty, subject to specific
limitations, to provide a worker with information, including personal
information, related to a risk of workplace violence from a person with a history
of violent behaviour.


CAN A CUSTODIAN STORE, ACCESS OR DISCLOSE PERSONAL HEALTH
INFORMATION OUTSIDE OF ONTARIO?

PHIPA does not require that personal health information be retained and stored
in Ontario or Canada. There is no legislative prohibition on storing and accessing
personal health information outside of Ontario. For example, a custodian may
decide to outsource the storage of personal health information to a service
provider in another jurisdiction. However, the custodian is ultimately accountable
for the actions of its agent and must be satisfied that appropriate administrative,
physical and technical safeguards are in place, for example, through contractual
arrangements.

PHIPA permits disclosures of personal health information to a person outside
of Ontario in certain situations. A custodian may disclose personal health
information about an individual collected in Ontario to a person outside of
Ontario if:

     ‚Ä¢ the individual consents to the disclosure,

     ‚Ä¢ PHIPA permits the disclosure,




Frequently Asked Questions: Personal Health Information Protection Act                 31
   Can a custodian    ‚Ä¢ the person receiving the information performs functions comparable to
   store, access or     the functions of certain persons to whom the disclosure in Ontario is
 disclose personal      permitted,
health information
outside of Ontario?   ‚Ä¢ the custodian is a prescribed entity and the disclosure is for the purpose of
                        health planning or health administration, the information relates to health
                        care provided in Ontario to a person who is a resident of another province
                        or territory of Canada and the disclosure is made to the government of that
                        province or territory,

                      ‚Ä¢ the disclosure is reasonably necessary for the provision of health care to
                        the individual and the individual has not expressly instructed the custodian
                        not to make the disclosure or

                      ‚Ä¢ the disclosure is reasonably necessary for the administration of payments
                        in connection with the provision of health care to the individual or for
                        contractual or legal requirements in that connection.




        32                                        Frequently Asked Questions: Personal Health Information Protection Act
FUNDRAISING AND MARKETING
CAN CUSTODIANS COLLECT, USE OR DISCLOSE
PERSONAL HEALTH INFORMATION FOR FUNDRAISING
ACTIVITIES?
The regulations contain specific requirements and restrictions that apply to all
collections, uses and disclosures of personal health information for fundraising,
including the following:

     ‚Ä¢ The collection, use or disclosure of personal health information for
       fundraising purposes is only permitted where the fundraising relates to a
       charitable or philanthropic purpose related to the custodian‚Äôs operations.

     ‚Ä¢ All solicitations must contain an easy opt-out from any further solicitations.

     ‚Ä¢    No solicitations may contain information about an individual‚Äôs health care
         or state of health.

In general, custodians are only permitted to collect, use or disclose personal
health information for purposes that are not related to the provision of health
care with the express consent of the individual in question. However, PHIPA and
its regulations provide that a collection, use or disclosure of an individual‚Äôs name
and mailing address (or the name and mailing address of a substitute decision-
maker, if applicable) for fundraising may take place with the implied consent of
the individual in question, as long as the following requirements are met:

     ‚Ä¢ at the time the service has been provided to the individual, the custodian
       has posted, or has made available to the individual, a notice informing the
       individual of the custodian‚Äôs intention to use or disclose the information
       for fundraising purposes, along with information on how the individual can
       easily opt out and

     ‚Ä¢ the individual had not opted out within 60 days from the time the notice
       had been provided.

For personal health information collected before November 1, 2004, a custodian
may assume implied consent to use or disclose an individual‚Äôs name and contact
information for fundraising, unless the custodian is aware that the individual has
expressly withheld or withdrawn consent.

For further information please see the IPC fact sheet, Fundraising under PHIPA.




Frequently Asked Questions: Personal Health Information Protection Act                  33
     CAN PERSONAL HEALTH INFORMATION BE COLLECTED,
     USED OR DISCLOSED FOR MARKETING PURPOSES?
     A custodian can only collect, use or disclose personal health information about
     an individual for market research or for marketing purposes with the express
     consent of the individual.

     Note that the following activities are excluded from the definition of marketing:

        ‚Ä¢ communications by health care practitioners about the availability of
          non-OHIP covered charges for a block fee or on the basis of a set fee for
          service and

        ‚Ä¢ communications by Canadian Blood Services for recruiting donors of
          blood and blood products.




34                                   Frequently Asked Questions: Personal Health Information Protection Act
RESEARCH
WHAT ARE THE REQUIREMENTS FOR THE COLLECTION,
USE AND DISCLOSURE OF PERSONAL HEALTH
INFORMATION FOR RESEARCH?
The general rule is that a custodian needs to obtain an individual‚Äôs consent to
collect, use and disclose personal health information, unless PHIPA allows the
collection, use or disclosure without consent. In recognizing the importance
of health research, PHIPA permits the collection, use or disclosure of personal
health information for research purposes without an individual‚Äôs consent, if strict
conditions are met.

For example, a custodian who uses personal health information for research
and, similarly, a researcher who seeks disclosure of personal health information
for research purposes, must both submit a detailed research plan to a Research
Ethics Board (REB) for approval. When deciding whether to approve a research
plan involving the use or disclosure of personal health information without
consent, a REB must consider:

     ‚Ä¢ whether the research can be reasonably accomplished without using the
       personal health information,

     ‚Ä¢ the public interest in conducting the research and in protecting privacy,

     ‚Ä¢ whether obtaining consent is impractical and

     ‚Ä¢ whether adequate safeguards will be in place to protect the privacy of
       individuals and the confidentiality of their personal health information.

A researcher requesting disclosure of personal health information from a
custodian must submit to the custodian a written application, a research plan
and a copy of the decision approving the research plan by a REB. In addition,
the custodian must enter into an agreement with the researcher that may impose
further restrictions, including the manner in which the researcher may use and
disclose the personal health information.

A researcher with an approved research plan who receives personal health
information from a custodian shall:

     ‚Ä¢ comply with the conditions, if any, imposed by the REB,




Frequently Asked Questions: Personal Health Information Protection Act                35
       What are the      ‚Ä¢ use personal health information only for the purpose set out in the
  requirements for         research plan,
the collection, use
                         ‚Ä¢ not publish information in a form that could reasonably enable a person to
 and disclosure of
   personal health         identify the individual,
    information for      ‚Ä¢ not disclose information unless required by law or if the disclosure is to
         research?         prescribed persons or registries,

                         ‚Ä¢ not attempt to contact the individual whose personal information is the
                           subject of the research project unless the custodian obtains the consent of
                           that individual,

                         ‚Ä¢ notify the custodian in writing of any breaches of either the agreement or
                           PHIPA and

                         ‚Ä¢ comply with the agreement between the researcher and the custodian.

                      Researchers are permitted to disclose personal health information to another
                      researcher or to a prescribed person or a prescribed entity if the disclosure
                      is either part of a research plan approved by a REB, or it is necessary for the
                      purpose of verifying or validating the information or the research.



                      ARE THERE ANY REQUIREMENTS FOR RESEARCH ETHICS
                      BOARDS AND RESEARCH PLANS?
                      Yes. The regulations specify that a REB must have at least five members,
                      including:

                         ‚Ä¢ a member who has no affiliation to the person who established the REB,

                         ‚Ä¢ a member who has knowledge in privacy issues,

                         ‚Ä¢ a member who has knowledge in research ethics and

                         ‚Ä¢ at least two members with expertise in the methods or the relevant areas
                           of research.

                      In addition, the regulations list a number of requirements that research plans
                      must include. For example, a research plan must include a description of why
                      consent to the disclosure of personal health information is not being sought
                      from the individual to whom the information relates; a description of how the
                      information will be used, the safeguards the researcher will put in place to
                      protect the confidentiality and security of the information and a description of all
                      persons who will have access to the information.




        36                                            Frequently Asked Questions: Personal Health Information Protection Act
ONTARIO HEALTH CARDS AND
HEALTH NUMBERS
WHO CAN COLLECT, USE OR DISCLOSE ONTARIO HEALTH
NUMBERS AND UNDER WHAT CIRCUMSTANCES?
Custodians and certain persons or organizations prescribed in the regulations
are permitted to collect, use or disclose Ontario health numbers.

A person or organization that is not a custodian or an agent of a custodian may
only collect or use an Ontario health number for the following purposes:

     ‚Ä¢ for purposes related to the provision of provincially funded health
       resources,

     ‚Ä¢ for the purposes for which the custodian disclosed the number,

     ‚Ä¢ for purposes related to the duties or powers of a governing body of health
       care practitioners who provide provincially funded health resources or

     ‚Ä¢ for health administration, health planning or health research or
       epidemiological studies by persons listed in the regulations including, the
       WSIB, prescribed persons and prescribed entities.

An organization or person who is not a custodian or agent of a custodian
may not disclose a health number, except as set out in the regulations or as
required by law.

These restrictions on the collection, use and disclosure of health numbers do not
apply to:

     ‚Ä¢ a person who collects, uses or discloses health numbers for a proceeding,

     ‚Ä¢ a prescribed entity or

     ‚Ä¢ the individual or the individual‚Äôs substitute decision-maker.



ARE OTHER ORGANIZATIONS PERMITTED TO REQUEST
THE PRODUCTION OF A HEALTH CARD?
PHIPA states that only a person who provides a provincially funded health care
resource may require the production of an individual‚Äôs health card.



Frequently Asked Questions: Personal Health Information Protection Act               37
      Are other   However, there is nothing in PHIPA that prevents an organization from requesting
 organizations    a health card, as long as it is made clear that disclosure is voluntary and the
   permitted to   information will only be used for purposes directly related to the provision of
    request the   provincially funded health resources. For instance, an employer may allow an
production of a   employee to voluntarily provide a health card in order to expedite the provision of
   health card?   health care services in the event of an emergency. A school, day care, or camp
                  may also request a child‚Äôs health number so that it is on record in the event of a
                  medical emergency.

                  Please note that any such disclosure must be voluntary, and non-custodians may
                  not require the production of health cards. It is an offence under PHIPA for any
                  organization to wilfully collect, use or disclose any personal health information‚Äî
                  including health numbers‚Äîin a manner that contravenes PHIPA.

                  An organization, for example a private sector business not directly involved
                  in the delivery of provincially funded health services, is not permitted to take
                  note of, record, collect, or use a health number for identification purposes.
                  However, nothing prevents individuals from voluntarily choosing to show their
                  health cards in order to verify identity. For example, individuals may voluntarily
                  decide to provide their health cards to librarians in order to confirm their identity
                  and obtain a library card. The librarians may view the health card, but are not
                  permitted to record the health number.

                  For further information, please see the IPC document, Frequently Asked
                  Questions: Health Cards and Health Numbers.




    38                                             Frequently Asked Questions: Personal Health Information Protection Act
ACCESS TO RECORDS
OF PERSONAL HEALTH
INFORMATION AND CORRECTION
ACCESS

ARE INDIVIDUALS PERMITTED TO ACCESS THEIR OWN PERSONAL
HEALTH INFORMATION?

With limited exceptions, PHIPA provides individuals with a general right to
access their personal health information held by a custodian and sets out a
formal procedure for access requests. The right of access does not apply to:

     ‚Ä¢ records that contain quality of care information,

     ‚Ä¢ personal health information required for quality assurance programs,

     ‚Ä¢ raw data from psychological tests or assessments,

     ‚Ä¢ personal health information used solely for research purposes or

     ‚Ä¢ personal health information that is in the custody or under the control of a
       laboratory in respect of a test, where an individual has the right of access
       to that information from the health care practitioner and the practitioner
       has not directed the laboratory to provide the information directly to the
       individual.

As previously noted, personal health information includes identifying information
that is not personal health information, but that is contained in a record that
contains personal health information. Personal health information does not
include identifying information if the information relates primarily to one or
more employees or other agents of the custodian and the record is maintained
primarily for a purpose other than the provision or assisting in the provision of
health care.

For further information please see the IPC fact sheet, Your health information:
Your access and correction rights.




Frequently Asked Questions: Personal Health Information Protection Act                39
     HOW DO AN INDIVIDUAL OBTAIN ACCESS TO THEIR PERSONAL HEALTH
     INFORMATION?

     An individual may exercise a right of access to a record of personal health
     information by making a written request for access to the custodian that has
     custody or control of the information.

     The custodian should then either make the record available for examination
     or provide a copy of the record. Otherwise, the custodian must give a written
     notice to the individual seeking access stating that, after a reasonable search,
     the record does not exist, cannot be found or is not a record to which access
     applies. If the custodian is entitled to refuse the request, in whole or in part, the
     custodian must give a written notice stating that the request is being refused and
     providing reasons for the refusal. The notice must also state that the individual is
     entitled to make a complaint about the refusal to the IPC. If an individual decides
     to complain to the IPC, the complaint must be in writing.

     The request must contain sufficient detail to allow the custodian to locate the
     record in question. Where the individual has not provided sufficient detail to
     enable the custodian to identify and locate the record, the custodian is required
     to assist the individual in reformulating the request.

     Nothing in PHIPA prevents a custodian from granting an individual access to
     a record of personal health information if the individual makes an oral request
     for access.

     For further information, please see the IPC‚Äôs PHIPA Practice Directions,
     Clarifying Access Requests and Drafting a Letter Responding to a Request for
     Access to Personal Health Information.


     HOW LONG DOES A CUSTODIAN HAVE TO RESPOND TO AN INDIVIDUAL‚ÄôS
     REQUEST FOR ACCESS TO PERSONAL HEALTH INFORMATION?

     A custodian must respond no later than 30 calendar days after the request was
     made.

     Extensions of up to a maximum of 30 additional calendar days are allowed,
     where meeting this time frame would unreasonably interfere with the custodian‚Äôs
     operations, or where the necessary consultations would not make it reasonably
     practical to reply within that time frame. In such situations, the custodian must
     inform the individual in writing of the extension and set out the length of the
     extension and the reasons for the extension.




40                                   Frequently Asked Questions: Personal Health Information Protection Act
CAN A CUSTODIAN REFUSE TO PROVIDE ACCESS TO AN INDIVIDUAL‚ÄôS
PERSONAL HEALTH INFORMATION?

Generally, custodians are responsible for providing individuals with access to
their records of personal health information.

Custodians may only refuse access in limited situations, including:

     ‚Ä¢ the information in question is subject to a legal privilege,

     ‚Ä¢ access could reasonably be expected to result in a risk of serious harm
       to the treatment or recovery of the individual or serious bodily harm to the
       individual or another person,

     ‚Ä¢ the information was collected in the course of an inspection, investigation
       or similar procedure and the resulting proceedings, appeals or processes
       have not yet been concluded or

     ‚Ä¢ another law prohibits the disclosure of that information.

If an exception applies, an individual still has a right of access to the part of the
record that can reasonably be severed from the part containing the information
to which the individual does not have the right of access. If a custodian denies
an individual access to the personal health information, the individual has the
right to file a written complaint with the IPC.


IS THERE A FEE ASSOCIATED WITH AN ACCESS REQUEST?

Custodians may charge a reasonable fee for providing access to an individual‚Äôs
records of personal health information. PHIPA also permits a custodian to waive
all or part of the fee associated with an access request. In charging a fee, PHIPA
requires custodians to first provide the individual with a fee estimate. The fee
amount must not exceed the prescribed amount set out in the regulations, if any,
or the amount of reasonable cost recovery.

There is currently no regulation prescribing the fee for providing access to an
individual‚Äôs records of personal health information. Order HO-009 found that a
custodian may charge a set fee of $30 for photocopying or printing the first 20
pages of a record and 25 cents per page for every additional page. The set fee
of $30 also includes additional activities, for example, locating and retrieving the
record, reviewing the contents of the record for not more than 15 minutes and
preparing a response letter to the individual.




Frequently Asked Questions: Personal Health Information Protection Act                  41
     WHAT IF THE CUSTODIAN WORKS FOR A NON-CUSTODIAN THAT IS
     COVERED UNDER PUBLIC SECTOR ACCESS AND PRIVACY LEGISLATION,
     SUCH AS A SCHOOL BOARD OR MUNICIPALITY?

     The provisions of PHIPA regarding access to, and correction of, personal health
     information do not apply to records in the custody or under the control of a
     health care practitioner who is employed by or acting for an institution within
     the meaning of the Freedom of Information and Protection of Privacy Act,
     which covers provincial ministries and most provincial boards, agencies and
     commissions, or the Municipal Freedom of Information and Protection of Privacy
     Act, which covers local government organizations such as municipalities, police,
     school, health and library boards, if the individual has the right to request access
     under either of those Acts. In that case, the individual would submit an access
     request under FIPPA or MFIPPA, as applicable. For example, if an individual
     wants to access personal health information compiled by a psychologist who
     works for a school board, the individual should make the request to the freedom
     of information coordinator of the custodian‚Äôs institution, for example the school
     board, in accordance with MFIPPA, rather than directly to the custodian.

     If the custodian works for a non-custodian that is not covered under FIPPA or
     MFIPPA, for example a private sector organization, the provisions of PHIPA
     would apply. In that case, the individual would submit an access request under
     PHIPA directly to the custodian.



     CORRECTION

     CAN INDIVIDUALS CORRECT ERRORS IN THEIR PERSONAL HEALTH
     INFORMATION?

     An individual who believes that personal health information is incomplete
     or inaccurate may request that a custodian correct the record. It is the
     responsibility of the custodian to ensure that personal health information is
     complete and accurate.

     For further information please see the IPC fact sheet, Your health information:
     Your access and correction rights.




42                                   Frequently Asked Questions: Personal Health Information Protection Act
HOW DOES AN INDIVIDUAL CORRECT ERRORS?

An individual seeking a correction to personal health information may submit
a request to the custodian who has custody or control of the records. The
custodian is permitted to make a correction based on an oral request; however,
the custodian may require that the request be made in writing.

The custodian must respond within 30 days of receiving a written correction
request. PHIPA provides limited grounds for extending this 30-day time frame.
Extensions of up to a maximum of 30 additional days are allowed, where replying
within 30 days would unreasonably interfere with the custodian‚Äôs operations, or
where the necessary consultations would not make it reasonably practical to
reply within that time frame. In such situations, the custodian must inform the
individual in writing of the extension and set out the length of the extension and
the reasons for the extension.


CAN A CUSTODIAN REFUSE TO CORRECT AN INDIVIDUAL‚ÄôS PERSONAL
HEALTH INFORMATION?

Subject to the exceptions set out in the next paragraph, a custodian is obligated to
correct a record of personal health information where an individual demonstrates,
to the satisfaction of the custodian, that the record is inaccurate or incomplete for
the purposes for which the custodian uses the information and the individual gives
the custodian the necessary information to correct the record.

However, a custodian may refuse to correct a record of personal health
information that was not originally created by the custodian and which the
custodian does not have sufficient knowledge, expertise and authority to
correct, or if the record consists of a professional opinion or an observation that
a custodian has made in good faith, for example, a medical diagnosis made by
a physician. If a correction is refused, the custodian is required to inform the
individual of the refusal, the reasons for the refusal, the individual‚Äôs right to file
a complaint regarding the refusal to the IPC and the right of the individual to
attach a statement of disagreement to the record.




Frequently Asked Questions: Personal Health Information Protection Act                   43
     ADMINISTRATION AND
     ENFORCEMENT
     HOW IS PHIPA ENFORCED?
     The IPC has been designated as the independent oversight body responsible for
     ensuring that custodians collect, use and disclose personal health information
     according to the rules set out in PHIPA. The IPC plays a significant role in
     enforcing overall compliance.

     The IPC has various powers under PHIPA, including the authority to review and
     adjudicate complaints. These include the authority to:

        ‚Ä¢ require a complainant to try to resolve the issue directly with the custodian,

        ‚Ä¢ appoint a mediator to resolve the complaint and/or

        ‚Ä¢ review a complaint initiated by an individual or, in the absence of a complaint,
          self-initiate a review where there are reasonable grounds to do so.

     The IPC also has the authority to issue orders requiring compliance with PHIPA.
     For example, the IPC may order a custodian to:

        ‚Ä¢ provide the individual with access to a record of personal health
          information,

        ‚Ä¢ correct a record of personal health information,

        ‚Ä¢ dispose of records of personal health information and

        ‚Ä¢ change or cease a particular information practice.


     HOW DOES AN INDIVIDUAL INITIATE A COMPLAINT?
     A person who believes that another person has contravened, or is about to
     contravene PHIPA, has the right to submit a written complaint to the IPC. For
     example, a person may complain about:

        ‚Ä¢ a custodian‚Äôs information practices,

        ‚Ä¢ a refusal to grant access to personal health information or

        ‚Ä¢ a refusal to correct or amend personal health information.




44                                   Frequently Asked Questions: Personal Health Information Protection Act
For further information please see the IPC document, Access and Correction            How does an
Complaints ‚Äì Personal Health Information Protection Act.                              individual initiate a
                                                                                      complaint?

IS THERE A TIME LIMIT WITHIN WHICH AN INDIVIDUAL
MAY COMPLAIN?
In general, an individual must file a complaint with the IPC within one year from
when the individual became aware of the problem. The legislation provides the
IPC with the discretion to extend this one year limitation period.

For complaints that deal with access or correction, an individual must file a
complaint with the IPC within six months from the time a custodian refuses an
access or correction request.



IF A PERSON IS NOT SATISFIED WITH AN IPC ORDER,
WHAT CAN BE DONE?
Persons affected by most types of orders issued by the IPC have the right to
appeal on a question of law to the Divisional Court of Ontario within 30 days of
receiving a copy of the order. Where the IPC issues an order relating to access or
correction of health records, there is no right of appeal. In such a case, a person
may apply to the Divisional Court of Ontario for judicial review.



CAN A PERSON SEEK COMPENSATION FOR DAMAGES?
A person affected by an order of the IPC or a person affected by conduct
leading to a conviction for an offence under PHIPA, that has become final, may
commence a proceeding in court for damages for actual harm suffered. If a court
determines that the harm suffered was caused by wilful or reckless misconduct,
PHIPA permits the court to award up to $10,000 in damages for mental anguish.



WHAT IS AN OFFENCE UNDER PHIPA?
Offences under PHIPA include:

     ‚Ä¢ wilfully collecting, using or disclosing personal health information in
       contravention of PHIPA or its regulations,




Frequently Asked Questions: Personal Health Information Protection Act                         45
        ‚Ä¢ requesting access to or correction of a record of personal health
          information under false pretences,

        ‚Ä¢ intentionally disposing of a record of personal health information to avoid
          providing access,

        ‚Ä¢ collecting, using or disclosing an individual‚Äôs health number in
          contravention of PHIPA,

        ‚Ä¢ obstructing the IPC, or one of its delegates, in the performance of its
          oversight functions,

        ‚Ä¢ dismissing, suspending, demoting, disciplining, harassing or
          disadvantaging an individual who has alerted the IPC of an alleged
          contravention of PHIPA or

        ‚Ä¢ failing to comply with an IPC order.



     WHAT ARE THE CONSEQUENCES FOR COMMITTING AN
     OFFENCE UNDER PHIPA?
     A natural person found guilty of committing an offence under PHIPA can be liable
     for a fine of up to $50,000. A person who is not a natural person for example, an
     organization or institution can be liable for a fine of up to $250,000.

     Any officer, member, employee or agent of a corporation found to have
     authorized or acquiesced to a breach of PHIPA can be held personally liable.

     In addition, persons who are convicted of an offence under PHIPA may be
     subject to a civil suit for damages for breach of privacy. Generally, custodians
     who have acted reasonably and in good faith will be protected from liability.


     WHO IS RESPONSIBLE FOR PROSECUTING OFFENCES
     UNDER PHIPA?
     No person other than the Attorney General, or an agent for the Attorney General,
     may commence a prosecution for an offence under PHIPA. A proceeding to
     prosecute an offence cannot be commenced after six months from the time the
     offence under PHIPA was alleged to have been committed.




46                                   Frequently Asked Questions: Personal Health Information Protection Act
ABOUT THE INFORMATION AND PRIVACY COMMISSIONER OF ONTARIO

The role of the Information and Privacy Commissioner of Ontario is set out
in three statutes: the Freedom of Information and Protection of Privacy Act,
the Municipal Freedom of Information and Protection of Privacy Act and
the Personal Health Information Protection Act. The Commissioner acts
independently of government to uphold and promote open government and
the protection of personal privacy.

Under the three Acts, the Commissioner:

   ‚Ä¢ Resolves access to information appeals and complaints when
     government or health care practitioners and organizations refuse to
     grant requests for access or correction;

   ‚Ä¢ Investigates complaints with respect to personal information held by
     government or health care practitioners and organizations;

   ‚Ä¢ Conducts research into access and privacy issues;

   ‚Ä¢ Comments on proposed government legislation and programs; and

   ‚Ä¢ Educates the public about Ontario‚Äôs access and privacy laws.
Information and Privacy Commissioner of Ontario
2 Bloor Street East, Suite 1400
Toronto, Ontario
Canada M4W 1A8

Web site: www.ipc.on.ca
Telephone: 416-326-3333
Email: info@ipc.on.ca

September 2015
PHIPA Order
 HO-013


 December 16, 2014
                                         Table of Contents

EXECUTIVE SUMMARY.........................................................................................1

BACKGROUND...................................................................................................3

   The Hospital..........................................................................................................3

   Electronic Information System..................................................................................3

   Clerical Staff.........................................................................................................4

   Reported Breach 1.................................................................................................4

   Reported Breach 2.................................................................................................6

   Further Notification to Patients ................................................................................7

REVIEW PROCESS...............................................................................................7

   Other Hospitals .....................................................................................................8

ISSUES............................................................................................................9

RESULTS OF THE INVESTIGATION ...........................................................................9

   Issue A: Is the information at issue ‚Äúpersonal health information‚Äù as defined
            in section 4 of the Act?................................................................................9

   Issue B: Is the person who operates the Hospital a ‚Äúhealth information custodian‚Äù
            as defined in section 3(1) of the Act?.............................................................10

   Issue C: Were Employee 1 and Employee 2 ‚Äúagents‚Äù of the Hospital as defined
            in section 2 of the Act? ..............................................................................11

   Issue D: Was personal health information ‚Äúused‚Äù and/or ‚Äúdisclosed‚Äù in accordance
            with the Act?............................................................................................18
   Issue E: Did the Hospital take steps that are reasonable in the circumstances to
            ensure that personal health information in its custody or control is protected
            against theft, loss and unauthorized use or disclosure in accordance
            with section 12(1) of the Act? .....................................................................21

   Issue F: Did the Hospital have in place information practices that comply with the
            Act and did it comply with its information practices in accordance with
            section 10(1) and 10(2) of the Act?..............................................................39

SUMMARY OF FINDINGS......................................................................................42

ORDER...........................................................................................................42

POSTSCRIPT....................................................................................................44
EXECUTIVE SUMMARY
Personal health information is considered to be among the most sensitive types of personal information,
deserving of the highest protection. Yet, in Ontario, we have seen a growing number of cases of agents
inappropriately accessing the personal health information of individuals. The type and magnitude of
these violations vary. Some involve celebrity ‚Äúgawkers,‚Äù others nosey neighbours, family members or
work colleagues. The circumstances of this case involve the unauthorized use and disclosure of personal
health information for financial gain. The message to take from all of these cases is clear. Authorized
users of electronic information systems can abuse their access privileges ‚Äî they pose a risk to patient
privacy. Health information custodians must implement reasonable measures and safeguards to elimi-
nate or reduce these risks and to mitigate the harms that may arise from them.

Within the span of less than a year, the Rouge Valley Health System (the Hospital) reported two separate
breaches of patient privacy to the Office of the Information and Privacy Commissioner of Ontario. The
first reported breach was received by this office in September 2013 and the second, seven months
later, in April 2014. Although separate incidents, the breaches were materially similar in that both
involved allegations that Hospital employees in clerical positions used and/or disclosed the personal
health information of mothers who had recently given birth at the Hospital for the purposes of selling
or marketing Registered Education Savings Plans (RESPs).

Given the pattern that appeared to be emerging, upon notification of the second breach, this office
decided to conduct a review under the Personal Health Information Protection Act, 2004 (the Act). During
this review, we conducted extensive interviews. We engaged in a thorough review of the Hospital‚Äôs
relevant policies, practices and procedures and received written representations from the Hospital.

As a consequence of the two reported breaches, the Hospital notified more than 14,000 current and
former patients of its Rouge Valley Centenary site and Rouge Valley Ajax and Pickering site, all of whom
may have been affected by the actions of the two employees. It was necessary to notify all of these
individuals because the Hospital was unable to identify the individuals who were actually affected by
the actions of the two employees involved in the reported breaches.

Following the first breach, the Hospital discovered that the audit functionality of its Meditech system,
the electronic information system at issue in this review, was limited and it undertook to address this
shortcoming. During this review, we learned that despite the actions taken and the similarity between
the two breaches, the Hospital was still unable to conduct an audit of user activities relating to the
second breach due to another ‚Äúgap‚Äù in the Meditech system‚Äôs audit functionality.

Audits are essential technical safeguards to protect personal health information. They can be used to
deter and detect collections, uses and disclosures of personal health information that contravene the
Act. In this way, they help to maintain the integrity and confidentiality of personal health information
stored in electronic information systems. The Hospital‚Äôs failure to implement full audit functionality
in its Meditech system meant that it could not comply with its own policies and that it did not comply
with the requirements of the Act.




       HO-013                                                                                         1
We also learned that the Hospital‚Äôs administrative measures or safeguards such as privacy policies,
procedures and practices as well as privacy training and awareness programs ‚Äî which are critical in
protecting personal health information ‚Äî were insufficient and therefore not in compliance with the
Act. These types of safeguards are particularly important in relation to electronic information systems
which provide agents with the ability to access a vast amount of personal health information.

In this Order, among other things, I find that the Hospital failed to comply with its obligations under
the Act to put in place technical and administrative measures or safeguards to protect personal health
information in compliance with section 12(1) of the Act and I order the Hospital to:

1. In relation to all of the Hospital‚Äôs electronic information systems, implement the measures neces-
   sary to ensure that the Hospital is able to audit all instances where agents access personal health
   information on its electronic information systems, including the selection of patient names on the
   patient index of its Meditech system.

2. In relation to the Hospital‚Äôs Meditech system:

    a) Work with the Hospital‚Äôs Hosting Provider to review and amend the service level agreement
       between the Hospital and the Hosting Provider to clarify the responsibility for the creation,
       maintenance and archiving of user activity logs generated by the Hospital‚Äôs use of its Meditech
       system, and ensure that the user activity logs are available to the Hospital for audit purposes.

    b) Work with Meditech or another software provider to develop a solution that will limit the
       search capabilities and search functionalities of the Hospital‚Äôs Meditech system so that agents
       are unable to perform open-ended searches for personal health information about individuals,
       including newborns and/or their mothers, and can only perform searches based on the follow-
       ing criteria: health number, medical record number, encounter number, or exact first name,
       last name and date of birth.

3. Review and revise its Privacy Audits policy, the Pledge of Confidentiality policy and the ‚ÄúPledge of
   Confidentiality,‚Äù and the Privacy Advisory in accordance with the comments and findings made in
   this Order, and take steps to ensure that it complies with the Privacy Audits policy.

4. Develop and implement a Privacy Training Program policy, a Privacy Awareness Program policy, and a
   Privacy Breach Management policy in accordance with the comments and findings made in this Order.

5. Immediately review and revise its privacy training tools and materials in accordance with the
   comments and findings made in this Order.

6. Using the privacy training materials developed in accordance with Order provision 5:

    a) immediately conduct privacy training for all agents in clerical positions in the Hospital; and

    b) conduct privacy training for all other agents by June 16, 2015.

7. Provide this office with proof of compliance with all of the Order provisions by September 16, 2015.




2                                                                                        HO-013
BACKGROUND
Within the span of less than a year, the Rouge Valley Health System (the Hospital) reported two
separate breaches to the Office of the Information and Privacy Commissioner of Ontario (IPC). The
first breach was reported to the IPC in September 2013 and the second, seven months later, in April
2014. Although the reported breaches involved separate incidents, they were materially similar in
that both involved allegations that Hospital employees in clerical positions used and/or disclosed the
personal health information of mothers who had recently given birth at the Hospital for the purposes
of selling or marketing Registered Education Savings Plans (RESPs). Given the pattern that appeared to
be emerging, upon receipt of the report of the second breach, the IPC decided to conduct a review
pursuant to section 58(1) of the Personal Health Information Protection Act, 2004 (the Act).

The circumstances surrounding the two reported breaches are complex. Before going into the details
of the two breaches, it is necessary to provide some background on the Hospital as well as on the
electronic information system at issue in this review.


The Hospital
The Hospital operates two community hospital sites, Rouge Valley Centenary (Centenary site) and
Rouge Valley Ajax and Pickering (Ajax and Pickering site). The Centenary site is located in east Toronto
and the Ajax and Pickering site is located in west Durham. The two employees who were the subject
of the reported breaches were employed at the Centenary site, but had access to the personal health
information of patients at both sites through one of the Hospital‚Äôs electronic information systems.


Electronic Information System
The Hospital uses electronic information systems to facilitate the provision of health care to its patients.
While the Hospital maintains records of personal health information in paper format, there has been
nothing to suggest that the two employees who were the subject of the reported breaches used and/
or disclosed personal health information in paper form for the purposes of selling or marketing RESPs.

The software that runs the electronic information system at issue is named after the company that
provides it. That company is Medical Information Technology, Inc. (Meditech). In this Order, I will
use ‚ÄúMeditech‚Äù to refer to the company and ‚ÄúMeditech system‚Äù to refer to the electronic informa-
tion system at issue. The information that the IPC received about the Hospital‚Äôs Meditech system was
provided by the Hospital in its representations filed during this review.

The Hospital‚Äôs Meditech system is a collection of different applications called ‚Äúmodules.‚Äù Different
modules assist employees and other agents of the Hospital in performing different high-level tasks; for
example, scheduling, admissions, payroll, billing, etc. At a lower level than modules are components
of modules. Components of modules perform specific functions. For example, the patient index is
a component that is present in the scheduling module. The patient index, an electronic list of every




       HO-013                                                                                             3
Hospital patient, allows employees and other agents of the Hospital with access to it to search for
patients in the Hospital‚Äôs database.

The scheduling module was used by the two employees to access the personal health information
of new mothers. The employees were granted access to this module to perform their duties, which
included registering patients and scheduling appointments and procedures for them. The first step in
scheduling an appointment or procedure is to determine whether the person is an existing Hospital
patient by searching for their name on the patient index. If the patient is not on the patient index,
then they must be registered and issued a medical record number (MRN). This two-step task requires
access to the entire patient index so as to prevent the same patient from being registered multiple times
and receiving multiple MRNs. For this reason, the two employees had access to the personal health
information of patients at both the Hospital‚Äôs Centenary site and Ajax and Pickering site, including
demographic information about patients, such as their name, address and phone numbers, date of
birth, health number and the dates of visits to the Hospital.

The Hospital shares a version of Meditech software with another hospital that runs the software and
hosts the Meditech system used by the Hospital. In this Order, this other hospital will be referred to as
the ‚ÄúHosting Provider.‚Äù The Hosting Provider owns the license for the shared Meditech software and
is responsible for implementing and operating a Meditech system on behalf of the Hospital according
to a service level agreement between them. A consequence of the fact that the Hospital and Hosting
Provider share a version of Meditech software is that some technical settings apply to both the Hospital
and the Hosting Provider.


Clerical Staff
As noted above, the responsibilities of the two employees who were the subject of the reported
breaches included performing tasks such as registering patients, and scheduling appointments and
procedures for them. However, the two employees‚Äô responsibilities were not limited to such tasks.
Throughout this Order, those who work in positions with similar responsibilities as the two employees
will be referred to generally as ‚Äúclerical staff.‚Äù Clerical staff are not responsible for directly providing
health care to patients.


Reported Breach 1
In September 2013, the Hospital contacted the IPC to report a breach involving an employee (Employee
1) who it determined had violated the Hospital‚Äôs privacy policy and the Act. Employee 1 began work-
ing in a clerical position at the Centenary site in May 2004. In July 2013, Employee 1 was transferred
to another department at the Centenary site, where he continued to work in a clerical position until
he was terminated in October 2013.

In 2009, Employee 1 advised the Hospital that he had applied for a part-time position as a sales rep-
resentative for an RESP company and he asked the Hospital to confirm, in writing, that selling RESPs




4                                                                                             HO-013
would not be a conflict of interest vis-√†-vis his employment with the Hospital. In July 2009, the Hospital
provided Employee 1 with a letter, which stated:

       It is our understanding that one of our employees, [name of Employee 1] has applied
       for an RESP sales representative position. Many of our staff hold jobs apart from work-
       ing here and all employees who hold other positions outside of their employment with
       Rouge Valley Health System must abide by our rules and regulations, including our
       Conflict of Interest Policy, which states, in part, that no employee shall solicit any busi-
       ness from patients, staff or visitors to support such outside employment.

When Employee 1 was transferred in July 2013, his access to Meditech system modules that included
personal health information was terminated, because the Hospital determined that he no longer required
that access to fulfil his job duties. According to the Hospital, shortly after his transfer, Employee 1
asked the Hospital to reinstate his access to Meditech system modules that included personal health
information, stating, at that time, that he was ‚Äúseeking access to phone numbers of patients who had
recently given birth in order to sell them RESPs in the course of his part-time employment.‚Äù Based on
this information, Employee 1 was suspended pending the outcome of an investigation by the Hospital.
His access to personal health information was also suspended.

The Hospital advised that during a subsequent interview by its human resources staff, Employee 1
denied that he was employed part-time selling RESPs or that he had contacted any patients of the
Hospital for that purpose. Shortly after that interview, the Hospital terminated Employee 1 having
concluded that he had violated the Hospital‚Äôs privacy policy and the Act.

During discussions we had with Employee 1, he continued to deny that he had contravened the
Hospital‚Äôs policies and the Act. However, given the Hospital‚Äôs findings and the fact that it reported
a privacy breach to this office, for the purposes of this Order, I accept the Hospital‚Äôs conclusion that
Employee 1 had contravened its policies and the Act.

When this breach was reported to the IPC in September 2013, the IPC believed that it was an isolated
incident and, based on the information provided by the Hospital as to the steps that it had taken
or would take to minimize the risk of a similar breach occurring in future, the IPC worked with the
Hospital to contain the breach and to ensure that appropriate notice to affected patients was given.
Further discussion of the Hospital‚Äôs response to this reported breach appears below.

In a letter dated October 8, 2013, the Hospital provided the IPC with the results of its internal inves-
tigation, including the following information:

   ‚Ä¢   The Hospital ‚Äúdetermined that the incident was a violation of the [Hospital] privacy policy and
       the PHIPA act, and self-reported the incident to the IPC.‚Äù

   ‚Ä¢   The Hospital‚Äôs IT department was not able to prove or disprove that Employee 1 had been
       ‚Äúaccessing patient records‚Äù because the Hospital‚Äôs ‚Äúsystem only has two weeks of user history‚Äù
       which was ‚Äúa limitation posed by [its] Meditech hosting partner.‚Äù




       HO-013                                                                                           5
    ‚Ä¢   The Hospital ‚Äú[e]stablished that the employee had access to schedule information which
        allowed him to view contact information (telephone and address) of expecting mothers without
        accessing the patient record.‚Äù

    ‚Ä¢   The Hospital was not ‚Äúable to view patient record level audit logs‚Äù and therefore was also
        ‚Äúnot able to quantify the number of patients whose information was viewed inappropriately
        by the employee.‚Äù

With respect to the limitations of its audit functionality, the Hospital stated:

        In order to overcome the audit log limitation that we discovered in our Meditech system,
        we are working with our hosting party, [ ], and the vendor, Meditech, on two enhance-
        ments of the access logs: (a) extend the length of the live Meditech log to ninety days
        and enable the archiving of the logs past ninety days, (b) create an export of the access
        logs from the Meditech proprietary format to a relational database that will allow us to
        maintain unlimited access history and report inappropriate access.

On December 12, 2013, the Hospital began notifying 7,613 current and former patients that their
personal health information may have been used by Employee 1 in contravention of the Act. Not
knowing for certain which patients‚Äô personal health information was used by Employee 1, the Hospital
notified all patients at the Centenary site who had given birth between July 2009 and August 2013,
which is the period of Employee 1‚Äôs employment for which he had access to the scheduling module
of the Meditech system.


Reported Breach 2
On April 24, 2014, the Hospital notified the IPC that it had discovered that a second employee
(Employee 2) had been selling the personal health information of patients who had recently given
birth at the Hospital to an RESP company.

Employee 2 began working in a clerical position at the Hospital‚Äôs Centenary site in July 2001 and
continued to work in that capacity until June 2013. In July 2013, Employee 2 began working again in
a clerical position at the Hospital‚Äôs Centenary site until her resignation in April 2014.

The Hospital explained that in early April 2014, one of its staff members found a number of documents
on a printer that appeared to be printed screen shots of the Meditech system. The Hospital stated that
the documents included the personal health information of patients who had recently given birth.
The screen shots were given to senior managers, who determined that Employee 2 had printed them.

In representations submitted by the Hospital during this review, the Hospital states that the printed
screen shots represented the results of searches conducted on the patient index. The results included
a ‚Äúlookup list of patients meeting the search parameters‚Äù used by Employee 2. In addition, the printed
screen shots included the patient ‚Äúselected‚Äù from the list which pulls up the patient‚Äôs contact informa-
tion, health number and Hospital visits. These search results indicated that Employee 2 was looking
for information about new mothers.




6                                                                                          HO-013
The Hospital conducted an internal investigation. During this internal investigation, Employee 2 admit-
ted to the Hospital that she had been selling personal health information to an RESP sales agent since
2010 and stated that she had sold the information of about 400 patients in the last nine months of
her employment for approximately $600. After the Hospital concluded its internal investigation in
April 2014, Employee 2 resigned.

On May 27, 2014, the Hospital notified an additional 669 former patients of its Centenary site that
‚Äúa staff member was inappropriately accessing hospital information through the Hospital‚Äôs electronic
scheduling system.‚Äù This group of patients was comprised of the mothers who had given birth at the
Hospital‚Äôs Centenary site between July 2013 and April 2014 ‚Äî the period of time between the first
reported breach and the second reported breach. The Hospital states in its representations that it was
not able to identify the actual patients affected by Employee 2‚Äôs actions.


Further Notification to Patients
On July 2, 2014, the Hospital informed the IPC that the screen shots that were found on the printer
that triggered its investigation into the activities of Employee 2, contained the personal health infor-
mation of patients who had received care at its Ajax and Pickering site, in addition to its Centenary
site. In light of this and given that Employee 1 also had access to the Meditech patient index, which
includes the personal health information of patients at the Ajax and Pickering site, the Hospital con-
cluded that some patients at the Ajax and Pickering site may have been affected by the activities of
the two Employees.

On August 19, 2014, the Hospital notified a further 6,150 former patients of the Ajax and Pickering site
that their personal health information may have been used and/or disclosed in contravention of the Act.



REVIEW PROCESS
Following the report of the second breach, the IPC commenced a review under section 58(1) of the
Act and began to gather further information from the Hospital including copies of relevant documents,
such as copies of the applicable policies, practices and procedures of the Hospital.

The IPC also met with and interviewed the Chief Information and Privacy Officer and other senior
managers at the Hospital, and was given a demonstration of the Meditech system and its scheduling
module that was used by both Employees in the regular course of their employment. Hospital IT staff
also gave a demonstration of the audit capabilities and limitations of the Meditech system.

The IPC contacted both Employees and asked them to meet with IPC staff. Employee 1 declined, but
Employee 2 agreed to be interviewed.




       HO-013                                                                                         7
Given the seriousness of the allegations, I issued a summons to both Employees pursuant to section
60(12) of the Act. The summons compelled them to attend at the office of the IPC and to give evi-
dence under oath or affirmation.

I also issued a Notice of Review asking the Hospital to submit written representations on the issues
relevant to this review. After receipt of representations in response to the initial Notice of Review, I
issued a Supplementary Notice of Review inviting the Hospital to submit further representations. I
received representations from the Hospital in response to the supplementary notice.


Other Hospitals
During the course of this review, the IPC received complaints from 20 different individuals who had
given birth at other Ontario hospitals and who had been contacted by telephone in the days or weeks
following their child‚Äôs birth by representatives of various RESP companies (‚Äúcomplainants‚Äù).

Seventeen of these complainants provided their consent to an investigation by this office into the cir-
cumstances surrounding their complaint. The IPC then contacted the hospitals identified and requested
that they conduct their own internal investigations and report back to the IPC on the results of those
investigations. The hospitals‚Äô internal investigations included audits of the personal health informa-
tion of the complainants and, in some cases, the hospitals contacted the RESP companies involved
to inquire as to how the RESP company received the contact details of the complainants. The IPC
received the full cooperation of the hospitals involved.

I am satisfied that in each of these cases involving other hospitals, the personal health information of
the complainants was not used and/or disclosed by agents of the other hospitals for the purposes of
selling or marketing RESPs. Based on the reports received from the other hospitals, the IPC learned
that the complainants had, at some point prior to the birth of their child, provided their consent to be
contacted by an RESP company. This consent was provided in some cases on ballot entries submitted
at baby shows or exhibitions, and/or by signing up for a loyalty card at a maternity clothing retailer.
Most of these complainants did not recall providing their consent and acknowledged that they might
not have thoroughly reviewed the information on the ballot or loyalty card application, or understood
what they were consenting to.¬†¬†

As a result of the above, I am satisfied that the personal health information of these complainants was
not used and/or disclosed by agents of these other hospitals for the purposes of selling or marketing
RESPs and each of these files has been closed.




8                                                                                         HO-013
ISSUES
In this Order, I will consider the following issues:

a) Is the information at issue ‚Äúpersonal health information‚Äù as defined in section 4 of the Act?

b) Is the person who operates the Hospital a ‚Äúhealth information custodian‚Äù as defined in section
   3(1) of the Act?

c) Were Employee 1 and Employee 2 ‚Äúagents‚Äù of the Hospital as defined in section 2 of the Act?

d) Was personal health information ‚Äúused‚Äù and/or ‚Äúdisclosed‚Äù in accordance with the Act?

e) Did the Hospital take steps that are reasonable in the circumstances to ensure that personal health
   information in its custody or control is protected against theft, loss and unauthorized use or dis-
   closure in accordance with section 12(1) of the Act?

f)   Did the Hospital have in place information practices that comply with the Act and did it comply
     with these practices in accordance with section 10(1) and 10(2) of the Act?



RESULTS OF THE INVESTIGATION

Issue A:         Is the information at issue ‚Äúpersonal health information‚Äù as
                 defined in section 4 of the Act?
Section 4(1) of the Act states, in part:

        In this Act,

        ‚Äúpersonal health information‚Äù, subject to subsections (3) and (4), means identifying
        information about an individual in oral or recorded form, if the information,

            (a) relates to the physical or mental health of the individual, including informa-
                tion that consists of the health history of the individual‚Äôs family,

            (b) relates to the providing of health care to the individual, including the iden-
                tification of a person as a provider of health care to the individual,

            ‚Ä¶

            (f) is the individual‚Äôs health number, or

            (g) identifies an individual‚Äôs substitute decision-maker.




        HO-013                                                                                      9
Section 4(2) of the Act states:

        In this section,

        ‚Äúidentifying information‚Äù means information that identifies an individual or for which
        it is reasonably foreseeable in the circumstances that it could be utilized, either alone
        or with other information, to identify an individual.

In its representations, the Hospital stated that it does not know the exact nature and type of information
that was used and/or disclosed by Employee 1 and Employee 2. Based on the information available
to the Hospital, the Hospital ‚Äúsurmises that the information used and/or disclosed by the Employees
was: patient names of the mother and baby, baby‚Äôs gender, baby‚Äôs date of birth, and the mother‚Äôs
telephone number.‚Äù

The Hospital acknowledges that this information is ‚Äúpersonal health information‚Äù as that term is
defined in the Act. In its representations, the Hospital states that section 32(2) of the Act suggests that
an individual‚Äôs name and contact information constitute ‚Äúpersonal health information‚Äù even if they
do not on their own relate to the physical or mental health of the individual, the health history of the
family or the provision of health care to the individual.

I find that the information at issue is ‚Äúpersonal health information‚Äù as defined in section 4 of the Act. It
identifies the name of the mother and the baby and identifies that the mother and baby were patients
of the Hospital. Section 4(1) of the Act clearly states that personal health information includes the
identification of a person, in this case the Hospital, as a provider of health care to the individual, in
this case the mother and the baby.


Issue B:        Is the person who operates the Hospital a ‚Äúhealth information
                custodian‚Äù as defined in section 3(1) of the Act?
Section 3(1) of the Act states, in part:

        ‚Äúhealth information custodian‚Äù, subject to subsections (3) to (11), means a person or
        organization described in one of the following paragraphs who has custody or control
        of personal health information as a result of or in connection with performing the per-
        son‚Äôs or organization‚Äôs powers or duties or the work described in the paragraph, if any:

                ‚Ä¶

        4. A person who operates one of the following facilities, programs or services:

        i. A hospital within the meaning of the Public Hospitals Act‚Ä¶

Section 2 of the Act defines a ‚Äúperson‚Äù to include a partnership, association or other entity. Section
87 of the Legislation Act further provides that a ‚Äúperson‚Äù includes a corporation.




10                                                                                            HO-013
Consistent with the IPC‚Äôs findings in previous Orders, I find that the Hospital is a ‚Äúperson‚Äù who operates
a hospital within the meaning of the Public Hospitals Act and that it is a health information custodian
with custody or control of the personal health information at issue as defined in section 3(1)4i of the
Act. The Hospital does not dispute this finding.


Issue C:          Were Employee 1 and Employee 2 ‚Äúagents‚Äù of the Hospital as
                  defined in section 2 of the Act?
The issue to be decided here is whether Employee 1 and Employee 2 were ‚Äúagents‚Äù when they used
and/or disclosed personal health information in the custody or control of the Hospital for the pur-
poses of selling or marketing RESPs. The issue is relevant to determining whether the personal health
information was ‚Äúused‚Äù and/or ‚Äúdisclosed‚Äù within the meaning of the Act.1

Section 2 of the Act defines an ‚Äúagent‚Äù as:

         ‚Äúagent‚Äù, in relation to a health information custodian, means a person that, with the
         authorization of the custodian, acts for or on behalf of the custodian in respect of per-
         sonal health information for the purposes of the custodian, and not the agent‚Äôs own
         purposes, whether or not the agent has the authority to bind the custodian, whether
         or not the agent is employed by the custodian and whether or not the agent is being
         remunerated.

The Hospital submits that Employee 1 and Employee 2 were not ‚Äúagents‚Äù for these purposes. The
Hospital argues that it did not authorize Employee 1 or Employee 2 to use and/or disclose personal
health information for the purposes of selling or marketing RESPs and, in doing so, the Employees acted
beyond the authority delegated by the Hospital. It argues that these uses and/or disclosures of personal
health information by the Employees were not in the course of their duties and were not carried out
for, or on behalf of and for the purposes of, the Hospital, but rather, were ‚Äúclearly motivated by self-
interest.‚Äù Therefore, the Hospital argues that the Employees were not ‚Äúagents‚Äù within the meaning of
the Act in using and/or disclosing personal health information for these purposes.

Having carefully considered these representations, I disagree. In the usual course of their duties,
Employee 1 and Employee 2 acted for or on behalf of, for the purposes of and with the authoriza-
tion of the Hospital in respect of personal health information, and not for their own purposes. They
were authorized to collect, use, disclose, retain or dispose of personal health information to assist the
Hospital in carrying out its duties. Therefore, they were ‚Äúagents‚Äù under the Act even though they may
have acted beyond the authority delegated by the Hospital in the particular instances when they used
and/or disclosed personal health information to market or sell RESPs.

This interpretation is consistent with previous Orders of the IPC. As held in Orders HO-002 and
HO-010:
1 The provision of personal health information between a health information custodian and an agent of the custodian
is a use by the custodian, and not a disclosure by the person providing the information or a collection by the person to
whom the information is provided. See section 6(1) of the Act.




        HO-013                                                                                                         11
        A cursory reading of the definition of ‚Äúagent‚Äù in the circumstances of this complaint
        might suggest that, because in this instance the nurse did not have the hospital‚Äôs autho-
        rization to use or disclose the health information in question, and was in fact doing
        so for her own purposes, she was not an ‚Äúagent.‚Äù That is not my view. For the reasons
        that follow, I have concluded that this interpretation is not sustainable, and that the
        nurse was in fact an agent.

        A careful reading of the definition, particularly when viewed in the context of the Act
        as a whole, makes it clear that the Legislature intended that the phrase, ‚Äúacts for or on
        behalf of the custodian in respect of personal health information for the purposes of
        the custodian‚Äù should be read as a reference to the person‚Äôs usual duties and activi-
        ties, as opposed to an action taken in the particular circumstances of a complaint‚Ä¶ It
        is also important that the definition of ‚Äúagent‚Äù expressly contemplates the inclusion of
        employees in this category.2

My finding is also supported by the modern rule of statutory interpretation which states: ‚Äúthe words
of an Act are to be read in their entire context, in their grammatical and ordinary sense harmoniously
with the scheme of the Act, the object of the Act, and the intention of Parliament.‚Äù3


Grammatical and Ordinary Meaning of ‚ÄúAgent‚Äù
In R v Conception, the Supreme Court of Canada emphasized that the starting point of statutory inter-
pretation ‚Äúis the text of the provisions in their grammatical and ordinary sense,‚Äù especially where the
key term is expressly defined by statute.4 Section 2 of the Act defines an ‚Äúagent‚Äù as ‚Äúa person that,
with the authorization of the custodian, acts for or on behalf of the custodian in respect of personal
health information for the purposes of the custodian, and not the agent‚Äôs own purposes.‚Äù

The Hospital argues that the Employees were not acting ‚Äúwith the authorization of‚Äù and ‚Äúfor or on
behalf of‚Äù and ‚Äúfor the purposes of‚Äù the Hospital, but rather for their ‚Äúown purposes,‚Äù and therefore
were not ‚Äúagents‚Äù within the meaning of the Act in using and/or disclosing personal health information
for the purposes of selling or marketing RESPs.

I do not agree with this position. Employee 1 and Employee 2 meet the definition of ‚Äúagent‚Äù in the
ordinary sense of the word used in the Act. They are ‚Äúpersons‚Äù who acted ‚Äúwith the authorization of,‚Äù
‚Äúfor or on behalf of,‚Äù and ‚Äúfor the purposes of‚Äù the Hospital in respect of personal health information
in the usual course of their duties. But for the fact that they were agents, the Employees would not
have had access to the personal health information at issue.

This interpretation is consistent with the grammatical and ordinary meaning of the term ‚Äúagent.‚Äù
‚ÄúAgent‚Äù is defined by Merriam-Webster as ‚Äúa person who does business for another person,‚Äù ‚Äúa person

2 (July 2006), HO-002, online: IPC <http://www.ipc.on.ca/images/Findings/up-HO_002.pdf> at 5 [HO-002];
(December 31, 2010), HO-010, online: IPC <http://www.ipc.on.ca/images/Findings/ho-010.pdf> at 7 [HO-010].
3 Ruth Sullivan, Sullivan on the Construction of Statutes, 5th ed (Markham: LexisNexis Canada Inc., 2008) at 1; Re Rizzo
& Rizzo Shoes Ltd., [1998] 1 SCR 27 at para 41; and R. v. Conception, 2014 SCC 60 at para 14 [Conception].
4 R. v. Conception, supra note 3.




12                                                                                                      HO-013
who acts on behalf of another,‚Äù or ‚Äúa person or thing that causes something to happen.‚Äù5 Similarly,
Oxford defines ‚Äúagent‚Äù as ‚Äúa person who acts on behalf of another‚Äù or ‚Äúa person or thing that takes
an active role or produces a specified effect.‚Äù6 None of the dictionary definitions consulted indicate
that a person must act within the authorization of, for or on behalf of and for the purposes of the other
person at all times in order to be an agent.

The words ‚Äúwith the authorization of the custodian,‚Äù ‚Äúacts for or on behalf of the custodian,‚Äù and
‚Äúfor the purposes of the custodian, and not the agent‚Äôs own purposes‚Äù in section 2 of the Act ensure
that third parties who do not have an employment, contractual or other agency relationship with the
custodian fall outside the scope of the definition of ‚Äúagent.‚Äù These words make it clear that third
parties who may be permitted to access personal health information in health care settings for their
own purposes, such as independent researchers, assessors or inspectors of regulatory colleges and
government inspectors, are not ‚Äúagents‚Äù within the meaning of the Act and therefore the custodian is
not responsible for their actions in respect of the personal health information in its custody or control.


Objects and Scheme of the Act
The Legislation Act states that a statute shall be interpreted as being remedial and shall be given ‚Äúsuch
fair, large and liberal interpretation as best ensures the attainment of its objects.‚Äù7

The Hospital‚Äôs argument that the term ‚Äúagent‚Äù should be narrowly interpreted to exclude a person
who is authorized to collect, use, disclose, retain or dispose of personal health information for or
on behalf of and for the purposes of a health information custodian in the usual course of his or her
duties, but who, in a particular instance or instances, collects, uses, discloses, retains or disposes of
that information for an unauthorized purpose, is inconsistent with the objects of the Act and with the
scheme of the Act.


Objects of the Act
The objects of the Act are set out in section 1, which provides in part:

        1.¬†¬†The purposes of this Act are,

        (a) to establish rules for the collection, use and disclosure of personal health information
        about individuals that protect the confidentiality of that information and the privacy
        of individuals with respect to that information, while facilitating the effective provision
        of health care;

        [‚Ä¶]

        (e) to provide effective remedies for contraventions of this Act.

5 Merriam-Webster Online Dictionary, sub verbo ‚Äúagent.‚Äù
6 Oxford Online Dictionary, sub verbo ‚Äúagent‚Äù; likewise, Cambridge Dictionaries Online‚Äôs definition of ‚Äúagent‚Äù includes
‚Äúa person who acts for or represents another‚Äù, sub verbo ‚Äúagent.‚Äù
7 Legislation Act, 2006, SO 2006, c 21 Sched F at s 64(1).




        HO-013                                                                                                       13
At its core, the objects or purposes of the Act are to protect the privacy of individuals in respect of
their personal health information, to protect the confidentiality of that information and to provide
effective remedies for contraventions of the Act. Privacy and confidentiality are best protected by
holding health information custodians accountable for the conduct of persons who act for or on their
behalf and for their purposes in the usual course of their duties.

As the law of vicarious liability demonstrates, ‚Äú[e]mployers are often in a position to reduce accidents
and intentional wrongs by efficient organization and supervision.‚Äù8 Vicarious liability is designed to
ensure that the employer remains responsible for the reasonably foreseeable risks attributable to or
arising from the employer‚Äôs activities so that the employer takes reasonable steps to reduce the risk.
This has been acknowledged on numerous occasions by the Supreme Court of Canada. In London
Drugs v Kuehne & Nagel International Ltd., the Court noted ‚Äú[v]icarious liability has the broader func-
tion of transferring to the enterprise itself the risks created by the activity performed by its agents.‚Äù9
Further, in John Doe v Bennett, the Court stated ‚Äúthe hope is that holding the employer or principal
liable will encourage such persons to take steps to reduce the risk of harm in the future.‚Äù10

But for the fact that they were employees, Employee 1 and Employee 2 would not have had access to
the personal health information at issue. Therefore, the Hospital provided the opportunity and created
the risk of unauthorized use and disclosure. The Hospital is also in the best position to take reasonable
steps to reduce the risk of further contraventions of the Act not only by Employee 1 and Employee 2,
but by all persons. The Hospital, and not the Employees, can develop, amend and implement poli-
cies, procedures, practices and safeguards that apply to all persons, including those acting for or on
its behalf and for its purposes in the usual course of their duties.

If the Hospital‚Äôs submissions were accepted, a health information custodian would arguably have
less responsibility for those acting for or on its behalf and for its purposes in the usual course of their
duties under the Act than under the law of vicarious liability. This clearly does not protect the privacy
of individuals with respect to their personal health information and the confidentiality of that infor-
mation. If the Legislature intended to limit the responsibility of health information custodians for the
actions of those acting for or on their behalf in the usual course of their duties, it would have included
clear and unambiguous language in the Act. Absent such clear and unambiguous language, there is
no basis for interpreting the term ‚Äúagent‚Äù in such a way that is fundamentally inconsistent with the
purposes of the Act.

The Hospital argues that ‚Äúit is not necessary for a person to be an ‚Äòagent‚Äô to be covered by the restric-
tions and potential sanctions in the Act.‚Äù In particular, it argues that the Commissioner may make an
order under section 61(1) of the Act against ‚Äúany person‚Äù and that ‚Äúany person‚Äù may be charged with
an offence under section 72 of the Act, suggesting that such orders and prosecutions would, in these
circumstances, achieve the objects or purposes of the Act.



8 Bazley v Curry, [1999] 2 SCR 534 at para 32.
9 [1992] SCR 299 at 339.
10 2004 SCC 17 at para 20.




14                                                                                           HO-013
While an order of the Commissioner directed at ‚Äúany person‚Äù or a prosecution commenced by the
Attorney General against ‚Äúany person‚Äù may have a deterrent effect on others, such measures would
not adequately address the systemic issues that an order directed at a health information custodian
would achieve. As previously noted, an order directed at a custodian to implement policies, proce-
dures, practices and safeguards would reduce the risk of further contraventions of the Act not only
by the ‚Äúperson‚Äù whose acts or omissions are at issue, but all ‚Äúpersons‚Äù acting for, on behalf of and for
the purposes of the custodian in the usual course of their duties.

There are further problems with the Hospital‚Äôs proposed interpretation. If the Hospital‚Äôs submissions
were accepted, it would result in inconsistent treatment or accountability of health information cus-
todians under the Act, depending on whether or not they act through other persons. For example,
custodians that are corporations (such as community care access corporations and corporations that
operate hospitals, long-term care homes and pharmacies) and other custodians that act through other
persons would have less responsibility for contraventions of the Act than a custodian who may do so
to a lesser degree, such as a sole health care practitioner. Such an interpretation is inconsistent with
the objects and purposes of the Act.

Moreover, if the Hospital‚Äôs submissions were accepted, it would result in persons constantly transitioning
between acting as agents and non-agents, potentially from one moment to the next, throughout the
course of a day. The effort that would be required to determine exactly when each person was acting
as an agent would create unnecessary confusion and ultimately frustrate the ability of the Commissioner
and the courts to achieve the objects and purposes of the Act. 11 The objects and purposes of the
Act are not to apportion liability between the health information custodian and persons acting for or
on its behalf. Its main object or purpose is to protect privacy and confidentiality of individuals in a
health care setting.


Scheme of the Act
My finding as to the proper interpretation of the term ‚Äúagent‚Äù in section 2 of the Act is also consistent
with other provisions in the Act.

Section 17(1) provides, in part, as follows:

         A health information custodian is responsible for personal health information in the
         custody or control of the health information custodian and may permit the custodian‚Äôs
         agents to collect, use, disclose, retain or dispose of personal health information on the
         custodian‚Äôs behalf only if ‚Ä¶

This section unequivocally states that a health information custodian is responsible for personal health
information in its custody or control. A health information custodian may permit others to collect,
use, disclose, retain or dispose of personal health information for or on its behalf, but the Act clearly

11 This type of impractical time-based interpretation was expressly criticized in reference to the Freedom of Information
and Protection of Privacy Act, RSO 1990, c F-31, in Ontario (Solicitor General) v Mitchinson, (2001) 55 OR (3d) 355 (CA),
[2001] OJ No 3223 at paras 38-40.




        HO-013                                                                                                         15
states that the custodian remains responsible. Nothing in the Act permits a custodian to delegate or
assign that responsibility.

In these circumstances, there is no dispute that the personal health information at issue was and con-
tinues to be in the custody and control of the Hospital. Therefore, pursuant to section 17(1), as the
health information custodian, the Hospital ‚Äúis responsible‚Äù for that information.

In fact, the majority of the obligations under the Act are imposed on health information custodians,
not on other persons, including agents. This clearly points to the fact that accountability for personal
health information remains with the custodian.

The Hospital‚Äôs suggestion that a person is not an ‚Äúagent‚Äù when they act beyond the authority delegated
by the Hospital is also inconsistent with sections 17(1)(b) and (2), which state:

     (1)¬†¬†A health information custodian is responsible for personal health information in the custody or
          control of the health information custodian and may permit the custodian‚Äôs agents to collect,
          use, disclose, retain or dispose of personal health information on the custodian‚Äôs behalf only if,

     ‚Ä¶

     (b) the collection, use, disclosure, retention or disposition of the information, as the case may be,
          is in the course of the agent‚Äôs duties and not contrary to the limits imposed by the custodian,
          this Act or another law; and

     (2) Except as permitted or required by law and subject to the exceptions and additional require-
         ments, if any, that are prescribed, an agent of a health information custodian shall not collect,
         use, disclose, retain or dispose of personal health information on the custodian‚Äôs behalf unless
         the custodian permits the agent to do so in accordance with subsection (1).

Section 17(2) of the Act expressly permits agents to collect, use, disclose, retain or dispose of personal
health information without the permission or authorization of the health information custodian in cer-
tain circumstances, including those prescribed in section 7 of Regulation 329/04 under the Act.12 As
a result, the Act clearly contemplates that a person does not cease to be an agent simply because the
custodian did not permit or authorize the agent to collect, use, disclose, retain or dispose of personal
health information for a specific purpose. In addition, sections 17(1)(b) and (2) clearly contemplate the
possibility of unauthorized collection, use, disclosure, retention or disposal by agents, which would be
impossible if the Hospital‚Äôs submissions were accepted. As stated in both Orders HO-002 and HO-010:

         Section 17 of the Act clearly contemplates the possibility of improper collection, use or
         disclosure by agents, which would be impossible if their status as agents ended when
         they ceased acting for the custodian‚Äôs purposes and began acting for their own‚Ä¶ these
         provisions would be rendered meaningless if a person who would usually be an agent
         is converted to a non-agent in the event that they act improperly. The Legislature could
         not possibly have intended this result.13
12 Personal Health Information Protection Act, 2004, Ontario Regulation 329/04 at s 7.
13 Orders HO-002 and HO-010, supra note 2.




16                                                                                            HO-013
The Hospital refers to section 12(2) of the Act in support of its position that Employee 1 and Employee
2 were not ‚Äúagents‚Äù when they used and/or disclosed personal health information for the purposes of
selling or marketing RESPs. Section 12(2) states:
       Subject to subsection (3) and subject to the exceptions and additional requirements, if
       any, that are prescribed, a health information custodian that has custody or control of
       personal health information about an individual shall notify the individual at the first
       reasonable opportunity if the information is stolen, lost, or accessed by unauthorized
       persons.

The Hospital states that an agent is by definition not ‚Äúan unauthorized person‚Äù and therefore sug-
gests that because the Hospital was required to notify affected individuals under section 12(2), the
Employees could not possibly have been agents.
Again, I do not agree. If the position of the Hospital were accepted, a health information custodian
would also not be required to notify affected individuals under section 12(2) if the custodian authorized
a person to use and/or disclose personal health information in contravention of the Act on the basis
that the personal health information would not have been ‚Äúaccessed by an unauthorized person.‚Äù
Such a result would not conform with the scheme of the Act.

Section 12(2) cannot be read in isolation. It must be read in the context of the section in which it is
found as well as the other provisions of the Act. The immediately preceding section states:

       12(1) A health information custodian shall take steps that are reasonable in the circum-
       stances to ensure that personal health information in the custodian‚Äôs custody or control
       is protected against theft, loss and unauthorized use or disclosure and to ensure that
       the records containing the information are protected against unauthorized copying,
       modification or disposal.

The notice requirement in section 12(2) of the Act stems from the obligation in section 12(1) which
requires a health information custodian to take steps that are reasonable in the circumstances to pro-
tect personal health information against ‚Äúunauthorized use or disclosure.‚Äù Section 12(2) should be
interpreted to encompass unauthorized use or disclosure of personal health information.


The Legislative Intent
The Hospital has not referenced any legislative history to support its narrow interpretation of the term
‚Äúagent.‚Äù In fact, the legislative history supports a broad interpretation.

Statements made by individuals who were instrumental in advising the Ministry of Health and Long-
Term Care (the Ministry) on the development of the Act, make it clear that the term ‚Äúagent‚Äù is to
be interpreted broadly. For example, when explaining the term ‚Äúagent‚Äù to the Standing Committee
considering the bill that led to the Act, legal counsel for the Ministry stated:




       HO-013                                                                                         17
        There‚Äôs always someone who is responsible. The hospital is responsible for all the health-
        care practitioners who work within it. As well, the doctor is responsible for his or her
        own staff in the office‚Ä¶ The definition of ‚Äúagent‚Äù is an expansive definition. It includes
        students, it would include volunteers; it is all of those who work within a custodian.14

Before the same committee, the Acting Director of the Health Information, Privacy and Sciences
Branch of the Ministry confirmed the breadth of the definition of ‚Äúagent‚Äù in the Act:

        You‚Äôll see in section 17 the point that we made earlier, that custodians are respon-
        sible for the actions of their agents. Whether it‚Äôs a volunteer working in a hospital or
        an information manager that you‚Äôve hired to transcribe your records, ultimately, the
        custodian is responsible.15

In fact, the definition of ‚Äúagent‚Äù was further broadened by the Standing Committee to include the
phrase ‚Äúwhether or not the agent has the authority to bind the custodian,‚Äù16 which is how the term
is currently defined in section 2 of the Act. The Standing Committee‚Äôs expansion of the definition is
further evidence that the term ‚Äúagent‚Äù is meant to be interpreted broadly.

Upon consideration of the grammatical and ordinary meaning of ‚Äúagent,‚Äù the objects and scheme
of the Act and the legislative intent, I find that Employee 1 and Employee 2 were ‚Äúagents‚Äù of the
Hospital in the particular instances when they used and/or disclosed personal health information for
the purposes of selling or marketing RESPs.


Issue D:         Was personal health information ‚Äúused‚Äù and/or ‚Äúdisclosed‚Äù in
                 accordance with the Act?
Section 2 of the Act defines ‚Äúuse‚Äù and ‚Äúdisclose‚Äù as follows:

        ‚Äúuse‚Äù, in relation to personal health information in the custody or under the control of
        a health information custodian or a person, means to handle or deal with the informa-
        tion, subject to subsection 6(1), but does not include to disclose the information, and
        ‚Äúuse‚Äù, as a noun, has a corresponding meaning;

        ‚Äúdisclose‚Äù, in relation to personal health information in the custody or under the control
        of a health information custodian or a person, means to make the information available
        or to release it to another health information custodian or to another person, but does
        not include to use the information, and ‚Äúdisclosure‚Äù has a corresponding meaning;



14 Ontario, Standing Committee on General Government (Hansard), 38th Parl, 1st Sess, (January 26, 2004) at 1050
(Halyna Perun) [emphasis added].
15 Ontario, Standing Committee on General Government (Hansard), 38th Parl, 1st Sess, (January 26, 2004) at 1110 (Carol
Appathurai).
16 Ontario, Standing Committee on General Government (Hansard), 38th Parl, 1st Sess, (April 28, 2004) at 1600 (Kathleen
Wynne).




18                                                                                                     HO-013
Section 6(1) of the Act is also relevant. It states, in part, that ‚Äúthe providing of personal health informa-
tion between a health information custodian and an agent of the custodian is a use by the custodian,
and not a disclosure by the person providing the information‚Ä¶‚Äù

Personal health information is permitted to be used or disclosed if the use or disclosure complies with
section 29 of the Act, which states:

        A health information custodian shall not collect, use or disclose personal health infor-
        mation about an individual unless,

            (a) it has the individual‚Äôs consent under this Act and the collection, use or
                disclosure, as the case may be, to the best of the custodian‚Äôs knowledge, is
                necessary for a lawful purpose; or

            (b) the collection, use or disclosure, as the case may be, is permitted or required
                by this Act.

As previously discussed, in July 2013, Employee 1 was transferred to another department at the
Hospital‚Äôs Centenary site and his access rights to personal health information in the Meditech system
were terminated. According to the information provided by the Hospital, Employee 1 asked his man-
ager to reinstate his previous access rights and stated that he had been accessing the Meditech system
to obtain the contact information of new mothers so that he could contact them for the purposes of
selling them RESPs. Following an investigation by the Hospital, the Hospital concluded that there had
been a violation of the Hospital‚Äôs privacy policy and of the Act and reported the breach to the IPC.
The Hospital has indicated to the IPC that it has no information to suggest that Employee 1 disclosed
personal health information.

Employee 2 admitted that she accessed personal health information for the purpose of selling it to an
RESP sales agent and sold that information to the RESP agent for that purpose, and that she had been
doing so since 2010. Employee 2 sold the personal health information knowing that the RESP agent
was using this information to sell or market RESPs to patients.

Employee 2 used the Meditech scheduling module to search the patient index and retrieve the contact
information of the patients. The printouts of Meditech screen shots found in April 2014 show that
Employee 2 was able to return a list of newborns by searching for a patient with the name ‚ÄúAA‚Äù and
a recent date of birth. Because the name ‚ÄúAA‚Äù did not match any patients in the patient index, the
system relaxed the search criteria and searched for any patients with the specified date of birth only.
In this way, an open-ended search for newborns was performed. By selecting the name of a newborn
from the results of the patient index search, Employee 2 was able to access information about the
newborn‚Äôs mother.




       HO-013                                                                                             19
Use of Personal Health Information
Based on the information provided by the Hospital and the information gathered in this review, and
given that I have found that the Employees were agents of the Hospital, applying section 6(1) of the
Act, I find that their handling and dealing with the personal health information described above was a
‚Äúuse‚Äù within the meaning of section 2 of the Act. I also find that this use of personal health information
was for the purposes of selling or marketing RESPs or for the purpose of selling the personal health
information to an RESP sales agent who in turn was selling or marketing RESPs to patients.

There is no information or evidence before me to suggest that patients consented to this use of their
personal health information. In addition, no section in the Act permits or requires such a use of per-
sonal health information without the consent of patients. Section 37 of the Act sets out the purposes
for which personal health information is permitted to be used without consent. I find that none of
these purposes applies in the circumstances before me.

The Hospital acknowledges that personal health information was used without patient consent and
that this use was not permitted by section 37 of the Act. In its representations, the Hospital states ‚Äúthe
Employees used [personal health information]. They did not have patient consent to do so and were
not using the [personal health information] for any purpose permitted under section 37 of [the Act]
or permitted by the Hospital.‚Äù

Not only does the Act not permit or require such a use of personal health information without consent,
the Act prohibits such a use. Section 33 of the Act states:

       A health information custodian shall not collect, use or disclose personal health infor-
       mation about an individual for the purpose of marketing anything or for the purpose
       of market research unless the individual expressly consents and the custodian collects,
       uses or discloses the information, as the case may be, subject to the prescribed require-
       ments and restrictions, if any.

Therefore, I find that the use of personal health information for the purposes of selling RESPs or for the
purpose of selling the personal health information to an RESP sales agent who in turn was selling or
marketing RESPs to patients was not permitted and, in fact, was expressly prohibited without express
consent, and therefore contravened section 29 of the Act.


Disclosure of Personal Health Information
Based on the information provided by the Hospital, there is no evidence to suggest Employee 1 dis-
closed personal health information. However, as acknowledged by the Hospital, Employee 2 admitted
that she provided personal health information to an RESP sales agent for the purpose of selling or
marketing RESPs. I find that in making this information available or releasing it to an RESP sales agent,
personal health information was ‚Äúdisclosed‚Äù within the meaning of section 2 of the Act. I also find
that this disclosure of personal health information was for the purposes of selling or marketing RESPs.




20                                                                                          HO-013
There is no information or evidence before me to suggest that patients consented to this disclosure of
their personal health information. In fact, the Hospital states that ‚ÄúEmployee 2 did not obtain patient
consent‚Äù for the disclosure.

In addition, no section in the Act permits or requires such a disclosure of personal health information
without the consent of patients. Sections 38 - 50 of the Act set out the purposes for which personal
health information is permitted to be disclosed without consent. I find that none of these purposes
applies in the circumstances before me. Not only does the Act not permit or require such a disclosure
of personal health information without consent, but as noted above, section 33 of the Act expressly
prohibits such a disclosure.

Therefore, I find that the disclosure of personal health information for the purpose of selling or market-
ing RESPs to patients was not permitted and, in fact, was expressly prohibited without express consent,
and therefore contravened section 29 of the Act.

While I previously found that both Employees were agents of the Hospital within the meaning of
section 2 of the Act, if they were not agents for these purposes, as argued by the Hospital, the provi-
sion of personal health information to them would have been disclosures to them by the Hospital.
Consistent with my previous findings, such disclosures were not made with the consent of patients
and were not permitted or required by the Act and, in fact, were expressly prohibited. Therefore, I
find that even if the Employees were not agents of the Hospital for these purposes, these disclosures
would have contravened section 29 of the Act. Therefore, regardless of whether the Employees were
agents of the Hospital for these purposes, and regardless of whether the provision of personal health
information by the Hospital to them were ‚Äúuses‚Äù or ‚Äúdisclosures,‚Äù the Hospital is responsible for such
uses or disclosures.


Issue E:        Did the Hospital take steps that are reasonable in the
                circumstances to ensure that personal health information
                in its custody or control is protected against theft, loss and
                unauthorized use or disclosure in accordance with section
                12(1) of the Act?
Section 12(1) of the Act sets out the obligation of health information custodians to implement steps
that are reasonable in the circumstances to protect personal health information against unauthorized
use or disclosure. It states:

       A health information custodian shall take steps that are reasonable in the circumstances
       to ensure that personal health information in the custodian‚Äôs custody or control is
       protected against theft, loss and unauthorized use or disclosure and to ensure that
       the records containing the information are protected against unauthorized copying,
       modification or disposal.




       HO-013                                                                                          21
In Order HO-010, the IPC stated that measures or safeguards must be reviewed from time to time to
ensure that they continue to be ‚Äúreasonable in the circumstances‚Äù in order to protect personal health
information from theft, loss and unauthorized use or disclosure and to protect records of personal
health information against unauthorized copying, modification or disposal. As new technologies are
developed, adopted or implemented and as new threats and vulnerabilities emerge, ‚Äústeps that are
reasonable in the circumstances,‚Äù the standard in section 12(1) of the Act, will also evolve.

This means that, among other things, health information custodians must identify the risks to privacy
and confidentiality of personal health information and implement measures or safeguards that are
reasonable in the circumstances to eliminate or reduce these risks and to mitigate the harms that
may arise from these risks. The risks to privacy and to the confidentiality of personal health informa-
tion posed by agents who use or disclose personal health information for purposes that contravene
the Act are well known. The IPC has issued two previous Orders17 and other privacy commissioners
have issued a number of orders or reports stemming from this issue.18 Articles in major newspapers
evidence increased public concern over this issue and its impact on patients.19 There have been a
number of prosecutions of agents for uses and disclosures in contravention of privacy legislation in
other provinces. In Ontario, the Attorney General has commenced a prosecution against a nurse who
worked at a hospital in northern Ontario for allegedly accessing the personal health information of
more than 5,000 patients in contravention of the Act. This prosecution is ongoing.20

Accordingly, in my view, the Hospital should have known about the risks to privacy and to the confi-
dentiality of personal health information posed by its own agents before the first breach was discovered
in 2013 and should have taken steps that were reasonable in the circumstances, as outlined below,
before that time. Its failure to do so contravened section 12(1) of the Act. Even if the Hospital was not
aware of this risk prior to the time the first breach was discovered, it should have become aware of the
risk at that time. In addition, after the first breach, the Hospital clearly knew or ought to have known
that it did not have in place sufficient measures or safeguards to detect or confirm uses and disclosures
of personal health information in contravention of the Act by agents using the Meditech scheduling
module. Based on this, I find that even if the Hospital did not contravene section 12(1) of the Act
prior to discovering the first breach in 2013, the Hospital contravened section 12(1) of the Act when
it failed to take steps that were reasonable in the circumstances, as outlined below, after that time.

The Hospital states that it has complied with section 12(1) because the Hospital ‚Äúhas taken steps that
are reasonable in the circumstances, as measured against Ontario health sector practices and more
specifically the practices of other similarly situated (size, region, resources) public hospitals and the
information technology assets at the Hospital‚Äôs disposal.‚Äù It states that ‚Äúthe Hospital‚Äôs safeguards to
protect [personal health information] have been tested at different times against the requirements

17 Orders HO-002 and HO-010, supra note 2.
18 Investigation Report H-2013-001 Office of the Information and Privacy Commissioner-Saskatchewan, 2013 CanLII
5640 (SK IPC); Investigation Report H2011-IR-004 Information and Privacy Commissioner of Alberta, http://www.oipc.
ab.ca/downloads/documentloader.ashx?id=2912; Manitoba Ombudsman Report 2011-0513 and 2011-0514, https://
www.ombudsman.mb.ca/uploads/document/files/cases2011-0513-0514-en.pdf.
19 See Olivia Carville, ‚ÄúHospital Privacy Violations Rife in Ontario,‚Äù The Toronto Star, October 29, 2014, (URL)
20 See Maria Calabrese, ‚ÄúHospital Defends Private Records,‚Äù North Bay Nugget, June 12, 2013, http://www.nugget.
ca/2013/06/11/hospital-defends-private-records.




22                                                                                                   HO-013
established for regional information technology initiatives,‚Äù and refers to the Hospital Diagnostic
Imaging Repository Services as an example. It also states that it uses a standard configuration of the
Meditech system and the same safeguards used by other Meditech clients and added that any new
audit functionality that it implemented following the first reported breach is a ‚Äúcustom feature.‚Äù

The health information custodian has the onus of establishing compliance with section 12(1) of the
Act. While the Hospital claims that it has complied with section 12(1), it has not provided me with any
information or evidence to support its claims about the practices in place in the Ontario health sec-
tor. Nor has it explained how its safeguards ‚Äúhave been tested‚Äù against ‚Äúthe requirements established
for regional information technology initiatives,‚Äù including the Hospital Diagnostic Imaging Repository
Services. Nor has it provided me with any information or evidence about the safeguards used by other
Meditech clients, other than its Hosting Provider. Even if it had provided information to support these
claims, these factors would not be determinative of the question of whether the Hospital has complied
with its obligations under section 12(1).

Below I provide further details of the deficiencies in the Hospital‚Äôs compliance with section 12(1). These
deficiencies are addressed in the following two general headings, Technical Measures or Safeguards
and Administrative Measures or Safeguards.


Technical Measures or Safeguards

Audit Functionality
As in other industries, audits play an important role in the health sector. Auditing of electronic informa-
tion systems is particularly important in ensuring that the privacy of individuals and the confidentiality
of personal health information are protected. Audits are essential technical safeguards for electronic
information systems. They can be used to deter and detect collections, uses and disclosures of per-
sonal health information and the copying, modification or disposal of records of personal health
information that contravene the Act. As such, they help to maintain the integrity and confidentiality
of personal health information stored in electronic information systems. The ability to conduct audits
of personal health information and the activities of agents or users (referred to in this section as users)
in an electronic information system also ensures that a health information custodian is able to respond
to requests from patients for information about who has collected, used or disclosed their personal
health information.

In order to be effective, audits require analyzable data about the full extent to which users collected,
used, disclosed, copied, modified or disposed of personal health information within a given time
period. If such data is not available or is only available in part, then a health information custodian
will not be able to conduct a complete audit in relation to the personal health information stored in
its electronic information system.

As noted above, the two Employees had access to the scheduling module of the Hospital‚Äôs Meditech
system which contains the personal health information of patients, including new mothers. The sched-




       HO-013                                                                                           23
uling module provides access to demographic information about patients such as their name, address
and phone numbers, as well as information about their date of birth, health number and the dates of
visits to the Hospital.

In the Meditech system, analyzable data about user activities within the scheduling module is gener-
ated in the form of user activity logs. As the name suggests, these logs, if available, can be used to
create an audit report of user activities.

User activity logs are not the only means of conducting audits in the Hospital‚Äôs Meditech system. For
example, the Hospital states that it is able to audit the activities of users by generating ‚Äúsystem utili-
zation reports‚Äù which capture user access to personal health information in the Patient Care Inquiry
(PCI) module. Although they are important elements of the Hospital‚Äôs overall auditing system, system
utilization reports are specific to the PCI module and do not capture information about access to
personal health information within the scheduling module. As such, these reports do not address
the personal health information at issue in this review. The only means of conducting audits on user
activities within the scheduling module are user activity logs, which, according to the Hospital, are
different from system utilization reports.

Having carefully reviewed the information provided by the Hospital in response to the Notice of
Review and the Supplementary Notice of Review, I find that the Hospital did not take steps that were
reasonable in the circumstances with respect to the audit functionality of the scheduling module of
the Meditech system and therefore failed to comply with section 12(1). In particular, I take issue with
three aspects of the Hospital‚Äôs auditing functionality: the user activity log history, the service level
agreement and the user activity log information. With respect to each of these aspects, the reasons
for my finding of non-compliance are as follows:

     ‚Ä¢   User activity log history. At the time of the first reported breach, the Hospital‚Äôs Meditech system
         did not archive user activity logs for a period longer than 14 days. This meant that the Hospital
         was unable to conduct any audits of user activities within the scheduling module that occurred
         more than two weeks prior.

     ‚Ä¢   Service level agreement. The service level agreement between the Hospital and the Hosting
         Provider did not include a requirement for the Hosting Provider to ensure that user activ-
         ity logs generated by the Hospital‚Äôs users were archived and available to the Hospital for
         auditing purposes.

     ‚Ä¢   User activity log information. In its report to this office following the first reported breach, the
         Hospital committed to addressing the lack of user activity logs by ensuring that user activity
         logs were permanently archived and available to the Hospital for audit purposes. However,
         despite this improvement, it became apparent during this review that the Hospital was still
         unable to audit Employee 2‚Äôs activities within the scheduling module because the user activity
         logs generated by its Meditech system lacked key information. Specifically, the user activity logs
         did not capture the selection of a patient‚Äôs name on the patient index within the scheduling
         module. The Hospital came to this realization only after the second reported breach whereas




24                                                                                             HO-013
        it should have discovered this shortcoming in its auditing system immediately after the first
        reported breach and taken the appropriate actions to address it at that time.


User Activity Log History
As I indicated above, the Hospital shares a version of Meditech software with another hospital, which
is referred to throughout this Order as the Hosting Provider. The Hosting Provider owns the license for
the software and is responsible for implementing and operating a Meditech system on behalf of the
Hospital. The Hospital is entitled to use the Meditech system pursuant to a service level agreement
which sets out the roles and responsibilities of the parties in relation to the Meditech system and the
information stored in the system.

At the time of the first reported breach, the Hospital‚Äôs Meditech system was configured to retain user activ-
ity logs for a maximum period of 14 days. This meant that any audit log information about users‚Äô activities
within the scheduling module was automatically overwritten if the activities were older than 14 days.

In addition, these logs were not archived by the Hosting Provider so as to enable their long-term stor-
age and retrieval for audit or any other purposes. Thus once overwritten, any audit log information
about user activities within the scheduling module was permanently deleted. Since the Meditech
system is shared between the Hospital and the Hosting Provider, this included the user activity logs
for the Hospital.

Upon discovery of the first reported breach, the Hospital became aware of these limitations in the
auditing functionality of its Meditech system and that it was unable to perform an audit of Employee
1‚Äôs activities.

One consequence of this lack of user activity logs and the inability to conduct an audit of user activi-
ties within the scheduling module was that the Hospital was unable to identify the patients whose
personal health information was accessed. This was confirmed in a letter from the Hospital to the IPC
dated October 8, 2013, stating that ‚Äú[d]ue to the fact that we were not able to view patient record
level audit logs we were not able to quantify the number of patients whose information was viewed
inappropriately [‚Ä¶].‚Äù

During our investigation into the first reported breach, the Hospital committed to addressing the
archiving limitations in the auditing functionality of its Meditech system. In the same October 8, 2013
letter, the Hospital states:

        In order to overcome the audit log limitation that we discovered in our Meditech system,
        we are working with our hosting party, [], and the vendor, Meditech, on two enhance-
        ments of the access logs: (a) extend the length of the live Meditech log to ninety days
        and enable the archiving of the logs past ninety days, (b) create an export of the access
        logs from the Meditech proprietary format to a relational database that will allow us to
        maintain unlimited access history and report inappropriate access.




       HO-013                                                                                             25
In its representations, the Hospital was given an opportunity to explain its position in regards to this
lack of auditing capabilities at the time of the first reported breach. When asked specifically why, at the
time of the first reported breach, user activity logs were not archived, the Hospital replied that ‚Äú[it] did
not have the option of archiving logs because it does not own the Meditech Archiving Module (MAM).‚Äù

This answer is unacceptable. As a custodian of personal health information, the Hospital is responsible
for personal health information in its custody or control. The fact that it does not ‚Äúown the Meditech
Archiving Module‚Äù does not absolve it of its responsibilities under the Act. In the words of a former
Commissioner, ‚Äúyou can outsource services, but you cannot outsource accountability.‚Äù21 Regardless of
who is actually implementing and operating the Meditech system, the Hospital is responsible for ensuring
that measures or safeguards that are reasonable in the circumstances are in place to protect personal
health information in its custody or control against theft, loss and unauthorized use or disclosure and
to protect records of personal health information in its custody or control against unauthorized copy-
ing, modification or disposal. If the Hosting Provider was responsible for maintaining and archiving
user activity logs, then the service level agreement should have reflected that. If the Hosting Provider
was not responsible for maintaining those logs, then the Hospital should have taken steps to ensure
that it maintained the logs through other means.


Service Level Agreement
The Hospital‚Äôs position in regard to its lack of auditing capabilities at the time of the first reported
breach raised the question of the adequacy of the service level agreement and whether it complied
with section 12(1) of the Act. The service level agreement stipulates that the Hospital‚Äôs ‚ÄúMeditech
databases‚Äù will be ‚Äúindependent‚Äù of the Hosting Provider‚Äôs; that the Hospital ‚Äúowns‚Äù the ‚Äúdata con-
tained within [its] databases;‚Äù and that the Hosting Provider is responsible for maintaining the security,
confidentiality and integrity of the Hospital‚Äôs ‚Äúdata‚Äù by providing controlled access to it and performing
daily backups of it.

While the service level agreement contains many provisions that stipulate much of the required func-
tionality of the shared Meditech system, it does not address the responsibility for ensuring that the
user activity logs generated by the Hospital‚Äôs use of its Meditech system are archived and available to
the Hospital for auditing purposes.

In particular, the service level agreement does not contain a provision that explicitly sets down the
requirements for logging the activity of agents or users and the archiving of user activity logs gener-
ated by the Meditech system. Indeed, the service level agreement makes no mention at all of ‚Äúuser
activity logs‚Äù or even ‚Äúaudit logs.‚Äù

Second, where the service level agreement discusses the Hosting Provider‚Äôs requirement to ‚Äúperform [
] daily backups of the software and data,‚Äù it is not clear whether these daily backups include backups
of user activity logs.


21 Reviewing the Licensing Automation System of the Ministry of Natural Resources: A Special Investigation Report, June
27, 2012, http://www.ipc.on.ca/images/Findings/2012-06-28-MNR_report.pdf, p. 6.




26                                                                                                       HO-013
In Order HO-010, issued in December 2010, the IPC dealt with a complaint related to the use of
personal health information by a technologist at The Ottawa Hospital without consent and in contra-
vention of the Act. During the IPC‚Äôs review, it was determined that one of the hospital‚Äôs electronic
information systems included audit functionality, but that it had not been turned on. With respect to
the obligations imposed pursuant to section 12(1), the IPC found:

       The fact that audit functions are either non-existent or have not been turned on in rela-
       tion to any of the electronic information systems of the hospital that contain personal
       health information falls short of meeting the requirements of section 12(1) of the Act.

I agree. I find that the Hospital‚Äôs failure to ensure that user activity logs were available to conduct
audits was a contravention of section 12(1) of the Act.

Although the Hospital states that it has taken steps to address the limitations in its auditing system
such that it no longer relies upon the Hosting Provider for the archiving of user activity logs, in the
Order provisions that follow, I will require the Hospital to work with the Hosting Provider to review
and amend the service level agreement between the Hospital and the Hosting Provider to clarify
the responsibility for the creation, maintenance and archiving of user activity logs generated by the
Hospital‚Äôs use of its Meditech system, and to ensure that the user activity logs are available to the
Hospital for audit purposes.


User Activity Log Information
As noted above, in the case of both breaches, the two Employees had access to the scheduling module
of the Hospital‚Äôs Meditech System. Employee 2 used and/or disclosed the personal health information
about new mothers by selecting their name or the name of their newborn from the results of a patient
index search within the scheduling module.

In the Meditech system, when a patient name is selected from the results of a patient index search,
additional information about that patient is displayed at the bottom of the screen. This information
includes the patient‚Äôs address, phone number, health number, and a list of dates of Hospital visits. This
information is provided for verification purposes, i.e., so that the user can confirm that the selected
patient is in fact the patient the user is searching for. Thus, alongside the information about the patient
displayed at the bottom of the screen, a dialog box appears to the user. The dialog box prompts the
user with the question ‚ÄúIs this the one?‚Äù and gives the user the option of answering ‚ÄúYes‚Äù or ‚ÄúNo.‚Äù

If the user selects ‚ÄúYes,‚Äù then the selected ‚Äúpatient‚Äôs record‚Äù is shown. However, if the user selects
‚ÄúNo,‚Äù the user is taken back to the results of the patient index search. A key configuration of the
Hospital‚Äôs Meditech system is that it is only when the user goes on to view the selected ‚Äúpatient‚Äôs
record‚Äù (by selecting ‚ÄúYes‚Äù in the dialog box) that an event regarding the user‚Äôs access to that patient‚Äôs
personal health information is recorded in the user activity log. If the user clicks ‚ÄúNo,‚Äù then an event
regarding the access to that patient‚Äôs personal health information is not recorded in the user activity
log, despite the fact that the user was able to view the patient‚Äôs personal health information displayed
at the bottom of the screen. In its representations, the Hospital explained this aspect of its auditing
system as follows:




       HO-013                                                                                           27
        If the user selects to proceed once the user has determined from the demographic
        information that the patient is the correct patient, by clicking ‚ÄúYes‚Äù in the dialogue box
        entitled ‚Äúis this the right one,‚Äù the user triggers an audit trail. If the user clicks ‚ÄúNo,‚Äù
        there is currently no audit trail.

In the case of the first breach, this limitation in the Hospital‚Äôs auditing capabilities did not affect its
ability to perform an audit on Employee 1, since, as noted above, the short retention period and lack
of archiving of audit logs ruled out the possibility of conducting an audit on Employee 1‚Äôs activities
within the scheduling module.

However, in the case of the second breach, this limitation played an important role. At the time of the
second breach, the Hospital had roughly six months of audit logs available to it. Despite this, however,
because Employee 2 did not click ‚ÄúYes‚Äù in the dialog box when selecting the results of patient index
searches, the user activity logs provided no information on which patients‚Äô personal health informa-
tion Employee 2 had used and disclosed in contravention of the Act.

At the time of both breaches, the Hospital‚Äôs Meditech system did not have the ability to record the
selection of patient names on the patient index list. Simply put, the Hospital‚Äôs Meditech system did not
provide that functionality despite the fact that by selecting a patient‚Äôs name on the patient index, the
Employees were able to view demographic and personal health information about the patient including
information relating to the dates of visits to the Hospital and health number. Thus, the Hospital rightly
points to a ‚Äúgap‚Äù in the capabilities of its Meditech system when explaining why, at the time of the
second reported breach, it continued to lack information regarding the affected patients and therefore
was not able to identify them for purposes of the notification required under section 12(2) of the Act.

Having said that, it is important to note that the Employees had similar duties and responsibilities with
similar access rights to the Meditech system in both breaches ‚Äî they were both in clerical positions with
access to the entire patient index through the scheduling module of the Hospital‚Äôs Meditech system.
However, according to the Hospital, it was only after the second reported breach that it discovered
this deficiency in its audit functionality.

In my view, this was too late. The Hospital should have come to this realization during its initial
investigation into the first breach. The Hospital concluded that Employee 1 was using the personal
health information about new mothers in contravention of the Act. In addition, the Hospital knew
that Employee 1 was able to perform patient index searches and that selecting a patient name on the
patient index would reveal personal health information about that patient. The Hospital confirmed
this was the case in its letter to the IPC dated October 8, 2013, where it stated that the Hospital:

        [e]stablished that the employee had access to schedule information which allowed
        him to view contact information (telephone and address) of expecting mothers without
        accessing the patient record. [Emphasis added]

On the basis of this information alone, the Hospital should have taken further steps to ensure that
sufficient information about user activities within the scheduling module of its Meditech system was
being captured in its user activity logs in the fall of 2013, if not sooner. With this critical information,




28                                                                                            HO-013
it then could have put additional measures or safeguards in place, which may have mitigated the harm
arising from the second breach. Why the Hospital did not carry through with a full assessment of its
auditing capabilities in 2013, given the information available to it, is not clear to me.

In its representations, the Hospital stated that it did not know how Employee 1 accessed personal health
information for the purpose of selling RESPs. I accept that the Hospital may not have had specific details
of the manner in which Employee 1 accessed the personal health information of patients. However,
statements from the Hospital in its letter of October 8, 2013 show that, at the very least, the Hospital
knew what options were open to Employee 1 for accessing the personal health information of patients.

As noted above, to comply with section 12(1) of the Act, health information custodians must review
from time to time the measures or safeguards that they have implemented to ensure that they con-
tinue to be ‚Äúreasonable in the circumstances.‚Äù After the discovery of a contravention of the Act, such
a review is absolutely essential. A health information custodian must conduct a thorough review to
identify limitations or ‚Äúgaps‚Äù in the measures or safeguards directly related to the contravention of the
Act and address these limitations or ‚Äúgaps‚Äù in a timely manner so as to prevent similar contraventions
in the future.

In my view, the Hospital did not undertake a thorough enough review of its safeguards upon discovery
of the first breach and so failed to introduce reasonable measures or safeguards in advance of the sec-
ond breach that could have mitigated the harm and facilitated the identification of affected patients.
As such, I find that the Hospital did not have measures or safeguards in place that were reasonable
in the circumstances at the time of the second breach with respect to the information contained in
its user activity logs.

According to the Hospital, it has been working with Meditech since the discovery of the second breach
to enhance the logging functionality of its auditing system. The Hospital submits that Meditech has
provided it with a custom auditing feature that would log the selection of a patient name on the patient
index. The Hospital has tested this feature and authorized its migration to the Hospital‚Äôs Meditech
system; however, I understand that the feature will not be available for use at the Hospital until the
migration is accepted by the Hosting Provider.

In the Order provisions below, I will require the Hospital to implement this custom auditing feature
in its Meditech system. In addition, I will require the Hospital to implement any other measures nec-
essary to ensure that the Hospital is able to audit all instances where agents access personal health
information in its Meditech system and in any other electronic information systems it uses, including
the selection of a patient name on the patient index.


Search Controls
As noted in the background section of this Order, Employee 2 accessed the personal health informa-
tion about new mothers by selecting their names or the names of their newborns from the results
of a patient index search. To perform a patient index search in the Hospital‚Äôs Meditech system, one
must enter certain search criteria, for example, the patient‚Äôs name and date of birth. The printouts of




       HO-013                                                                                          29
Meditech screen shots found in the second reported breach show that Employee 2 was able to return
a list of newborns by searching for a patient with the name ‚ÄúAA‚Äù and a recent date of birth. When the
term ‚ÄúAA‚Äù did not match any patients in the patient index, the Meditech system relaxed the search
criteria and searched for any patients with the specified date of birth only. In this way, an open-ended
search for newborns was performed by Employee 2 and by selecting the name of a newborn from the
results of the search, Employee 2 was able to access information about the newborn‚Äôs mother.

Another way to retrieve a list of newborns from patient index searches, and as a result gain access
to personal health information about new mothers, is to search for a patient with the name ‚Äúbaby.‚Äù
Because the Hospital may not be aware of a newborn‚Äôs given name until sometime after the birth, the
newborn‚Äôs record of personal health information will often initially list the newborn‚Äôs first name as ‚Äúbaby
girl‚Äù or ‚Äúbaby boy.‚Äù Because of this, if an agent of the Hospital searches for a patient with the name
‚Äúbaby,‚Äù and specifies a gender or date of birth, the system will return a list of newborns as matches.

A third way to retrieve a list of newborns from the patient index searches involves the fact that, accord-
ing to the Hospital, the Meditech system defaults to a search for ‚Äúbaby‚Äù when the search criteria do
not match any patient. In other words, one does not need to actually search for the name ‚Äúbaby‚Äù
for results with that name to show up. The Hospital explained that when performing a search using a
combination of name with gender or date of birth, the search algorithm works as a series of steps in
which at each step, if a match is not found, the search criteria are relaxed from specific to more general
and the search is performed again. As a final step, if the algorithm finds no approximate matches on
the name, gender or date of birth entered, it ‚Äúsearches for the last name, ‚ÄòBABY‚Äô.‚Äù

The result of these three search configurations is that one does not need to know the name, address,
health number or any other identifiable information about a particular patient to produce a list of
newborns and their parents‚Äô contact details within the scheduling module of the Hospital‚Äôs Meditech
system through its search functionality.

The Hospital explained that its Meditech system does not have built-in functionality to limit the ability
of agents of the Hospital to perform open-ended searches of the nature used by Employee 2. According
to the Hospital, ‚Äú[t]he search algorithm for the patient index is standard Meditech functionality‚Äù and
‚Äú[t]he number of search results is not customizable.‚Äù

Accordingly, at the time of the breaches, the Hospital did not have any search controls in place. The
Hospital submits that, since this review was commenced, it has requested Meditech to remove the
search term ‚Äúbaby‚Äù from its search algorithm as a possible future safeguard. This would prevent a
list of newborns from being returned if the Meditech search algorithm finds no exact or approximate
matches for the other search criteria. The Hospital has indicated that it does not know when Meditech
will provide this functionality.

However, I note that this is only one of the ways of retrieving a list of newborns through the Hospital‚Äôs
search system. It would not affect the ability of agents to retrieve a list of newborns by searching for
a patient with a meaningless name ‚Äî e.g., ‚ÄúAA‚Äù‚Äî and a recent date of birth, nor would it prevent
agents from searching for patients with the name ‚Äúbaby.‚Äù




30                                                                                           HO-013
Another approach available to the Hospital is to look at ways in which agents use the Meditech system
for authorized purposes and restrict the system‚Äôs functionality to only those uses. The Hospital explained
that in addition to partial first or last name plus date of birth or gender, patient index searches can be
initiated by any of the following criteria:

    1. health number;

    2. medical record number (MRN);

    3. encounter number; or

    4. exact first name, last name and date of birth.

What is important to note about these searches is that in contrast to open-ended searches where a
list of patients who partially match the criteria is returned, these would only return a single patient, if
there was in fact a match, for the majority of cases.

If the Hospital‚Äôs Meditech system had been configured to allow only these four types of searches, the
occurrence of both reported breaches may have largely, if not entirely, been prevented. Since open-
ended searches that return lists of patients who partially match the search criteria would not have been
allowed, the Employees would not have been able to go ‚Äúfishing‚Äù for information about new mothers.

The Hospital states that disallowing open-ended searches that return lists of patients who partially
match the search criteria would adversely affect the ability of agents to schedule appointments and
procedures for patients. According to the Hospital, the ability to relax search criteria from specific to
more general is necessary for the following reasons:

        (a) the patient name may not be always spelled correctly (long names are particularly
        challenging); (b) the patient name may be spelled phonetically; (c) common names or
        very short names may produce hundreds of matches in which case, additional search
        criteria such as gender or date of birth are required to narrow the search.

With respect to (a) and (b), if the Hospital‚Äôs Meditech system were configured to allow only four types
of searches, namely health number, MRN, encounter number, or exact first name, last name and date
of birth, then agents would still be able to find patients on the patient index list with misspelled names
or phonetically spelled names by using one of the other allowable search criteria.

With respect to (c), I do not see how this demonstrates a need for open-ended searches. Rather than
a case where search criteria are relaxed, it describes a case where search criteria are further restricted.
As such, it is not an argument for open-ended searches, but rather an argument against them.

For these reasons, I do not find the Hospital‚Äôs arguments convincing. Disallowing open-ended searches
that return lists of patients who partially match the search criteria would not adversely affect the ability
of agents to perform their duties and are measures that are reasonable in the circumstances to ensure
that personal health information is protected against unauthorized use or disclosure. The Hospital




       HO-013                                                                                            31
should have asked Meditech to address this shortcoming in the Meditech system or should have looked
at other technical solutions if not after the first reported breach, then definitely after the second.

In the Order provisions below, I will require that the Hospital work with Meditech or another software
provider to develop a solution that will limit the ability of its agents to perform open-ended searches for
personal health information about patients in accordance with the comments above.


Administrative Measures or Safeguards
In order to comply with the requirement in section 12(1) of the Act to take steps that are reasonable in
the circumstances to protect personal health information, health information custodians must imple-
ment administrative measures or safeguards, including privacy policies, procedures and practices,
as well as privacy training and awareness programs and initiatives. Comprehensive privacy policies,
procedures and practices, as well as comprehensive privacy training, are critical in protecting personal
health information from unauthorized use and disclosure and from other contraventions of the Act.
This is particularly important in relation to electronic information systems which provide agents with
the ability to access a vast amount of personal health information.

The Hospital states that, in addition to the technological measures or safeguards in place, it has imple-
mented a number of administrative measures or safeguards to protect personal health information. It
provided the IPC with policies, procedures and practices to support its position.

The Hospital also states that following the first breach it understood that it needed to implement more
frequent privacy training and that ‚Äúit is considering additional means it can use to reinforce its culture
of privacy.‚Äù It adds that the administrative measures or safeguards it has implemented are reasonable
in the circumstances as measured against Ontario health sector practices and more specifically the
practices of similarly situated public hospitals, but it provides no information or evidence to support
this claim. Even if it had provided more information or evidence, these factors would not be deter-
minative of the question whether the hospital has complied with its obligations under section 12(1).

I have reviewed all of the policies, procedures and practices provided by the Hospital. In the discussion
that follows, I comment on a number of those policies, practices and procedures. I also find that the
Hospital has not developed some policies, practices and procedures which it should have developed
in order to meet its obligations under section 12(1) of the Act.


Privacy Audits Policy
The Privacy Audits policy was revised in May 2009. Its stated purpose is to ensure that the Hospital‚Äôs
‚Äúclinical information computer systems‚Äù have ‚Äúregular audits and any findings of non-compliance
with privacy policies [of the Hospital] and/or legislation are investigated by the Manager or Chief of
Program.‚Äù The Privacy Audits policy also provides that the Privacy Coordinator is responsible for con-
ducting ‚Äúaudits on a random basis to review access by staff, physicians or volunteers, with the same
last name as a patient, next of kin, person to notify, guarantor or RVHS employer, in the Master Patient




32                                                                                           HO-013
Index.‚Äù The policy also sets out the steps that will be taken by the Privacy Coordinator following an
audit if it is determined that further investigation is required.

In the Notice of Review sent to the Hospital, I asked it to provide information concerning the audits
it conducted, the frequency with which and the circumstances in which audits are conducted, the
process that is followed in conducting the audits, the number and nature of the records of personal
health information audited and how the findings of any such audits are addressed.

In response, the Hospital stated:

       In the past, the Hospital audited user activity on request and following a suspected
       incident. In October 2013, the Hospital intensified its auditing program and in June
       2014, introduced weekly random audits. [Emphasis added.]

       Since user-based auditing is not highly effective in identifying inappropriate access, the
       Hospital created data mining programs to look for unusual access patterns. These pro-
       grams identify suspicious access which the Hospital investigates with applicable users.

The Privacy Audits policy requires the conduct of ‚Äúrandom audits.‚Äù Random audits are restricted to
a review of access ‚Äúby staff, physicians or volunteers, with the same last name as a patient, next of
kin, person to notify, guarantor or RVHS employer in the Master Patient Index.‚Äù These types of audits
are important but by themselves insufficient. The Hospital must conduct random audits on all users‚Äô
activities and it must also conduct random audits of the records of personal health information of high
profile individuals.

The Hospital must also implement measures to ensure it is able to conduct random audits on all
activities in its electronic information systems and these measures must be reflected in the Privacy
Audits policy. The Hospital‚Äôs failure to ensure that it has the ability to conduct random audits on all
uses of the scheduling module and its failure to conduct random audits are a contravention of sec-
tion 12(1) of the Act.

I am equally concerned that the Hospital was not following its own Privacy Audits policy because it
was only conducting audits in response to requests and following ‚Äúa suspected incident‚Äù despite the
requirement to conduct random audits. In my view, reactive auditing is inadequate and does not
meet the Hospital‚Äôs obligations pursuant to section 12(1) of the Act and is contrary to the Hospital‚Äôs
own policy.

My other concern with the Privacy Audits policy is that, although it sets out the steps the Hospital
must take if a privacy breach is suspected, it does not set out what actions must be taken if a breach
is identified. I will discuss this further in the context of my discussion about the Hospital‚Äôs obligations
under section 10 of the Act, which appears later in this Order.

Having found that the Hospital contravened section 12(1), and in view of the Hospital‚Äôs lack of aware-
ness about the limitations in the auditing functionality of the Meditech system, in the Order provisions
below, I will require the Hospital to review and revise its Privacy Audits policy to require that measures
be implemented to ensure that the activities of all agents on all of its electronic information systems




       HO-013                                                                                           33
can be audited. The policy must also require that audits be conducted on request, following the report
of an actual or suspected privacy breach and on a random basis. In addition, in relation to high profile
patients, audits must be conducted frequently.


Privacy Training Program Policy
At the time of the two reported breaches, the Hospital‚Äôs practice was to conduct privacy training
during the orientation of new employees. It also conducted training in 2004 when the Act was pro-
claimed in force. With respect to the training program, the Hospital provided the IPC with a copy
of its general orientation program, its ‚Äú2004 PHIPA rollout‚Äù document and a copy of a PowerPoint
presentation on the Act.

In its representations, the Hospital acknowledged that it needs to implement more frequent privacy
training and that it was investigating options for delivering supplemental ‚Äúrefresher‚Äù training when it
learned that Employee 2 was using and disclosing personal health information in contravention of the
Act. It also stated that:

       In July of 2014, as an interim step, the Hospital conducted two privacy education
       sessions, one in a leadership forum attended by managers and one in a town hall for
       staff. The Hospital intends to implement on-line privacy modules for employees to
       complete on an annual basis, or more frequently if recommended by their manager.
       The Hospital is consulting with the Ontario Hospital Association about available on-
       line privacy training programs to expedite the implementation of this type of privacy
       training for all staff members.

I note that in September 2014, approximately one year after the first breach was reported to the IPC,
the Hospital stated that it was consulting with the Ontario Hospital Association (OHA) regarding the
availability of on-line training programs. I am aware that the OHA has an on-line training resource
that deals specifically with the issue of unauthorized access to personal health information by agents.
Steps should have been taken by the Hospital much earlier to ensure that all agents of the Hospital
were provided with this type of training.

In addition, the Hospital does not record whether or not agents attend privacy training and therefore
was unable to confirm that Employee 1 and Employee 2 had received any privacy training when they
were initially hired by the Hospital. The Hospital must take steps to ensure that attendance at privacy
training is documented.

A comprehensive privacy training program is an essential tool to combat the risk of uses and disclo-
sures of personal health information by agents in contravention of the Act, including agents who are
‚Äúcurious‚Äù or who are motivated by their own interests, such as financial gain. The training program
must be detailed in a Privacy Training Program policy. The Privacy Training Program policy must require
agents to complete privacy training upon the commencement of their employment, contractual or other
relationship with the Hospital and before they are given access to personal health information in the
custody or control of the Hospital. The Privacy Training Program policy must further require that agents




34                                                                                        HO-013
complete privacy training annually. It must also clearly define who is responsible for developing the
privacy training materials and for providing the training. Further, it must require that the attendance
of agents at the initial and annual privacy training be documented and must identify the person(s)
responsible for documenting attendance, for identifying agents who do not attend such training and
for ensuring that such training is completed. It must also require that the privacy training materials be
reviewed and updated on a regular basis to address:

   ‚Ä¢   Any orders, guidelines, fact sheets and best practices issued by the IPC under the Act;

   ‚Ä¢   Evolving industry standards and best practices;

   ‚Ä¢   The implementation of new technologies, programs or services;

   ‚Ä¢   Amendments to the Act;

   ‚Ä¢   New or amendments to privacy policies, procedures or practices implemented by the Hospital; and

   ‚Ä¢   Recommendations arising from privacy and security audits, privacy impact assessments and
       investigations into privacy complaints, privacy breaches and information security breaches.

In the Order provisions below, I will require the Hospital to develop and implement a Privacy Training
Program policy in accordance with the comments above.


Privacy Training Materials
The Hospital‚Äôs training materials contain information about the Act and best practices. However, this
training material lacks detail in some areas and essential information. The training materials must be
amended to include detailed information in relation to the following:

   ‚Ä¢   the purposes for which agents of the Hospital are permitted to collect, use and disclose personal
       health information and any limitations imposed by the Hospital;

   ‚Ä¢   the privacy policies, procedures and practices implemented by the Hospital and the obligations
       imposed on agents by these policies, procedures and practices;

   ‚Ä¢   the obligations of agents under the Act, including the duty to notify the Hospital at the first
       reasonable opportunity if personal health information is stolen, lost or accessed by unauthor-
       ized persons and the procedure for doing so;

   ‚Ä¢   the potential consequences that may be imposed on agents who collect, use or disclose per-
       sonal health information in contravention of the Act and/or the privacy policies, procedures
       and practices implemented by the Hospital;

   ‚Ä¢   the potential consequences for the Hospital arising from agents who collect, use or disclose
       personal health information in contravention of the Act;




       HO-013                                                                                         35
     ‚Ä¢   the circumstances surrounding the contraventions of the Act by Employee 1 and Employee 2,
         including the findings made in this Order regarding these contraventions, and the consequences
         of these contraventions for the Hospital and the employees involved.

Comprehensive and frequent privacy training is essential to the development and maintenance of a
culture of privacy within any organization. It is even more essential in an organization with custody
or control of sensitive personal health information that is made widely available through electronic
information systems. In the Order provisions that follow, I will require that the Hospital review and
revise its privacy training materials in accordance with the comments above.


Privacy Awareness Programs and Policies
The development of a culture of privacy within any organization is also dependent on the level of
awareness beyond training. When asked to describe any steps that the Hospital has taken to foster a
culture of privacy and raise awareness among agents of their duties under the Act and of their duties
under the privacy policies, procedures and practices implemented by the Hospital, the Hospital
responded as follows:

         Privacy awareness is fostered through means that include ‚Äúe-Echo mail blasts‚Äù which
         are a form of internal electronic bulletin. In the past, there would typically be at least
         one article per year on a privacy-related matter. An alert reminding users of the restric-
         tions on access pops up when a user logs on to Meditech.

         The Hospital is considering additional means it can use to reinforce its culture of privacy
         including the use of posters and annual attestations by employees that they are not
         engaged in activities outside of the Hospital that place them in a conflict of interest with
         their obligations to the Hospital. The Hospital is cognizant of its obligations under labour
         and employment laws and is working with its HR department in this regard: it has found
         enforcement more of a challenge in its unionized environment than privacy awareness.

I am satisfied that the Hospital is considering additional communications tools to assist it in complying
with section 12(1). However, more than one year has passed since the first reported breach which is
more than sufficient time to develop and implement measures such as these. As such, the Hospital
must review its current communications practices to ensure that they are promoting and fostering a
strong culture of privacy and it must develop a Privacy Awareness Program policy. The policy must
require the development of a communications program to frequently remind agents of the privacy
and security policies, procedures and practices implemented by the Hospital and of the obligations
imposed on agents by these policies, procedures and practices, as well as their obligations under the
Act. It must also identify the individual responsible for implementing the Privacy Awareness Program
and set out the frequency, method and nature of the privacy awareness communications to be deliv-
ered to all agents.

In the Order provisions below, I will require the Hospital to develop and implement a Privacy Awareness
Program policy in accordance with the comments above.




36                                                                                             HO-013
Pledge of Confidentiality policy
The Hospital submits that its ‚Äúnew employees and others, including temporary employees and vol-
unteers‚Äù are required to sign a ‚ÄúPledge of Confidentiality‚Äù when they are hired as well as ‚Äúat other
points in the employment relationship.‚Äù This requirement is set out in a Pledge of Confidentiality policy.

The Hospital did not explain what it meant by the words ‚Äúat other points in the employment relation-
ship.‚Äù However, the Hospital confirmed that both Employees signed a ‚ÄúPledge of Confidentiality‚Äù
upon hiring. Employee 2 was also required to re-execute the ‚ÄúPledge of Confidentiality‚Äù following a
gap in her employment.

By signing this pledge, these two Employees acknowledged that they understood that they were
prohibited from accessing patient information ‚Äúwithout authorization to do so and without a ‚Äòneed-
to-know‚Äô basis for direct patient care or the performance of one‚Äôs duties.‚Äù

The Hospital must clarify that the Hospital‚Äôs Pledge of Confidentiality policy and the ‚ÄúPledge of
Confidentiality‚Äù apply to all agents, not just employees of the Hospital. It must also:

   ‚Ä¢   require agents to comply with the Act and its regulations;

   ‚Ä¢   require agents to securely return all property of the Hospital including keys and records of
       personal health information, if any, at the conclusion of their employment or contractual or
       other relationship; and

   ‚Ä¢   require agents to notify the Hospital at the first reasonable opportunity in accordance with
       the Hospital‚Äôs Privacy Breach Management policy, if they believe that there may have been a
       breach of the ‚ÄúPledge of Confidentiality‚Äù or if the agent breaches or believes there may have
       been a breach of privacy policies, procedures and practices implemented by the Hospital, or
       a breach of the Act.

The Pledge of Confidentiality policy states that breaches of the ‚ÄúPledge of Confidentiality‚Äù will result in
discipline up to and including termination of employment or hospital privileges and/or hospital affili-
ation as applicable. This must be amended to clarify that a breach may also result in the termination
of a contractual relationship and a report to the agents‚Äô health regulatory college, where applicable.

Unfortunately, the policy also states ‚Äú[r]andom audits may be carried out to ensure compliance with this
policy.‚Äù The Pledge of Confidentiality policy must be amended to read that the Hospital will conduct
random audits in order to ensure that agents are deterred from using or disclosing personal health
information in the Hospital‚Äôs electronic information systems in contravention of the Act.

In addition, the Hospital‚Äôs current practice of having employees sign the pledge upon hiring is not
sufficient. All agents of the Hospital must be reminded annually of their obligations under the Act
and under the privacy policies, procedures and practices implemented by the Hospital. A ‚ÄúPledge of
Confidentiality‚Äù is one of the many administrative measures open to the Hospital to do this. To be
effective, a ‚ÄúPledge of Confidentiality‚Äù must be signed by all agents on an annual basis. The Hospital‚Äôs
failure to adopt this practice is a shortcoming in its current administrative measures and safeguards.




       HO-013                                                                                           37
The Hospital must ensure that all agents of the Hospital sign the ‚ÄúPledge of Confidentiality‚Äù and that
it be signed at the commencement of an agent‚Äôs employment, contractual or other relationship with
the Hospital and then on an annual basis.

In the Order provisions that follow, I will require the Hospital to review and revise its Pledge of
Confidentiality policy and ‚ÄúPledge of Confidentiality‚Äù in accordance with the comments and findings
made above.


Privacy Advisory
The Hospital states that when an agent logs on to the Meditech system, an alert appears on the login
screen reminding him or her of the restrictions on access. The alert states:

                                         PRIVACY ADVISORY

        This system contains personal information about our patients and staff. Access to this
        information is permitted for patient care purposes and/or for the performance of your
        work duties. Access to information in this system is audited regularly. Inappropriate
        access may result in suspension or termination of your access privileges and disciplinary
        action up to and including termination of employment or affiliation.

                Privacy and Confidentiality Policies must be reviewed and understood
                                    before entering this system.

Notices alerting agents of the consequences of using or disclosing personal health information in
contravention of the Act, such as this Privacy Advisory, can be effective tools for protecting privacy.22
However, this Privacy Advisory lacks essential features.

The Privacy Advisory must be amended to clarify that the use or disclosure of personal health infor-
mation in the system is permitted ‚Äúonly‚Äù for the purposes of providing health care to the patient and/
or in the performance of the agent‚Äôs duties. The Privacy Advisory must also appear on its own screen
and it must require the agent to acknowledge that he or she has read, understood and agrees to
comply with these terms and with the privacy policies, procedures and practices of the Hospital prior
to permitting the agent or user to access the Hospital‚Äôs electronic information systems. In the Order
provisions below, I will require the Hospital to review and revise the Privacy Advisory in accordance
with the comments and findings above.




22 Order HO-010, supra note 2.




38                                                                                         HO-013
Issue F:        Did the Hospital have in place information practices that comply
                with the Act and did it comply with its information practices in
                accordance with section 10(1) and 10(2) of the Act?
Section 10(1) of the Act states:

       A health information custodian that has custody or control of personal health informa-
       tion shall have in place information practices that comply with the requirements of this
       Act and its regulations.

Section 10(2) of the Act states:

       A health information custodian shall comply with its information practices.

Section 2 of the Act defines ‚Äúinformation practices‚Äù as follows:

       ‚Äúinformation practices‚Äù, in relation to a health information custodian, means the policy
       of the custodian for actions in relation to personal health information, including,

           (a) when, how and the purposes for which the custodian routinely collects,
               uses, modifies, discloses, retains or disposes of personal health informa-
               tion, and

           (b) the administrative, technical, and physical safeguards and practices that the
               custodian maintains with respect to the information.

Health information custodians must review their information practices on an ongoing basis to ensure
that they are current and take into account: evolving industry standards and best practices; new
technologies, programs or services; any orders, guidelines, fact sheets and best practices issued by
the IPC under the Act; amendments to the Act; and recommendations arising from privacy audits,
privacy impact assessments and investigations into privacy complaints and privacy breaches. It is also
important for health information custodians to review their information practices on an ongoing basis
to ensure their information practices, as set out in privacy policies and procedures, continue to be
consistent with their actual practices.

In Order HO-004, the IPC stated:

       Health information custodians should review their information practices regularly to
       ensure that they remain appropriate for their operations. As the health information
       custodian‚Äôs operations evolve and grow, and as a result of the introduction of new
       information technology, it is important to update information practices to reflect these
       changes. A health information custodian should take steps to ensure that the contents
       of its policies and procedures are kept current to reflect actual practices. In addition,
       a health information custodian should keep abreast of developments relating to safe-
       guards to ensure that they comply with the Act.




       HO-013                                                                                      39
        In addition, when adopting policies and procedures, a health information custodian
        needs to ensure that staff members and independent contractors are made aware of
        new policies and procedures by proper notice, either through the use of internal mail
        system, electronic mail and/or educational sessions.

Privacy policies and procedures on their own, however, are not sufficient. Health information cus-
todians must also take steps to ensure that agents are aware of and understand their obligations and
limitations under the Act and under the privacy policies, practices and procedures that custodians
have implemented and that agents are aware of and understand the consequences of failing to comply
with these obligations and limitations.

In its written representations, the Hospital takes the position that its information practices complied
with sections 10(1) and (2) of the Act and restates its position that the issue in this review is that the
Employees did not comply with the Hospital‚Äôs information practices.

I reviewed the relevant Hospital‚Äôs policies, procedures and practices in the preceding discussion and
found that they did not meet the Hospital‚Äôs obligations under section 12(1) of the Act. For the same
reasons, I find that these same policies, practices and procedures do not comply with the require-
ments of section 10(1) of the Act.

In addition, as I mentioned above, the Hospital has not developed a Privacy Breach Management policy
and in the Order provisions below I require the Hospital to do so. A Privacy Breach Management policy
is necessary to ensure the proper identification, reporting, containment, notification, investigation
and remediation of privacy breaches, including contraventions of the Act, and that agents understand
their duties and responsibilities in this regard. The policy must include a requirement that a review be
conducted following a breach to ensure that steps are taken to prevent further unauthorized use or
disclosure of personal health information, by identifying any risks and taking steps to mitigate those risks.

If a policy, practice or procedure had been in place requiring a complete review of the Hospital‚Äôs
Meditech system as a result of the first reported breach, the Hospital may have identified the limita-
tions or gaps in its auditing program and in its Privacy Audits policy, and taken steps to address these
limitations or gaps before the second breach. If it had done so, the Hospital may have been in a posi-
tion to prevent some of the unauthorized uses and disclosures by Employee 2 and would have been
able to identify patients whose personal health information was used and disclosed by Employee 2
in contravention of the Act in the period of time between the first and the second reported breach.

As a result, the Hospital must develop a Privacy Breach Management policy that:

     a) Imposes an obligation on agents to notify the Hospital if personal health information is stolen,
        lost or accessed by unauthorized persons and identifies who at the Hospital must be notified
        and the time frame for notification;

     b) Mandates that agents report a breach to senior management, and sets out who is responsible
        for such reporting, the time frame within which this reporting must be completed and to whom
        it must be reported;




40                                                                                             HO-013
   c) Sets out the circumstances in which a privacy breach should be reported to others including
      police, health regulatory colleges and the IPC;

   d) Requires immediate measures be taken to contain the breach to ensure that steps are taken
      that are reasonable in the circumstances to protect personal health information from further
      theft, loss or unauthorized use or disclosure and to protect records of personal health informa-
      tion from further unauthorized copying, modification or disposal;

   e) Requires notification of the affected individual(s) pursuant to the Act, and sets out who is
      responsible for providing notification and the information to be provided;

   f)   Requires that an investigation of the breach be conducted including a review of all relevant
        information systems and policies, practices and procedures;

   g) Sets out who is responsible for investigating, the nature and scope of the investigation and the
      process to be followed in the investigation; and

   h) Sets out the process by which the findings of the investigation, including any recommendations
      are communicated and implemented and the person responsible for implementation.

As noted above, in relation to its Privacy Audits policy, the Hospital stated:

        In the past, the Hospital audited user activity on request and following a suspected
        incident. In October 2013, the Hospital intensified its auditing program and in June
        2014, introduced weekly random audits. [Emphasis added.]

Given that the Privacy Audits policy requires the conduct of ‚Äúrandom audits,‚Äù the Hospital‚Äôs practice
of conducting audits on request and following a breach amounts to a failure to comply with its own
policies contrary to section 10(2) of the Act. The Hospital must take steps to ensure that it complies
with its Privacy Audits policy.

In the Order provisions below, I will require that the Hospital develop a Privacy Breach Management
policy in accordance with the comments and findings above and that it take steps to ensure that it
complies with the Privacy Audits policy.




        HO-013                                                                                     41
SUMMARY OF FINDINGS
I have made the following findings in this review:

     1. The information at issue in this review is ‚Äúpersonal health information‚Äù as defined in section
        4 of the Act.

     2. The Hospital is a ‚Äúperson‚Äù who operates a hospital within the meaning of the Public Hospitals
        Act and it is a health information custodian with custody or control of the personal health
        information at issue as defined in section 3(1)4i of the Act.

     3. Employee 1 and Employee 2 were agents of the Hospital as defined in section 2 of the Act.

     4. The personal health information at issue in this review was used and/or disclosed in contraven-
        tion of the Act.

     5. The Hospital did not take steps that are reasonable in the circumstances to ensure that personal
        health information in its custody or control is protected against unauthorized use or disclosure
        in contravention of section 12(1) of the Act.

     6. The Hospital did not have information practices that comply with the Act and did not comply
        with its information practices, in contravention of sections 10(1) and 10(2) of the Act.



ORDER
I order the Hospital to:

1. In relation to all of the Hospital‚Äôs electronic information systems, implement the measures neces-
   sary to ensure that the Hospital is able to audit all instances where agents access personal health
   information on its electronic information systems, including the selection of patient names on the
   patient index of its Meditech system.

2. In relation to the Hospital‚Äôs Meditech system:

     a) Work with the Hospital‚Äôs Hosting Provider to review and amend the service level agreement
        between the Hospital and the Hosting Provider to clarify the responsibility for the creation,
        maintenance and archiving of user activity logs generated by the Hospital‚Äôs use of its Meditech
        system, and ensure that the user activity logs are available to the Hospital for audit purposes.

     b) Work with Meditech or another software provider to develop a solution that will limit the
        search capabilities and search functionalities of the Hospital‚Äôs Meditech system so that agents
        are unable to perform open-ended searches for personal health information about individuals,
        including newborns and/or their mothers, and can only perform searches based on the follow-




42                                                                                        HO-013
       ing criteria: health number, medical record number, encounter number, or exact first name,
       last name and date of birth.

3. Review and revise its Privacy Audits policy, the Pledge of Confidentiality policy and the ‚ÄúPledge of
   Confidentiality,‚Äù and the Privacy Advisory in accordance with the comments and findings made in
   this Order, and take steps to ensure that it complies with the Privacy Audits policy.

4. Develop and implement a Privacy Training Program policy, a Privacy Awareness Program policy, and
   a Privacy Breach Management policy in accordance with the comments and findings made in this
   Order.

5. Immediately review and revise its privacy training tools and materials in accordance with the
   comments and findings made in this Order.

6. Using the privacy training materials developed in accordance with Order provision 5:

   a) immediately conduct privacy training for all agents in clerical positions in the Hospital; and

   b) conduct privacy training for all other agents by June 16, 2015.

7. Provide this office with proof of compliance with all of the Order provisions by September 16, 2015.




                                                                     December 16, 2014
Brian Beamish								Date
Commissioner (Acting)




      HO-013                                                                                        43
POSTSCRIPT
The Personal Health Information Protection Act, 2004 (the Act) was enacted 10 years ago, on November
1, 2004, to establish rules governing the collection, use and disclosure of personal health information
within the health sector. Over the last decade, this office has seen a growing number of privacy breaches
involving unauthorized use, often described as unauthorized access, to personal health information
by employees, staff and other agents of health information custodians. Indeed, while this review was
underway, three additional cases of unauthorized access were reported in the media.

Efforts to combat this issue require action by multiple stakeholders. Health regulatory colleges have
a role to play where regulated health professionals breach standards of professional conduct. Health
information custodians, such as hospitals, must also ensure that staff are fully aware of their duties and
obligations to protect the privacy of patients and the confidentiality of their personal health information.
As this Order makes clear, custodians have an important role in auditing access to electronic health
records and taking appropriate disciplinary and other actions when unauthorized access is detected.
Full disclosure of the actions taken in response to a breach, including the disciplinary actions taken
against staff, may assist in deterring other similar conduct and will demonstrate a commitment to
transparency and accountability.

Part of this office‚Äôs role is to investigate and review instances of unauthorized access to ensure that
health information custodians are meeting their responsibilities by implementing proper privacy poli-
cies, practices and procedures; conducting staff training; monitoring access to health records; and
implementing technical, physical and administrative safeguards to protect the privacy of patients.

There are measures in the Act designed to promote greater accountability that are not being used to
the extent that they should. Section 72(1)(a) of the Act states that a person is guilty of an offence if
the person ‚Äúwilfully collects, uses or discloses personal health information in contravention of this Act
or its regulations.‚Äù Individuals found guilty of an offence under this section are liable, on conviction,
to a fine of up to $50,000. No person other than the Attorney General or his agent may commence
a prosecution for an offence under the Act.

The fact that charges might be laid should be a significant deterrent to agents, but the prospect of
charges will only have that effect if health information custodians and their agents know that this
provision is likely to be used in appropriate cases.

Since the Act was passed 10 years ago, charges under section 72(1)(a) of the Act have been laid in
only one case, and that case is still pending before the courts. More needs to be done to address
what appears to be a growing problem. To that end, we have initiated discussions with the Ministry
of Health and Long-Term Care and the Ministry of the Attorney General with a view to developing a
protocol for the Attorney General to commence prosecutions in appropriate cases.

The Legislature clearly contemplated that there would be serious consequences for failure to comply
with the Act ‚Äî it is time to make use of all the tools available to send a strong message to health
information custodians and their agents that breaches of this kind will not be tolerated.




44                                                                                            HO-013
 Office of the Information and Privacy
       Commissioner of Ontario
            2 Bloor Street East
               Suite 1400
             Toronto, Ontario
                 CANADA
      M4W 1A8 416-326-3333
          1-800-387-0073
         Fax: 416-325-9195
TTY (Teletypewriter): 416-325-7539
        Web site: www.ipc.on.ca
                                              Ann Cavoukian, Ph.D.
                                       Information & Privacy Commissioner
                                                 Ontario, Canada




Number 8
July 2005


                                   Lock-box Fact Sheet
            What does the term ‚Äòlock-box‚Äô                     These provisions have come to be referred
            mean?                                             to as the ‚Äúlock-box‚Äù provisions, although
                                                              lock-box is not a defined term in PHIPA.
            The Personal Health Information Protection
            Act (PHIPA) is a consent-based statute,
            meaning that it gives Ontarians control over      What information can individuals
            the collection, use and disclosure of their       ‚Äòlock,‚Äô and from whom can they
            personal health information by stipulating        lock it?
            that health information custodians can only       The withholding or withdrawal of consent or
            collect, use and disclose an individual‚Äôs         the express instructions cited above may take
            personal health information with the express      various forms, including communications
            or implied consent of that individual, subject    from individuals to health information
            to limited exceptions.                            custodians:
            Integral to the concept of consent is the
                                                               ‚Ä¢ not to collect, use or disclose a particular
            notion that individuals have the ability to
                                                                 item of information contained in their
            withhold or withdraw their consent to the
                                                                 record of personal health information
            collection, use or disclosure of their personal
                                                                 (for example, a particular diagnosis);
            health information for a particular purpose,
            including for the provision of health care.        ‚Ä¢ not to collect, use or disclose the contents
            Section 20(2) of PHIPA makes it clear that           of their entire record of personal health
            individuals may withhold or withdraw their           information;
            consent to the collection, use or disclosure of
            their personal health information by health        ‚Ä¢ not to disclose their personal health
            information custodians for the purposes of           information to a particular health
            providing or assisting in providing health           information custodian, a particular agent
            care.                                                of a health information custodian or a
                                                                 class of health information custodians or
            Further, under PHIPA, individuals may
                                                                 agents (e.g. physicians, nurses or social
            provide express instructions to health
                                                                 workers); or
            information custodians not to use or
            disclose their personal health information         ‚Ä¢ not to enable a particular health
            for health care purposes without consent in          information custodian, a particular agent
            the circumstances set out in sections 37(1)          of a health information custodian or a
            (a), 38(1)(a) and 50(1)(e) of PHIPA.                 class of health information custodians
INFORMATION
AND PRIVACY
COMMISSIONER OF
ONTARIO


     or agents (e.g. physicians, nurses or social      the individual‚Äôs consent or without the
     workers) to use their personal health             individual‚Äôs consent pursuant to section
     information.                                      36(1)(b) because the collection was
                                                       reasonably necessary for the provision of
Although it is up to the individual to whom the
                                                       health care (section 37(1)(a) of PHIPA).
information relates to decide what personal
health information to lock, if any, and to whom     However, this one-year delay in the application
the lock should apply, a health information         of the lock-box for public hospitals does not
custodian may discuss with the individual how       apply to any withholding or withdrawal of
locking personal health information might           consent to the collection, use or disclosure of
affect the individual‚Äôs health care and why a       personal health information for purposes of
health information custodian may need more          providing or assisting in providing health care
personal health information to provide the best     based on implied consent.
possible care.


When do the lock-box provisions take                 What ‚Äúlock-box‚Äù obligations are
effect?                                             imposed on health information
                                                    custodians?
In the majority of cases, the lock-box provisions
have been in effect since November 1, 2004,         Health information custodians are required
when PHIPA came into force.                         to respect the decisions of individuals to
                                                    withhold or withdraw their consent to the
Public hospitals were granted a one-year            collection, use or disclosure of their personal
exemption from having to comply with                health information for purposes of providing or
certain lock-box requirements of PHIPA until        assisting in providing health care and to respect
November 1, 2005. The exemption applies             express instructions not to use or disclose their
only with respect to express instructions related   personal health information for health care
to:                                                 purposes without consent in the circumstances
    ‚Ä¢ disclosures of personal health information    set out in sections 37(1)(a), 38(1)(a) and 50(1)
      to certain health information custodians      (e) of PHIPA (subject, in the case of express
      without consent that are reasonably           instructions, to a one-year extension for public
      necessary for the provision of health care    hospitals).
      (section 38(1)(a) of PHIPA);                  To ensure that no unauthorized collection, use
                                                    or disclosure occurs, it is important for health
    ‚Ä¢ disclosures of personal health information    information custodians to record any such
      to persons outside of Ontario without         express instructions or limitations on consent
      consent that are reasonably necessary for     to the collection, use or disclosure of personal
      the provision of health care (section 50(1)   health information for health care purposes.
      (e) of PHIPA); and
                                                    Compliance with the lock-box provisions of
    ‚Ä¢ uses of personal health information           PHIPA may be achieved by health information
      where the information was collected with      custodians through:

2
                                                                                         INFORMATION
                                                                                          AND PRIVACY
                                                                                     COMMISSIONER OF
                                                                                             ONTARIO


 ‚Ä¢ policies, procedures or manual processes;       What notice obligations do health
 ‚Ä¢ electronic or technological means; or
                                                   information custodians have when an
                                                   individual locks his or her personal
 ‚Ä¢ a combination of policies, procedures           health information?
   or manual processes and technological
                                                   PHIPA (section 20(3)) provides that, where
   means;
                                                   personal health information has been locked
depending on the avenue chosen by the health       and a health information custodian is prevented
information custodian.                             from disclosing to certain health information
                                                   custodians personal health information that the
Once an individual locks personal health           first custodian considers reasonably necessary
information by withholding or withdrawing          for the provision of health care, the disclosing
consent to its collection, use or disclosure for   health information custodian must notify the
health care purposes, or providing an express      other health information custodian(s) of that
instruction to that effect, a health information   fact.
custodian who is subject to the withdrawal or
withholding of consent or express instruction      The receiving health information custodian
cannot collect, use or disclose, as the case may   would then be able to explore the matter of
be, that personal health information for health    the ‚Äúlocked‚Äù personal health information with
care purposes unless:                              the individual and seek his or her consent to
                                                   access the locked information. (The disclosing
 ‚Ä¢ the individual changes his or her mind and      health information custodian would need to
   informs the health information custodian        obtain the express consent of that individual
   accordingly; or                                 to disclose the locked information.)
 ‚Ä¢ the collection, use or disclosure can be made
   without consent (except as set out in section
   37(1)(a), 38(1)(a) and 50(1)(e) of PHIPA),
   such as where the health information
   custodian believes on reasonable and
   probable grounds that the disclosure is
   necessary for the purpose of eliminating or
   reducing a significant risk of serious bodily
   harm to a person or group of persons, subject
   to any applicable constitutional restrictions
   (section 40(1) of PHIPA).




                                                                                                   3
Fact Sheet                                              Communications Department
                                                        Information and Privacy Commissioner of Ontario
                                                        2 Bloor Street East, Suite 1400                   30% recycled
                                                        Toronto, Ontario CANADA                              paper
is published by the Office of the Information and
                                                        M4W 1A8
Privacy Commissioner of Ontario.                        Telephone: 416-326-3333 ‚Ä¢ 1-800-387-0073
                                                        Facsimile: 416-325-9195
If you have any comments regarding this news¬≠¬≠letter,   TTY (Teletypewriter): 416-325-7539
wish to advise of a change of address, or be added      Website: www.ipc.on.ca
to the mailing list, contact:                           Cette publication, intitul√©e ¬´ Feuille-info ¬ª,
                                                        est √©galement disponible en fran√ßais.
                                               Ann Cavoukian, Ph.D.
                                  Information and Privacy Commissioner of Ontario



Number 15
December 2008




                     Obtaining Personal Health Information
                          About a Deceased Relative
                There may be circumstances in which a          information a person is able to obtain
                person may wish to obtain the personal         about a deceased relative.
                health information of a deceased relative.
                For example, a person may wish to obtain       This Fact Sheet provides answers to some
                the personal health information of a           common questions that are important in
                parent who has resided in a long-term          determining whether a person has the
                care facility or who has been treated at a     right under PHIPA to obtain personal
                hospital emergency department, a sibling       health information about a deceased
                who has been estranged for some time           relative from a health information
                or, tragically, a son or daughter who has      custodian. It also provides some
                died suddenly in an accident. There may        information about whether a person has
                be many reasons why obtaining such             the right to obtain medical information
                information is important to the surviving      about a deceased relative held by a
                relative. For example, it may assist him       government institution covered by the
                or her in administering the deceased           Freedom of Information and Protection
                relative‚Äôs estate, coping with the grieving    of Privacy Act (FIPPA) or the Municipal
                process, or making knowledgeable               Freedom of Information and Protection
                decisions about his or her own health          of Privacy Act (MFIPPA) that is not also
                care or that of another relative.              a health information custodian.

                Ontario‚Äôs Personal Health Information
                                                               What is Personal Health
                Protection Act, 2004 (PHIPA) sets out
                                                               Information?
                rules for the collection, use and disclosure
                of personal health information by health       In accordance with section 4 of PHIPA,
                information custodians. Since these rules      ‚Äúpersonal health information‚Äù means
                continue to apply for a period of time         identifying information about an
                after an individual has died, there may be     individual in oral or recorded form, if
                limits to the amount of personal health        the information,
INFORMATION
AND PRIVACY
COMMISSIONER
OF ONTARIO



(a) relates to the physical or mental             How long is personal health
    health of the individual, including           information protected under PHIPA?
    information that consists of the health
    history of the individual‚Äôs family,           PHIPA continues to protect an individual‚Äôs
                                                  personal health information for a period
(b)    relates to the providing of health         of time after death. Specifically, PHIPA
      care to the individual, including the       ceases to apply to an individual‚Äôs personal
      identification of a person as a provider    health information 120 years after a record
      of health care to the individual,           containing the information was created or
(c)   is a plan of service within the meaning     50 years after the death of the individual,
      of the Long-Term Care Act, 1994 for         whichever comes first.
      the individual,

(d) relates to payments or eligibility for        What is a health information
    health care, or eligibility for coverage      custodian?
    for health care, in respect of the            As defined at section 3 of PHIPA, health
    individual,                                   information custodians include health
(e)   relates to the donation by the individual   care practitioners (defined at section 2),
      of any body part or bodily substance        hospitals, psychiatric facilities, pharmacies,
      of the individual or is derived from        laboratories, nursing homes and long-term
      the testing or examination of any such      care facilities, homes for the aged and
      body part or bodily substance,              homes for special care, community care
                                                  access corporations, ambulance services,
(f)   is the individual‚Äôs health number, or       boards of health, the Minister of Health
(g) identifies an individual‚Äôs substitute         and Long-Term Care and the Canadian
    decision-maker.                               Blood Services.

Personal health information also includes
identifying information about an individual       Does the estate trustee have a
that is not personal health information,          right to obtain personal health
but that is included in a record containing       information about a deceased
personal health information.                      individual?

                                                  When a person dies, the estate trustee or the
                                                  person who has assumed responsibility for
                                                  the administration of the deceased‚Äôs estate,
                                                  if the estate does not have an estate trustee,

                                                                                               2
INFORMATION
AND PRIVACY
COMMISSIONER
OF ONTARIO



becomes the substitute decision-maker for                   How does a health information
the deceased individual. In many cases,                     custodian determine whether a
the estate trustee will be a member of the                  person is the substitute decision-
deceased‚Äôs family.                                          maker for the deceased individual?
The term ‚Äúsubstitute decision-maker‚Äù                        As noted above, when an individual dies,
generally refers to a person who is authorized              the estate trustee or the person who has
under PHIPA to consent on behalf of an                      assumed responsibility for the administration
individual to the collection, use or disclosure             of the deceased‚Äôs estate becomes the
of personal health information about the                    substitute decision-maker for the deceased
individual. The substitute decision maker is                individual.
also authorized to make a request, give an
instruction, or take a step on behalf of the                ‚ÄúEstate trustee‚Äù is not a defined term in
deceased individual where PHIPA permits                     PHIPA. It is, however, defined in Rules 74
or requires such a step or other action.                    and 75 of Regulation 194 under the Courts
                                                            of Justice Act as ‚Äúan executor, administrator
Under PHIPA, every individual has the right                 or administrator with the will annexed.‚Äù
to request access to records containing his or
her own personal health information in the                  The term ‚Äúexecutor‚Äù generally means a
custody or control of a health information                  person named by the deceased in his or her
custodian. Where the individual is deceased,                will to carry out the provisions in that will.
the substitute decision-maker may make                      The term ‚Äúadministrator‚Äù generally means a
such a request on behalf of the deceased                    person appointed by the court to administer
individual. Where the health information                    the deceased‚Äôs estate where there is no
custodian grants the access, the substitute                 will. The term ‚Äúadministrator with the will
decision-maker also has the right to request                annexed‚Äù generally means an administrator
that the record be corrected.                               appointed by the court to carry out the
                                                            provisions of a will when the will names no
There are some exceptions to the right                      executor or the executor named refuses, is
to access a record of personal health                       incompetent to act or has died.1
information. Requests may be made using
the form entitled, Request to Access Personal               The ‚Äúperson who has assumed responsibility
Health Information (PHIPA), available at:                   for the administration of the deceased‚Äôs
                                                            estate‚Äù is also not a defined term in PHIPA,
www.ipc.on.ca/images/Resources/up-
                                                            but it may include, for example, a person
phipa_accfrm_e.pdf
1 See definitions of ‚Äúexecutor,‚Äù ‚Äúadministrator‚Äù and ‚Äúadministrator cum testamento annexo‚Äù in Black‚Äôs Law Dictionary,
 8th ed.


                                                                                                                   3
INFORMATION
AND PRIVACY
COMMISSIONER
OF ONTARIO



who has been appointed by the court to          What obligations are imposed on
represent the interests of the deceased‚Äôs       the substitute decision-maker for
estate where there is no estate trustee, or a   the deceased individual?
person who is not so appointed but who,
for valid reasons, has assumed responsibility   A substitute decision-maker is required
for managing the affairs of the deceased‚Äôs      to take certain factors into consideration
estate.                                         when acting on behalf of another individual
                                                regarding a collection, use or disclosure of
To date, the IPC has not issued any orders      personal health information by a health
under PHIPA that interpret the terms ‚Äúestate    information custodian.
trustee‚Äù or ‚Äúperson who has assumed
responsibility for the administration of the    In the case of a deceased individual, the
deceased‚Äôs estate.‚Äù                             substitute decision-maker must consider the
                                                wishes, values and beliefs that the substitute
  Under PHIPA, a health information             decision-maker knows the individual held
custodian is entitled to rely on the accuracy   when alive, and believes the individual
of a person‚Äôs assertion that he or she is the   would have wanted reflected in decisions
substitute decision-maker for the deceased      made concerning his or her personal
individual, unless it is not reasonable to do   health information. Further, the substitute
so in the circumstances.                        decision-maker must consider whether:

In circumstances where it is not reasonable      ‚Ä¢ the benefits from the collection, use or
to rely on a person‚Äôs assertion that he or         disclosure of the information outweigh
she is the authorized substitute decision-         the risk of negative consequences
maker for the deceased individual, the             occurring as a result of the collection,
health information custodian may decide            use or disclosure, as the case may be;
to ask the person asserting that he or she
                                                 ‚Ä¢ the purpose for which the collection,
is the substitute decision-maker to provide
                                                   use or disclosure is sought can be
documentation verifying his or her authority
                                                   accomplished without the collection,
as such or attesting to such authority, and
                                                   use or disclosure; and
request identification to be satisfied as to
the individual‚Äôs identity.                       ‚Ä¢ the collection, use or disclosure is
                                                   necessary to satisfy a legal obligation.




                                                                                            4
INFORMATION
AND PRIVACY
COMMISSIONER
OF ONTARIO



Can a relative, who is not the                   ‚Ä¢ for the purpose of identifying the
substitute decision-maker, obtain                  individual;
the personal health information of a
                                                 ‚Ä¢ for the purpose of informing any person
deceased individual?
                                                   whom it is reasonable to inform in the
Under PHIPA, A health information                  circumstances of,
custodian may only disclose personal               ‚Ä¢ the fact that the individual is deceased
health information to a person who is not            or reasonably suspected to be
a custodian (such as a relative of a deceased        deceased, and
individual), if:
                                                   ‚Ä¢ the circumstances of death, where
 ‚Ä¢ the individual whose personal health              appropriate; or
   information is at issue has given express
   consent;                                      ‚Ä¢ to the spouse, partner, sibling or child
                                                   of the individual if the recipients of
 ‚Ä¢ the disclosure is permitted or required         the information reasonably require the
   by PHIPA; or                                    information to make decisions about
                                                   their own health care or their children‚Äôs
 ‚Ä¢ the disclosure is permitted or required
                                                   health care.
   by another law.
                                                PHIPA also permits the disclosure of personal
In the case of a deceased individual, the
                                                health information by a health information
consent may be given by the deceased
                                                custodian without consent in a number of
individual‚Äôs substitute decision-maker.
                                                other circumstances (e.g., to eliminate or
This means that personal health information     reduce a significant risk of serious bodily
about a deceased individual may be disclosed    harm). All of the permitted disclosures
by a health information custodian to a          are set out in sections 38 through 48 and
relative of the deceased individual, if the     section 50 of PHIPA. While some of these
substitute decision-maker consents to the       provisions may be applicable in disclosing
disclosure.                                     personal health information to relatives of
                                                deceased individuals, it is important to note
In addition, PHIPA permits a health             that these disclosures are discretionary not
information custodian to disclose personal      mandatory.
health information about a deceased
individual without consent in the following     In general, persons who are not custodians
circumstances:                                  and who obtain personal health information
                                                about a deceased individual from a health
                                                information custodian may only use or

                                                                                            5
INFORMATION
AND PRIVACY
COMMISSIONER
OF ONTARIO



disclose the information for the purpose for   Freedom of Information and Protection
which it was disclosed or for the purpose of   of Privacy Act (FIPPA), and the Municipal
carrying out a statutory or legal duty.        Freedom of Information and Protection of
                                               Privacy Act (MFIPPA).
If a health information custodian refuses
to disclose personal health information        In these two Acts, ‚Äúpersonal information‚Äù
to a relative of the deceased individual or    is defined to mean recorded information
refuses to provide access to the substitute    about an identifiable individual, including
decision-maker, is there any recourse?         information relating to the medical,
                                               psychiatric, or psychological history of
If a health information custodian refuses      the individual. The complete definition of
to grant access to a substitute decision-      ‚Äúpersonal information‚Äù is found in section
maker or refuses to disclose personal health   2 of these Acts. Personal information does
information about a deceased individual to     not include information about an individual
a relative, a complaint may be made to the     who has been deceased for more than thirty
IPC. Complaints may be made using the          years.
form entitled, Access/Correction Complaint
Form, available at:                            Under FIPPA and MFIPPA, a deceased
http://www.ipc.on.ca/images/Resources/         individual‚Äôs personal representative can
up-1phipa_accorrcmp_e.pdf                      exercise any right or power conferred on
                                               the deceased individual by the Act, including
                                               making a request for information, if doing
What if a deceased relative‚Äôs                  so relates to the administration of the
personal information, including                individual‚Äôs estate.
medical information, is not in
the custody or control of a health             In addition, FIPPA and MFIPPA contain
information custodian?                         provisions that may permit the disclosure
                                               of personal information about a deceased
If the personal information about a            relative in certain circumstances. Section
deceased individual is held by a provincial    42(1) of FIPPA and section 32 of MFIPPA set
or municipal government institution that       out the circumstances in which an institution
is not a health information custodian, such    may disclose personal information. These
as a provincial government ministry, the       provisions are discretionary, not mandatory.
Ontario Provincial Police, or a local police   For example, an institution may disclose
services board, then this information would    personal information in compassionate
be subject to Ontario‚Äôs two public sector      circumstances, to facilitate contact with
access and privacy laws: the provincial        the spouse, a close relative or a friend of a

                                                                                           6
INFORMATION
AND PRIVACY
COMMISSIONER
OF ONTARIO



deceased individual. Whether a particular        If a government institution refuses to
disclosure of personal information fits          provide a relative or personal representative
within any of the provisions in section          with the personal information of a deceased
42(1) of FIPPA or section 32 of MFIPPA will      individual in response to an access request,
depend on the specific circumstances.            the institution‚Äôs decision may be appealed
                                                 to the IPC using the Appeal Form available
FIPPA and MFIPPA also provide that in the        at:
case of a request by the spouse or a close
                                                 http://www.ipc.on.ca/images/Resources/
relative of a deceased individual for personal
                                                 up-appfrm_e.pdf
information about the deceased individual,
disclosure of the personal information to
those individuals does not constitute an         How are the results of a coroner‚Äôs
unjustified invasion of privacy if the head      investigation obtained and who is
of the institution is satisfied that, in the     entitled to the information?
circumstances, the disclosure is desirable
for compassionate reasons.                       For information about whether and how a
                                                 person may obtain the results of a coroner‚Äôs
A spouse or close relative requesting            investigation, including pathologist‚Äôs and
information about a deceased individual is       toxicologist‚Äôs reports, contact the Regional
required to give the head of the institution     Supervising Coroner‚Äôs Office, or the Office
all information that he or she has regarding     of the Chief Coroner, part of Ontario‚Äôs
whether the deceased individual has a            Ministry of Community Safety and
personal representative and how to contact       Correctional Services. Their office is located
him or her.                                      at 26 Grenville Street, 2nd floor, Toronto,
                                                 Ontario M7A 2G9, and the phone number
Requests for information under FIPPA or          is (416) 314-4000. For more information
MFIPPA may be made using the Request             you can visit their website at:
Form, available at:
www.ipc.on.ca/images/Resources/up-               http://www.mcscs.jus.gov.on.ca/english/
form_2e.pdf                                      office_coroner/about_coroner/coroner_
                                                 what/coroner_what.html
If a government institution refuses to
provide a deceased individual‚Äôs personal
information to a relative or to the personal
representative in response to an access
request, is there any recourse?


                                                                                              7
INFORMATION
AND PRIVACY
COMMISSIONER
OF ONTARIO



How can a death certificate                                              In some circumstances, an application may
and other related information                                            be made for additional information, such
concerning a deceased relative be                                        as a medical certificate of death. For more
obtained?                                                                information, contact the Office of the
                                                                         Registrar General, part of Ontario‚Äôs Ministry
Applications for death certificates for                                  of Government Services, at:
deceased individuals may be made online.
The Office of the Registrar General‚Äôs Online                             Office of the Registrar General
Certificate Application form is available                                P.O. Box 4600
through ServiceOntario at:                                               189 Red River Road, 3rd Floor
https://www.orgforms.gov.on.ca/eForms/                                   Thunder Bay ON P7B 6L8
start.do?lang=en                                                         In Toronto 416-325-8305
                                                                         Toll-free Outside Toronto 1-800-461-2156
                                                                         Fax: (807) 343-7459




               Fact Sheet                                              Communications Department
                                                                       Information and Privacy Commissioner of Ontario
                                                                       2 Bloor Street East, Suite 1400                   30% recycled
                                                                                                                            paper
               is published by the Office of the Information and       Toronto, Ontario M4W 1A8
               Privacy Commissioner of Ontario.                        Telephone: 416-326-3333 ‚Ä¢ 1-800-387-0073
                                                                       Facsimile: 416-325-9195
                                                                       TTY (Teletypewriter): 416-325-7539
               If you have any comments regarding this news¬≠¬≠letter,   Website: www.ipc.on.ca
               wish to advise of a change of address, or be added      Cette publication, intitul√©e ¬´ Feuille-info ¬ª,
               to the mailing list, contact:                           est √©galement disponible en fran√ßais.
                                                                                                                                        8
PRIVACY        IN     THE      RESEARCH         CONTEXT



‚Ä¶THE REQUIREMENTS AND THE CHALLENGES



Debra Grant, Senior Health Privacy Specialist
Manuela Di Re, Health Law Counsel

Information and Privacy Commissioner/Ontario
                   Presentation Outline

 Unique nature of personal health information

 Privacy versus the public interest in the secondary use
  and disclosure of personal health information

 Research provisions in the Personal Health Information
  Protection Act, 2004 (‚Äúthe Act‚Äù)

 Requirements and challenges for health information
  custodians, researchers and research ethics boards
             Unique Characteristics of
            Personal Health Information

 Highly sensitive and personal in nature

 Must be shared immediately and accurately among a range
  of health care providers for the benefit of the individual

 Widely used and disclosed for secondary purposes seen to
  be in the public interest (e.g., research, planning and
  evaluation, surveillance and quality assurance)

 Dual nature of personal health information is reflected in
  health privacy legislation
  The Case for Secondary Uses and Disclosures

 Canada has a publicly-funded health care system
 It makes economical and ethical sense to use information
  collected in the course of providing health care to improve
  the efficiency and effectiveness of the health system
 In the past there has been pressure to use administrative
  database for such purposes
 With the digitization of information, there is increasing
  pressure to use information to create disease specific (e.g.,
  diabetes) or population specific (e.g., children) databases
  and registries.
    Benefits of Secondary Use and Disclosure


 Identifying causes     of   diseases,     risk   factors   and
  populations at risk
 Protecting public safety with respect to infectious diseases
  and environmental hazards
 Assessing needs for health services, monitoring and
  evaluating services and allocating resources
 Educating the public about proactive steps to improve
  their health, minimize risks, etc.
 Developing clinical practice guidelines
                     The Challenge

 In the health context, personal health information is
  generally collected for the purpose of providing or
  facilitating the provision of health care to the individual
 It is usually collected on the basis of the implied or
  assumed implied consent of the individual
 A requirement for express consent could delay or impede
  the delivery of health care
 Pursuant to fair information practices, personal health
  information should only be used and disclosed for the
  identified purpose for which it was collected, unless the
  individual consents to its use or disclosure for other
  purposes
                Some Privacy Concerns

 Personal health information may fall into the wrong hands
 Third parties (e.g., researchers) may not have sufficient
  safeguards to protect personal health information
 Third parties may not be subject to the same professional,
  institutional confidentiality and ethical standards
 Third parties may not be subject to privacy legislation
 Personal health information may be used to create profiles
 Personal health information may be used to discriminate
  and for commercial purposes, marketing and other
  purposes for which the individual might object
 Personal health information may be used for identity theft
        Privacy Fundamentalists‚Äô Position

Believe individuals have a fundamental right to determine
 how their personal health information is being collected,
 used and disclosed (informational self-determination)
Concerned about any use and disclosure of personal health
 information even if the information is de-identified
Privacy is an absolute right that should not be traded off
 against other interests
Most believe that consent should be sought for secondary
 uses and disclosures even if the information is de-
 identified
            Privacy Pragmatists‚Äô Position

 Also concerned about their privacy but they are often
  willing to allow their personal health information to be
  used and disclosed for secondary purposes if they or
  society as a whole will receive some benefit

 Concerns allayed by privacy laws, privacy policies, etc.

 Often still want to be asked for their consent for secondary
  use and disclosure of their personal health information
              Attitudes Towards Research

‚Ä¢ More than eight in ten Canadians (84 per cent) support the
  use of electronic health records in health research provided
  that personal identifiers are not known to the researcher
‚Ä¢ Support for such research drops dramatically if personal
  identifiers are not removed from the record (50 per cent)
‚Ä¢ If consent is obtained ahead of time, there is support for
  linking of personal health information to other records that
  may be related to health outcomes (e.g., education,
  income) but this is considerably more tepid (66 per cent).
             Attitudes Towards Research


 Studies have shown that individuals want to be asked for
  consent even if their personal health information will be
  de-identified

 Studies have also shown that individuals are extremely
  sensitive when a third party is likely to benefit
  commercially from their personal health information
RESEARCH PROVISIONS IN THE ACT
                Overview of the Act


The Personal Health Information Protection Act
 (‚Äúthe Act‚Äù) came into effect on November 1, 2004

The Act governs the collection, use and disclosure
 of ‚Äúpersonal health information‚Äù by ‚Äúhealth
 information custodians‚Äù and their ‚Äúagents.‚Äù

Also governs the use and disclosure of personal
 health information by a recipient who received the
 information from a ‚Äúhealth information custodian‚Äù
   Definition of Health Information Custodian

Health information custodians include:
 Health care practitioners who provide health care or persons who
  operates a group practice of such health care practitioners
 Hospitals, psychiatric facilities, independent health facilities
 Long-term care homes, care homes and homes for special care
 Pharmacies, laboratories, specimen collection centres
 Ambulance services
 Community care access corporations
 A medical officer of health of a board of health
 Ontario Agency for Health Protection and Promotion
 Minister/Ministry of Health and Long-Term Care
 Minister/Ministry of Health Promotion
       Accountability for Privacy in Research

 The Act contains requirements for health information
  custodians, research ethics boards and researchers when
  personal health information is collected, used and
  disclosed for research purposes

 All three parties have a role to play in ensuring safeguards
  are in place to protect personal health information
  collected, used and disclosed for research purposes
               General Obligations for
            Health Information Custodians

 Duty to take steps that are reasonable in the circumstances
  to ensure personal health information is protected against
  theft, loss and unauthorized use or disclosure and to ensure
  records are protected against unauthorized copying,
  modification or disposal (section 12);
 Duty to ensure records of personal health information are
  retained, transferred and disposed of in a secure manner
  (section 13(1));
 These duties apply at all times, even when personal health
  information is collected, used or disclosed for research
                 Legal Authority for
            Health Information Custodians

 Health information custodians are permitted to collect, use
  and disclose personal health information for research with
  the consent of the individual or as permitted by the Act,
  provided that the requirements in the Act are met
 In general, personal health information should only be
  collected, used and disclosed for research with the consent
  of the individual ‚Äì in exceptional circumstances, the
  consent requirement may be waived
 The research provisions in the Act were drafted to ensure
  appropriate safeguards are in place in those exceptional
  circumstances when personal health information needs to
  collected, used or disclosed by health information
  custodians for research without consent of the individual
          Collection, Use and Disclosure
        by Health Information Custodians

Health information custodians may collect
 personal health information from a person who is
 not a custodian or from another health information
 custodian for research and may use and disclose
 personal health information for research purposes,
 without the consent of the individual provided the
 requirements of the Act are satisfied.
        Requirements That Must be Satisfied
         by Health Information Custodians

 If the researcher is not conducting the research on behalf
  of the custodian (i.e., the researcher is a recipient), the
  researcher must submit an application in writing
 Whether the researcher is a recipient or an agent of the
  custodian, there must be a written research plan
 A research ethics board must consider the research plan
  and render a written decision approving the research plan
 The researcher must       agree   to   comply    with   the
  requirements in the Act
 If the researcher is a recipient, there must be an written
  agreement between the researcher and the custodian
      General Obligations for Researchers

Researchers that are not agents of the custodian
 must submit an application to the custodian and
 must enter into an agreement with the custodian

All researchers (both agents and recipients) must
 develop a research plan that fulfills the
 requirements of the Act and submit the research
 plan to a research ethics board for approval.
          General Obligations for Researchers
 All researchers (both agents and recipients) must agree to:
 Comply with any conditions specified by the research ethics board
 Use the personal health information only for the purposes set out in
  the research plan as approved by the research ethics board
 Not publish the information in a form that could reasonably enable a
  person to ascertain the identity of the individual
 Not disclose the information except as required by law
 Not contact or attempt to contact the individual, directly or
  indirectly, unless the health information custodian first obtains the
  individual‚Äôs consent to being contacted;
 Notify the health information custodian immediately in writing if the
  researcher becomes aware of any breach
 Comply with the agreement with the health information custodian
   Consideration by the Research Ethics Board

In deciding whether to approve a research plan, a research
ethics board shall consider relevant matters, including:
 Whether the objectives can be accomplished without
  using personal health information
 Whether adequate safeguards will be in place to protect
  privacy and preserve confidentiality
 The public interest in conducting the research and the
  public interest in protecting privacy
 Whether obtaining consent would be impractical
              What Does This Mean?

 Research ethics boards must:
    consider whether the research could be conducted with
     aggregate or de-identified information
    consider the physical, technological and administrative
     safeguards that will be put in place to protect the
     personal health information
    conduct a harm/benefit analysis ‚Äì harm includes any
     risk to privacy
    consider if it would be practical to obtain consent
                     CIHR Best Practices:
                      Waiver of Consent

 Harm-benefit analysis ‚Äì potential harm to individuals
  must be minimized

 Impracticability     or   inappropriateness   of   consent
  requirement

 Necessity of the personal data

 Consideration of individuals‚Äô expectations

 Legal requirements
          CIHR ‚Äì Harm-Benefit Analysis

 Ideally, the research should pose only a minimal risk
  of harm to individuals and the benefits of the research
  should clearly outweigh any risk of harm

 Probability and magnitude of harm depend on the
  sensitivity of the information and the adequacy of
  safeguards
             CIHR ‚Äì Impracticability or
            Inappropriateness of Consent

 Inappropriate because of:
   Potential harm from direct contact due to conditions or
    circumstances
   Need to link coded data to obtain contact information

 Impracticable where there are difficulties contacting
  individuals due to:
    Size of population
    Relocation or death
    Lack of ongoing relationship with participants
  such that there is risk of introducing bias into the sample or
  termination of the research due to the financial burden.
      Obligation to De-identify Under the Act

 The health information custodian has an obligation to
  minimize the collection, use and disclosure of personal
  health information to the greatest extent possible
 Under the Act, a custodian is not permitted to collect, use
  or disclose personal health information if other
  information will serve the purpose or to collect, use or
  disclose more than is reasonably necessary to meet the
  purpose
 This means that if research could be conducted with
  aggregate or de-identified information, custodians have a
  duty to only use or disclose aggregate or de-identified
  information.
 The Act Permits the Use of Personal Health
     Information for De-identification



 Section 37 permits health information custodians to
  use personal health information for the purpose of
  modifying the information in order to conceal the
  identity of the individual
      De-identified Information Under the Act

 To the extent personal health information is de-identified it
  may fall outside the scope of privacy legislation
 Under the Act, personal health information only includes
  information that ‚Äúidentifies an individual or for which it is
  reasonably foreseeable in the circumstances that it could
  be utilized, either alone or with other information, to
  identify an individual‚Äù
 If information can be used to identify an individual (e.g.,
  re-identification), then it must be treated as personal health
  information
           De-identification Challenges

Even if personal health information is de-
 identified, the risk of re-identification can never be
 eliminated
Most custodians take a risk based approach to
 disclosing personal health information
Ensuring appropriate safeguards will assist in
 removing or reducing barriers to disclosure
There are tools available to facilitate proper de-
 identification
         Exercising Discretion to Disclose

 In exercising their discretion to disclose, custodians
  should consider all relevant factors including:
   The identity of the recipient and whether the recipient is bound
    by professional or legal obligations to maintain privacy and
    confidentiality
   Whether or not the recipient has in place information practices
    that are comparable to those of the custodian
   Whether or not the information practices of the recipient have
    been scrutinized by a reputable, independent third party or
    privacy oversight body, such as the IPC;
   The track record of the recipient with respect to privacy
    complaints and privacy breaches;
   Any commercial benefits that may be accrued by the recipient as
    a consequence of the disclosure
   Whether the public interest in disclosure outweighs privacy risks
RESEARCH AND NEW TECHNOLOGIES
          Why is the Need to Protect
    Personal Health Information So Critical?


The need to protect the privacy of individuals‚Äô personal
health information has never been greater given the:

 Extreme sensitivity of personal health information

 Increased portability of personal health information

 Emphasis on information technology and electronic
  exchanges of personal health information
  What Consequences Arise When Inadequate
 Attention is Paid to the Protection of Privacy?

May result in discrimination, stigmatization and economic
 or psychological harm to individuals
May lose trust or confidence in the public health system
  ‚ÄúA public health agenda that ignores privacy will ultimately fail because the public will
  lose trust and confidence in the very system that is striving to safeguard its health. If people
  fear that actions taken in the name of public health are unjustifiably coercive or that their
  sensitive medical information is being collected and shared for unrelated purposes, then
  they will not fully and honestly participate in and support critical public health activities‚Äù

Individuals may be deterred from seeking information,
 testing or treatment or may withhold or falsify information
Significant time and financial and human resources
 may be expended in dealing with privacy breaches
          Provisions Governing the Security
           of Personal Health Information

 Health information custodians are required to maintain
  the security of personal health information, including to:
   Take reasonable steps to protect personal health information
    from theft, loss and unauthorized use or disclosure and records
    from unauthorized copying, modification or disposal (s. 12(1))
   Ensure records of personal health information are retained,
    transferred and disposed of in a secure manner (s. 13)
   Enter an agreement with the researcher containing conditions and
    restrictions, if any, with respect to the security of personal health
    information with which the researcher must comply (s. 44(5))
   Notify individuals at the first reasonable opportunity if personal
    health information is stolen, lost or accessed by unauthorized
    persons (s. 12(2))
         Provisions Governing the Security
          of Personal Health Information
 Researchers are required to maintain the security of
  personal health information, including to:
  Enter into an agreement with the health information custodian
   containing the conditions and restrictions, if any, with respect to
   the security of personal health information (s. 44(5)
  Comply with the conditions specified by the research ethics
   board in respect of the research plan including conditions relating
   to the security of personal health information (s.44(6))
  notify the health information custodian immediately in writing if
   the researcher becomes aware of any breach (s. 44(6))

 Research ethics boards are also required to consider the
  adequacy of the safeguards implemented to protect
  privacy and to preserve confidentiality (s.44(3))
       Order HO-004, HO-007 and HO-008
             Nature of the Incidents

Order HO-004
 Theft of a laptop containing the unencrypted personal
  health information of 2,900 individuals
Order HO-007
 Loss of a USB memory stick containing the unencrypted
  personal health information of 83,524 individuals

Order HO-008
 Theft of a laptop containing the unencrypted personal
  health information of 20,000 individuals.
   How to Protect Personal Health Information
         on Mobile or Portable Devices
 Avoid retaining personal health information on such
  devices unless it is necessary for the purpose

 Consider alternatives to retaining personal health
  information on a mobile or portable device, such as:
     Retaining de-identified information on the device,
     Retaining encoded information on the device and storing the
      code needed to unlock the identifying information separately on
      a secure computing device, or
     Retaining personal health information on a secure server and
      accessing it remotely through a secure connection or
      virtual private network
    How to Protect Personal Health Information
          on Mobile or Portable Devices
 If it is necessary to retain personal health information on
  a mobile or portable device:
     Only retain the minimal amount of personal health information
      and for the minimal amount of time necessary,
     Ensure personal health information is strongly encrypted,
     Ensure that the encryption keys are not stored with or on the
      mobile or portable device, and
     Ensure the use of strong password protection

 Develop a policy and procedures for secure retention on
   mobile or portable devices
     Provide training to agents on the policy and procedures, and
     Regularly audit compliance with the policy and procedures,
     Regularly review the policy and procedures to ensure they
      continue to be effective in minimizing privacy risks
              What is Encryption?

To encode information to render it meaningless
 through an array of letters, numbers and symbols
It is accomplished through the use of a computer
 algorithm and encryption keys


                   ENCRYPTION

                   ?b64L+7N6lX2$bhSq
                    *&*Sg$%*#@)‚Äô:{Q
   STOP. THINK. PROTECT
PUBLIC AWARENESS CAMPAIGN
                 STOP.



Ask yourself:
Do I really need to store any personal health
information on this device?
                  THINK.

Consider the alternatives:

Would de-identified or encoded
 information serve the purpose?

Could you access the information remotely
 through a secure connection or virtual
 private network instead?
                 PROTECT.
If you need to retain personal health
information on a mobile/portable device:
Ensure it is strongly encrypted and
 protected with strong passwords
Retain the least amount of personal
 health information for the shortest
 amount of time
Develop policy and procedures for
 secure retention on mobile or portable devices
Provide training for agents
Audit compliance
PREVENTING PRIVACY BREACHES
                     Privacy by Design



Term developed by the Information and Privacy
 Commissioner of Ontario in the 1990s in response
 to growing and emerging threats to online privacy

‚ÄúPrivacy by Design‚Äù seeks to build privacy up
 front into the design specifications and architecture
 and to use privacy-enhancing technologies
             Privacy by Design:
             The 7 Foundational Principles

1. Proactive not Reactive;
   Preventative not Remedial
2. Privacy as the Default
3. Privacy Embedded into Design
4. Full Functionality: Positive-
   Sum, not Zero-Sum
5. End-to-End Lifecycle
   Protection
6. Visibility and Transparency
7. Respect for User Privacy
                    www.ipc.on.ca/images/Resources/7foundationalprinciples.pdf
              Case Study for Privacy by Design
                Data De-Identification Tool
 Developed by a senior investigator at
  the Children‚Äôs Hospital of Eastern
  Ontario Research Institute
 De-identification tool that minimizes
  the risk of re-identification based on:
    The probability of re-identification
    Whether mitigation controls in place
    Motives and capacity of the recipient
    The extent a breach would invade privacy
 Simultaneously         minimizes    the
  distortion to the original database
 Maximizes privacy and data quality
WHAT TO DO IN THE EVENT
 OF A PRIVACY BREACH
Implementation, Identification and Containment

 Implement the privacy breach management policy
 Determine whether a privacy breach has in fact occurred
 Identify the personal health information breached
 Implement containment measures to ensure the personal
  health information is protected from further theft, loss or
  unauthorized use or disclosure, for example:
    Ensure no copies of the records have been made
    Ensure the records are either retrieved or disposed of securely
    Obtain confirmation that the records were securely disposed of
                      Notification


 The Act requires researchers to notify the health
  information custodian immediately in writing if the
  researcher becomes aware of a breach
The Act requires health information custodians to
 notify individuals at the first reasonable opportunity if
 personal health information is lost, stolen or accessed by
 unauthorized persons
            Investigation and Remediation

 Conduct an internal investigation in order to:
    Review the containment measures implemented
    Determine whether the breach has been effectively contained
    Ensure notification has been provided to affected individuals
    Review the circumstances surrounding the privacy breach
    Review the adequacy of existing policies and procedures
    To develop recommendations to prevent similar future breaches

 Document the investigation and any recommendations

 Implement the recommendations to prevent similar
  breaches in the future
                        Conclusion

 We have a publicly-funded health system in which vast
  amounts of personal health information are collected with
  the implied consent of individuals
 The personal health information is a valuable resource that
  can be leveraged for a wide range of secondary uses to
  improve the overall efficiency and effectiveness of our
  health care system and to protect public health
 Custodians, researchers and research ethics boards all
  have an important role to play in protecting the privacy of
  individuals in the research context and thereby promoting
  confidence in such uses and disclosures of information
   How to Contact Us



Information and Privacy Commissioner/Ontario
2 Bloor Street East, Suite 1400
Toronto, Ontario M4W 1A8

Phone:    (416) 326-3333
Web:      www.ipc.on.ca
E-mail:   info@ipc.on.ca
                          PHIPA: The Basics

  Presentation to the Verney PHIPA Summit 2010
      Meeting the Challenge of Managing Health Information

                           November 29-30, 2010




Lonny J. Rosen, Partner
Health Law Group
                The Personal Health Information
               and Protection Act, 2004 (PHIPA)

‚Ä¢ Applies to all Health Information Custodians (HICs) and to
  individuals and organizations that receive information from
  HICs
‚Ä¢ Regulates the way in which HICs collect, use and disclose
  Personal Health Information (PHI)
‚Ä¢ Balances individual‚Äôs right to privacy with the legitimate
  needs of persons and organizations providing health care
  services to access and share this information




                                                                2
                                  PHIPA: Introduction

‚Ä¢ The Information and Privacy
  Commissioner of Ontario
  (the ‚ÄúCommissioner‚Äù) has
  been designated as the
  independent oversight body
  responsible for ensuring that
  HICs collect, use and
  disclose PHI in accordance
  with PHIPA




                                                    3
                     Personal Health Information
                     Protection Act, 2004 - PHIPA
‚Ä¢ Creates rules for the collection, use and
  disclosure of personal health information
  (PHI) by health information custodians
‚Ä¢ Also applies to organizations outside the
  health care system that receive information
  such as insurance companies, employers
  and schools
‚Ä¢ With some exceptions, it provides
  individuals the right to access and correct
  their PHI



                                                4
                      Personal Health Information
                      Protection Act, 2004 - PHIPA
General Principles
‚Ä¢ A custodian is prohibited from collecting, using or
  disclosing PHI about an individual unless it has the consent
  of the individual or it is required under law
‚Ä¢ Consent may be express or implied (depending on the
  situation)




                                                                 5
                     Personal Health Information
                     Protection Act, 2004 - PHIPA
General Principles, cont‚Äôd
‚Ä¢ A custodian is not permitted
  to collect, use or disclose
  PHI if other information will
  serve the purpose
‚Ä¢ A custodian is not permitted
  to collect, use or disclose
  more PHI than is reasonably
  necessary to serve the
  purpose



                                                6
                      Personal Health Information
                      Protection Act, 2004 - PHIPA
Other Responsibilities of Custodians
‚Ä¢ Must appoint a contact person
‚Ä¢ Must establish information practices
‚Ä¢ Must take care to ensure security of patient records
‚Ä¢ Must make available to the public a written statement that
  sets out the following:
    ‚Ä¢ Description of the custodian‚Äôs information practices
    ‚Ä¢ How to contact the contact person
    ‚Ä¢ How to access and correct a patient record
    ‚Ä¢ How to make a complaint to the Information and Privacy
      Commissioner

                                                               7
                                   Purposes of PHIPA

‚Ä¢ Provides rules for the collection, use and disclosure of PHI
  that protect the confidentiality and privacy of individuals
  with respect to that information, while facilitating the
  effective provision of health care
‚Ä¢ Ensures a right of access to an individual‚Äôs own PHI,
  subject to certain exceptions
‚Ä¢ Extends to individuals the right to require the correction or
  amendment of their PHI, subject to certain exceptions
‚Ä¢ Sets out Information practice requirements
‚Ä¢ Provides effective remedies for privacy violations



                                                                  8
                                To whom does
                          PHIPA apply? - HIC‚Äôs
‚Ä¢ Health care practitioners
‚Ä¢ Hospitals
‚Ä¢ Psychiatric facilities
‚Ä¢ Pharmacies
‚Ä¢ Laboratories
‚Ä¢ Nursing homes and long-term
  care facilities
‚Ä¢ Ambulance services
‚Ä¢ Community Care Access Centres



                                             9
                                                HIC‚Äôs

‚Ä¢ Boards of health
‚Ä¢ The Minister of Health and Long-Term Care
‚Ä¢ Retirement homes and homes for special care
‚Ä¢ Assessor (Substitute Decisions Act)
‚Ä¢ Evaluator (Health care Consent Act)
‚Ä¢ Public Health Laboratory
‚Ä¢ Canadian Blood Services

* and agents for all of the above


                                                    10
                       Personal Health Information

What is PHI?
‚Ä¢ Any identifying information in oral or recorded form that
  relates to:
    ‚Ä¢ Physical or mental health, including family history
    ‚Ä¢ Payments or eligibility for health care
    ‚Ä¢ Donation of body parts or substances
    ‚Ä¢ Name of substitute decision maker
    ‚Ä¢ Care previously provided
    ‚Ä¢ A plan of services
    ‚Ä¢ Person‚Äôs health number



                                                              11
                                                               PHI

‚Ä¢ Patients do not have to be named for information to be
  considered personal health information
   ‚Ä¢ Test: is the information identifying or can it be used with
     other information to identify the patient?
‚Ä¢ Mixed Records ‚Äì contain both health and non-health
  information
   ‚Ä¢ Must be careful to discern between protected health
     information and non-protected material




                                                                   12
                                                    Duties

‚Ä¢ Each custodian that has custody or control of PHI must
  establish and comply with information practices in
  accordance with PHIPA.
‚Ä¢ Each custodian must implement a privacy policy that
  outlines the following:
    ‚Ä¢ when, how and the purposes for which
      the custodian routinely collects, uses,
      modifies, discloses, retains or disposes
      of PHI, and
    ‚Ä¢ the administrative, technical and
      physical safeguards and practices that
      the custodian maintains with respect
      to the information.
                                                           13
                                                       Duties
‚Ä¢ Responsible for ensuring that you use client information
  only for purpose for which it was collected
‚Ä¢ Ensure information lawfully disclosed to outside health
  team is accurate, complete and up-to-date as possible
‚Ä¢ In the event that an individual‚Äôs PHI is stolen, lost or
  accessed by unauthorized persons, the custodian must
  notify the affected individual of this occurrence at the first
  reasonable opportunity
‚Ä¢ Ensure that PHI remains secure within the health care team




                                                                   14
                                         PHI ‚Äì Section 4(1)

‚Ä¢ PHI generally includes any identifying information about
  an individual in oral or recorded form (i.e. paper and/or
  electronic records), including the following:
   ‚Ä¢ Information relating to the physical or mental health of an
     individual;
   ‚Ä¢ Medical history;
   ‚Ä¢ Information relating to the provision of health care to an
     individual, including the identification of their health care
     provider; and
   ‚Ä¢ An individual‚Äôs health card number




                                                                     15
                             Responsibilities of HICs
                                  to Safeguard PHI
 ‚Ä¢ Under PHIPA, the responsibility of HICs to safeguard PHI
   in their custody or control is set out in the following
   sections:
Section 12(1)
 ‚Ä¢ A health information custodian shall take steps that are
   reasonable in the circumstances to ensure that personal
   health information in the custodian‚Äôs custody or control is
   protected against theft, loss and unauthorized use or
   disclosure and to ensure that the records containing the
   information are protected against unauthorized copying,
   modification or disposal.


                                                                 16
                            Responsibilities of HICs
                                 to Safeguard PHI
Section 13(1)
 ‚Ä¢ A health information custodian shall ensure that the
   records of personal health information that it has in its
   custody or under its control are retained, transferred and
   disposed of in a secure manner and in accordance with the
   prescribed requirements, if any.




                                                            17
                                                     Storage

 ‚Ä¢ Under PHIPA, HICs must ensure that PHI and PHI Records
   are securely stored:
Physical Safeguards
 ‚Ä¢ locking filing cabinets storing PHI Records;
 ‚Ä¢ ensuring that PHI Records are supervised when they are not
   locked;
 ‚Ä¢ restricting office access to authorized personnel
   (i.e. through locks, pass codes and alarm systems);
 ‚Ä¢ ensuring that faxes and printers are kept in a restricted area
   and are directly monitored while running; and
 ‚Ä¢ protecting against the effects of fire

                                                                18
                                                    Storage

Technological Safeguards
 ‚Ä¢ all staff should have their own computers, wherever
   possible. At the very least, all staff should have their own
   login names and passwords to access PHI;
 ‚Ä¢ passwords should not be easy to guess and should be
   changed regularly;
 ‚Ä¢ access should be removed as soon as a staff member leaves;
 ‚Ä¢ an administrator login and password should be established;
 ‚Ä¢ automatic back-up for file recovery should be set up; and




                                                              19
                                          Storage

Technological Safeguards, cont‚Äôd.
 ‚Ä¢ all electronic PHI Records should
   have an audit trail that records the
   date and time of each entry,
   showing any changes in a record
   and preserving the original
   information when PHI is changed,
   updated or corrected.




                                                20
                              Communication of PHI

‚Ä¢ In communicating PHI, a HIC is
  obligated to ensure that
  unauthorized disclosure of PHI
  does not occur
‚Ä¢ HICs and their agents must follow
  up appropriate safeguards when
  sending PHI by fax or e-mail
‚Ä¢ The following are some of the
  Privacy Commissioner of
  Canada‚Äôs best practices
  recommendations to protect
  personal information:


                                                 21
                                  Communication of PHI

Fax Transmissions
   ‚Ä¢ the faxing of PHI should be limited to situations where the
     information is required immediately;
   ‚Ä¢ fax machines should be located in a secure area that is locked when
     unsupervised;
   ‚Ä¢ before sending a fax that is particularly sensitive, the recipient
     should be contacted by telephone to confirm the fax number and that
     the recipient fax machine is in a secure area;
   ‚Ä¢ a fax cover sheet should always be completed, clearly identifying
     the sender and recipient. The fax cover sheet should also include a
     warning that the information is intended for the named recipient
     only and to contact the sender if the communication is misdirected;
   ‚Ä¢ the recipient should be called after the fax has been sent to confirm
     receipt, and
   ‚Ä¢ a single individual should be made responsible for faxing.

                                                                        22
                            Communication of PHI

E-mail Communications
 ‚Ä¢ Potentially insecure
 ‚Ä¢ E-mail should only be used in
   communications between HICs,
   where both the sender and
   recipient can confirm that the
   browser is secure.




                                               23
                            Transfer of PHI Records

‚Ä¢ HICs are also responsible for the secure transfer of PHI
  Records, which may include transfers to other HICs,
  facilities, or successors.
‚Ä¢ To ensure the secure transfer of PHI Records, HICs would
  be well advised to only transfer records personally or
  through a staff member agent.
‚Ä¢ If a third party agent is used (i.e. a courier company), they
  should be required to enter into an agreement
  acknowledging that they understand the importance of
  safeguarding PHI Records and will provide written
  confirmation when the transfer is completed.


                                                                  24
                            Transfer of PHI Records

‚Ä¢ In the case of a transfer of a
  PHI Record to a successor,
  section 42(2) of PHIPA
  requires a HIC to make
  reasonable efforts to contact
  the patient before
  transferring records to a
  successor, or if that is not
  possible, as soon as possible
  after the transfer.




                                                  25
                                                       Disposal

‚Ä¢ Another important responsibility of HICs under section
  12(1) of PHIPA is to take reasonable steps to ensure the
  secure disposal of PHI
‚Ä¢ In the Commissioner‚Äôs first Order, which was released on
  October 30, 2005, the Commissioner offers a great deal of
  guidance as to how HICs should dispose of PHI:
   ‚Ä¢ For paper records, secure disposal must consist of
     permanently destroying the documents by irreversible
     shredding (i.e. cross-cutting) or pulverizing, thus making
     them unreadable;




                                                                  26
                                            Disposal

‚Ä¢ HICs are responsible for putting into
  place a written agreement with any
  agent it retains to dispose of PHI
  Records. The agreement must set out
  the obligations for secure disposal and
  require the agent to provide written
  confirmation through an attestation
  once secure disposal has been
  conducted;
‚Ä¢ The HIC is responsible for ensuring
  that steps are taken to ensure that no
  unauthorized person will have access
  to the records throughout the disposal
  process.
                                                   27
                                            Disposal

Electronic PHI
 ‚Ä¢ A HIC must also take steps to ensure
   secure disposal of electronic PHI.
 ‚Ä¢ This may involve physical
   destruction, deletion or overwriting
   the information.
 ‚Ä¢ Given the increasing complexity of
   technology, a HIC would be well
   advised to consult an expert to ensure
   that electronic PHI is permanently
   destroyed.
 ‚Ä¢ Destruction must be Secure!


                                                   28
29
                                                     Disposal

Reasonable steps include:
 ‚Ä¢ Written agreements with agents
 ‚Ä¢ Policy outlining disposal policies
 ‚Ä¢ Maintaining a ledger outlining the following particulars in
   the disposal process:
    ‚Ä¢ identification of the PHI disposed of;
    ‚Ä¢ the manner in which PHI was disposed of;
    ‚Ä¢ the name of the individual whose PHI was disposed of;
    ‚Ä¢ the date of disposal; and
    ‚Ä¢ the name and telephone number of the individual who
      disposed of the PHI.

                                                                 30
                           Administrative Controls

‚Ä¢ In order to ensure that PHI is
  stored, transferred,
  communicated and disposed
  of in a reasonable and
  appropriate manner, it is
  imperative that administrative
  controls are put into place.




                                                 31
                             Administrative Controls

Security Protocol
 ‚Ä¢ The most important of these controls is the establishment of
   a security protocol outlining the a HIC‚Äôs rules and policies
   regarding PHI security, including instructions as to how to
   deal with a privacy breach, if and when it occurs.
 ‚Ä¢ All staff should receive thorough training to ensure that
   they are completely familiar with all aspects of the security
   protocol




                                                               32
                                   Contact Person

‚Ä¢ PHIPA requires custodians that
  are not individual persons to
  designate a ‚Äúcontact person‚Äù




                                                33
                          Role of the Contact Person

‚Ä¢ A contact person is an agent of the custodian and is
  responsible for the following functions:
   ‚Ä¢ facilitate the custodian‚Äôs compliance with PHIPA;
   ‚Ä¢ ensure that all agents of the custodian are appropriately
     informed of their duties under PHIPA;
   ‚Ä¢ respond to inquiries about the custodian‚Äôs information
     practices;
   ‚Ä¢ respond to requests for access to PHI and/or corrections to
     PHI that is in the custody of the custodian; and
   ‚Ä¢ receive complaints about alleged breaches of PHIPA




                                                                   34
                                          Policy Statement

‚Ä¢ A key feature of PHIPA is that each custodian must make
  available a written statement that includes the following:
   ‚Ä¢ a general description of the custodian‚Äôs information practices;
   ‚Ä¢ how to contact the contact person;
   ‚Ä¢ how an individual may obtain access to or request correction
     of a record of their PHI, and
   ‚Ä¢ how to make a complaint to the custodian and to the Privacy
     Commissioner




                                                                   35
                            Collection of Information

‚Ä¢ Can only collect that information which is needed to meet
  purpose of collection
‚Ä¢ Information may be collected from third party if:
   ‚Ä¢ Client unable to provide
   ‚Ä¢ Question as to accuracy of the information that the patient
     provides
   ‚Ä¢ Obtaining consent would affect the timeliness of the care




                                                                   36
                           Administrative Controls

Other Administrative Controls
‚Ä¢ appointment of a staff member who is responsible for
  security issues;
‚Ä¢ security clearances for staff members and other applicable
  agents;
‚Ä¢ confidentiality agreements for all staff members and other
  applicable agents;
‚Ä¢ restrict access to certain PHI to certain employees; and
‚Ä¢ regular audits to ensure compliance with the security
  protocol and to determine whether changes should be made
  to the security protocol and the HIC‚Äôs information
  practices.
                                                           37
                                PHIPA vs. PIPEDA

‚Ä¢ The Personal Information Protection and Electronic
  Documents Act (PIPEDA) is federal legislation designed to
  regulate the collection, use and disclosure of personal
  information within the personal sector
‚Ä¢ PIPEDA was not designed to address the intricacies of PHI
‚Ä¢ PHIPA has been declared substantially similar to PIPEDA,
  and HICs covered by PHIPA are not also subject to
  PIPEDA with respect to PHI records




                                                          38
                                                        Consent

‚Ä¢ Custodian needs to obtain a patient‚Äôs knowledgeable
  consent to collect, use and disclose personal health
  information, unless PHIPA allows said collection without
  consent
   ‚Ä¢ Knowledge means the individual knows why the information
     is being collected, used, disclosed and that they have the right
     to withhold consent
‚Ä¢ Consent can be either:
   ‚Ä¢ Implied; or
   ‚Ä¢ Express




                                                                    39
                                                Why Consent?

‚Ä¢ Consent is at the heart of PHIPA
‚Ä¢ Consent is required for the collection, use, disclosure of
  personal health information (PHI) unless PHIPA authorizes
  the collection, use, or disclosure of PHI without consent -
  s.29
‚Ä¢ Purposes of PHIPA include establishment of rules for the
  collection, use and disclosure of PHI that:
   ‚Ä¢ protect the confidentiality of the information,
   ‚Ä¢ protect the privacy of individuals with respect to that
     information, and at the same time,
   ‚Ä¢ facilitate the effective provision of health care. - s.1(a)


                                                                   40
                                          Why Consent?

‚Ä¢ Generally, need express or implied consent to collect, use
  or disclose PHI
‚Ä¢ Collection, use or disclosure of PHI in some circumstances
  does not require consent:
   ‚Ä¢ Where required or authorized by law (including where a
     search warrant, court order or summons/subpoena has been
     issued)
   ‚Ä¢ Where necessary, to prevent significant harm
‚Ä¢ These are specifically provided for in PHIPA




                                                                41
                                      Why Consent?

‚Ä¢ Confidentiality is at the heart
  of health professional-patient
  or clinician-client relationship,
  and relationship of trust leads
  to better care

‚Ä¢ Where clients have opportunity
  to consent to collection, use or
  disclosure, they can be secure
  in belief that confidentiality
  will be maintained, enhancing
  the relationship of trust




                                                 42
                                  What is Required for
                                      Valid Consent?
A HIC needs to obtain an individual‚Äôs knowledgeable consent
to collect, use and disclose personal health information, unless
PHIPA allows the collection, use or disclosure without consent
‚Ä¢ Consent is knowledgeable if it is reasonable in the circumstances
  to believe that the individual knows the purposes of the
  collection, use or disclosure and that the individual may give or
  withhold consent ‚Äì s.18(5)
‚Ä¢ A HIC may rely on a notice of purposes (posted or made readily
  available) as reasonable belief of the individual‚Äôs knowledge of
  the purposes, where reasonable in the circumstance - s. 18(6)
‚Ä¢ Consent must satisfy the requirements of PHIPA, whether it is
  express or implied.


                                                                  43
                                    What is Required for
                                        Valid Consent?
Consent must:
‚Ä¢ be a consent of the individual (or the individual‚Äôs Substitute
  Decision Maker (SDM) if the individual is incapable)
‚Ä¢ be knowledgeable
‚Ä¢ relate to the information that is collected, used or disclosed
‚Ä¢ not be obtained through deception or coercion - s.18(1)




                                                                   44
                                          Implied Consent

‚Ä¢ PHIPA specifies several conditions when consent is
  implied:
   ‚Ä¢ Information exchanged between custodians within the ‚Äúcircle
     of care‚Äù for the purpose of providing direct health care
   ‚Ä¢ Name or address of patient collected, used or disclosed for
     fundraising
   ‚Ä¢ Name and location of patient to a religious organization if the
     patient has previously provided information concerning
     affiliation




                                                                   45
46
                                          Implied Consent
                                          ‚ÄúCircle of Care‚Äù
‚Ä¢ Not defined under PHIPA
‚Ä¢ Describes health information custodians and their
  authorized agents
   ‚Ä¢ eg. (physician‚Äôs office): includes the physician, nurse,
     specialist and any health care professional selected by the
     patient
   ‚Ä¢ Therefore, for Circle of Care, it would include nurses, social
     workers, volunteers and anyone with direct responsibility for
     providing care to the client




                                                                      47
                                          Implied Consent
                                          ‚ÄúCircle of Care‚Äù
‚Ä¢ Does not include:
   ‚Ä¢ Any health care provider who is not a part of the direct or
     follow-up team
   ‚Ä¢ Medical officer of health or a board of health
   ‚Ä¢ Evaluator under the Health Care Consent Act
   ‚Ä¢ Assessor under the Substitute Decisions Act
   ‚Ä¢ The Minister
   ‚Ä¢ Any other person prescribed in the proposed Regulations




                                                                   48
                                         Express Consent

‚Ä¢ Needed for:
   ‚Ä¢ Disclosure to a non-custodian outside the circle of care
     (eg. insurance provider)
   ‚Ä¢ Disclosure of information by one custodian to another
     custodian for a purpose outside of providing health care
   ‚Ä¢ Collection of information for marketing research
   ‚Ä¢ Collection of information (other than name and address)
     for fundraising purposes




                                                                49
                                    Consent Summary

‚Ä¢ Most routine activities will be covered by ‚Äúimplied
  consent‚Äù
‚Ä¢ A patient‚Äôs express wishes to withhold or restrict consent
  must be strictly adhered to
‚Ä¢ A patient can withdraw consent at anytime
‚Ä¢ If incapable, a substitute decision maker may provide
  consent




                                                               50
                              A Note on Fundraising

‚Ä¢ A HIC may collect, disclose or use personal health
  information for fundraising if the individual expressly
  consents
‚Ä¢ If the information consists only of the individual's name and
  mailing address, consent may be implied for fundraising
  purposes
‚Ä¢ All solicitations must provide any easy way to opt out




                                                              51
                                            Persons Who May
                                             Provide Consent
‚Ä¢ A capable individual, regardless of age, can consent to collection,
  use, or disclosure of his or her own PHI. Capacity is presumed.‚Äì
  s. 21(4)
‚Ä¢ Consent may be given on behalf of someone else:
   ‚Ä¢ if the individual is capable and 16 or over, anyone who is 16 or over
     whom the individual has authorized to act on his or her behalf may
     give or refuse consent on behalf of the individual
   ‚Ä¢ if the individual is less than 16 years of age, a parent of the child or
     youth may give consent on the child‚Äôs behalf, with some exceptions
   ‚Ä¢ if the individual is incapable of consenting, a person authorized to
     consent on behalf of the individual under this Act (i.e. the SDM)
     may give or refuse consent
   ‚Ä¢ if the individual is deceased, the deceased‚Äôs estate trustee or the
     person who has assumed responsibility for the administration of the
     estate may give or refuse consent

                                                                            52
                                      Persons Who May
                                       Provide Consent
‚Ä¢ In some circumstances, an Act of Ontario or Canada may
  authorize or require someone to act on behalf of the individual;
  in these circumstances that person may provide consent
‚Ä¢ Where an individual can make a request, express an instruction
  or take a step, his or her SDM may make the request, express the
  instruction or take the step
‚Ä¢ Where a HIC determines that a person is incapable of giving,
  refusing or withdrawing consent, that determination is
  reviewable by the Consent and Capacity Board
‚Ä¢ An SDM can make decisions respecting collection, use or
  disclosure of PHI on behalf of the incapable person



                                                                 53
                             Capacity to Give Consent

Capacity is:

 ‚Ä¢ The ability to understand the
   information that is relevant to deciding
   whether to consent to the collection, use,
   or disclosure

and

 ‚Ä¢ The ability to appreciate the reasonably
   foreseeable consequences of giving, not
   giving, or withholding or withdrawing
   consent ‚Äì s. 21


                                                    54
                               Ranking Order of SDMs
‚Ä¢ PHIPA provides a hierarchy of SDMs who are authorized to
  consent on behalf of an incapable individual
‚Ä¢ These SDMs are, in order of priority:
   ‚Ä¢ A statutory or court appointed guardian of the person or guardian of
     property (with authority)
   ‚Ä¢ attorney for personal care or attorney for property (with authority)
   ‚Ä¢ a representative appointed by the Consent and Capacity Board
   ‚Ä¢ the individual‚Äôs spouse or partner
   ‚Ä¢ individual‚Äôs child or parent (where ‚Äúparent‚Äù is defined to include a
     child‚Äôs custodial parent, a children‚Äôs aid society or other legal
     guardian, but not a parent who has only a right of access)
   ‚Ä¢ a parent with only a right of access
   ‚Ä¢ a brother or sister
   ‚Ä¢ any other relative
   ‚Ä¢ Public Guardian and Trustee (as last resort)
                                                                        55
                      Substitute Decision Making

‚Ä¢ An SDM who makes decisions for an incapable person
  under the Health Care Consent Act is the SDM with respect
  to information decisions necessary for, or ancillary to, a
  decision about treatment, a long term care admission or a
  personal assistance service, as the case may be




                                                           56
                                 Consent and Children

‚Ä¢ If a child is capable, the child may consent
‚Ä¢ If a child (young adult) is capable and 16 or older he or she
  is treated as an adult: he or she may authorize another
  capable person who is at least 16 years old to provide
  consent on his or her behalf, provided it is in writing
‚Ä¢ If a child is capable and less than 16, a custodial parent,
  Children‚Äôs Aid Society or person lawfully entitled to stand
  in the place of a parent may consent EXCEPT:
   ‚Ä¢ If the PHI relates to treatment about which the child has made
     a decision on his or her own pursuant to Health Care Consent
     Act, 1996
   ‚Ä¢ If the PHI relates to counseling in which child participated on
     his or her own under the Child and Family Services Act ‚Äì s.
     23(1)
                                                                   57
                                 Consent and Children

‚Ä¢ If a child is incapable, the same
  persons who may consent for an
  incapable individual can provide
  consent for an incapable child

‚Ä¢ If a child is capable, even if less than
  16, the child‚Äôs decision prevails over
  that of the parent or other SDM
  (where it conflicts)
  ‚Äì s.23(3)



                                                    58
                           Withdrawal of Consent

‚Ä¢ An individual may withdraw consent by notifying the
  Custodian in writing - s.19(1)
‚Ä¢ A withdrawal does not have retroactive effect




                                                        59
                           Withdrawal of Consent:
                                  The ‚Äúlock box‚Äù
‚Ä¢ The ‚Äúlock box‚Äù allows a person to
  restrict the use that may be made of
  PHI, and to restrict who can see and
  use part or all of the person‚Äôs PHI
  that has been collected
‚Ä¢ Lock box protocols give ongoing
  effect to the principle that
  collection, use and disclosure of
  PHI is to be done with consent




                                                60
                            Withdrawal of Consent:
                                   The ‚Äúlock box‚Äù
‚Ä¢ But if disclosure is restricted, and the HIC is instructed to
  disclose some but not all of an individual‚Äôs PHI, if the HIC
  considers disclosure of the PHI in the ‚Äúlock box‚Äù, the HIC
  must advise the other HIC (and possibly other providers of
  health care who access the PHI record) that it has not been
  authorized to disclose all information ‚Äì s. 38(2)




                                                                  61
                                    Consent Summary

‚Ä¢ Most routine activities will be covered by ‚Äúimplied
  consent‚Äù
‚Ä¢ A patient‚Äôs express wishes to withhold or restrict consent
  must be strictly adhered to
‚Ä¢ A patient can withdraw consent at anytime
‚Ä¢ If incapable, a substitute decision maker may provide
  consent




                                                               62
Orders Under PHIPA




                     63
                                 Health Order No. 1

‚Ä¢ The Commissioner was advised by
  a member of the media that health
  records were strewn across
  downtown Toronto as part of a film
  set recreating the 9/11 disaster
‚Ä¢ The film company had no
  knowledge that the papers it had
  purchased from a recycling
  company were actually health
  records, intended to be shredded



                                                  64
                                      Health Order No. 1

‚Ä¢ The clinic was ordered to do the following:
   ‚Ä¢ review its practices re secure storage of PHI; and
   ‚Ä¢ put in place a written contractual agreement with any agent it
     retains to dispose of PHI. The agreement must set out the
     obligation for secure disposal and require the agent to provide
     written confirmation through an attestation once secure
     disposal has been conducted.
‚Ä¢ This Order establishes the practice to be followed by all
  custodians and their agents in Ontario, with respect to
  the secure disposal of health information records under
  PHIPA


                                                                   65
                                    Health Order No. 1

‚Ä¢ The paper disposal company was ordered to do the
  following:
   ‚Ä¢ put into place a written agreement with any custodian for
     whom it will shred PHI that includes the obligation for it to
     shred securely and irreversibly and provide an attestation of
     destruction;
   ‚Ä¢ any handling of PHI by a third party is documented in a
     written agreement that binds the third party to the
     requirements of PHIPA and its contractual agreement with the
     custodian; and




                                                                 66
                                   Health Order No. 2:
                                  Unauthorized Access
‚Ä¢ Health Order No. 2(HO-02) showed that the hospital‚Äôs policies
  and procedures failed to prevent ongoing privacy breaches by an
  employee, even after the hospital became aware that such
  breaches had occurred repeatedly;
‚Ä¢ Even when the patient alerted the hospital to her concerns upon
  admission, the staff did not recognize the obvious threat to
  privacy posed by the estranged husband and his girlfriend-both
  employees of the hospital;
‚Ä¢ Staff only recognized the threat to the physical security of the
  patient, not the threat to her privacy;
‚Ä¢ After learning about the breach, the hospital was more concerned
  about the employee‚Äôs right to due process (Human Resources
  Policy) than the patient‚Äôs right to privacy;
‚Ä¢ Hospitals can have both ‚Äìbut HR cannot trump privacy.
                                                                 67
                               Health Order No. 2:
                            Commissioner‚Äôs Findings
‚Ä¢ Hospital had not taken steps that were reasonable in the
  circumstances to ensure that the personal health information was
  protected against theft, loss and unauthorized use or disclosure;
‚Ä¢ Hospital was ordered to review its practices and procedures to
  ensure that human resource issues did not trump privacy;
‚Ä¢ Hospital was ordered to implement a protocol that would require
  immediate steps to be taken upon being notified of an actual or
  potential privacy breach.




                                                                  68
                                    Health Order No. 3:
                                    Abandoned Records
‚Ä¢ Commissioner advised that medical and rehabilitation clinic
  (Clinic) ceased operations and abandoned records with PHI;
‚Ä¢ Commissioner immediately contacted landlord and personally
  retrieved the records pursuant to 60(13) of PHIPA;
‚Ä¢ The majority of records retrieved from the Clinic consisted of
  invoices; notes on patients; financial records relating to patient
  services; sign-in sheets and appointment books; and insurance
  carrier and benefits information;
‚Ä¢ Corporate search revealed Clinic owned and operated by
  numbered company solely directed by licensed doctor;
‚Ä¢ Landlord wrote to Clinic three times regarding abandonment and
  requested that the Clinic notify him if it wished to claim any
  property on the premises;
‚Ä¢ No provision in the lease for storage and/or retention of records
  of PHI.
                                                                   69
                                   Health Order No. 3:
                                 Commissioner‚Äôs Order
‚Ä¢ As a result of this Order, HICs must:
   ‚Ä¢ enter into a written agreement with any record storage
     company used to retain records stipulating that PHI must be
     treated according to all aspects of PHIPA;
   ‚Ä¢ Put in place practices and procedures to ensure that records of
     PHI are safeguarded at all times;
   ‚Ä¢ Appoint a staff member to facilitate compliance with PHIPA;
   ‚Ä¢ Enter into written contracts with health care practitioners
     acting as independent contractors outlining PHIP obligations
     of both parties regarding records of PHI;
   ‚Ä¢ If impending closure of the group practice of HICs, make
     available to patients a written statement that describes how
     their records will be retained or disposed of and how they may
     obtain access to or transfer of their records.
                                                                   70
                                    Health Order No. 4:
                                         Stolen Laptop
‚Ä¢ Health Order No. 4(HO-04) resulted from a hospital not having
  adequate policies and procedures to permit compliance with
  PHIPA;
‚Ä¢ In spite of the known high risk of loss or theft, extremely
  sensitive personal health information was transported on a
  portable device (laptop) without adequate safeguards;
‚Ä¢ This is clearly unacceptable, more than two years after PHIPA
  came into force;
‚Ä¢ Safeguards would include encryption, or better still, maintaining
  PHI on a secure server accessible remotely.




                                                                      71
                                   Health Order No. 5:
                                   Wireless Technology
‚Ä¢ Health Order No. 5(HO-05) resulted from a methadone clinic
  that installed a wireless video surveillance system in its
  washroom to monitor patients providing urine samples.
‚Ä¢ Video images were intercepted by a wireless rear view backup
  camera in a car outside of the clinic.
‚Ä¢ Clinic immediately agreed to shut down the cameras and
  replaced the wireless surveillance system with a more secure
  wired system.
‚Ä¢ As a result, all HICs must assess the use of their wireless
  communication technology and ensure that PHI is protected
  through strong security measures such as encryption, to preclude
  unauthorized access.


                                                                 72
                              Health Order No. 6:
                     Medical Records on the Street
‚Ä¢ Records containing PHI were found scattered on the street
  outside a medical centre housing a medical laboratory.
‚Ä¢ Commissioner found that the HIC did have appropriate
  information practices in place in accordance with the
  requirements of the Act and its regulations, but did not protect
  the records against theft, loss, unauthorized use or disclosure,
  protection against unauthorized copying, modification or
  disposal in accordance with the Act.
‚Ä¢ The records were not retained, transferred or disposed of in a
  secure manner.




                                                                     73
                            Health Order No. 6:
                   Medical Records on the Street
‚Ä¢ Commissioner‚Äôs Order:
  ‚Ä¢ The HIC must implement its plan to place cross-cut shredders
    in every location
  ‚Ä¢ The HIC must ensure that all contracts or agreements in place
    with third party shredding companies comply with the
    requirements set out in Order HO-001, binding the shredding
    company to the requirements of the Act and its contractual
    agreement with the health information custodian.




                                                                74
                                     Health Order No: 7
                                        Lost USB Stick
‚Ä¢ Order resulted from a USB memory stick, containing PHI of over
  80,000 people, that was lost by a public health nurse
‚Ä¢ Memory stick was not encrypted and person who issued stick did
  not know this was a requirement
‚Ä¢ Found to be completely unacceptable in light of Order No: 4
‚Ä¢ Act requires HIC to take reasonable steps to ensure PHI
  protected from theft, loss and unauthorized use and to ensure that
  OHI is retained and disposed of ion a secure manner
‚Ä¢ Also concern that they collected too much information (i.e.
  health card number when everyone was entitled to H1N1 shot,
  regardless of OHIP status).



                                                                  75
                                        Health Order No: 7
                                           Lost USB Stick
‚Ä¢ Lessons (that should have been) Learned from Past Orders:
   ‚Ä¢ Order HO-004 (Sick Kids Hospital) directed that PHI must be
     encrypted whenever it is stored on mobile devices
   ‚Ä¢ Commissioner stated:
     It is my view that it is no longer reasonable to store PHI on mobile
     computing devices, unless steps are taken to ensure that any PHI
     stored on such devices is protected against unauthorized access, in
     the event that the device is lost or stolen




                                                                       76
                               Health Order No: 7
                         What the HIC did right
‚Ä¢ informing patients about the loss of their PHI
‚Ä¢ planned to use virtual private network (‚ÄúVPN‚Äù) systems
  for storing and accessing PHI records
‚Ä¢ posting a privacy statement at all registration areas
‚Ä¢ informing patients that they were not obliged to provide
  all of the information requested
‚Ä¢ training staff with respect to PHIPA
‚Ä¢ having staff acknowledge their obligations under PHIPA;
‚Ä¢ immediately switching to a paper-based system as soon as
  the breach was found to have occurred
‚Ä¢ immediately engaging a security firm to develop suitable
  encryption for PHI records
‚Ä¢ updating policies regarding the storage of PHI so that
  these now conform to the reality of mobile devices.
                                                             77
                                           Health Order No: 7
                                         Commissioner‚Äôs Order
‚Ä¢   The HIC was ordered to:
    ‚Ä¢   implement procedures to safeguard PHI including by encrypting mobile
        devices
    ‚Ä¢   Revise information practices
    ‚Ä¢   Ensure clinics cease collecting more PHI than was necesssary
    ‚Ä¢   Securely destroy PHI records collected inappropriately
‚Ä¢   The Region was advised to
    ‚Ä¢ inform the public about the Order, through advertisements in local
       newspapers
    ‚Ä¢ develop and implement a policy for mobile devices to ensure
       strong encryption of PHI




                                                                               78
                                     Health Order No: 7
                                   Commissioner‚Äôs Order
‚Ä¢   the Commissioner recommended that the Ministry and Chief
    Medical Officer of Health:
    ‚Ä¢ request each public health unit review of its practices and
       procedures with regard to the encryption of mobile devices;
    ‚Ä¢ request that each Medical Officer of Health confirm that no
       unencrypted PHI is being transported on mobile devices
    ‚Ä¢ audit public health units to verify this information
    ‚Ä¢ provide resources for the development of training materials
       to ensure that all public health unit staff are aware of the
       need for proper safeguards for PHI stored on mobile devices.




                                                                  79
                                  Health Order No: 8
                      Laptop Stolen from Nurse‚Äôs Car
‚Ä¢ Computer left in front seat of nurse‚Äôs car overnight
‚Ä¢ University Health Network Policy is to encrypt all mobile devices, but
  this device was not encrypted
‚Ä¢ Significant amount of PHI Records on computer
‚Ä¢ Commissioner found that UHN failed to comply with PHIPA and failed
  to take reasonable steps to safeguard PHI records in its custody and
  control




                                                                       80
                                   Health Order No: 8
                                 Commissioner‚Äôs Order
‚Ä¢ Commissioner ordered that UHN:
   ‚Ä¢ Develop and implement policies and procedures for secure
     retention of records in mobile devices, including that:
      ‚Ä¢ Any PHI stored on mobile devices to be strongly encrypted
      ‚Ä¢ Information Management Department to ensure encryption
        of all mobile devices
      ‚Ä¢ Chief Information Officer to receive notice of encryption
        errors
      ‚Ä¢ Guidelines for staff using new mobile devices
   ‚Ä¢ Conduct review of all policies to ensure that all PHI on
     mobile devices is identified or strongly encrypted
   ‚Ä¢ Enhance education and awareness programs with
     comprehensive, ongoing, role-based privacy and security
     training re risks with use of mobile devices.
                                                                81
                                      Health Order No: 9
                   Excessive Cost of Copying Records
‚Ä¢ Patient required copy of PHI records from physician
‚Ä¢ Physician followed Ontario Medical Association fee guide, and
  proposed to charge $125 for 34 pages, on basis that 50 hours
  were required to review the records, apply judgment and respond
  to request
‚Ä¢ Patient sought waiver of fee but was refused




                                                                82
                                      Health Order No: 9
                   Excessive Cost of Copying Records
‚Ä¢ Commissioner‚Äôs Findings:
   ‚Ä¢ Physician‚Äôs cost of copying records exceeded reasonable cost
     recovery; $33.50 would have been appropriate ($30 for first
     20 pages and 25 cents per page thereafter)
   ‚Ä¢ Physician‚Äôs denial of waiver of fee upheld
   ‚Ä¢ Proposed regulation under PHIPA, circulated in draft but
     never adopted, prescribes maximum fees that are close to
     ‚Äúreasonable cost recovery.‚Äù




                                                                    83
                                     For More Information

Lonny J. Rosen, C.S.*
Gardiner Roberts LLP
40 King Street West, Suite 3100
Toronto, Ontario
M5H 3Y25

Tel: (416) 369-4345
E-mail: lrosen@gardiner-roberts.ca
Website: http://www.gardiner-roberts.com
* Certified by the Law Society of Upper Canada as a Specialist in Health
Law




                                                                           84
  INTERACTION ISSUES BETWEEN THE PERSONAL
HEALTH INFORMATION PROTECTION ACT (PHIPA) AND
 THE FREEDOM OF INFORMATION AND PROTECTION
           FOR PRIVACY ACT (FIPPA):
 HOW TO NAVIGATE SUCCESSFULLY BETWEEN THE
                  TWO ACTS



                             PHIPA SUMMIT, 2010
                                  November 2010
                              Lise Hendlisz, Counsel
                      Ministry of Health and Long-
                                             Long-Term Care

The contents of this presentation should not be construed as legal advice. For any issues of
           interpretation of PHIPA or FIPPA please consult your legal counsel.
                TO WHOM AND WHAT DOES
                     PHIPA APPLY?
To whom:
 Health information custodians (‚Äúcustodians‚Äù) who collect, use or disclose
   personal health information (‚Äúphi‚Äù); [s.7]

 Agents of custodians who collect, use or disclose phi on behalf of custodians;
     [s.17]

 Non-custodians to whom custodians disclose phi; [s.49]
 Non-custodians who want to collect and use an individual‚Äôs health number.
     PHIPA ‚Äúhealth number rules‚Äù apply specifically to non-custodians. [s.34]

To what:
 Personal health information about an individual in oral or recorded form, that is
   in the custody or under the control of a custodian. [s.4]

* Every underlined term/word is defined in s.2 of PHIPA



                                                                                 2
        TO WHOM AND WHAT DOES
            (M)FIPPA APPLY?
To whom:
 institutions:
    ‚óè all provincial ministries, and entities listed in the FIPPA
        Schedule of Institutions; [FIPPA Reg. 460];
    ‚óè   municipalities, school boards, boards of health, etc.
        [MFIPPA s.2; Reg. 372/91]

To what:
 Records: any record of information however recorded,
   that is in the custody or under the control of the institution
   (even if it does not contain personal information). [s.2]




                                                                    3
    ESSENTIAL DIFFERENCE BETWEEN
         (M)FIPPA AND PHIPA
 (M)FIPPA is a ‚Äúfreedom of information‚Äù AND ‚Äúprotection of privacy‚Äù
    statute: has a dual purpose;

 PHIPA is only a privacy statute, not an access to information statute
    as well;

 The right to privacy includes a right of access to one‚Äôs own
    information, so PHIPA includes ‚Äúaccess‚Äù provisions - - but only for
    individuals wanting access to their own records of phi;

 PHIPA does not provide a right of access to a custodian‚Äôs general
    administrative, financial, etc. records;

 By contrast, (M)FIPPA does provide a right of access to an
    institution‚Äôs general administrative, financial, etc. records.

                                                                          4
                     OTHER DIFFERENCES
Difference in Notice requirements:
‚óè (M)FIPPA ss.29/39 require Notice of Collection for direct and indirect collections,
      and regardless of consent;
‚óè PHIPA s.18 Notice of Purpose requirement is tied to consent and serves as
      evidence of ‚Äúknowledgeable‚Äù consent under s.18(5). Notice of Purpose required
      only where collection, use or disclosure is with consent - - express or implied. If
      consent not required, no notice obligation. However, all custodians must have
      publicly available ‚Äúinformation practices‚Äù: s.16.

Difference in Collection/Use/Disclosure Authorities:
‚óè PHIPA authorities are more extensive, to ensure no interference with provision of
      health care. Note that ‚Äúuse‚Äù in PHIPA is broader concept than in (M)FIPPA;
      includes flow of phi between custodian and its agents/employees. [s.6(1)]

Difference in Right to access pi/phi on behalf of others:
‚óè (M)FIPPA s.54/66 is more restrictive than PHIPA Substitute Decision Maker
      provisions in ss.23-26.
‚óè However, (M)FIPPA ss.14/21, now extends access to ‚Äúclose relatives‚Äù of
      deceased, for ‚Äúcompassionate reasons‚Äù. Is comparable (though not identical) to
      PHIPA ss.38(4)


                                                                                            5
WHO IS A CUSTODIAN UNDER PHIPA?
                  [see PHIPA s.3 & Reg. 329/04, s.3(1)]

   Health care practitioners:                           A person who operates a:
     ‚óè a member defined under the Regulated                ‚óè hospital, psychiatric facility or
          Health Professions Act and who                        independent health facility
          provides health care;                            ‚óè A long-term care home within the
     ‚óè a drugless practitioner under Drugless                   meaning of the Long-Term Care
          Practitioners Act and who provides                    Homes Act, 2007,
          health care;                                     ‚óè community care access centre
     ‚óè a member of the Ontario College of                  ‚óè pharmacy
          Social Workers and Social Service                ‚óè laboratory
          Workers who provides health care;                ‚óè ambulance service
     ‚óè a person whose primary function is to               ‚óè a centre, program or service for
          provide health care for payment.                      community health or mental health
                                                                whose primary purpose is the
   A service provider within the meaning of the                provision of health care
    Home Care and Community Services Act, 1994             ‚óè home for special care
   A community care access corporation within
    the meaning of the Community Care Access             Any other person or class of persons
    Corporations Act, 2001                                prescribed in PHIPA Reg. 329/04, s.3(1)
                                                          (e.g., Canadian Blood Services, the Ministry
   Minister/ Ministry of Health and Long-Term            of Health Promotion)
    Care

   Medical officers of health/ boards of health
    within the meaning of the Health Protection and
    Promotion Act

                                                                                                         6
           WHAT IS PERSONAL HEALTH INFORMATION (PHI)?
                         [see PHIPA s.4]
 ‚ÄúPHI‚Äù is a defined PHIPA concept
 PHI means identifying information about an individual in oral or recorded form that:
     ‚óè relates to his or her physical or mental health, including family health history
     ‚óè relates to providing health care, including identifying a provider of health care
     ‚óè is a plan of service within the meaning of the Long-Term Care Act
     ‚óè relates to the donation of a body part or bodily substance
     ‚óè relates to payments or eligibility for health care/health care insurance coverage in respect of
       the individual
     ‚óè is a health number, in and of itself
     ‚óè identifies a substitute decision-maker of that individual
 ‚ÄúIdentifying Information‚Äù means:
    information that identifies an individual or for which it is reasonably foreseeable in the
    circumstances that it could be utilized, either alone or with other information, to identify an
    individual. [See IPC Order PO-2744 for discussion of issue]

    Effectively extends the definition of phi to include information from which the recipient can deduce
    an individual‚Äôs identity, even where the individual‚Äôs name does not appear on the record.

 The Mixed Record Rule: Is critical for entities whose records are subject to both PHIPA and
    (M)FIPPA
    It provides that the definition of ‚ÄúPHI‚Äù includes all ‚Äúidentifying information‚Äù in a record that also
    contains the type of information defined above as phi. (e.g. address and telephone number on an
    OHIP record is phi because the record also contains individual‚Äôs health number ‚Äì which is PHI in
    and of itself)                                                                                        7
          WHAT IS personal information (pi)?
                                       [see (M)FIPPA s.2(1)]
   ‚ÄúPI‚Äù is a FIPPA concept. Is broader than definition of phi in PHIPA; phi is a subset of pi.

    ‚Äúpersonal information‚Äù means recorded information about an identifiable individual, including,

    (a)    information relating to the race, national or ethnic origin, colour, religion, age, sex, sexual orientation
           or marital or family status of the individual,

    (b)    information relating to the education or the medical, psychiatric, psychological, criminal or employment
           history of the individual or information relating to financial transactions in which the individual has been
           involved,

    (c)    any identifying number, symbol or other particular assigned to the individual,

    (d)    the address, telephone number, fingerprints or blood type of the individual,

    (e)    the personal opinions or views of the individual except where they relate to another individual,

    (f)    correspondence sent to an institution by the individual that is implicitly or explicitly of a private or
           confidential nature, and replies to that correspondence that would reveal the contents of the original
           correspondence,

    (g)    the views or opinions of another individual about the individual, and

    (h)    the individual‚Äôs name where it appears with other personal information relating to the individual or where the
           disclosure of the name would reveal other personal information about the individual.


                                                                                                                          8
       WHO IS AFFECTED BY PHIPA/
     (M)FIPPA INTERACTION ISSUES?

 (M)FIPPA institutions that are also PHIPA custodians
   (‚ÄúDual institutions‚Äù);

 (M)FIPPA institutions that employ/retain PHIPA
   custodians;

 PHIPA custodians who work for (M)FIPPA institutions;
 (M)FIPPA institutions and PHIPA custodians who need
   to exchange personal information for
   operational/business reasons

                                                         9
  HOW TO DETERMINE WHICH ACT
APPLIES TO YOU AND YOUR RECORDS
 Are you a PHIPA custodian, a (M)FIPPA institution, or both?
 If you are not listed as a custodian in PHIPA, you are not subject to PHIPA,
    even if your records contain phi. If you are an institution, those records are
    subject to (M)FIPPA, and are records of pi. (e.g. school board records about
    exceptional pupils)

 If you are a custodian, are you acting as one when performing your ordinary
    work? (eg. a nurse working as a university lecturer is not a custodian)

 If you are a custodian, is the information at issue phi as defined under
    PHIPA, pi under (M)FIPPA, or general, administrative information?

 Does PHIPA state whether PHIPA or (M)FIPPA applies in the
    circumstances?


                                                                              10
  (M)FIPPA INSTITUTIONS SUBJECT TO
      PHIPA: ‚ÄúDUAL INSTITUTIONS‚Äù
 Some (M)FIPPA institutions (or parts of such institutions) are also PHIPA
    custodians: are subject to both Acts:

         Ministry of Health and Long-Term Care;
         Ministry of Health Promotion;
         Boards of Health / Medical Officer of Health;
         Municipal Ambulance Services; and
         Parts of a Ministry/City that operates a centre / program / service for
          community or mental health if primary purpose is provision of health
          care;

 Acts are complementary, not conflicting; for dual Institutions, determining
    which Act applies to a given transaction depends on whether information
    at issue is phi, pi or neither.



                                                                              11
 DISENTANGLING (M)FIPPA AND PHIPA
                                 [see PHIPA s. 8]

If you are a Dual Institution:

 PHIPA applies to all your ‚Äúrecords of phi‚Äù, including ‚Äúmixed‚Äù records that
    also contain pi (i.e. OHIP record listing insured services provided to Mr. X,
    and his address and phone number: is all phi subject to PHIPA);

 (M)FIPPA applies to your records of pi, if no phi is also in the record. (i.e.
    financial information about individuals);

 (M)FIPPA continues to apply to your general records that contain no pi or
    phi (i.e. financial, governance or management records);

 (M)FIPPA access rules continue to apply to general information contained in
    records of phi - - if all phi is readily severable from record.

 PHIPA provides that certain sections of (M)FIPPA continue to apply to
    records of phi.
                                                                              12
     (M)FIPPA INSTITUTIONS THAT
  EMPLOY/RETAIN PHIPA CUSTODIANS
 (M)FIPPA institutions -that are not also custodians- may employ or retain
    persons who are custodians and who are required to provide health care in
    the course of their duties: (i.e. physician in a university clinic; school board
    social worker).

 Institution and custodian each have distinct obligations and responsibilities
    under (M)FIPPA and PHIPA.

 An (M)FIPPA institution that employs PHIPA custodians does not become a
    custodian and is not subject to PHIPA.

 A PHIPA custodian that works for a (M)FIPPA institution remains subject to
    PHIPA, and does not become a (M)FIPPA institution.

 If the institution has custody/control of a record containing phi, that record
    remains subject to (M)FIPPA, and the information remains pi under that Act.

                                                                                13
  IMPACT OF PHIPA CUSTODIANS
WORKING FOR (M)FIPPA INSTITUTIONS
 PHIPA custodians employed/retained by (M)FIPPA institutions
   must comply with ‚Äúinformation practices‚Äù applicable to all
   custodians; e.g.
    ‚óè Appoint or act as a contact person;
    ‚óè Develop and comply with information practices; and
    ‚óè Ensure accuracy and security of phi.

 A PHIPA custodian that works for an institution must comply with
   PHIPA rules governing consent, and the collection, use and
   disclosure of phi without consent.

 A PHIPA custodian working for an institution must look to PHIPA ‚Äì
   not (M)FIPPA ‚Äì in order to determine if it has authority to collect,
   use or disclose phi in a given situation. [PHIPA ss.36-50]

                                                                      14
  PHIPA ACCESS/CORRECTION RULES
                       DO NOT APPLY
 Records in the custody or control of health care practitioners
    working within (M)FIPPA institutions are not subject to PHIPA Part
    V: access & correction rules, if individual has a right to request
    access from the institution (ie. if individual can make an FOI request
    to the institution for the record)

 Requests for custodian‚Äôs records of phi are processed under
    (M)FIPPA by the institution, as a request for pi . [PHIPA ss. 51(3);
    ss.1(11) O.Reg. 329/04]

 To facilitate processing of these access requests, PHIPA expressly
    permits custodians in this situation to disclose records of phi to the
    institution they work for, specifically to enable the institution to
    process an individual's access request under (M)FIPPA. [s.51(4)]



                                                                       15
     INTERACTION ISSUES IN ACCESS
              REQUESTS
   For M/FIPPA Custodians, the most complex interaction issues arise in the context of
    access requests by individuals for their own phi/pi.

   Provisions to be aware of when processing access requests under PHIPA or
    M/FIPPA:
     ‚óè s.4(3): mixed record rule: extends definition of phi to include pi (identifying
          information) of a third party, if contained in individual‚Äôs record of phi;
     ‚óè s.8(1): M/FIPPA does not apply to phi unless PHIPA states otherwise;
     ‚óè s.8(4): M/FIPPA continues to apply to access requests for non-phi; preserves
          right of access to general records;
     ‚óè s.51(3): M/FIPPA applies to access requests for records of a custodian
          working within a (M)FIPPA institution
     ‚óè s.52(3): PHIPA right of access limited to phi about individual if record is ‚Äúnot
          dedicated primarily‚Äù requester‚Äôs phi
     ‚óè O.Reg. 329/04, s.24(3): PHIPA requester has no right of access to phi in a
          record ‚Äúdedicated primarily‚Äù to the phi of another individual


                                                                                   16
   ACCESS REQUESTS FOR GENERAL
    RECORDS OF ‚ÄúDual Institutions‚Äù

 Right of access to general records of M/FIPPA custodian is
   preserved, if phi is severable from record; [s.8(4)]

 PHIPA (not M/FIPPA) requires that all phi be removed: is not an
   ‚Äúexemption‚Äù; FIPPA simply does not apply to custodian‚Äôs records of
   phi; [s.8(1)]

 Once phi is removed, institution can process request under usual
   (M)FIPPA rules to determine if remaining information is subject to
   any (M)FIPPA exemptions/exclusions, or must be disclosed in
   whole.




                                                                   17
      (M)FIPPA INSTITUTIONS AND PHIPA
        CUSTODIANS EXCHANGING PHI/PI
Q: When can a custodian working for an institution disclose phi to that institution?

     Custodians can disclose phi to the institution they work for if any PHIPA disclosure authorities apply to that
      particular disclosure. Custodian must look to PHIPA ss.38-50 to determine if it has authority to disclose
      without consent, and to s.18 if it is disclosing with the individual‚Äôs consent; some examples:

       ‚óè      s.43(1)(f): consistent purpose; s.40(1): preventing risk of physical harm; s.41(a): for proceedings/
              litigation purposes; s.43(1)(h): if permitted/required by another law; s.43(1)(g): for purpose of
              facilitating inspection/investigation.

Q: When can a custodian working for an institution collect phi from that institution?

     If institution has determined it has the authority under (M)FIPPA to disclose the pi to the custodian, the
      custodian has authority under s. 36(1)(g) of PHIPA to collect it indirectly: the collection is from a ‚Äúperson
      who is permitted or required by law‚Äù to disclose it‚Äù. Other collection authorities under s. 36(1) may apply,
      depending on circumstances.

Q: Can an institution collect phi from a custodian it has retained?

     Institutions may collect pi from custodians based on (M)FIPPA s.29/39(1)(h) ‚Äì ‚Äúcollection authorized by or
      under a statute‚Äù - because this collection is specifically authorized under PHIPA s. 49((6):

       ‚óè      ‚Äúwhere PHIPA permits or requires a custodian to disclose phi to an institution that is not a
              custodian, the institution may collect the information from the custodian‚Äù.

                                                                                                             18
              IPC‚ÄôS ROLE UNDER PHIPA
 Ken Anderson is Assistant Commissioner for Health;
 IPC has broader powers of investigation under PHIPA than under (M)FIPPA.
    May investigate a privacy complaint, or, on its own motion, when it has
    reasonable grounds to believe that a person has or is about to contravene
    PHIPA.

 IPC has power to enter and inspect premises (without a warrant, unless a
    dwelling), require access to phi and compel testimony.

 IPC can issue Orders re: complaints (ie. privacy breaches) and access
    decisions, but prefers mediating resolutions between parties.

 To date, the IPC has issued only nine Orders under PHIPA: eight relate to
    privacy breaches; one deals with fees.

 IPC Orders, other than for access or correction, may be appealed on
    questions of law.

                                                                          19
               RESOURCES & CONTACT
                   INFORMATION
 Perun, Orr & Dimitriadis, The Guide to the Ontario Personal Health
    Information Protection Act (Irwin Law, 2005)

 IPC website for (M)FIPPA and PHIPA Fact Sheets, Orders and Privacy
    Reports/Resolutions: www.ipc.on.ca

 Ministry of Health and Long Term Care website for PHIPA:
    www.health.gov.on.ca

 Lise Hendlisz, Legal Counsel, Ministry of Health and Long-Term Care,
    (416) 327-8593, email: lise.hendlisz@ontario.ca




                                                                       20
Frequently Asked Questions
Personal Health Information
Protection Act

September 2015
This Frequently Asked Questions (FAQ) provides a general
overview of the Personal Health Information Protection Act
and Regulation 329/04. The information contained in this
document is for general reference purposes only and should
not be considered as legal advice. Legal counsel should be
consulted for all purposes of interpretation. This FAQ is not
binding on the Information and Privacy Commissioner of
Ontario (IPC) and should not be construed to interfere with
the IPC‚Äôs ability to discharge its duties under the Personal
Health Information Protection Act.

This publication is also available on the IPC website.

Cette publication est √©galement disponible en fran√ßais.
CONTENTS
Introduction.............................................................................................. 1
     What is the Personal Health Information Protection Act
     and why is it necessary?........................................................................ 1

Overview................................................................................................... 3
     What is the purpose of PHIPA?............................................................... 3
     What rights do individuals have?............................................................ 3
     What is the relationship between PHIPA and the federal Personal
     Information Protection and Electronic Documents Act (PIPEDA)?.............. 4
     What is the relationship between PHIPA, the Freedom of Information
     and Protection of Privacy Act (FIPPA) and the Municipal Freedom of
     Information and Protection of Privacy Act (MFIPPA)?............................... 5

Interpretation and Application of PHIPA..................................................... 6
     To whom does PHIPA apply?.................................................................. 6
     What is personal health information?...................................................... 6
     What does ‚Äúhealth care‚Äù mean?.............................................................. 7
     What is a custodian?.............................................................................. 7
     Is a health care practitioner working for a non-custodian
     considered to be a custodian? ............................................................... 8
     What is an agent?................................................................................. 9
     Does PHIPA apply to insurance companies or employers?....................... 10
     What is an electronic service provider?................................................. 10
     What is a health information network provider?..................................... 10
     Who is a prescribed person?................................................................. 11
     What is a prescribed entity?................................................................. 12

Practices to Protect Personal Health Information...................................... 13
     How does PHIPA protect personal health information?........................... 13
     What are the notification requirements in PHIPA in the event
     of a breach?....................................................................................... 14
     Do custodians have responsibilities with respect to accountability
     and openness?.................................................................................... 14
     What are the requirements for the treatment of personal
     health records in the event of a change in practice?............................... 15


Frequently Asked Questions: Personal Health Information Protection Act
Consent Concerning Personal Health Information...................................... 16
    What are the requirements for consent?............................................... 16
    What is the difference between express and implied consent?................. 16
    When is express consent required?....................................................... 17
    When is implied consent sufficient?...................................................... 17
    What is the ‚Äòcircle of care‚Äô?.................................................................. 18
    When can custodians assume implied consent?...................................... 19
    Are pharmacists required to obtain express consent from an
    individual to disclose personal health information to a third party
    benefits payor?................................................................................... 20
    Can individuals control what personal health information is recorded
    in their file?........................................................................................ 20
    Can individuals withdraw their consent?................................................ 20
    What is a ‚Äòlock-box‚Äô?........................................................................... 21
    What are the restrictions and limitations on the lock-box?..................... 21
    What happens when an individual is incapable of providing consent?....... 22
    Can a child under 16 years old provide consent?.................................... 23
    Can another person, such as a family member, provide consent
    on an individual‚Äôs behalf when picking up or dropping off
    a prescription?.................................................................................... 23

Collection, Use and Disclosure of Personal Health Information................... 24
    What are the general limitations on the collection, use and disclosure
    of personal health information?............................................................ 24
    Collection........................................................................................... 24
       What is a collection of personal health information
       under PHIPA?................................................................................. 24
       What are the rules regarding the collection of personal health
       information?................................................................................... 25
       When can custodians indirectly collect personal
       health information?......................................................................... 25
    Use.................................................................................................... 26
       What is a use of personal health information under PHIPA? ............... 26
       What are the rules regarding the use of personal
       health information?......................................................................... 26
       When can personal health information be used without consent?........ 26


                                          Frequently Asked Questions: Personal Health Information Protection Act
     Disclosure.......................................................................................... 27
         What is a disclosure of personal health information
         under PHIPA?................................................................................. 27
         What are the rules regarding the disclosure of personal
         health information?......................................................................... 27
         When can personal health information be disclosed without
         consent?........................................................................................ 28
         Can personal health information be disclosed in the event of
         an emergency?............................................................................... 29
         Does PHIPA permit disclosure of personal health information
         about a deceased individual?........................................................... 30
         Can a custodian disclose personal health information to the
         Workplace Safety and Insurance Board (WSIB) about an injured
         worker without the individual‚Äôs consent?........................................... 31
         Can a custodian store, access or disclose personal health
         information outside of Ontario?........................................................ 31

Fundraising and Marketing....................................................................... 33
     Can custodians collect, use or disclose personal health information
     for fundraising activities?.................................................................... 33
     Can personal health information be collected, used or disclosed for
     marketing purposes?........................................................................... 34

Research................................................................................................. 35
     What are the requirements for the collection, use and disclosure
     of personal health information for research?......................................... 35
     Are there any requirements for research ethics boards and
     research plans?................................................................................... 36

Ontario Health Cards and Health Numbers................................................ 37
     Who can collect, use or disclose Ontario health numbers and
     under what circumstances?.................................................................. 37
     Are other organizations permitted to request the production
     of a health card?................................................................................. 37

Access to Records of personal Health Information and Correction.............. 39
     Access............................................................................................... 39
         Are individuals permitted to access their own personal health
         information?................................................................................... 39




Frequently Asked Questions: Personal Health Information Protection Act
       How do an individual obtain access to their personal
       health information?......................................................................... 40
       How long does a custodian have to respond to an individual‚Äôs
       request for access to personal health information?............................ 40
       Can a custodian refuse to provide access to an individual‚Äôs
       personal health information?........................................................... 41
       Is there a fee associated with an access request?............................... 41
       What if the custodian works for a non-custodian that is covered
       under public sector access and privacy legislation, such as a
       school board or municipality?.......................................................... 42
    Correction.......................................................................................... 42
       Can individuals correct errors in their personal
       health information?......................................................................... 42
       How does an individual correct errors?............................................. 43
       Can a custodian refuse to correct an individual‚Äôs personal health
       information?................................................................................... 43

Administration and Enforcement.............................................................. 44
    How is PHIPA enforced?...................................................................... 44
    How does an individual initiate a complaint?.......................................... 44
    Is there a time limit within which an individual may complain?................. 45
    If a person is not satisfied with an IPC order, what can be done?.............. 45
    Can a person seek compensation for damages? .................................... 45
    What is an offence under PHIPA?......................................................... 45
    What are the consequences for committing an offence
    under PHIPA?..................................................................................... 46
    Who is responsible for prosecuting offences under PHIPA?..................... 46




                                         Frequently Asked Questions: Personal Health Information Protection Act
INTRODUCTION
WHAT IS THE PERSONAL HEALTH INFORMATION
PROTECTION ACT AND WHY IS IT NECESSARY?
The Personal Health Information Protection Act (PHIPA) is Ontario‚Äôs health-
specific privacy legislation which came into force on November 1, 2004. PHIPA
governs the manner in which personal health information may be collected, used
and disclosed within the health sector. It regulates health information custodians
(custodians), as well as individuals and organizations that receive personal health
information from custodians.

Personal health information is among the most sensitive of personal
information. People are understandably protective about sharing personal
details relating to their medical conditions. At the same time, personal health
information must flow freely between health care practitioners in order to
ensure the best care for patients.

The nature of our health system is that personal health information passes
through many links in the health care chain: from a doctor‚Äôs office, to a referral
to a specialist, to a medical lab, to a hospital or to an insurance company for
reimbursement of claims. There are many circumstances in which personal health
information must be readily, as well as expeditiously shared, such as in the case
of a medical emergency. Beyond patient care, personal health information is
needed for important activities, such as health research which is vital to develop
new treatments and cures.

PHIPA creates a consistent approach to protecting personal health information
across the health sector. The legislation was designed to give individuals
greater control over how their personal health information is collected, used or
disclosed. PHIPA balances the privacy rights of individuals with the legitimate
need of custodians to collect, use and disclose personal health information in
order to deliver effective and timely health care and to plan and manage our
publicly funded health system.

With limited exceptions, PHIPA requires custodians to obtain consent before
personal health information is collected, used or disclosed. In addition, PHIPA
provides individuals with a right to access and request correction of their
personal health information. PHIPA also provides a means for redress through
the Office of the Information and Privacy Commissioner of Ontario (IPC) when
privacy rights relating to personal health information have been violated.




Frequently Asked Questions: Personal Health Information Protection Act                1
     What is the   The IPC is the designated oversight body responsible for administering
Personal Health    and enforcing these health sector privacy rules. As such, we have prepared
    Information    the following questions and answers to guide Ontarians and custodians in
 Protection Act    understanding their respective privacy rights and obligations.
   and why is it
     necessary?




     2                                           Frequently Asked Questions: Personal Health Information Protection Act
OVERVIEW
WHAT IS THE PURPOSE OF PHIPA?
PHIPA establishes rules for the collection, use and disclosure of personal health
information and includes provisions that:

     ‚Ä¢ require consent for the collection, use and disclosure of personal health
       information, with necessary but limited exceptions,

     ‚Ä¢ require that custodians treat all personal health information as confidential
       and keep it secure,

     ‚Ä¢ provide individuals with a right of access to their personal health
       information, as well as the right to correct errors,

     ‚Ä¢ give individuals the right to withhold or withdraw consent to the collection,
       use or disclosure of personal health information or to expressly instruct
       custodians not to use or disclose their personal health information for
       health care purposes,

     ‚Ä¢ establish clear rules for the collection, use and disclosure of personal
       health information for fundraising and marketing purposes,

     ‚Ä¢ set guidelines for the collection, use and disclosure of personal health
       information for research purposes,

     ‚Ä¢ ensure accountability by granting individuals the right to complain to the
       IPC about the practices of custodians and

     ‚Ä¢ establish remedies for breaches of the legislation.



WHAT RIGHTS DO INDIVIDUALS HAVE?
PHIPA gives individuals the right to:

     ‚Ä¢ be informed of the purposes for the collection, use and disclosure of
       personal health information,

     ‚Ä¢ be notified by a custodian if personal health information has been stolen,
       lost or accessed by unauthorized persons,

     ‚Ä¢ refuse or give consent to the collection, use or disclosure of personal
       health information, except in circumstances specified in PHIPA,




Frequently Asked Questions: Personal Health Information Protection Act                 3
   What rights do      ‚Ä¢ withdraw consent by providing notice to the custodian,
individuals have?
                       ‚Ä¢ expressly instruct a custodian not to use or disclose personal health
                         information for health care purposes without consent,

                       ‚Ä¢ access a copy of their own personal health information, except in limited
                         circumstances specified in PHIPA,

                       ‚Ä¢ request corrections to be made to their personal health information,

                       ‚Ä¢ complain to the IPC about a custodian‚Äôs refusal to give access to all or
                         part of a record of personal health information,

                       ‚Ä¢ complain to the IPC about a custodian‚Äôs refusal to grant a correction
                         request,

                       ‚Ä¢ complain to the IPC about any breach or potential breach of PHIPA or its
                         regulations and

                       ‚Ä¢ begin a proceeding in court for damages for actual harm suffered, if
                         affected by a final order or conduct leading to a final conviction for an
                         offence under PHIPA.

                    PHIPA establishes a formal process for individuals to access and correct
                    their personal health information, within specified time frames and the right to
                    complain if an access or correction request is denied.



                    WHAT IS THE RELATIONSHIP BETWEEN PHIPA AND THE
                    FEDERAL PERSONAL INFORMATION PROTECTION AND
                    ELECTRONIC DOCUMENTS ACT (PIPEDA)?
                    The collection, use and disclosure of personal information within the commercial
                    sector is regulated by federal privacy legislation‚Äîthe Personal Information
                    Protection and Electronic Documents Act. PIPEDA was enacted to regulate the
                    collection, use or disclosure of personal information in the hands of private sector
                    organizations. PIPEDA does not apply to personal information in provinces and
                    territories that have ‚Äúsubstantially similar‚Äù privacy legislation in place.

                    The federal government has deemed PHIPA to be ‚Äúsubstantially similar‚Äù to
                    PIPEDA. Custodians and their agents are exempted from having to comply with
                    the provisions of PIPEDA to the extent that they collect, use and disclose personal
                    health information within Ontario. PIPEDA continues to apply to all commercial
                    activities relating to the exchange of personal health information between
                    provinces and territories and to information transfers outside of Canada.




      4                                             Frequently Asked Questions: Personal Health Information Protection Act
WHAT IS THE RELATIONSHIP BETWEEN PHIPA, THE
FREEDOM OF INFORMATION AND PROTECTION OF
PRIVACY ACT (FIPPA) AND THE MUNICIPAL FREEDOM
OF INFORMATION AND PROTECTION OF PRIVACY ACT
(MFIPPA)?
Organizations that are both custodians under PHIPA and institutions under
public sector privacy and access to information legislation, namely the provincial
FIPPA or its municipal counterpart MFIPPA, include hospitals, the Ontario
Agency for Health Protection and Promotion, the Ministry of Health and Long-
Term Care, medical officers of health and municipally operated long-term care
homes and ambulance services.

The general rule is that, subject to certain exceptions, a custodian that is also an
institution or a part of an institution is governed by PHIPA, not FIPPA or MFIPPA,
with respect to personal health information in its custody or under its control.
All other recorded information about an individual that is not personal health
information and that is in the custody or under the control of an organization that
is both a custodian and an institution or part of an institution is subject to FIPPA
or MFIPPA, as the case may be.

PHIPA also contains provisions that are specific to custodians that are
institutions. For example, PHIPA provides a number of exceptions to the general
rule that custodians are only permitted to collect personal health information
directly from the individual to whom the information relates. In addition to the
exceptions available to all custodians, a custodian that is also an institution
under FIPPA or MFIPPA may collect personal health information indirectly for a
purpose related to investigating a breach of an agreement or a contravention or
alleged contravention of laws of Ontario or Canada, the conduct of a proceeding
or possible proceeding or the statutory function of the custodian.

For further information, please see the IPC documents Applying PHIPA and
FIPPA/MFIPPA to Personal Health Information, Freedom of Information at Ontario
Hospitals: Frequently Asked Questions, and Applying PHIPA and FIPPA to
Personal Health Information: Guidance for Hospitals.




Frequently Asked Questions: Personal Health Information Protection Act                 5
    INTERPRETATION AND
    APPLICATION OF PHIPA
    TO WHOM DOES PHIPA APPLY?
    PHIPA applies to a wide variety of persons and organizations defined as health
    information custodians. PHIPA also applies to agents who are authorized to
    act for or on behalf of custodians. Additionally, PHIPA applies to the use and
    disclosure of personal health information by those who receive personal health
    information from custodians (recipients) and to electronic service providers,
    including health information network providers.



    WHAT IS PERSONAL HEALTH INFORMATION?
    Personal health information is ‚Äúidentifying information‚Äù about an individual,
    whether oral or recorded if the information:

       ‚Ä¢ relates to the individual‚Äôs physical or mental condition, including family
         medical history,

       ‚Ä¢ relates to the provision of health care to the individual,

       ‚Ä¢ is a plan of service for the individual,

       ‚Ä¢ relates to payments, or eligibility for health care or for coverage for health
         care,

       ‚Ä¢ relates to the donation of any body part or bodily substance or is derived
         from the testing or examination of any such body part or bodily substance,

       ‚Ä¢ is the individual‚Äôs health number or

       ‚Ä¢ identifies a health care provider or a substitute decision-maker for the
         individual.

    ‚ÄúIdentifying information‚Äù includes information that identifies an individual or for
    which it is reasonably foreseeable that it could be used, either alone or with
    other information, to identify an individual.

    Personal health information includes identifying information that is not personal
    health information but that is contained in a record that contains personal health
    information. Personal health information does not include identifying information




6                                    Frequently Asked Questions: Personal Health Information Protection Act
about an employee or agent of the custodian that is not maintained primarily for    What is
the provision of health care. For example, a doctor‚Äôs note to support an absence    personal health
from work in the personnel file of a secretary employed by a custodian is not       information?
personal health information.



WHAT DOES ‚ÄúHEALTH CARE‚Äù MEAN?
‚ÄúHealth care‚Äù means any observation, examination, assessment, care,
service or procedure that is done for a health-related purpose and that is
carried out or provided:

     ‚Ä¢ for diagnosis, treatment or maintenance of an individual‚Äôs physical or
       mental condition,

     ‚Ä¢ for prevention of disease or injury or the promotion of health or

     ‚Ä¢ as part of palliative care.

It also includes:

     ‚Ä¢ the compounding, dispensing or selling of a drug, device or equipment
       pursuant to a prescription,

     ‚Ä¢ a community service that is described in the Home Care and Community
       Services Act and

     ‚Ä¢ taking blood or a blood product donation from an individual.



WHAT IS A CUSTODIAN?
A custodian is a person or organization listed in PHIPA that, as a result of his,
her or its power or duties or work set out in PHIPA, has custody or control of
personal health information. Examples of custodians include:

     ‚Ä¢ health care practitioners, (including doctors, nurses, speech-language
       pathologists, chiropractors, dental professionals, dieticians, medical
       laboratory technologists, massage therapists, midwives, occupational
       therapists, opticians and physiotherapists),

     ‚Ä¢ community care access corporations,

     ‚Ä¢ hospitals,

     ‚Ä¢ psychiatric facilities,

     ‚Ä¢ long-term care homes,



Frequently Asked Questions: Personal Health Information Protection Act                       7
  What is a      ‚Ä¢ pharmacies,
custodian?
                 ‚Ä¢ laboratories,

                 ‚Ä¢ ambulance services,

                 ‚Ä¢ retirement homes and homes for special care,

                 ‚Ä¢ medical officers of health of boards of health,

                 ‚Ä¢ the Minister of Health and Long-Term Care and

                 ‚Ä¢ Canadian Blood Services.

              A custodian does not include:

                 ‚Ä¢ a health care practitioner, service provider, evaluator or assessor who is an
                   agent of a custodian,

                 ‚Ä¢ a person authorized to act for or on behalf of a person that is not a
                   custodian, if the scope of duties of the authorized person does not include
                   the provision of health care,

                 ‚Ä¢ an aboriginal healer who provides traditional healing services to aboriginal
                   persons or members of an aboriginal community,

                 ‚Ä¢ an aboriginal midwife who provides traditional midwifery services to
                   aboriginal persons or members of an aboriginal community and

                 ‚Ä¢ a person who provides treatment solely by spiritual means or by prayer.



              IS A HEALTH CARE PRACTITIONER WORKING FOR A
              NON-CUSTODIAN CONSIDERED TO BE A CUSTODIAN?
              A health care practitioner, who provides health care, but who contracts with, is
              employed by or volunteers for an organization that is not defined as a custodian
              under PHIPA, would fall within the definition of a custodian under PHIPA and
              must comply with all requirements for custodians.

              Examples of custodians who work for non-custodians include:

                 ‚Ä¢ a nurse employed by a school board to provide health care services to
                   students,

                 ‚Ä¢ a doctor employed by a professional sports team in order to diagnose
                   sporting injuries,




8                                             Frequently Asked Questions: Personal Health Information Protection Act
     ‚Ä¢ a registered massage therapist providing health care services to clients of Is a health care
       a spa and                                                                   practitioner
                                                                                   working for a
    ‚Ä¢ a nurse employed in-house by a manufacturing firm in a health care capacity.
                                                                                   non-custodian
A custodian cannot disclose personal health information to a non-custodian,        considered to be a
including the non-custodian for whom the individual is working, unless the         custodian?
individual whose personal health information is at issue has given express
consent or the disclosure is permitted or required by PHIPA or another law.

For further information, please see the IPC fact sheet, Health Information
Custodians Working for Non-Health Information Custodians.



WHAT IS AN AGENT?
PHIPA defines an agent to include any person who is authorized by a custodian
to perform services or activities in respect of personal health information on the
custodian‚Äôs behalf and for the purposes of that custodian.

An agent may include a person or company that contracts with, is employed
by or volunteers for a custodian and, as a result, may have access to personal
health information. PHIPA permits custodians to provide personal health
information to their agents only if the custodian is permitted to collect, use,
disclose, retain or dispose of the information.

For example, an agency relationship under PHIPA includes a nurse who is
employed by, or a student who volunteers at, a hospital. An agency relationship
may also include a physician who is not employed by a hospital, but has
admitting privileges to use the hospital‚Äôs equipment or facilities. In such cases,
the custodian hospital is permitted to authorize the agent to handle or deal with
personal health information on its behalf, as long as the agent complies with
PHIPA and adopts the information practices of the custodian. An agent must
notify the custodian if the personal health information the agent is handling is
stolen, lost or accessed by unauthorized persons.

The custodian remains accountable for the personal health information in its
custody or under its control, even where the agent is authorized to act on its
behalf with respect to that personal health information. The custodian also
remains accountable for the personal health information in its custody or under
its control where the agent acted beyond what was authorized by the custodian.
For example, in Order HO-013, employees were found to be agents when they
used and/or disclosed personal health information in the custody or under the
control of a hospital for the purpose of selling or marketing Registered Education
Saving Plans. The custodian hospital was accountable for the contravention of
PHIPA, even though the agents may have acted beyond the authority delegated
by the hospital.



Frequently Asked Questions: Personal Health Information Protection Act                       9
     DOES PHIPA APPLY TO INSURANCE COMPANIES OR
     EMPLOYERS?
     Certain organizations, such as insurance companies and employers, who may
     hold personal health information in their files, are not governed by PHIPA, unless
     they receive personal health information from a custodian. When an insurance
     company or employer receives personal health information from a custodian,
     the receiving entity may, in general, only use or disclose the information for the
     authorized purpose for which the information was disclosed or for the purpose
     of carrying out a statutory or legal duty. This rule is colloquially referred to as the
     ‚Äúrecipient rule.‚Äù

     However, an exception to the recipient rule applies to insurance providers that
     receive personal health information from a pharmacist. In that situation, PHIPA
     permits the insurance provider to disclose personal health information to the
     pharmacist to assist the pharmacist in advising the individual or providing the
     individual with health care. For example, the insurance provider may disclose to
     a pharmacist the types of medications an individual has purchased from different
     pharmacies so that the pharmacist may advise of any incompatible prescriptions.



     WHAT IS AN ELECTRONIC SERVICE PROVIDER?
     An electronic service provider is a person who supplies services that enable a
     custodian to collect, use, modify, disclose, retain or dispose of personal health
     information electronically. If the electronic service provider is not an agent of the
     custodian, then it shall not use any personal health information to which it has
     access in the course of providing services to the custodian, except as necessary
     in the course of providing the service and it cannot disclose the information.
     Electronic service providers must also ensure their employees or any other
     persons acting on their behalf agree to comply with these restrictions.



     WHAT IS A HEALTH INFORMATION NETWORK PROVIDER?
     PHIPA contains requirements that apply to a specific type of electronic
     service provider, referred to as a health information network provider. A health
     information network provider is a person who provides services to two or more
     custodians, where the services are provided primarily to enable the custodians
     to use electronic means to disclose personal health information to one another,
     whether or not the person is an agent of any of the custodians. Among other
     requirements, health information network providers must:




10                                    Frequently Asked Questions: Personal Health Information Protection Act
     ‚Ä¢ notify the custodian of any breaches,                                          What is a health
                                                                                      information
     ‚Ä¢ perform threat risk assessments and privacy impact assessments,
                                                                                      network provider?
     ‚Ä¢ upon request, provide an electronic record to the custodian of all accesses
       and transfers of the personal health information,

     ‚Ä¢ ensure that retained third parties comply with necessary restrictions and
       conditions,

     ‚Ä¢ enter into a written agreement with the custodian and

     ‚Ä¢ make publicly available information about its services to the custodian.



WHO IS A PRESCRIBED PERSON?
The regulations prescribe a list of persons who compile and maintain registries
of personal health information for the purpose of facilitating or improving the
provision of health care or that relates to the storage or donation of bodily parts
or substances. Custodians are permitted to disclose personal health information
without consent to these listed persons. They consist of the following:

     ‚Ä¢ Cardiac Care Network of Ontario in respect of its registry of cardiac
       services,

     ‚Ä¢ INSCYTE (Information System for Cytology etc.) Corporation in respect of
       CytoBase,

     ‚Ä¢ Hamilton Health Sciences Corporation in respect of the Critical Care
       Information System,

     ‚Ä¢ Cancer Care Ontario in respect of the Ontario Cancer Screening Registry,

     ‚Ä¢ Children‚Äôs Hospital of Eastern Ontario in respect of the Better Outcomes
       Registry and Network and

     ‚Ä¢ Ontario Institute for Cancer Research in respect of the Ontario Tumour
       Bank.

The above-noted prescribed persons may use and disclose personal health
information for the purpose of facilitating or improving the provision of health
care or for the storage or donation of bodily parts or substances. They are
also permitted to use and disclose personal health information for research
purposes, with a research plan approved by a research ethics board (REB) in
certain circumstances. These prescribed persons are also permitted to disclose
personal health information to prescribed entities for the planning, management
or analysis of the health system.



Frequently Asked Questions: Personal Health Information Protection Act                       11
Who is a prescribed The regulations also require that prescribed persons make publicly available:
            person?
                      ‚Ä¢ a plain language description of the functions of the registry and

                      ‚Ä¢ a summary of the practices and procedures to protect the privacy of the
                        individuals whose personal health information they receive and to maintain
                        the confidentiality of the information.

                   The prescribed person must have its practices and procedures approved by the
                   IPC every three years.

                   For further information, please see the IPC‚Äôs Manual for the Review and Approval
                   of Prescribed Persons and Prescribed Entities.



                   WHAT IS A PRESCRIBED ENTITY?
                   The regulations prescribe a list of entities, including any registries maintained
                   within these listed entities, that custodians are permitted to disclose personal
                   health information to without consent for purposes of planning, management and
                   analysis of the health system. Prescribed entities consist of the following:

                      ‚Ä¢ Cancer Care Ontario,

                      ‚Ä¢ Canadian Institute for Health Information,

                      ‚Ä¢ Institute for Clinical Evaluative Sciences and

                      ‚Ä¢ Pediatric Oncology Group of Ontario.

                   In certain circumstances, with a research plan approved by a research ethics
                   board, these prescribed entities are permitted to use and disclose personal
                   health information for research purposes as if they were custodians. A
                   prescribed entity is permitted to disclose personal health information to a
                   prescribed person who compiles or maintains a registry of personal health
                   information, and to another prescribed entity for purposes related to the
                   planning, management and analysis of the health system.

                   The regulations also require that prescribed entities make publicly available:

                      ‚Ä¢ a plain language description of the functions of the entity and

                      ‚Ä¢ a summary of the practices and procedures to protect the privacy of the
                        individuals whose personal health information they receive and to maintain
                        the confidentiality of the information.

                   The prescribed entity must have its practices and procedures approved by the
                   IPC every three years.

                   For further information, please see the IPC‚Äôs Manual for the Review and Approval
                   of Prescribed Persons and Prescribed Entities.


       12                                          Frequently Asked Questions: Personal Health Information Protection Act
PRACTICES TO PROTECT
PERSONAL HEALTH
INFORMATION
HOW DOES PHIPA PROTECT PERSONAL HEALTH
INFORMATION?
PHIPA establishes certain privacy rights for individuals and imposes specific
obligations on custodians in protecting personal health information. Custodians
who have custody or control of personal health information must develop and
implement information practices that comply with the requirements of PHIPA.
Custodians must also ensure that personal health information is as accurate,
up-to-date and complete as is necessary for the purposes for which they use or
disclose personal health information.

PHIPA requires custodians to take steps that are reasonable in the
circumstances to ensure personal health information in their custody or under
their control is protected against theft, loss and unauthorized use and disclosure,
and to ensure that records of personal health information are protected against
unauthorized copying, modification or disposal.

PHIPA also requires custodians to ensure that records of personal health
information are retained, transferred and disposed of in a secure manner.
According to the definition of ‚Äúdisposed of in a secure manner‚Äù in the
regulations, records of personal health information must be destroyed in such a
manner that their reconstruction is not reasonably foreseeable.

PHIPA requires records of personal health information to be kept for as long as
needed to allow an individual to exhaust any legal recourse regarding an access
request. As PHIPA does not establish specific retention periods for personal
health information, custodians should refer to their governing legislation to
determine applicable record retention requirements. For example, regulations
under the Public Hospitals Act specify how long hospitals must retain records of
personal health information.

For further information, please see the IPC fact sheets, Safeguarding Personal
Health Information, Secure Destruction of Personal Health Information, Encrypting
Personal Health Information on Mobile Devices, Health-Care Requirement for
Strong Encryption and The Secure Transfer of Personal Health Information, and
the IPC discussion papers Get Rid of it Securely to keep it Private ‚Äì Best Practices
for the Secure Destruction of Personal Health Information and Detecting and
Deterring Unauthorized Access to Personal Health Information.



Frequently Asked Questions: Personal Health Information Protection Act                 13
     WHAT ARE THE NOTIFICATION REQUIREMENTS IN PHIPA
     IN THE EVENT OF A BREACH?
     PHIPA contains notification requirements for both agents and custodians.
     If personal health information handled by an agent on behalf of a custodian
     is stolen, lost or accessed by unauthorized persons, the agent must notify
     the custodian of the breach at the first reasonable opportunity. PHIPA also
     requires custodians to notify individuals at the first reasonable opportunity
     if personal health information is stolen, lost or accessed by an unauthorized
     person. However, a custodian who is a researcher and received personal health
     information for research purposes from another custodian must not notify an
     individual, unless the researcher is informed that the individual has given consent
     to being contacted.

     For further information please see the IPC guidelines, What to do When Faced
     With a Privacy Breach: Guidelines for the Health Sector.



     DO CUSTODIANS HAVE RESPONSIBILITIES WITH RESPECT
     TO ACCOUNTABILITY AND OPENNESS?
     In order to enhance transparency, PHIPA contains specific requirements for
     custodians that relate to accountability and openness. For example, a custodian
     must designate a contact person who is authorized on behalf of the custodian
     to facilitate compliance with PHIPA, ensure agents are appropriately informed
     of their duties, respond to inquiries about the custodian‚Äôs information practices,
     respond to access and correction requests and receive complaints from the
     public. A custodian that is a natural person may designate a contact person, or
     else perform the functions on their own.

     A custodian must also provide a written statement that is readily available to the
     public and describes the custodian‚Äôs information practices, how to reach the
     contact person, how to obtain access to or request a correction of a record of
     personal health information and how to make a complaint to the custodian and
     to the IPC.

     Unless an individual does not have a right of access, a custodian must notify the
     individual of any uses and disclosures of personal health information that occur
     without the individual‚Äôs consent in a manner that is outside of the scope of the
     custodian‚Äôs description of its information practices. The custodian must also
     make a note of these uses and disclosures and keep the note as a part of, or
     linked to, the records of personal health information.




14                                   Frequently Asked Questions: Personal Health Information Protection Act
WHAT ARE THE REQUIREMENTS FOR THE TREATMENT
OF PERSONAL HEALTH RECORDS IN THE EVENT OF A
CHANGE IN PRACTICE?
A change in practice occurs in a variety of circumstances, for example, due to
death, bankruptcy, retirement or relocation. It is important to identify who the
custodian of records of personal health information is in the event of a planned
or unforeseen change in practice. Generally, a custodian remains a custodian
with respect to a record of personal health information until complete custody
and control of the record passes to another person who is legally authorized to
hold it.

Upon the death of a custodian, the estate trustee or the person who assumed
responsibility for the administration of the estate becomes the custodian,
until custody and control passes to another person who is legally authorized
to hold the records. If another person, for example a trustee in bankruptcy,
obtains complete custody or control of the records as a result of the bankruptcy
or insolvency of the custodian, then that person becomes the custodian. A
custodian may also divest itself of responsibility for records by transferring them
to an archive.

When complete custody or control of the records is transferred to a successor,
then the successor becomes the custodian. The original custodian must
make reasonable efforts to notify the individual to whom the personal health
information relates before the transfer to the successor, or, if that is not
reasonably possible, as soon as possible after transferring the records.

If none of the above conditions apply, then the existing custodian of the records
remains the custodian. PHIPA requires custodians to protect personal health
information and to ensure that records of personal health information are
retained, transferred and disposed of in a secure manner. A custodian remains
responsible for records of personal health information even where the records
are being retained by an agent of the custodian, such as a record storage
company.

For further information, please see the IPC documents How to Avoid Abandoned
Records: Guidelines on the Treatment of Personal Health Information, in the
Event of a Change in Practice and Checklist for Health Information Custodians in
the Event of a Planned or Unforeseen Change in Practice.




Frequently Asked Questions: Personal Health Information Protection Act                15
     CONSENT CONCERNING
     PERSONAL HEALTH
     INFORMATION
     WHAT ARE THE REQUIREMENTS FOR CONSENT?
     The general rule is that a custodian needs to obtain an individual‚Äôs consent to
     collect, use and disclose personal health information, unless PHIPA allows the
     collection, use or disclosure without consent. An individual‚Äôs consent may be
     express or implied. Under PHIPA, regardless of whether it is express or implied,
     consent must be:

        ‚Ä¢ knowledgeable,

        ‚Ä¢ voluntary (not obtained through deception or coercion),

        ‚Ä¢ related to the information in question and

        ‚Ä¢ given by the individual.

     Knowledgeable consent means that it is reasonable in the circumstances to
     believe that an individual knows why a custodian collects, uses and discloses
     their personal health information and that they may give or withhold this consent.

     A custodian may ensure that consent is knowledgeable by posting or making
     readily available a notice that is likely to come to the individual‚Äôs attention,
     describing the purposes for the collection, use and disclosure of personal
     health information.



     WHAT IS THE DIFFERENCE BETWEEN EXPRESS AND
     IMPLIED CONSENT?
     Express consent to the collection, use or disclosure of personal health
     information by a custodian is consent that has been clearly and unmistakably
     given. Express consent may be explicitly provided, either orally or in writing.

     Implied consent to the collection, use or disclosure of personal health
     information is consent that a custodian concludes has been given based on an
     individual‚Äôs action or inaction in particular factual circumstances.




16                                   Frequently Asked Questions: Personal Health Information Protection Act
For example, when an individual discloses their personal health information          What is the
for the purposes of filling out a prescription, a pharmacist can reasonably infer    difference between
consent to the collection of that information.                                       express and
                                                                                     implied consent?

WHEN IS EXPRESS CONSENT REQUIRED?
Subject to very limited exceptions, express consent is required:

     ‚Ä¢ where personal health information is disclosed to a person or an
       organization, such as an insurance company, that is not a custodian and

     ‚Ä¢ where information is disclosed by one custodian to another for a purpose
       other than providing or assisting in providing health care.

Express consent is also required where a custodian:

     ‚Ä¢ collects, uses or discloses personal health information other than an
       individual‚Äôs name and mailing address for fundraising purposes,

     ‚Ä¢ collects, uses or discloses personal health information for marketing or
       marketing research and

     ‚Ä¢ collects, uses or discloses personal information for research purposes,
       unless certain conditions and restrictions are met.



WHEN IS IMPLIED CONSENT SUFFICIENT?
In practice, a custodian is not required to obtain an individual‚Äôs written or
verbal consent every time personal health information is collected, used or
disclosed. Custodians may rely on the implied consent of an individual to collect
and use personal health information for most purposes. They may also infer
consent to disclose personal health information to another custodian for the
purposes of providing or assisting in providing health care. Subject to limited
exceptions, custodians cannot rely on implied consent when disclosing personal
health information to a person or organization that is not a custodian, or when
disclosing personal health information for a purpose other than providing or
assisting in providing health care.

Subject to additional requirements and restrictions, implied consent is permitted
if a custodian collects, uses or discloses names or mailing addresses for the
purposes of fundraising. In addition, if individuals have provided information
about their religious affiliation to a health care facility, the facility may rely
on implied consent to provide the individual‚Äôs name and location within the
facility to a person representing their religious organization. Before making this



Frequently Asked Questions: Personal Health Information Protection Act                      17
   When is implied disclosure, the facility must provide the individual with an opportunity to withhold
consent sufficient? or withdraw consent. A health care facility may also disclose to a person the fact
                   that an individual is a patient or resident in the facility, the individual‚Äôs general
                   health status, and the location of the individual, if the individual is offered the
                   option, at the first reasonable opportunity after admission to the facility, to object
                   to such disclosures and does not do so.

                   PHIPA distinguishes between implied consent and assumed implied consent.
                   In the case of implied consent, custodians must ensure that all the required
                   elements of consent are fulfilled; whereas in the case of assumed implied
                   consent, custodians may assume that all the elements of consent are fulfilled,
                   unless it is not reasonable to do so in the circumstances.



                   WHAT IS THE ‚ÄòCIRCLE OF CARE‚Äô?
                   The ‚Äúcircle of care‚Äù is not a defined term under PHIPA. It is a term of reference
                   used to describe the provisions of PHIPA that enable custodians to rely on
                   an individual‚Äôs assumed implied consent when collecting, using or disclosing
                   personal health information for the purpose of providing or assisting in providing
                   health care. For example:

                      ‚Ä¢ With respect to a physician‚Äôs office, the circle of care may include: the
                        physician, a nurse, a specialist or other health care practitioner referred
                        by the physician and any other health care practitioner selected by the
                        patient, such as a pharmacist or physiotherapist.

                      ‚Ä¢ In the context of a hospital, the circle of care may include: the attending
                        physician and the health care team, for example residents, nurses, clinical
                        clerks and employees assigned to the patient, who have the responsibility
                        of providing care to the individual or assisting with that care. The circle
                        of care could also include, among others, custodians, external to the
                        hospital, who will be involved in providing health care to the patient upon
                        discharge from the hospital.

                   The circle of care does not include:

                      ‚Ä¢ custodians who are not part of the direct or follow-up treatment of an
                        individual and

                      ‚Ä¢ non-custodians, for example, insurance companies.




       18                                           Frequently Asked Questions: Personal Health Information Protection Act
WHEN CAN CUSTODIANS ASSUME IMPLIED CONSENT?
In order to rely on assumed implied consent to collect, use or disclose
personal health information, the custodian must first fall within a category
of custodians that are entitled to rely on assumed implied consent. Most
custodians, such as health care practitioners, hospitals, pharmacies, long-
term care homes and community care access centres, can rely on assumed
implied consent. However, some custodians are not entitled to assume implied
consent. For example, these include:

     ‚Ä¢ an evaluator under the Health Care Consent Act,

     ‚Ä¢ an assessor under the Substitute Decisions Act, 1992,

     ‚Ä¢ the Minister or Ministry of Health and Long-Term Care and

     ‚Ä¢ Canadian Blood Services.

In order for a custodian to rely on assumed implied consent a number of other
requirements must also be fulfilled, including:

     ‚Ä¢ The personal health information to be collected, used or disclosed by the
       custodian must be received from the individual, the substitute-decision
       maker or another custodian.

               o    For example, a custodian may not rely on assumed implied
                    consent if the personal health information was received from an
                    employer, insurance provider or educational institution.

     ‚Ä¢ The custodian must have received the personal health information that is
       being collected, used or disclosed for the purpose of providing or assisting
       in the provision of health care to the individual.

               o    A custodian may not rely on assumed implied consent if the
                    personal health information was received for other purposes such
                    as research, fundraising, marketing or providing or assisting in the
                    provision health care to another individual or group of individuals.

     ‚Ä¢ The purpose of the collection, use or disclosure of personal health
       information by the custodian must be for the provision of health care or
       assisting in the provision of health care to the individual.

               o    A custodian may not rely on assumed implied consent if the
                    collection, use or disclosure is for other purposes, such as
                    research, fundraising, marketing or providing or assisting in
                    the provision of health care to another individual or group of
                    individuals.



Frequently Asked Questions: Personal Health Information Protection Act                     19
         When can       ‚Ä¢ In the context of disclosure, the disclosure of personal health information
custodians assume         by the custodian must be to another custodian.
  implied consent?
                        ‚Ä¢ The custodian that receives the personal health information must not be
                          aware that the individual have expressly withheld or withdrawn consent to
                          the collection, use or disclosure.

                     For further information, please see the IPC guidance document Circle of Care:
                     Sharing Personal Health Information for Health-Care Purposes.



                     ARE PHARMACISTS REQUIRED TO OBTAIN EXPRESS
                     CONSENT FROM AN INDIVIDUAL TO DISCLOSE PERSONAL
                     HEALTH INFORMATION TO A THIRD PARTY BENEFITS
                     PAYOR?
                     No. The regulations provide an exception to the express consent requirement
                     where a pharmacist discloses personal health information to a third party who is
                     not a custodian and who is being asked to provide payment for a medication or
                     related goods or services provided to an individual. Pharmacists are permitted
                     to rely on an individual‚Äôs implied consent as long as they are satisfied that all the
                     required elements of consent are fulfilled.



                     CAN INDIVIDUALS CONTROL WHAT PERSONAL HEALTH
                     INFORMATION IS RECORDED IN THEIR FILE?
                     Yes, but any condition placed on the collection, use or disclosure of personal
                     health information cannot prohibit or restrict the recording of personal health
                     information that is required by law or by established standards of professional or
                     institutional practice.



                     CAN INDIVIDUALS WITHDRAW THEIR CONSENT?
                     Yes. An individual may, with limited exceptions, withdraw consent at any time
                     for the collection, use or disclosure of personal health information by providing
                     notice to the custodian. This applies to implied, as well as express consent.

                     A withdrawal of consent is not retroactive. For example, this means that where
                     a disclosure has been made on the basis of consent, the custodian is not
                     required to retrieve the information that has already been disclosed. However,
                     the custodian must stop disclosing the personal health information as soon as


        20                                            Frequently Asked Questions: Personal Health Information Protection Act
the notice of withdrawal is received. A withdrawal of consent would not apply        Can individuals
to a collection or use that had already occurred prior to receiving the notice of    withdraw their
withdrawal. It would only apply to new collections of personal health information    consent?
and future uses for the purpose of which the consent was initially obtained.



WHAT IS A ‚ÄòLOCK-BOX‚Äô?
The ‚Äúlock-box‚Äù is not a defined term under PHIPA. It is a term commonly used
to describe the right of individuals to withhold or withdraw their consent to the
collection, use or disclosure of their personal health information for health care
purposes. Individuals may expressly instruct custodians not to use or disclose
their personal health information for health care purposes without consent,
where PHIPA would otherwise permit a use or disclosure for such a purpose.

The withholding or withdrawal of consent or the express instructions may take
various forms, including communications from individuals to custodians: not to
collect, use or disclose a particular item of personal health information, such
as a specific diagnosis, for health care purposes, not to collect, use or disclose
the contents of their entire record of personal health information for health care
purposes and/or not to use and/or disclose their personal health information
to a particular custodian, a particular agent of a custodian or to a class of
custodians or agents, for example, physicians, nurses or social workers, for
health care purposes.



WHAT ARE THE RESTRICTIONS AND LIMITATIONS ON THE
LOCK-BOX?
Once an individual locks personal health information, a custodian subject to
the withdrawal or withholding of consent or express instruction cannot collect,
use or disclose, as the case may be, that personal health information for health
care purposes, unless the individual provides express consent and informs the
custodian accordingly or unless PHIPA otherwise permits the collection, use or
disclosure to be made without consent. For example, the custodian is permitted
to disclose the locked personal health information where the custodian believes,
on reasonable grounds, that the disclosure is necessary for the purpose of
eliminating or reducing a significant risk of serious bodily harm to an individual
or a group of persons.

Further, an individual‚Äôs restriction may not impede a custodian from recording
personal health information about an individual that is required by law or by
established standards of professional or institutional practice.




Frequently Asked Questions: Personal Health Information Protection Act                       21
     What are the    If a custodian discloses an individual‚Äôs personal health information to another
  restrictions and   custodian for the provision of health care and the disclosing custodian does not
limitations on the   have consent to disclose all the personal health information that it considers
        lock-box?    reasonably necessary for that purpose, the disclosing custodian must notify the
                     receiving custodian of that fact. The receiving custodian would then be able to
                     explore the matter of the locked personal health information with the individual
                     and seek their express consent to access the locked information.

                     For further information, please see the IPC fact sheet, Lock-Box Fact Sheet.



                     WHAT HAPPENS WHEN AN INDIVIDUAL IS INCAPABLE OF
                     PROVIDING CONSENT?
                     Under PHIPA, individuals are presumed to be capable of making their own
                     decisions regarding the collection, use or disclosure of their personal health
                     information. Individuals are capable of consent if they are able to understand
                     information relevant to deciding whether to consent to the collection, use
                     or disclosure of their personal health information, and to appreciate the
                     reasonably foreseeable consequences of giving, not giving, withholding or
                     withdrawing their consent.

                     If a custodian believes that an individual is incapable of providing consent, PHIPA
                     permits a substitute decision-maker, such as a relative, spouse, child‚Äôs parent, or
                     the Public Guardian and Trustee, to make a decision on an individual‚Äôs behalf.

                     PHIPA lists, in order of priority, the following substitute decision-makers who
                     may consent on behalf of an individual when consent is required, including:

                        ‚Ä¢ a substitute decision-maker within the meaning of section 9, section
                          39 and section 56 of the Health Care Consent Act, if the purpose of the
                          collection, use or disclosure is necessary for, or ancillary to, a decision
                          about a treatment under Part II, a decision about admission to a care
                          facility under Part III or a decision about a personal assistance service
                          under Part IV of the Health Care Consent Act respectively,,

                        ‚Ä¢ the attorney for personal care or for property,

                        ‚Ä¢ the representative appointed by the Consent and Capacity Board,

                        ‚Ä¢ the spouse or partner,

                        ‚Ä¢ a child or parent, including a children‚Äôs aid society,

                        ‚Ä¢ a parent who has a right of access,




       22                                            Frequently Asked Questions: Personal Health Information Protection Act
     ‚Ä¢ a sibling,

     ‚Ä¢ a relative and if no other person meets the requirements

     ‚Ä¢ the Public Guardian and Trustee.



CAN A CHILD UNDER 16 YEARS OLD PROVIDE CONSENT?
A custodian may obtain consent for the collection, use and disclosure of
personal health information from a capable child, regardless of age. As
discussed above, individuals are capable of consent if they are able to
understand information relevant to deciding whether to consent to the collection,
use or disclosure of their personal health information, and to appreciate the
reasonably foreseeable consequences of giving, not giving, withholding or
withdrawing their consent.

If the child is less than 16 years old, a parent of the child or a children‚Äôs aid
society or other person who is lawfully entitled to give or refuse consent in the
place of the parent may also give, withhold or withdraw consent. However, this
does not apply in the context of information that relates to treatment within
the meaning of the Health Care Consent Act, about which children have made
a decision on their own, or counselling in which children have participated on
their own under the Child and Family Services Act. A parent does not include a
parent who has only a right of access to the child. If there is a conflict between
a capable child who is less than 16 years old, and the person who is entitled to
act as the child‚Äôs substitute decision-maker, the decision of the capable child
regarding giving, withholding or withdrawing consent prevails.



CAN ANOTHER PERSON, SUCH AS A FAMILY MEMBER,
PROVIDE CONSENT ON AN INDIVIDUAL‚ÄôS BEHALF WHEN
PICKING UP OR DROPPING OFF A PRESCRIPTION?
Yes. The regulations permit a pharmacist to provide a prescription to another
person. This is also permitted under the Drug and Pharmacies Regulation Act.




Frequently Asked Questions: Personal Health Information Protection Act               23
     COLLECTION, USE AND
     DISCLOSURE OF PERSONAL
     HEALTH INFORMATION
     WHAT ARE THE GENERAL LIMITATIONS ON THE
     COLLECTION, USE AND DISCLOSURE OF PERSONAL
     HEALTH INFORMATION?
     A custodian is prohibited from collecting, using or disclosing personal health
     information unless consent has been obtained and the collection, use and
     disclosure is, to the best of the custodian‚Äôs knowledge, necessary for a lawful
     purpose or is permitted or required by PHIPA.

     According to PHIPA, a custodian must not collect, use or disclose personal
     health information if other information will serve the purpose of the collection,
     use or disclosure. For example, a custodian may be able to provide a researcher
     conducting a study with de-identified information, rather than disclosing
     personal health information. A custodian must not collect, use or disclose more
     personal health information than is reasonably necessary to meet the purpose
     of the collection, use or disclosure. For example, if a patient requests a doctor‚Äôs
     note to give to the employer, the doctor should only include the minimum
     information necessary, rather than the patient‚Äôs entire health history.



     COLLECTION

     WHAT IS A COLLECTION OF PERSONAL HEALTH INFORMATION
     UNDER PHIPA?

     PHIPA defines the term ‚Äúcollect‚Äù as the gathering, acquiring, receiving or
     obtaining of personal health information by any means from any source. This
     means that personal health information can be collected by a custodian or an
     authorized agent under PHIPA in several ways, such as when a doctor makes
     notes about a patient or when a pharmacist receives a prescription to be filled.




24                                   Frequently Asked Questions: Personal Health Information Protection Act
WHAT ARE THE RULES REGARDING THE COLLECTION OF PERSONAL
HEALTH INFORMATION?

As a general rule, consent is required for any collection of an individual‚Äôs
personal health information, unless PHIPA allows the collection without consent.
Custodians within the ‚Äúcircle of care‚Äù may rely on an individual‚Äôs implied
consent, or assumed implied consent if the requirements are fulfilled, to collect
personal health information for the purpose of providing health care.

With limited exceptions, custodians must collect personal health information
directly from the individual involved. Custodians must not collect personal health
information if other information will serve the purpose of the collection and may
only collect as much information as is necessary to meet the purpose of collection.


WHEN CAN CUSTODIANS INDIRECTLY COLLECT PERSONAL HEALTH
INFORMATION?

Custodians may collect personal health information indirectly where, for
example:

     ‚Ä¢ the individual consents,

     ‚Ä¢ the collection is necessary for providing health care and it is not possible
       to collect personal health information directly from the individual that can
       be relied on as accurate and complete,

     ‚Ä¢ the collection is necessary for providing health care and it is not possible
       to collect personal health information directly from the individual in a timely
       manner,

     ‚Ä¢ the custodian collects personal health information for the purposes of
       research from a person who is not a custodian, provided that certain
       conditions are met,

     ‚Ä¢ the indirect collection is required or permitted by law,

     ‚Ä¢ the custodian is a prescribed entity and is collecting personal health
       information from a person who is not a custodian for the purposes of the
       planning and management of the health system or

     ‚Ä¢ the IPC authorizes the indirect collection.




Frequently Asked Questions: Personal Health Information Protection Act                   25
     USE

     WHAT IS A USE OF PERSONAL HEALTH INFORMATION UNDER PHIPA?

     The term ‚Äúuse‚Äù in relation to personal health information in the custody or under
     the control of a custodian or a person, is defined under PHIPA as meaning to
     handle or deal with personal health information, but does not include to disclose
     the information. Where a custodian is authorized to use the information, the
     custodian may provide the information to an agent of the custodian to use it for
     that purpose on behalf of the custodian. The sharing of information between
     a custodian and its agent is considered to be a use and not a disclosure or a
     collection for the purposes of PHIPA. Order HO-013 found that handling and
     dealing with personal health information includes accessing/viewing personal
     health information.


     WHAT ARE THE RULES REGARDING THE USE OF PERSONAL HEALTH
     INFORMATION?

     As a general rule, consent is required for any use of an individual‚Äôs personal
     health information, unless PHIPA allows the use without consent. Custodians
     must not use personal health information if other information will serve the
     purpose of the use, and may only use as much information as is necessary to
     meet the purpose of the use of personal health information.

     When using personal health information, a custodian must take reasonable
     steps to ensure that the individual‚Äôs personal health information is as accurate,
     complete and up-to-date as is necessary for the purposes for which the
     custodian uses the information.


     WHEN CAN PERSONAL HEALTH INFORMATION BE USED WITHOUT
     CONSENT?

     PHIPA sets out a limited set of acceptable uses of personal health information
     without consent, including, for example, the following purposes:

        ‚Ä¢ planning or delivering programs or services,

        ‚Ä¢ risk management, error management or activities to improve or maintain
          the quality of care or any related program or service,

        ‚Ä¢ educating agents to provide health care,



26                                   Frequently Asked Questions: Personal Health Information Protection Act
     ‚Ä¢ obtaining payment or processing, monitoring, verifying or reimbursing           When can personal
       health care claims,                                                             health information
                                                                                       be used without
     ‚Ä¢ research, provided that specific requirements and conditions are met and
                                                                                       consent?
     ‚Ä¢ If permitted or required by law.

A custodian may provide personal health information to an agent of the
custodian for any of these purposes.



DISCLOSURE

WHAT IS A DISCLOSURE OF PERSONAL HEALTH INFORMATION UNDER
PHIPA?

The term ‚Äúdisclose‚Äù in relation to personal health information in the custody or
under the control of a custodian or a person, is defined under PHIPA as meaning
to make the personal health information available or to release it to another
custodian or person. It does not include providing personal health information
back to the person who provided it or disclosed it in the first place, whether or
not the personal health information has been manipulated or altered, as long as
it does not include additional identifying information.


WHAT ARE THE RULES REGARDING THE DISCLOSURE OF PERSONAL
HEALTH INFORMATION?

As a general rule, consent is required to disclose an individual‚Äôs personal health
information, unless PHIPA allows the disclosure without consent. Custodians
must not disclose personal health information if other information will serve the
purpose of the disclosure, and may only disclose as much information as is
necessary to meet the purpose.

A custodian and its authorized agents may rely on implied consent, or assumed
implied consent if the requirements are fulfilled, for the disclosure of personal
health information within the ‚Äúcircle of care‚Äù while providing health care, as long
as the disclosure is reasonably necessary for the provision of health care, and
the individual has not expressly withheld or withdrawn consent.

PHIPA permits custodians to disclose personal health information in certain
limited situations. However, simply because a disclosure is permitted does not
mean it is mandatory, unless it is necessary to carry out a statutory or legal duty.




Frequently Asked Questions: Personal Health Information Protection Act                         27
    What are the    Unless permitted or required by law, express consent is generally required when
 rules regarding    personal health information is disclosed by a custodian to a non-custodian,
the disclosure of   where a custodian discloses to another custodian for a purpose other than
 personal health    for health care or for market research, unless specific conditions are met and
    information?    fundraising, if more than contact information is provided.

                    When disclosing personal health information, the custodian should take care to
                    ensure that no information is inadvertently disclosed to third parties.


                    WHEN CAN PERSONAL HEALTH INFORMATION BE DISCLOSED WITHOUT
                    CONSENT?

                    PHIPA recognizes the need for a flexible approach to regulating information
                    exchanges between custodians in order to ensure the effective and efficient
                    operation of the health system. Consequently, custodians may disclose personal
                    health information without an individual‚Äôs consent in certain circumstances.
                    While these disclosures without consent are permitted by PHIPA, they are not
                    mandatory, unless they are necessary to carry out a statutory or legal duty.
                    Examples of permitted disclosures of personal health information without
                    consent include:

                       ‚Ä¢ if the disclosure is reasonably necessary for providing health care and
                         consent cannot be obtained in a timely manner, unless there is an express
                         request from the individual instructing otherwise,

                       ‚Ä¢ in order for the Minister of Health and Long-Term Care to provide funding
                         to the custodian for the provision of health care,

                       ‚Ä¢ for the purpose of contacting a relative or friend or potential substitute
                         decision-maker of an individual who is injured, incapacitated or ill and
                         unable to give consent personally,

                       ‚Ä¢ to inform any person that an individual is a patient or resident in a facility,
                         the individual‚Äôs general health status, and the location of the individual
                         in the facility, unless there is an express request from the individual
                         instructing otherwise,

                       ‚Ä¢ to eliminate or reduce a significant risk of serious bodily harm to a person
                         or group of persons,

                       ‚Ä¢ when transferring records to the archives for conservation,

                       ‚Ä¢ for the purpose of carrying out an inspection, investigation or similar
                         procedure that is authorized by a warrant, PHIPA or another Act,




      28                                            Frequently Asked Questions: Personal Health Information Protection Act
     ‚Ä¢ for determining or verifying eligibility for publicly funded health care or       When can personal
       related goods, services or benefits,                                              health information
                                                                                         be disclosed
     ‚Ä¢ for the purpose of administration and enforcement of various Acts by the
                                                                                         without consent?
       professional Colleges and other regulatory bodies,

     ‚Ä¢ to a prescribed person, listed in the regulations, that compiles and
       maintains a registry of personal health information for the purposes of
       facilitating or improving the provision of health care or that relates to the
       storage or donation of body parts or bodily substances,

     ‚Ä¢ to a prescribed entity, listed in the regulations, for the purpose of analysis
       or compiling information with respect to the management, evaluation or
       monitoring of the health system,

     ‚Ä¢ to the Public Guardian and Trustee, a children‚Äôs aid society and the
       Children‚Äôs Lawyer for the purpose of carrying out their statutory functions,

     ‚Ä¢ to a person conducting an audit or reviewing an accreditation or
       application for accreditation related to the services of a custodian,

     ‚Ä¢ for the purpose of legal proceedings, in specific circumstances,

     ‚Ä¢ for the purpose of research, subject to restrictions and conditions and

     ‚Ä¢ for any purpose as required or permitted by law.


CAN PERSONAL HEALTH INFORMATION BE DISCLOSED IN THE EVENT
OF AN EMERGENCY?

PHIPA does not prevent the rapid sharing of personal health information in
certain situations. PHIPA is not intended to stand in the way of the disclosure
of vital‚Äîand in some cases, life-saving‚Äîinformation in emergency or critical
situations affecting individuals or public health and safety, as well as in situations
that call for compassion.

Personal health information may be disclosed without consent if the custodian
believes on reasonable grounds that the disclosure is necessary for eliminating
or reducing a significant risk of serious bodily harm to a person or group
of persons. For example, a psychologist at a university could disclose a
student‚Äôs personal health information to the student‚Äôs family or physician, if the
psychologist believes it is necessary to reduce the risk of suicide. PHIPA also
allows a custodian to disclose personal health information without consent in
order to contact a relative, friend or potential substitute decision-maker if an
individual is injured, incapacitated or ill and unable to consent.




Frequently Asked Questions: Personal Health Information Protection Act                           29
     Can personal    Custodians may also disclose personal health information if permitted or
health information   required by another law. For example, the Health Protection and Promotion Act
   be disclosed in   (HPPA) requires certain custodians to report diseases defined as reportable,
   the event of an   communicable or virulent to the Medical Officer of Health. PHIPA also provides
      emergency?     that custodians may disclose personal health information without consent to
                     the Chief Medical Officer of Health or to a local medical officer of health for
                     the purposes of the HPPA and to the Ontario Agency for Health Protection and
                     Promotion for the purposes of the Ontario Agency for Health Protection and
                     Promotion Act. For example, a custodian may report an outbreak of a suspicious
                     condition that is not identified as a reportable, communicable or virulent disease,
                     but which the custodian believes could be dangerous.

                     For further information, please see the IPC fact sheet, Disclosure of Information
                     Permitted in Emergency or other Urgent Circumstances.


                     DOES PHIPA PERMIT DISCLOSURE OF PERSONAL HEALTH INFORMATION
                     ABOUT A DECEASED INDIVIDUAL?

                     PHIPA permits the disclosure of personal health information about a deceased
                     individual in certain circumstances. A custodian may only disclose personal
                     health information to a person who is not a custodian, such as a relative of a
                     deceased individual, if the individual whose personal health information is at
                     issue, has given express consent or the disclosure is permitted or required by
                     PHIPA or another law. In the case of a deceased individual, the consent may be
                     given by the deceased individual‚Äôs substitute decision-maker. This means that if
                     the substitute decision-maker consents, the personal health information may be
                     disclosed to a relative of the deceased individual.

                     PHIPA permits, but does not require, a custodian to disclose personal health
                     information without consent for the purposes of identifying the individual or
                     for informing people whom it is reasonable to inform, that the individual is
                     deceased or reasonably suspected to be deceased and the circumstances of
                     death, where appropriate. PHIPA also permits disclosure to the spouse, partner,
                     sibling or child of the deceased individual if the recipients reasonably require the
                     information to make decisions about their own, or their children‚Äôs health care.

                     For further information, please see the IPC fact sheet, Obtaining Personal Health
                     Information About a Deceased Relative.




       30                                            Frequently Asked Questions: Personal Health Information Protection Act
CAN A CUSTODIAN DISCLOSE PERSONAL HEALTH INFORMATION TO
THE WORKPLACE SAFETY AND INSURANCE BOARD (WSIB) ABOUT AN
INJURED WORKER WITHOUT THE INDIVIDUAL‚ÄôS CONSENT?

Yes. PHIPA permits the disclosure of personal health information without
consent, if permitted or required by another law. For example, this means that
PHIPA does not interfere with the Workplace Safety and Insurance Act (Act),
where that Act requires a hospital or health facility, which provides health care
to a worker claiming benefits under the insurance plan, to give the WSIB such
information relating to the worker as the WSIB may require. This requirement also
applies to a health care practitioner who provides health care to a worker or is
consulted with respect to a worker‚Äôs health care. When requested to do so by an
injured worker or the employer, the Act requires a health care practitioner treating
the worker to give the WSIB, the worker and the employer prescribed information
concerning the worker‚Äôs functional abilities.

PHIPA also does not interfere with the Occupational Health and Safety Act,
which sets out an employer‚Äôs and supervisor‚Äôs duty, subject to specific
limitations, to provide a worker with information, including personal
information, related to a risk of workplace violence from a person with a history
of violent behaviour.


CAN A CUSTODIAN STORE, ACCESS OR DISCLOSE PERSONAL HEALTH
INFORMATION OUTSIDE OF ONTARIO?

PHIPA does not require that personal health information be retained and stored
in Ontario or Canada. There is no legislative prohibition on storing and accessing
personal health information outside of Ontario. For example, a custodian may
decide to outsource the storage of personal health information to a service
provider in another jurisdiction. However, the custodian is ultimately accountable
for the actions of its agent and must be satisfied that appropriate administrative,
physical and technical safeguards are in place, for example, through contractual
arrangements.

PHIPA permits disclosures of personal health information to a person outside
of Ontario in certain situations. A custodian may disclose personal health
information about an individual collected in Ontario to a person outside of
Ontario if:

     ‚Ä¢ the individual consents to the disclosure,

     ‚Ä¢ PHIPA permits the disclosure,




Frequently Asked Questions: Personal Health Information Protection Act                 31
   Can a custodian    ‚Ä¢ the person receiving the information performs functions comparable to
   store, access or     the functions of certain persons to whom the disclosure in Ontario is
 disclose personal      permitted,
health information
outside of Ontario?   ‚Ä¢ the custodian is a prescribed entity and the disclosure is for the purpose of
                        health planning or health administration, the information relates to health
                        care provided in Ontario to a person who is a resident of another province
                        or territory of Canada and the disclosure is made to the government of that
                        province or territory,

                      ‚Ä¢ the disclosure is reasonably necessary for the provision of health care to
                        the individual and the individual has not expressly instructed the custodian
                        not to make the disclosure or

                      ‚Ä¢ the disclosure is reasonably necessary for the administration of payments
                        in connection with the provision of health care to the individual or for
                        contractual or legal requirements in that connection.




        32                                        Frequently Asked Questions: Personal Health Information Protection Act
FUNDRAISING AND MARKETING
CAN CUSTODIANS COLLECT, USE OR DISCLOSE
PERSONAL HEALTH INFORMATION FOR FUNDRAISING
ACTIVITIES?
The regulations contain specific requirements and restrictions that apply to all
collections, uses and disclosures of personal health information for fundraising,
including the following:

     ‚Ä¢ The collection, use or disclosure of personal health information for
       fundraising purposes is only permitted where the fundraising relates to a
       charitable or philanthropic purpose related to the custodian‚Äôs operations.

     ‚Ä¢ All solicitations must contain an easy opt-out from any further solicitations.

     ‚Ä¢    No solicitations may contain information about an individual‚Äôs health care
         or state of health.

In general, custodians are only permitted to collect, use or disclose personal
health information for purposes that are not related to the provision of health
care with the express consent of the individual in question. However, PHIPA and
its regulations provide that a collection, use or disclosure of an individual‚Äôs name
and mailing address (or the name and mailing address of a substitute decision-
maker, if applicable) for fundraising may take place with the implied consent of
the individual in question, as long as the following requirements are met:

     ‚Ä¢ at the time the service has been provided to the individual, the custodian
       has posted, or has made available to the individual, a notice informing the
       individual of the custodian‚Äôs intention to use or disclose the information
       for fundraising purposes, along with information on how the individual can
       easily opt out and

     ‚Ä¢ the individual had not opted out within 60 days from the time the notice
       had been provided.

For personal health information collected before November 1, 2004, a custodian
may assume implied consent to use or disclose an individual‚Äôs name and contact
information for fundraising, unless the custodian is aware that the individual has
expressly withheld or withdrawn consent.

For further information please see the IPC fact sheet, Fundraising under PHIPA.




Frequently Asked Questions: Personal Health Information Protection Act                  33
     CAN PERSONAL HEALTH INFORMATION BE COLLECTED,
     USED OR DISCLOSED FOR MARKETING PURPOSES?
     A custodian can only collect, use or disclose personal health information about
     an individual for market research or for marketing purposes with the express
     consent of the individual.

     Note that the following activities are excluded from the definition of marketing:

        ‚Ä¢ communications by health care practitioners about the availability of
          non-OHIP covered charges for a block fee or on the basis of a set fee for
          service and

        ‚Ä¢ communications by Canadian Blood Services for recruiting donors of
          blood and blood products.




34                                   Frequently Asked Questions: Personal Health Information Protection Act
RESEARCH
WHAT ARE THE REQUIREMENTS FOR THE COLLECTION,
USE AND DISCLOSURE OF PERSONAL HEALTH
INFORMATION FOR RESEARCH?
The general rule is that a custodian needs to obtain an individual‚Äôs consent to
collect, use and disclose personal health information, unless PHIPA allows the
collection, use or disclosure without consent. In recognizing the importance
of health research, PHIPA permits the collection, use or disclosure of personal
health information for research purposes without an individual‚Äôs consent, if strict
conditions are met.

For example, a custodian who uses personal health information for research
and, similarly, a researcher who seeks disclosure of personal health information
for research purposes, must both submit a detailed research plan to a Research
Ethics Board (REB) for approval. When deciding whether to approve a research
plan involving the use or disclosure of personal health information without
consent, a REB must consider:

     ‚Ä¢ whether the research can be reasonably accomplished without using the
       personal health information,

     ‚Ä¢ the public interest in conducting the research and in protecting privacy,

     ‚Ä¢ whether obtaining consent is impractical and

     ‚Ä¢ whether adequate safeguards will be in place to protect the privacy of
       individuals and the confidentiality of their personal health information.

A researcher requesting disclosure of personal health information from a
custodian must submit to the custodian a written application, a research plan
and a copy of the decision approving the research plan by a REB. In addition,
the custodian must enter into an agreement with the researcher that may impose
further restrictions, including the manner in which the researcher may use and
disclose the personal health information.

A researcher with an approved research plan who receives personal health
information from a custodian shall:

     ‚Ä¢ comply with the conditions, if any, imposed by the REB,




Frequently Asked Questions: Personal Health Information Protection Act                35
       What are the      ‚Ä¢ use personal health information only for the purpose set out in the
  requirements for         research plan,
the collection, use
                         ‚Ä¢ not publish information in a form that could reasonably enable a person to
 and disclosure of
   personal health         identify the individual,
    information for      ‚Ä¢ not disclose information unless required by law or if the disclosure is to
         research?         prescribed persons or registries,

                         ‚Ä¢ not attempt to contact the individual whose personal information is the
                           subject of the research project unless the custodian obtains the consent of
                           that individual,

                         ‚Ä¢ notify the custodian in writing of any breaches of either the agreement or
                           PHIPA and

                         ‚Ä¢ comply with the agreement between the researcher and the custodian.

                      Researchers are permitted to disclose personal health information to another
                      researcher or to a prescribed person or a prescribed entity if the disclosure
                      is either part of a research plan approved by a REB, or it is necessary for the
                      purpose of verifying or validating the information or the research.



                      ARE THERE ANY REQUIREMENTS FOR RESEARCH ETHICS
                      BOARDS AND RESEARCH PLANS?
                      Yes. The regulations specify that a REB must have at least five members,
                      including:

                         ‚Ä¢ a member who has no affiliation to the person who established the REB,

                         ‚Ä¢ a member who has knowledge in privacy issues,

                         ‚Ä¢ a member who has knowledge in research ethics and

                         ‚Ä¢ at least two members with expertise in the methods or the relevant areas
                           of research.

                      In addition, the regulations list a number of requirements that research plans
                      must include. For example, a research plan must include a description of why
                      consent to the disclosure of personal health information is not being sought
                      from the individual to whom the information relates; a description of how the
                      information will be used, the safeguards the researcher will put in place to
                      protect the confidentiality and security of the information and a description of all
                      persons who will have access to the information.




        36                                            Frequently Asked Questions: Personal Health Information Protection Act
ONTARIO HEALTH CARDS AND
HEALTH NUMBERS
WHO CAN COLLECT, USE OR DISCLOSE ONTARIO HEALTH
NUMBERS AND UNDER WHAT CIRCUMSTANCES?
Custodians and certain persons or organizations prescribed in the regulations
are permitted to collect, use or disclose Ontario health numbers.

A person or organization that is not a custodian or an agent of a custodian may
only collect or use an Ontario health number for the following purposes:

     ‚Ä¢ for purposes related to the provision of provincially funded health
       resources,

     ‚Ä¢ for the purposes for which the custodian disclosed the number,

     ‚Ä¢ for purposes related to the duties or powers of a governing body of health
       care practitioners who provide provincially funded health resources or

     ‚Ä¢ for health administration, health planning or health research or
       epidemiological studies by persons listed in the regulations including, the
       WSIB, prescribed persons and prescribed entities.

An organization or person who is not a custodian or agent of a custodian
may not disclose a health number, except as set out in the regulations or as
required by law.

These restrictions on the collection, use and disclosure of health numbers do not
apply to:

     ‚Ä¢ a person who collects, uses or discloses health numbers for a proceeding,

     ‚Ä¢ a prescribed entity or

     ‚Ä¢ the individual or the individual‚Äôs substitute decision-maker.



ARE OTHER ORGANIZATIONS PERMITTED TO REQUEST
THE PRODUCTION OF A HEALTH CARD?
PHIPA states that only a person who provides a provincially funded health care
resource may require the production of an individual‚Äôs health card.



Frequently Asked Questions: Personal Health Information Protection Act               37
      Are other   However, there is nothing in PHIPA that prevents an organization from requesting
 organizations    a health card, as long as it is made clear that disclosure is voluntary and the
   permitted to   information will only be used for purposes directly related to the provision of
    request the   provincially funded health resources. For instance, an employer may allow an
production of a   employee to voluntarily provide a health card in order to expedite the provision of
   health card?   health care services in the event of an emergency. A school, day care, or camp
                  may also request a child‚Äôs health number so that it is on record in the event of a
                  medical emergency.

                  Please note that any such disclosure must be voluntary, and non-custodians may
                  not require the production of health cards. It is an offence under PHIPA for any
                  organization to wilfully collect, use or disclose any personal health information‚Äî
                  including health numbers‚Äîin a manner that contravenes PHIPA.

                  An organization, for example a private sector business not directly involved
                  in the delivery of provincially funded health services, is not permitted to take
                  note of, record, collect, or use a health number for identification purposes.
                  However, nothing prevents individuals from voluntarily choosing to show their
                  health cards in order to verify identity. For example, individuals may voluntarily
                  decide to provide their health cards to librarians in order to confirm their identity
                  and obtain a library card. The librarians may view the health card, but are not
                  permitted to record the health number.

                  For further information, please see the IPC document, Frequently Asked
                  Questions: Health Cards and Health Numbers.




    38                                             Frequently Asked Questions: Personal Health Information Protection Act
ACCESS TO RECORDS
OF PERSONAL HEALTH
INFORMATION AND CORRECTION
ACCESS

ARE INDIVIDUALS PERMITTED TO ACCESS THEIR OWN PERSONAL
HEALTH INFORMATION?

With limited exceptions, PHIPA provides individuals with a general right to
access their personal health information held by a custodian and sets out a
formal procedure for access requests. The right of access does not apply to:

     ‚Ä¢ records that contain quality of care information,

     ‚Ä¢ personal health information required for quality assurance programs,

     ‚Ä¢ raw data from psychological tests or assessments,

     ‚Ä¢ personal health information used solely for research purposes or

     ‚Ä¢ personal health information that is in the custody or under the control of a
       laboratory in respect of a test, where an individual has the right of access
       to that information from the health care practitioner and the practitioner
       has not directed the laboratory to provide the information directly to the
       individual.

As previously noted, personal health information includes identifying information
that is not personal health information, but that is contained in a record that
contains personal health information. Personal health information does not
include identifying information if the information relates primarily to one or
more employees or other agents of the custodian and the record is maintained
primarily for a purpose other than the provision or assisting in the provision of
health care.

For further information please see the IPC fact sheet, Your health information:
Your access and correction rights.




Frequently Asked Questions: Personal Health Information Protection Act                39
     HOW DO AN INDIVIDUAL OBTAIN ACCESS TO THEIR PERSONAL HEALTH
     INFORMATION?

     An individual may exercise a right of access to a record of personal health
     information by making a written request for access to the custodian that has
     custody or control of the information.

     The custodian should then either make the record available for examination
     or provide a copy of the record. Otherwise, the custodian must give a written
     notice to the individual seeking access stating that, after a reasonable search,
     the record does not exist, cannot be found or is not a record to which access
     applies. If the custodian is entitled to refuse the request, in whole or in part, the
     custodian must give a written notice stating that the request is being refused and
     providing reasons for the refusal. The notice must also state that the individual is
     entitled to make a complaint about the refusal to the IPC. If an individual decides
     to complain to the IPC, the complaint must be in writing.

     The request must contain sufficient detail to allow the custodian to locate the
     record in question. Where the individual has not provided sufficient detail to
     enable the custodian to identify and locate the record, the custodian is required
     to assist the individual in reformulating the request.

     Nothing in PHIPA prevents a custodian from granting an individual access to
     a record of personal health information if the individual makes an oral request
     for access.

     For further information, please see the IPC‚Äôs PHIPA Practice Directions,
     Clarifying Access Requests and Drafting a Letter Responding to a Request for
     Access to Personal Health Information.


     HOW LONG DOES A CUSTODIAN HAVE TO RESPOND TO AN INDIVIDUAL‚ÄôS
     REQUEST FOR ACCESS TO PERSONAL HEALTH INFORMATION?

     A custodian must respond no later than 30 calendar days after the request was
     made.

     Extensions of up to a maximum of 30 additional calendar days are allowed,
     where meeting this time frame would unreasonably interfere with the custodian‚Äôs
     operations, or where the necessary consultations would not make it reasonably
     practical to reply within that time frame. In such situations, the custodian must
     inform the individual in writing of the extension and set out the length of the
     extension and the reasons for the extension.




40                                   Frequently Asked Questions: Personal Health Information Protection Act
CAN A CUSTODIAN REFUSE TO PROVIDE ACCESS TO AN INDIVIDUAL‚ÄôS
PERSONAL HEALTH INFORMATION?

Generally, custodians are responsible for providing individuals with access to
their records of personal health information.

Custodians may only refuse access in limited situations, including:

     ‚Ä¢ the information in question is subject to a legal privilege,

     ‚Ä¢ access could reasonably be expected to result in a risk of serious harm
       to the treatment or recovery of the individual or serious bodily harm to the
       individual or another person,

     ‚Ä¢ the information was collected in the course of an inspection, investigation
       or similar procedure and the resulting proceedings, appeals or processes
       have not yet been concluded or

     ‚Ä¢ another law prohibits the disclosure of that information.

If an exception applies, an individual still has a right of access to the part of the
record that can reasonably be severed from the part containing the information
to which the individual does not have the right of access. If a custodian denies
an individual access to the personal health information, the individual has the
right to file a written complaint with the IPC.


IS THERE A FEE ASSOCIATED WITH AN ACCESS REQUEST?

Custodians may charge a reasonable fee for providing access to an individual‚Äôs
records of personal health information. PHIPA also permits a custodian to waive
all or part of the fee associated with an access request. In charging a fee, PHIPA
requires custodians to first provide the individual with a fee estimate. The fee
amount must not exceed the prescribed amount set out in the regulations, if any,
or the amount of reasonable cost recovery.

There is currently no regulation prescribing the fee for providing access to an
individual‚Äôs records of personal health information. Order HO-009 found that a
custodian may charge a set fee of $30 for photocopying or printing the first 20
pages of a record and 25 cents per page for every additional page. The set fee
of $30 also includes additional activities, for example, locating and retrieving the
record, reviewing the contents of the record for not more than 15 minutes and
preparing a response letter to the individual.




Frequently Asked Questions: Personal Health Information Protection Act                  41
     WHAT IF THE CUSTODIAN WORKS FOR A NON-CUSTODIAN THAT IS
     COVERED UNDER PUBLIC SECTOR ACCESS AND PRIVACY LEGISLATION,
     SUCH AS A SCHOOL BOARD OR MUNICIPALITY?

     The provisions of PHIPA regarding access to, and correction of, personal health
     information do not apply to records in the custody or under the control of a
     health care practitioner who is employed by or acting for an institution within
     the meaning of the Freedom of Information and Protection of Privacy Act,
     which covers provincial ministries and most provincial boards, agencies and
     commissions, or the Municipal Freedom of Information and Protection of Privacy
     Act, which covers local government organizations such as municipalities, police,
     school, health and library boards, if the individual has the right to request access
     under either of those Acts. In that case, the individual would submit an access
     request under FIPPA or MFIPPA, as applicable. For example, if an individual
     wants to access personal health information compiled by a psychologist who
     works for a school board, the individual should make the request to the freedom
     of information coordinator of the custodian‚Äôs institution, for example the school
     board, in accordance with MFIPPA, rather than directly to the custodian.

     If the custodian works for a non-custodian that is not covered under FIPPA or
     MFIPPA, for example a private sector organization, the provisions of PHIPA
     would apply. In that case, the individual would submit an access request under
     PHIPA directly to the custodian.



     CORRECTION

     CAN INDIVIDUALS CORRECT ERRORS IN THEIR PERSONAL HEALTH
     INFORMATION?

     An individual who believes that personal health information is incomplete
     or inaccurate may request that a custodian correct the record. It is the
     responsibility of the custodian to ensure that personal health information is
     complete and accurate.

     For further information please see the IPC fact sheet, Your health information:
     Your access and correction rights.




42                                   Frequently Asked Questions: Personal Health Information Protection Act
HOW DOES AN INDIVIDUAL CORRECT ERRORS?

An individual seeking a correction to personal health information may submit
a request to the custodian who has custody or control of the records. The
custodian is permitted to make a correction based on an oral request; however,
the custodian may require that the request be made in writing.

The custodian must respond within 30 days of receiving a written correction
request. PHIPA provides limited grounds for extending this 30-day time frame.
Extensions of up to a maximum of 30 additional days are allowed, where replying
within 30 days would unreasonably interfere with the custodian‚Äôs operations, or
where the necessary consultations would not make it reasonably practical to
reply within that time frame. In such situations, the custodian must inform the
individual in writing of the extension and set out the length of the extension and
the reasons for the extension.


CAN A CUSTODIAN REFUSE TO CORRECT AN INDIVIDUAL‚ÄôS PERSONAL
HEALTH INFORMATION?

Subject to the exceptions set out in the next paragraph, a custodian is obligated to
correct a record of personal health information where an individual demonstrates,
to the satisfaction of the custodian, that the record is inaccurate or incomplete for
the purposes for which the custodian uses the information and the individual gives
the custodian the necessary information to correct the record.

However, a custodian may refuse to correct a record of personal health
information that was not originally created by the custodian and which the
custodian does not have sufficient knowledge, expertise and authority to
correct, or if the record consists of a professional opinion or an observation that
a custodian has made in good faith, for example, a medical diagnosis made by
a physician. If a correction is refused, the custodian is required to inform the
individual of the refusal, the reasons for the refusal, the individual‚Äôs right to file
a complaint regarding the refusal to the IPC and the right of the individual to
attach a statement of disagreement to the record.




Frequently Asked Questions: Personal Health Information Protection Act                   43
     ADMINISTRATION AND
     ENFORCEMENT
     HOW IS PHIPA ENFORCED?
     The IPC has been designated as the independent oversight body responsible for
     ensuring that custodians collect, use and disclose personal health information
     according to the rules set out in PHIPA. The IPC plays a significant role in
     enforcing overall compliance.

     The IPC has various powers under PHIPA, including the authority to review and
     adjudicate complaints. These include the authority to:

        ‚Ä¢ require a complainant to try to resolve the issue directly with the custodian,

        ‚Ä¢ appoint a mediator to resolve the complaint and/or

        ‚Ä¢ review a complaint initiated by an individual or, in the absence of a complaint,
          self-initiate a review where there are reasonable grounds to do so.

     The IPC also has the authority to issue orders requiring compliance with PHIPA.
     For example, the IPC may order a custodian to:

        ‚Ä¢ provide the individual with access to a record of personal health
          information,

        ‚Ä¢ correct a record of personal health information,

        ‚Ä¢ dispose of records of personal health information and

        ‚Ä¢ change or cease a particular information practice.


     HOW DOES AN INDIVIDUAL INITIATE A COMPLAINT?
     A person who believes that another person has contravened, or is about to
     contravene PHIPA, has the right to submit a written complaint to the IPC. For
     example, a person may complain about:

        ‚Ä¢ a custodian‚Äôs information practices,

        ‚Ä¢ a refusal to grant access to personal health information or

        ‚Ä¢ a refusal to correct or amend personal health information.




44                                   Frequently Asked Questions: Personal Health Information Protection Act
For further information please see the IPC document, Access and Correction            How does an
Complaints ‚Äì Personal Health Information Protection Act.                              individual initiate a
                                                                                      complaint?

IS THERE A TIME LIMIT WITHIN WHICH AN INDIVIDUAL
MAY COMPLAIN?
In general, an individual must file a complaint with the IPC within one year from
when the individual became aware of the problem. The legislation provides the
IPC with the discretion to extend this one year limitation period.

For complaints that deal with access or correction, an individual must file a
complaint with the IPC within six months from the time a custodian refuses an
access or correction request.



IF A PERSON IS NOT SATISFIED WITH AN IPC ORDER,
WHAT CAN BE DONE?
Persons affected by most types of orders issued by the IPC have the right to
appeal on a question of law to the Divisional Court of Ontario within 30 days of
receiving a copy of the order. Where the IPC issues an order relating to access or
correction of health records, there is no right of appeal. In such a case, a person
may apply to the Divisional Court of Ontario for judicial review.



CAN A PERSON SEEK COMPENSATION FOR DAMAGES?
A person affected by an order of the IPC or a person affected by conduct
leading to a conviction for an offence under PHIPA, that has become final, may
commence a proceeding in court for damages for actual harm suffered. If a court
determines that the harm suffered was caused by wilful or reckless misconduct,
PHIPA permits the court to award up to $10,000 in damages for mental anguish.



WHAT IS AN OFFENCE UNDER PHIPA?
Offences under PHIPA include:

     ‚Ä¢ wilfully collecting, using or disclosing personal health information in
       contravention of PHIPA or its regulations,




Frequently Asked Questions: Personal Health Information Protection Act                         45
        ‚Ä¢ requesting access to or correction of a record of personal health
          information under false pretences,

        ‚Ä¢ intentionally disposing of a record of personal health information to avoid
          providing access,

        ‚Ä¢ collecting, using or disclosing an individual‚Äôs health number in
          contravention of PHIPA,

        ‚Ä¢ obstructing the IPC, or one of its delegates, in the performance of its
          oversight functions,

        ‚Ä¢ dismissing, suspending, demoting, disciplining, harassing or
          disadvantaging an individual who has alerted the IPC of an alleged
          contravention of PHIPA or

        ‚Ä¢ failing to comply with an IPC order.



     WHAT ARE THE CONSEQUENCES FOR COMMITTING AN
     OFFENCE UNDER PHIPA?
     A natural person found guilty of committing an offence under PHIPA can be liable
     for a fine of up to $50,000. A person who is not a natural person for example, an
     organization or institution can be liable for a fine of up to $250,000.

     Any officer, member, employee or agent of a corporation found to have
     authorized or acquiesced to a breach of PHIPA can be held personally liable.

     In addition, persons who are convicted of an offence under PHIPA may be
     subject to a civil suit for damages for breach of privacy. Generally, custodians
     who have acted reasonably and in good faith will be protected from liability.


     WHO IS RESPONSIBLE FOR PROSECUTING OFFENCES
     UNDER PHIPA?
     No person other than the Attorney General, or an agent for the Attorney General,
     may commence a prosecution for an offence under PHIPA. A proceeding to
     prosecute an offence cannot be commenced after six months from the time the
     offence under PHIPA was alleged to have been committed.




46                                   Frequently Asked Questions: Personal Health Information Protection Act
ABOUT THE INFORMATION AND PRIVACY COMMISSIONER OF ONTARIO

The role of the Information and Privacy Commissioner of Ontario is set out
in three statutes: the Freedom of Information and Protection of Privacy Act,
the Municipal Freedom of Information and Protection of Privacy Act and
the Personal Health Information Protection Act. The Commissioner acts
independently of government to uphold and promote open government and
the protection of personal privacy.

Under the three Acts, the Commissioner:

   ‚Ä¢ Resolves access to information appeals and complaints when
     government or health care practitioners and organizations refuse to
     grant requests for access or correction;

   ‚Ä¢ Investigates complaints with respect to personal information held by
     government or health care practitioners and organizations;

   ‚Ä¢ Conducts research into access and privacy issues;

   ‚Ä¢ Comments on proposed government legislation and programs; and

   ‚Ä¢ Educates the public about Ontario‚Äôs access and privacy laws.
Information and Privacy Commissioner of Ontario
2 Bloor Street East, Suite 1400
Toronto, Ontario
Canada M4W 1A8

Web site: www.ipc.on.ca
Telephone: 416-326-3333
Email: info@ipc.on.ca

September 2015
PHIPA Conference ‚Äì November
29,2010
      Introduction to the
Public Services Health & Safety
          Association
Who we are‚Ä¶
‚Ä¢ We serve Ontario‚Äôs public service sector
‚Ä¢ We assist over 9,000 organizations, employing more
  than one million workers, to achieve safer and
  healthier work environments
‚Ä¢ Our highly skilled staff are located across the
  province, providing ready access and timely response
  to all our clients




3
New Directions in Prevention

‚Ä¢ Ontario‚Äôs prevention system is restructuring to better
  meet the needs of employers and workers

    2008 ‚Äì WSIB embarked on the 5-year Road to Zero strategy to
           reduce lost-time injuries by 7% each year

    2009 ‚Äì WSIB Board approves plan to realign Health & Safety
           Associations to achieve Road to Zero objectives

    2010 ‚Äì Amalgamation of 12 health and safety associations
           into four new associations as of January 1st

4
What does the new system look like?
‚Ä¢ 4 new health and safety associations coming together to
  form Health & Safety Ontario:
    ‚Äì Public Services Health & Safety Association
    ‚Äì Infrastructure Health & Safety Association
    ‚Äì Workplace Safety & Prevention Services
    ‚Äì Workplace Safety North

‚Ä¢ The Workers Health & Safety Centre and Occupational
  Health Clinics for Ontario Workers continue to operate
  within the new model

5
Serving a broader range of sectors‚Ä¶
                                  Hospitals
                                  Nursing Services
              Community &         Residential Care
              Healthcare Sector   Community Care
                                  Treatment Clinics
                                  Group Homes
              Education           Universities & Colleges
Public        Sector              School boards
Services                          Libraries
                                  Museums & Art Galleries
Health &      Municipal           Training Centres
              Sector              Municipalities
Safety                            Provincial Govt / Agencies
Association   Provincial
                                  Police, Fire & Paramedics
                                  First Nations
              Government          Conservation Authorities
              & Agencies          Transit


6
So‚Ä¶what will all this mean to you?
Restructuring will help us be more client focused‚Ä¶

‚Ä¢ more resources to devote to Ontario‚Äôs employers and workers
‚Ä¢ more sector specific products and services

‚Ä¢ easier to access information and resources

‚Ä¢ more responsive to clients and increase access to services in
  the North



7
‚Ä¶and tackle common problems as a system.

‚Ä¢ Many of leading hazards are common across employers and
  sectors
‚Ä¢ New system provides:
    ‚Äì ability to leverage strengths across the prevention system
    ‚Äì capacity and expertise to ensure products are evidence-
      based leading practices
‚Ä¢ We will continue to tailor our services to meet the sector
  specific needs



8
Looking Ahead ‚Äì the Road to Zero
‚Ä¢ We need to work in partnership with you to realize
  our goal of Zero workplaces injuries, accidents and
  fatalities and:
    ‚Äì Create a culture of safety in every organization
    ‚Äì Make health and safety a core business value
    ‚Äì Invest and innovate to tackle key issues
    ‚Äì Provide leadership to achieve our goals



9
     Privacy, Confidentiality & the
     Return to Work (RTW) Process




10
     Maintaining Medical Confidentiality

     ‚Ä¢ No employee personal health information may
       be communicated without the express,
       written consent of the employee
     ‚Ä¢ Organizations must ensure that employees‚Äô
       personal health information is acquired, used,
       disclosed, retained and disposed of in
       accordance with legislative requirements



11
     Privacy Legislation

     ‚Ä¢ Personal Health Information Protection Act
     ‚Ä¢ Regulated Health Professions Act
     ‚Ä¢ Freedom of Information and Protection of
       Privacy Act
     ‚Ä¢ Occupational Health and Safety Act




12
     PHIPA
     ‚Ä¢ The Personal Health Information Protection Act is based on
       the CSA Model Code for Protection of Personal
       Information‚Äôs 10 principles:
        ‚Äì Accountability
        ‚Äì Identifying purposes
        ‚Äì Consent
        ‚Äì Limiting collection
        ‚Äì Limiting use, disclosure and retention
        ‚Äì Safeguards
        ‚Äì Accuracy
        ‚Äì Openness
        ‚Äì Individual access
        ‚Äì Challenging compliance



13
     Personal Health Information(PHI) in RTW
     ‚Ä¢ PHI includes identifying information about an individual in oral or
       recorded form that:
         ‚Äì relates to his or her physical or mental health
         ‚Äì relates to providing health care, including identifying a provider of
           health care
         ‚Äì relates to payments or eligibility for health care in respect of the
           individual
         ‚Äì is a health number
         ‚Äì identifies a substitute decision-maker of that individual
         ‚Äì is in a record held by a HIC where the record contains any of the above
           information
     ‚Ä¢ PHI does not include a record of information about an employee or
       other agent of the HIC, unless the record is primarily related to the
       provision of health care to the employee/agent




14
     Personal Health Information(PHI) in RTW

     ‚Ä¢ What might be PHI in the RTW process
       ‚Äì Personal or family history
       ‚Äì A Form 7
       ‚Äì Physician notes for absences
       ‚Äì Notes or consultations from care providers
       ‚Äì Restriction memos issued to managers




15
     Privacy - Initial Contact
     ‚Ä¢ Early contact and intervention is essential
        ‚Äì Day 1 of absence for work-related injury/illness
        ‚Äì 3 to 5 days following non work-related injury/illness
     ‚Ä¢ Integrate existing injury/absence reporting
       protocols
     ‚Ä¢ Offer support and care
     ‚Ä¢ Ensure employee is receiving treatment and is
       aware of RTW program, processes and benefits
     ‚Ä¢ Advise employee of his/her responsibilities


16
     Privacy - Maintain Contact
     ‚Ä¢ Provide support and ‚Äúconnection‚Äù with employee:
        ‚Äì Include informal contact (telephone calls, emails,
          organization‚Äôs newsletter, invitation to visit the workplace
          for coffee)
     ‚Ä¢ The purpose of ongoing contact includes:
        ‚Äì Updating the employee‚Äôs progress and treatment
        ‚Äì Determining employee‚Äôs ability to perform activities of
          daily living
        ‚Äì Explaining benefit coverage
        ‚Äì Making referrals for assistance/support
        ‚Äì Assessing the likelihood of RTW


17
     Collaborating with Physicians

     ‚Ä¢ Proactive communication with health care
       professionals:
       ‚Äì Positively influences return to work
       ‚Äì Is associated with higher rates of return to work
       ‚Äì Contributes to improved satisfaction and
         outcomes
       ‚Äì Results in fewer adverse issues and events and
         fewer requests for unnecessary services



18
     Collaborating with Physicians

     ‚Ä¢ CMA and OMA role of the treating physician:
       ‚Äì Assess, diagnose and treat the injured/ill
         employee
       ‚Äì Work to develop a return to work/functional plan
       ‚Äì Monitor and communicate functional information
       ‚Äì Facilitate safe and timely return to proactive,
         productive employment




19
     Collaborating with Physicians
     ‚Ä¢ A standard letter should be developed for
       treating practitioners on organization‚Äôs
       letterhead and include:
       ‚Äì Brief introduction to RTW coordinator and the
         organization‚Äôs RTW program
       ‚Äì Brief history of the employee‚Äôs case
       ‚Äì Request for specific information and the limits to
         the request of this information
       ‚Äì Explanation of possible job modifications,
         adaptations and supporting internal resources


20
     RTW File

     ‚Ä¢ RTW file is legal documentation of all
       information, activities and conversations
       pertaining to the employee‚Äôs RTW process:
       ‚Äì Must be thorough, accurate and concise
       ‚Äì Entries must be in chronological order
       ‚Äì Must be maintained in a confidential manner and
         stored in a secure location




21
     Valid Consent

     ‚Ä¢ A consent is valid if:
        ‚Äì The individual knows the purpose for which the
          information is to be obtained
        ‚Äì The individual voluntarily agrees to consent
        ‚Äì The consent directly relates to the information
          requested
        ‚Äì The individual knows how the information will be
          used and disclosed



22
     Express Consent

     ‚Ä¢ Express consent is required in the following situations:
        ‚Äì personal health information is to be disclosed outside of the
          health care team (e.g., submitting personal health information
          on a claim form to an insurance company);
        ‚Äì information is to be disclosed (within the health care team) for
          purposes other than providing, or helping to provide, care;
        ‚Äì personal health information is used for fundraising (e.g., contact
          information can be provided without express consent); and
        ‚Äì personal information is being collected for marketing research
          or marketing activities.
        ‚Äì (College of Nurses of Ontario)



23
     Withdrawal of Consent

     ‚Ä¢ An individual may withdraw his/her consent at
       any time for the collection, use and disclosure
       of personal health information
     ‚Ä¢ Withdrawal of consent is not retroactive:
       ‚Äì The individual may not retrieve information that
         has already been disclosed, but may withdraw
         consent for further disclosure of health
         information



24
     Disclosure
     ‚Ä¢ Disclosure of an employee‚Äôs personal health
       information should be limited to:
       ‚Äì Fitness to work following a health assessment
       ‚Äì Confirmation that:
          ‚Ä¢ A medical condition exists
          ‚Ä¢ The employee is under medical care
          ‚Ä¢ The dates of appointments, referrals to specialists
          ‚Ä¢ Treatment programs (no diagnosis or treatment information)
       ‚Äì Expected and confirmed return to work date
       ‚Äì The employee‚Äôs functional abilities and precautions

25
     Safeguarding PHI
     ‚Ä¢ Staff involved in RTW planning, who have access
       to employees‚Äô PHI, must sign a pledge of
       confidentiality
     ‚Ä¢ Only authorized individuals have access to PHI
     ‚Ä¢ PHI records/documents must be stored in a
       locked filing cabinet
     ‚Ä¢ Ensure formal, authorized access to electronic
       PHI (e.g. use of passwords, sign-on identifiers and
       encryption)
     ‚Ä¢ Back up computerized data frequently
     ‚Ä¢ Ensure secure of fax/email transmission

26
     Retention and Storage

     ‚Ä¢ PHI must be kept as long as needed to allow
       an individual to exhaust any legal recourse
     ‚Ä¢ The PHIPA does not specify retention
       requirements
     ‚Ä¢ Refer to retention requirements in Designated
       Substance Regulations and/or College of
       Nurses of Ontario practice standard



27
     Accountability and Transparency
     ‚Ä¢ An organization should develop and
       communicate policies and procedures
       including:
       ‚Äì How PHI is obtained, stored, used, disclosed and
         disposed
       ‚Äì Security and safeguarding precautions
       ‚Äì Employee access to and correction of his/her PHI
         documents
       ‚Äì Breaches of security or confidentiality and
         disciplinary actions


28
     Offences and Enforcement
     ‚Ä¢ An offence under the PHIPA may include:
        ‚Äì Willfully collecting, using or disclosing personal health
          information in contravention of the PHIPA
        ‚Äì Obtaining or attempting to obtain health information
          under false pretenses
        ‚Äì Knowingly disposing of health records to avoid providing
          access
        ‚Äì Obstructing the IPC or one of its delegates in the
          performance of its oversight functions
        ‚Äì Disciplining or harassing an individual who has alerted the
          IPC of an alleged contravention
     ‚Ä¢ An individual may be fined up to $50,000 and an
       organization may be fined up to $250,000


29
     Privacy and Confidentiality Policy
     ‚Ä¢ Who
        ‚Äì Has access to information?
        ‚Äì Will collect and use the information?
        ‚Äì Can complaints be reported to?
     ‚Ä¢ Why
        ‚Äì Is the information being collected?
     ‚Ä¢ What
        ‚Äì Will be collected/ shared?
     ‚Ä¢ When
        ‚Äì Will the information be used?
        ‚Äì Will consent be required?
     ‚Ä¢ How
        ‚Äì Will the information be collected/ used/ accessed/ secured?
        ‚Äì Long will the information be retained?
     ‚Ä¢ Where
        ‚Äì Will the information be secured?



30
   Where to Find Us



       www.pshsa.ca




www.healthandsafetyontario.ca
See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/380721493



Data Privacy and Protection in the Digital Age: Emerging Trends and
Technologies

Article ¬∑ May 2024



CITATION                                                                                                  READS

1                                                                                                         694


2 authors, including:

           Ijeasm Journal
           International Journal of Engineering Applied Science and Management
           65 PUBLICATIONS 1 CITATION

              SEE PROFILE




 All content following this page was uploaded by Ijeasm Journal on 20 May 2024.

 The user has requested enhancement of the downloaded file.
                                        International Journal of Engineering Applied Science and Management
                                                                                    ISSN (Online): 2582-6948
                                                                                     Vol. 5 Issue 4, April 2024



   Data Privacy and Protection in the Digital Age: Emerging Trends and
                              Technologies

                                        Nisha Tusharkumar Gajjar
    Assistant Professor, Department of Computer Science and Application, Faculty of Computer Science
                   and Application College, Ganpat University, Kherva-Mehsana, Gujarat

 Abstract: In this paper, we delve into the vital issues of data privacy and protection in the context of the
 rapidly evolving digital landscape. The surge in digital data generation, driven by advancements in
 technology and an increase in online activity, has brought forth significant challenges in safeguarding
 personal information. As data becomes more integral to our personal and professional lives, the risks
 associated with data breaches, unauthorized access, and privacy infringements also escalate. This paper
 examines these challenges and underscores the importance of emerging technologies and trends that
 promise to enhance data protection measures. Specifically, we explore innovative solutions such as
 artificial intelligence, blockchain technology, quantum cryptography, and privacy-enhancing
 technologies (PETs), which offer new avenues for securing data and maintaining privacy. By
 highlighting the current state of data protection, evaluating the latest technological advancements, and
 suggesting future directions, this paper aims to contribute to the ongoing discourse on improving data
 security in an increasingly digital world. Our comprehensive overview not only addresses the technical
 aspects but also considers the regulatory and ethical dimensions of data privacy, offering insights into a
 holistic approach to data protection.

 Keywords: Data Privacy, Artificial Intelligence (AI), Blockchain Technology, Quantum Cryptography,
 Privacy-Enhancing Technologies (PETs), Regulatory Compliance, Cybersecurity Threats.

1. Introduction                                                 platforms. As a result, there is an urgent need to address
                                                                these privacy challenges, which are compounded by the
The digital era has ushered in unprecedented levels of data     increasing sophistication of cyber threats. Cybercriminals
creation and consumption, fundamentally altering the            are continually evolving their methods, leveraging
landscape of information exchange and storage. This             advanced technologies to orchestrate breaches that are more
transformation is facilitated by the widespread adoption of     intricate and difficult to detect.
digital technologies, including mobile devices, cloud           Moreover, the regulatory landscape concerning data privacy
computing, and the Internet of Things (IoT). Each of these      is complex and varied across regions, presenting additional
technologies generates a vast trove of data every second,       challenges for compliance and enforcement. High-profile
encapsulating everything from consumer behavior to              regulations such as the General Data Protection Regulation
personal communication, and business operations. While          (GDPR) in Europe and the California Consumer Privacy
this data proliferation has enabled significant advances in     Act (CCPA) in the United States reflect a growing
technology and services, it has also brought to the forefront   legislative emphasis on personal data protection. However,
serious concerns regarding privacy and data protection.         these regulations also require organizations to adopt
The importance of safeguarding personal data cannot be          comprehensive and often costly measures to ensure
overstated, given its implications for individual privacy,      compliance.
security, and freedom. Data breaches and unauthorized           This paper seeks to explore the intricate balance between
access to personal information can lead to financial fraud,     technological advancements and the need for robust data
identity theft, and a significant loss of trust in digital      privacy protections. By examining emerging trends and



 Paper ID: 2024/IJEASM/5/2024/2046a                                                                                   1
                                            International Journal of Engineering Applied Science and Management
                                                                                        ISSN (Online): 2582-6948
                                                                                         Vol. 5 Issue 4, April 2024

technologies, it aims to shed light on how innovations can        In response to the escalating challenges in data privacy,
not only pose challenges but also provide solutions that          several emerging trends and technologies have begun to
enhance data security in the digital age. Through this            reshape the landscape of data protection. These innovations
exploration, we aim to offer a roadmap for navigating the         offer promising solutions to enhance the security of digital
complex terrain of digital data privacy and protection.           information and manage privacy risks more effectively.
                                                                  Artificial Intelligence (AI) and Machine Learning (ML):
2. Current Challenges in Data Privacy                             AI and ML are at the forefront of transforming data
                                                                  protection strategies. These technologies enable the
Data privacy faces numerous challenges in today‚Äôs digital         automation of complex processes such as threat detection
landscape, primarily fueled by the rapid pace of                  and response. AI systems can analyze vast datasets quickly
technological advancements and the escalating volume of           and identify patterns that may indicate potential security
data being generated. Among the most pressing challenges          breaches or vulnerabilities. Moreover, machine learning
are the management of massive data volumes, sophisticated         algorithms adapt and improve over time, increasing their
cyber threats, and the complex regulatory environment.            effectiveness in detecting new and evolving threats.
Volume of Data: The digital age has been marked by an             Blockchain Technology: Known for its robust security
exponential increase in data generation. Every interaction        features, blockchain technology offers a decentralized
on the internet, every transaction on digital platforms, and      approach to data management. Each transaction on a
every communication through modern devices adds to the            blockchain is encrypted and linked to the previous
growing data pool. This vast amount of data, often personal       transaction, making it virtually immutable and resistant to
and sensitive, necessitates advanced management and               tampering. This feature is particularly beneficial for
protection strategies. However, the ability of organizations      applications requiring high levels of transparency and
to manage and secure this data has not kept pace with the         security, such as financial transactions, supply chain
rate at which it is being collected, leading to vulnerabilities   management, and even voting systems.
where data can be easily misused or exposed.                      Quantum Cryptography: As the potential of quantum
Advanced Persistent Threats (APTs): Cybersecurity                 computing grows, so does the threat it poses to traditional
threats have evolved into more sophisticated forms, such as       encryption methods. Quantum cryptography, however, uses
APTs, where attackers infiltrate systems to mine data over        the principles of quantum mechanics to secure data in a
extended periods without detection. These threats are             way that is virtually unbreakable by any computer,
particularly challenging because they are not only hard to        including quantum computers. This technology is still in its
detect but also capable of causing significant long-term          early stages but promises to revolutionize data security in
damage by targeting sensitive or critical information.            the coming years.
Legal and Regulatory Landscape: Data privacy                      Privacy-Enhancing Technologies (PETs): PETs are
regulations are intended to protect user data, but they also      designed to allow data processing and sharing while
introduce significant compliance challenges. Different            protecting user privacy. Techniques like differential
countries have different regulations, such as GDPR in             privacy, which adds randomness to datasets to obscure
Europe and CCPA in California, which can vary widely in           individual data points, and homomorphic encryption, which
their requirements and penalties for non-compliance. For          enables computations on encrypted data without needing to
global organizations, this creates a labyrinth of legal           decrypt it, are gaining popularity. These technologies
obligations, making it difficult to ensure consistent             ensure that data can be used for analysis without
adherence across all operations. Moreover, these                  compromising individual privacy.
regulations     are     continually     evolving,    requiring    These emerging trends and technologies not only address
organizations to stay abreast of changes to remain                current data protection challenges but also set the stage for
compliant.                                                        a more secure digital future. As these technologies mature
These challenges underscore the need for innovative               and their adoption increases, they will play a crucial role in
solutions and robust strategies to safeguard data privacy.        enhancing the overall security and privacy of digital
Addressing them effectively is crucial for maintaining trust      systems.
in digital platforms and ensuring the security of personal
information in the global digital economy.                        4. Case Studies

3. Emerging Trends and Technologies in Data                       4.1 Implementation of GDPR Compliance Tools
Protection



Paper ID: 2024/IJEASM/5/2024/2046a                                                                                       2
                                           International Journal of Engineering Applied Science and Management
                                                                                       ISSN (Online): 2582-6948
                                                                                        Vol. 5 Issue 4, April 2024

The General Data Protection Regulation (GDPR), enforced         healthcare. Additionally, the regulatory acceptance of
since May 2018, has set a new benchmark for data                blockchain technologies is still in nascent stages, with legal
protection laws globally, emphasizing transparency,             frameworks varying widely across jurisdictions, potentially
security, and accountability by data processors and             hindering widespread adoption.
controllers. To comply with these stringent requirements,       These case studies illustrate both the potential and the
organizations have turned to automated GDPR compliance          hurdles in implementing advanced technologies for data
tools. These tools assist in various aspects such as data       protection and management. While tools and technologies
mapping, risk assessment, and consent management, which         like GDPR compliance software and blockchain present
are essential for maintaining compliance.                       effective solutions, they also require organizations to
One prominent example is the use of automated data              navigate technical complexities, resource constraints, and
mapping tools that help organizations understand where          evolving regulatory landscapes.
personal data resides across their systems. This mapping is
crucial for GDPR's data minimization and storage limitation     5. Future Directions
principles, ensuring that personal data is not unnecessarily
stored or processed. Compliance software also facilitates       As we look toward the future of data privacy and
regular audits, generating reports that help organizations      protection, continuous innovation in technology and
identify and rectify compliance gaps.                           evolving regulatory frameworks will play pivotal roles. The
However, implementing these tools presents challenges.          ongoing development of artificial intelligence and machine
For instance, integrating them within existing IT               learning will further enhance threat detection and response
infrastructures can be complex and costly, especially for       capabilities, making predictive security measures more
small to medium enterprises (SMEs) with limited resources.      accurate and dynamic. Additionally, as quantum computing
Additionally, these tools require continuous updates to keep    advances, quantum cryptography is expected to become
pace with evolving regulatory interpretations and               mainstream, offering unprecedented security levels that can
guidelines, imposing a recurring expense on businesses.         counteract the potential threats posed by quantum
                                                                computers to current encryption methodologies.
4.2 Blockchain in Healthcare Data Management                    Blockchain technology is also poised to expand beyond
                                                                finance and healthcare, becoming integral in industries like
Blockchain technology's potential to revolutionize data         retail, logistics, and public administration, where
management in healthcare is increasingly recognized, given      transparency and security are crucial. Privacy-enhancing
its inherent features of decentralization, immutability, and    technologies (PETs) will gain broader acceptance as
transparency. Healthcare data, characterized by its             organizations seek to balance data utility with privacy,
sensitivity and the critical need for privacy and security,     employing techniques like differential privacy and
can benefit significantly from blockchain-based solutions.      homomorphic encryption in more widespread and practical
A notable application of blockchain in healthcare is the        applications.
management of electronic health records (EHRs). By using        Furthermore, the regulatory landscape will likely continue
blockchain, healthcare providers can create a decentralized     to evolve, with potential global harmonization of privacy
and secure database where patient records are not just          laws, or at least, the establishment of common standards
protected from unauthorized access but also made                that facilitate international data flows while protecting
accessible across various authorized institutions in real-      personal information. This regulatory evolution will
time. This capability significantly improves the efficiency     necessitate that businesses remain agile, adapting to new
and accuracy of medical services.                               laws and technologies to stay compliant and secure. These
Moreover, blockchain enables patients to control who            directions indicate a robust framework for the future, where
accesses their data, enhancing patient consent                  privacy, technology, and regulation intersect to foster a
management‚Äîa key aspect of medical ethics and privacy           safer digital environment.
regulations. Patients can grant or revoke access
permissions, and any access to their medical records is         6. Conclusion
immutably logged, providing a transparent audit trail.
However, deploying blockchain in healthcare also involves       This paper highlights the importance of advancing
significant challenges. The integration with existing digital   technologies and trends in the field of data privacy and
health systems can be technically demanding and costly.         protection. As digital threats evolve, so must our
There are also concerns about blockchain's scalability,         approaches to securing personal and organizational data.
especially given the large volume of data generated in          The continued development and adoption of advanced



Paper ID: 2024/IJEASM/5/2024/2046a                                                                                     3
                                                      International Journal of Engineering Applied Science and Management
                                                                                                  ISSN (Online): 2582-6948
                                                                                                   Vol. 5 Issue 4, April 2024

technologies are crucial in the fight against data breaches                   10. Pasha, Shaik Imran, and Harsh Pratap Singh. "A Novel
and privacy infringements.                                                        Model Proposal Using Association Rule Based Data
In conclusion, the complexities of data privacy and                               Mining Techniques for Indian Stock Market Analysis."
protection in the digital age are influenced by an intricate                      Annals of the Romanian Society for Cell Biology
                                                                                  (2021): 9394-9399.
interplay of technological advancements and regulatory                        11. Md, Abdul Rasool, Harsh Pratap Singh, and K. Nagi
demands. As we have explored in this paper, the landscape                         Reddy. "Data Mining Approaches to Identify
of data privacy is continuously evolving, driven by both the                      Spontaneous Homeopathic Syndrome Treatment."
challenges posed by new threats and the opportunities                             Annals of the Romanian Society for Cell Biology
presented by emerging technologies. Artificial intelligence,                      (2021): 3275-3286.
blockchain technology, quantum cryptography, and                              12. Vijay Vasanth, A., et al. "Context-aware spectrum
privacy-enhancing technologies are at the forefront of this                       sharing and allocation for multiuser-based 5G cellular
evolution, offering novel solutions that can significantly                        networks." Wireless Communications and Mobile
enhance the security and privacy of digital information.                          Computing 2022 (2022).
                                                                              13. Singh, Harsh Pratap, and Rashmi Singh. "Exposure and
The importance of these technologies cannot be overstated;                        Avoidance Mechanism Of Black Hole And Jamming
they not only provide the tools needed to combat modern                           Attack In Mobile Ad Hoc Network." International
threats but also help in building trust between users and                         Journal of Computer Science, Engineering and
digital platforms. However, the effective implementation of                       Information Technology 7.1 (2017): 14-22.
these technologies requires that they be complemented by                      14. Singh, Harsh Pratap, et al. "Design and Implementation
robust regulatory frameworks that can adapt to the pace of                        of an Algorithm for Mitigating the Congestion in Mobile
technological change and cover global discrepancies in                            Ad Hoc Network." International Journal on Emerging
privacy laws.                                                                     Technologies 10.3 (2019): 472-479.
                                                                              15. Singh, Harsh Pratap, et al. "Congestion Control in
                                                                                  Mobile Ad Hoc Network: A Literature Survey."
                                                                              16. Rashmi et al.. "Exposure and Avoidance Mechanism Of
References                                                                        Black Hole And Jamming Attack In Mobile Ad Hoc
                                                                                  Network." International Journal of Computer Science,
        1.        Smith, J., & Doe, A. (2022). Artificial Intelligence in         Engineering and Information Technology 7.1 (2017):
                  Cybersecurity: Emerging Trends and Applications. New            14-22.
                  York: Springer.                                             17. Sharma et al., "Guard against cooperative black hole
        2.        Johnson, L. (2021). Blockchain for Data Security: A             attack in Mobile Ad-Hoc Network." Harsh Pratap Singh
                  Comprehensive Guide. Oxford: Oxford University                  et al./International Journal of Engineering Science and
                  Press.                                                          Technology (IJEST) (2011).
        3.        Brown, R., & Green, T. (2023). Quantum Cryptography         18. Singh, et al., "A mechanism for discovery and
                  and the Future of Cyber Defense. Cambridge: MIT                 prevention of coopeartive black hole attack in mobile ad
                  Press.                                                          hoc network using AODV protocol." 2014 International
        4.        Edwards, S. (2022). Privacy-Enhancing Technologies:             Conference on Electronics and Communication Systems
                  Concepts and Implementations. San Francisco: Jossey-            (ICECS). IEEE, 2014.
                  Bass.                                                       19. Harsh et al., "Design and Implementation of an
        5.        Kumar, N., & Singh, V. (2021). Regulatory Compliance            Algorithm for Mitigating the Congestion in Mobile Ad
                  in the Digital Age: A Global Perspective. London:               Hoc Network." International Journal on Emerging
                  Routledge.                                                      Technologies 10.3 (2019): 472-479.
        6.        Chen, M., & Li, J. (2020). Data Privacy Management          20. Davidson, R. (2021). The Impact of IoT on Data
                  Techniques: Solutions from the Field. Stanford: Stanford        Security and Privacy. Boston: Pearson Education.
                  University Press.                                           21. Harris, L. (2022). Decentralized Systems and Their
        7.        Wilson, P., & Murphy, K. (2022). The GDPR and                   Applications in Data Privacy. Chicago: University of
                  Beyond: The Evolving World of Data Protection. Berlin:          Chicago Press.
                  De Gruyter.
        8.        Thompson, C., & Yates, F. (2023). Emerging
                  Cybersecurity Threats and Countermeasures. New
                  York: Academic Press.
        9.        Naiyer, Vaseem, Jitendra Sheetlani, and Harsh Pratap
                  Singh. "Software Quality Prediction Using Machine
                  Learning Application." Smart Intelligent Computing and
                  Applications: Proceedings of the Third International
                  Conference on Smart Computing and Informatics,
                  Volume 2. Springer Singapore, 2020.



Paper ID: 2024/IJEASM/5/2024/2046a                                                                                                 4



View publication stats
A Guide for Policy Engagement
on Data Protection




PART 1:

Data Protection,
Explained
A Guide for Policy Engagement on Data Protection | PART 1: Data Protection, Explained




Data Protection, Explained




What is Data Protection?


Data protection is commonly defined as the law designed to protect your personal
data. In modern societies, in order to empower us to control our data and to protect
us from abuses, it is essential that data protection laws restrain and shape the
activities of companies and governments. These institutions have shown repeatedly
that unless rules restricting their actions are in place, they will endeavour to collect
it all, mine it all, keep it all, share it with others, while telling us nothing at all.1


Why is Data Protection Needed?


Every time you use a service, buy a product online, register for email, go to your
doctor, pay your taxes, or enter into any contract or service request, you have to
hand over some of your personal data. Even without your knowledge, data and
information about you is being generated and captured by companies and agencies
that you are likely to have never knowingly interacted with. The only way citizens and
consumers can have confidence in both government and business is through strong
data protection practices, with effective legislation to help minimise state and
corporate surveillance and data exploitation.

Since the 1960s and the expansion of information technology capabilities, business
and government have been storing this personal data in databases. Databases
can be searched, edited, cross-referenced, and their data shared with other
organisations across the world.

Once the collection and processing of data became widespread, people started
asking questions about was happening to their data once they provided it. Who had
the right to access the data? Was it kept accurately? Was it being collected and
disseminated without their knowledge? Could it be used to discriminate or violate
other fundamental rights?

From all these questions, and amid growing public concern, data protection
principles were devised through numerous national and international consultations.
The German region of Hesse passed the first law in 1970, while the US Fair Credit
Reporting Act 1970 also contained elements of data protection. 2 The US-led
development of a ‚Äòcode of fair information practices‚Äô in the early 1970s continues
to shape data protection law today. At around the same time, the UK established
a committee to review threats by private companies, which came to similar
conclusions.




09/98
A Guide for Policy Engagement on Data Protection | PART 1: Data Protection, Explained




National laws emerged soon afterwards, beginning with Sweden, Germany, and
France. As of January 2018, over 100 countries had adopted data protection laws,
with pending bills or initiatives to enact a law in a further 40.3

Over time, regional legal frameworks were also adopted. In 1980, the Organisation
for Economic Cooperation and Development (OECD) developed its guidelines,
which included ‚Äòprivacy principles‚Äô; shortly afterwards, the Council of Europe‚Äôs
Convention for the Protection of Individuals with regard to Automatic Processing of
Personal Data entered into force - this was modernised in 2018.4

The sheer volume of data generated and the rapid development of technology,
including sophisticated profiling and tracking, and artificial intelligence, means that
some existing data protection laws are out of date and unfit to deal with processing
as it currently functions. Frameworks fail to reflect the new potential for data
processing which emerged with advancement of technologies which were deployed
and embedded within governance systems and business models.

It has been reported that 90% of data in the world today was created in the last
two years, and every two days we create as much data as we did from the start of
time until 2013 5 . When many data protection frameworks were drafted the world
was a very different place. For example, many laws were adopted before Google,
Facebook or smartphones were even created, let alone widely used.

A data protection framework may have its limitations (which we are trying to
identify and address by exploring what other regulations are needed to provide the
necessary safeguards) but it does provide an important and fundamental starting
point to ensure that strong regulatory and legal safeguards are implemented to
protect personal data.

A strong data protection framework can empower individuals, restrain harmful data
practices, and limit data exploitation. It essential to provide the much-needed
governance frameworks nationally and globally to ensure individuals have strong
rights over their data, stringent obligations are imposed on on those processing
personal data (in both the public and private sectors), and strong enforcement
powers can be used against those who breach these obligations and protections.




10/98
A Guide for Policy Engagement on Data Protection | PART 1: Data Protection, Explained




Data Protection: Essential for Exercise of Right to Privacy


Privacy is an internationally recognised human right. Article 12 of the Universal
Declaration of Human Rights (UDHR) proclaims that


 [n]o one shall be subjected to arbitrary interference
 with his privacy, family, home or correspondence ....
 Everyone has the right to the protection of the law
							 6
 against such interference or attacks.


The UDHR has formed the basis for the major international human rights treaties,
which similarly enshrine the right to privacy, including the International Covenant on
Civil and Political Rights (ICCPR) in Article 17.

As early as 1988, the UN Human Rights Committee, the treaty body charged with
monitoring implementation of the ICCPR, recognised the need for data protection
laws to safeguard the fundamental right to privacy recognised by Article 17 of the
ICCPR:

           The gathering and holding of personal information on
           computers, data banks, and other devices, whether by
           public authorities or private individuals or bodies, must
           be regulated by law. ... [E]very individual should have
           the right to ascertain in an intelligible form, whether,
           and if so, what personal data is stored in automatic data
           files, and for what purposes. Every individual should
           also be able to ascertain which public authorities or
           private individuals or bodies control or may control their
           files. If such files ... have been collected or processed
           contrary to the provisions of the law, every individual
           should have the right to request rectification
		                          7
           or elimination


In 2011, the then-UN Special Rapporteur on the Promotion and Protection of the
Right to Freedom of Opinion and Expression issued a report similarly noting that
‚Äúthe protection of personal data represents a special form of respect for the right to
privacy.‚Äù8 The report further noted that:


  [t]he necessity of adopting clear laws to protect
  personal data is further increased in the current
  information age, where large volumes of data are
  collected and stored by intermediaries, and there is a
  worrying trend of States obliging or pressuring these
  private actors to hand over information of their users.9 			
						




11/98
A Guide for Policy Engagement on Data Protection | PART 1: Data Protection, Explained




And in 2013, he also noted that the right to privacy includes:


           the ability of individuals to determine who holds
           information about them and how [...] that information
		
           [is] used.10 		


In December 2016, the UN General Assembly passed a resolution (by consensus)
on the Right to Privacy in the Digital Age, GA Res. 71/199, which reaffirmed previous
General Assembly resolutions on the subject, emphasising that:


  States must respect international human rights
  obligations regarding the right to privacy [...] when they
  require disclosure of personal data from third parties,
  including private companies. 11
					




Privacy and data protection are intrinsically linked. Individuals, as citizens,
customers, and consumers, need to have the means and tools to exercise their right
to privacy and protect themselves and their data from abuse. It is also important
that the obligations of those processing data are clear, so that they take measures
to protect personal data, mitigate interference with the right to privacy, and are
held to account when they fail to comply with obligations. This is particularly the
case when it comes to our personal data. Personal data, as described below in
detail, is data (information processed by automated means or kept in a structured
filing system) which relates to an individual. Data protection is about safeguarding
our fundamental right to privacy by regulating the processing of personal data:
providing the individual with rights over their data, and setting up systems of
accountability and clear obligations for those who control or undertake the
processing of the data.




12/98
A Guide for Policy Engagement on Data Protection | PART 1: Data Protection, Explained




        Data Protection: A Right?

        The protection of personal data has long been recognised as
        a fundamental aspect of the right to privacy. In recent years it
        has been recognised as a standalone right. For example, data
        protection has been included as a standalone right under the
        Charter of Fundamental Rights of the European Union (2012/C
        326/02) under Article 8 (in addition to Article 7 of the Charter which
        upholds the right to privacy). Article 8 reads:

        Protection of personal data

           1. Everyone has the right to the protection of personal data
           concerning him or her.
           2. Such data must be processed fairly for specified purposes
           and on the basis of the consent of the person concerned or
           some other legitimate basis laid down by law. Everyone has the
           right of access to data which has been collected concerning
           him or her, and the right to have it rectified.
           3. Compliance with these rules shall be subject to control by an
           independent authority.

        In many countries around the world, there is a Constitutional
        right of habeas data, which is designed to protect the data of an
        individual by granting them the right to access the information
        held about them, and providing for the individual concerned to
        submit a complaint to the Constitutional Court.

        Article 5, 1988 Brazilian Constitution:
        Habeas Data shall be granted: a) to ensure the knowledge of
        information related to the person of the petitioner, contained in
        records or databanks of government agencies or of agencies of a
        public character; b) for the correction of data, when the petitioner
        does not prefer to do so through a confidential process, either
        judicial or administrative.

        Article 15, Constitution of Colombia, as amended in 1995:
        All individuals have the right to personal and family privacy and
        to their good reputation, and the State has to respect them and to
        make others respect them. Similarly, individuals have the right to
        know, update, and rectify information collected about them in data
        banks and in the records of public and private entities.

        Freedom and the other guarantees approved in the Constitution
        will be respected in the collection, processing, and circulation of
        data.




13/98
A Guide for Policy Engagement on Data Protection | PART 1: Data Protection, Explained




        Correspondence and other forms of private communication may
        not be violated. They may only be intercepted or recorded on
        the basis of a court order in cases and following the formalities
        established by law.

        For tax or legal purposes and for cases of inspection, the oversight
        and intervention of the State may demand making available
        accounting records and other private documents within the limits
        provided by law.




How Does Data Protection Work?


There are no universally-recognised data protection standards, but regional and
international bodies have created internationally-agreed-upon codes, practices,
decisions, recommendations, and policy instruments.


        The most significant instruments are:

        - The Council of Europe Convention for the Protection of Individuals
          with regard to Automatic Processing of Personal Data (No. 108), 1981
          as amended in 2018
        - The Organization for Economic Co-operation and Development
          Guidelines on the Protection of Privacy and Transborder Data Flows
          of Personal Data (1980) as amended in 2013
        - The Guidelines for the regulation of computerized personal data files
          (General Assembly resolution 45/95 and E/CN.4/1990/72).


Other regional frameworks also exist including the APEC Privacy Framework - Asia-
Pacific Economic Cooperation.12

Where a comprehensive data protection law exists, organisations (public or private)
that collect and use your personal data, have the obligation to handle this data
according to the data protection law.




14/98
A Guide for Policy Engagement on Data Protection | PART 1: Data Protection, Explained




Data protection should ensure the following:

  ‚Ä¢     There should be limits on the collection of personal data, and it should be
        obtained by lawful and fair means, as well as being done in a transparent
        manner
  ‚Ä¢     The purposes for which the data and information is to be used should be
        specified (at the latest) at the time of collection, and should only be used
        for those agreed purposes. Personal data can only be disclosed, used, or
        retained for the original purposes (i.e. the purpose at the time of collection),
        except with the consent of the individual or under law: accordingly, it must be
        deleted when no longer necessary for that purpose
  ‚Ä¢     Personal data, as generated and processed, should be adequate, relevant,
        and limited to necessity of the purposes for which it is to be used
  ‚Ä¢     The data should be accurate and complete, and measures should be taken
        to ensure it is up to date
  ‚Ä¢     Reasonable security safeguards should be used to protect personal data from
        loss, unauthorised access, destruction, use, modification, or disclosure
  ‚Ä¢     There should be no secret processors of data, sources, or processing.
        Individuals must be made aware of the collection and processing of their data,
        as well as the purpose of its use, who is controlling it, and who is processing it
  ‚Ä¢     Individuals have a range of rights which enables them to control their personal
        data and any processing
  ‚Ä¢     Those that use personal data must be accountable for and demonstrate
        compliance with the above principles, and facilitate and fulfil the exercise of
        these rights, abiding by applicable laws that enshrine those principles


        OECD Guidelines on the Protection of Privacy and Transborder
        Flows of Personal Data, updated in 2013

        1. Collection Limitation Principle
        2. Data Quality Principle
        3. Purpose Specification Principle
        4. Use Limitation Principle
        5. Security Safeguards Principle
        6. Openness Principle
        7. Individual Participation Principle
        8. Accountability Principle


        Convention for the Protection of Individuals with regard to
        Automatic Processing of Personal Data, No. 108, as amended
        by 2018

        Article 5 (4):
        Personal data undergoing processing shall be:
        a. processed fairly and in a transparent manner
        b. collected for explicit, specified and legitimate purposes and
        not processed in a way incompatible with those purposes; further


15/98
A Guide for Policy Engagement on Data Protection | PART 1: Data Protection, Explained




        processing for archiving purposes in the public interest, scientific
        or historical research purposes or statistical purposes is, subject to
        appropriate safeguards, compatible with those purposes
        c. adequate, relevant and not excessive in relation to the purposes for
        which they are processed
        d. accurate and, where necessary, kept up to date
        e. preserved in a form which permits identification of data subjects for
        no longer than is necessary for the purposes for which those data are
        processed


        General Directive Personal Data, Regulation (EU) 2016/679 of the
        European Parliament and of the Council of 27 April 2016

        Principles presented in Article 5:
        1. Lawfulness, fairness and Transparency
        2. Purpose limitation
        3. Data minimisation
        4. Accuracy
        5. Storage limitation
        6. Integrity and confidentiality
        7. Accountability


Accountability should be at the core of any law regulating of the processing of
personal data and the protection of the rights of individuals, and data protection
rules thus need to be enforced by a regulator or authority. The strength of
powers invested in these authorities varies from country to country, as does their
independence from government. Some jurisdictions have established more than one
regulatory body for oversight regulation and enforcement, with powers depending
on if the data is being processed by public or private entities, e.g. Colombia. These
powers, for example, can include the ability to conduct investigations, act on
complaints, and impose fines when an organisation has broken the law.

Redress for breaches of data protection law should also be available through the
courts, both through individual actions and collective redress (brought by NGOs
and consumer groups).

In summary, data protection works through key principles which give individuals
rights over their data: those that process data have obligations in relation to the
data, and enforcement and redress must be available when these principles, rights
and obligations are not adhered to.




16/98
A Guide for Policy Engagement on Data Protection | PART 1: Data Protection, Explained




Data Protection in Practice Today


As of January 2018, over 100 countries around the world have enacted
comprehensive data protection legislation, and around 40 countries are in the
process of enacting such laws. Other countries may have privacy laws applying
to certain areas, for example for children or financial records, but do not have a
comprehensive law on data protection.




Source: Banisar, David, National Comprehensive Data Protection/Privacy Laws and Bills 2018 (January 25, 2018).
Available at SSRN:https://ssrn.com/abstract=1951416 or http://dx.doi.org/10.2139/ssrn.1951416


In countries where there is no comprehensive data protection framework, data
protection is regulated through sectorial laws where it is regulated at all. For
instance, though an early leader in the field of data protection, the US Privacy
Act 1974 applies only to the Federal Government, and subsequent laws apply
to specific sectors or groups of individuals (e.g. the Children‚Äôs Online Privacy
Protection Act (COPPA)), but there is no comprehensive data protection law to
date. This sectorial approach is still in place in many countries, including India.

A significant development in data protection law occurred with the adoption of the
EU General Data Protection Regulation (GDPR), which will take effect on 25 May
2018. The GDPR is comprehensive, covering almost all personal data processing.
It is also significant, as its implementation will affect not only data controllers
based within the EU, but also those that offer goods or services to, or monitor the
behaviour of, individuals based in the EU.




17/98
A Guide for Policy Engagement on Data Protection | PART 1: Data Protection, Explained




In May 2018, there was a further development with the amendment of the Council
of Europe‚Äôs Convention for the Protection of Individuals with regard to Automatic
Processing of Personal Data (No. 108). Since its adoption in 1981, over 40 European
countries and nine non-Members of Council of Europe have used the Convention as
a foundation of their own data protection frameworks. The modernised text of the
Convention reaffirms existing principles, and adopts new provisions to strengthen
obligations, accountability, and enforcement mechanisms. 13

For more information on data protection laws, broken down by country,
see Privacy International‚Äôs comprehensive reports. 14


Data Protection: A Piece of the Puzzle


In protecting the right to privacy of individuals as well as their data, data protection
is only a piece of the puzzle.

A general data protection framework does not preclude the adoption or application
of sectoral laws regulating particular sectors. Any data protection law should make
it clear that its scope is to protect the fundamental rights of individuals, such as the
right to privacy and personal data protection, and therefore any laws (current or
future) which contradict such protection, e.g. by limiting those fundamental rights,
should be considered null and void.



        There is a need to ensure that necessary legislation be adopted
        to regulate government and private sector policies and practices
        which interfere with the right to privacy and entail the processing
        of personal data. These could include laws regulating, but are not
        limited to:

        - Communications surveillance
        - Information and technology
        - Law enforcement
        - Trade
        - Education
        - E-governance
        - Health care services
        - Financial and banking institutions
        - Consumer protection
        - Cyber-security
        - Product liability



These should ensure the protection of the individual and their data as well as
respect their right to privacy.




18/98
A Guide for Policy Engagement on Data Protection | PART 1: Data Protection, Explained




A Step-by-Step Guide to Data Protection


While data protection laws vary from country to country, there are some
commonalities and minimum requirements, underpinned by data protection
principles and standards.


        Laws tend to have some general provisions providing for:

        - The scope of the law
        - Definitions
        - Data protection principles
        - The obligation of controllers and processors
        - The rights of data subjects
        - Oversight and enforcement


The different chapters of the guide outline and explain these general provisions in
more detail, presenting the key components of data protection through a variety of
national and global examples.




19/98
A Guide for Policy Engagement on Data Protection | PART 1: Data Protection, Explained




References


1       See full text: https://www.privacyinternational.org/explainer/41/101-data-protec
        tion

2       Robert Gellman, ‚ÄòFair Information Practices: A Basic History‚Äô,
        April 2017, available [PDF] at: https://bobgellman.com/rg-docs/rg-FIPshistory.pdf

3       David Banisar, ‚ÄòNational Comprehensive Data Protection/Privacy Laws and Bills
        2018‚Äô, available at: https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1951416
        (last revised 25 Jan 2018)

4       Protocol amending the Convention for the Protection of Individuals with regard
        to Automatic Processing of Personal Data (ETS No. 108), 128th Session of the
        Committee of Ministers, 18 May 2018, CM(2018)2-final. Available at: https://
        search.coe.int/cm/Pages/result_details.aspx?ObjectId=090000168089ff4e

5       Thomas A Singlehurst et al, ‚ÄòePrivacy and Data Protection‚Äô, CitiGroup,
        March 2017, p4. Available (PDF) at https://www.citibank.com/commercialbank/in
        sights/assets/docs/ePrivacyandData.pdf

6       GA Res. 217 (III) A, UDHR, art. 12 (Dec. 10, 1948)

7       UN Doc. HRI/GEN/1/Rev.9, General Comment No. 16: Article 17, para 10.

8       UN Doc. A/HRC/17/27, para 58 (May 16, 2011).

9       Id. para 56

10      UN Doc. A/HRC/23/40, ¬∂ 22 (Apr. 17, 2013).

11      GA Res. 71/199, at 3; accord Human Rights Council Res. 34/7.


12      APEC Privacy Framework, December 2005, available at
        https://www.apec.org/Publications/2005/12/APEC-Privacy-Framework

13      Council of Europe, ‚ÄòModernisation of Convention 108‚Äô, Council of Europe Portal,
        available at https://www.coe.int/en/web/data-protection/convention108/modernised

14      Privacy International, ‚ÄòState of Privacy‚Äô, available at
        https://www.privacyinternational.org/reports/state-of-privacy




20/98
                                                                    WHY DIGITAL PRIVACY IS SO COMPLICATED




Why Digital Privacy
Is So Complicated
JORDAN SHAPIRO
PROGRESSIVE POLICY INSTITUTE

MAY 2022




     @ppi |   @progressivepolicyinstitute |   /progressive-policy-institute
                                                                   WHY DIGITAL PRIVACY IS SO COMPLICATED




Why Digital Privacy                                                                   JORDAN SHAPIRO




Is So Complicated
                                                                                              MAY 2022




EXECUTIVE SUMMARY


The exact definition of digital                The discussion of digital privacy is complicated
                                               precisely because it operates on three distinct
privacy is complex, imperfectly                but interrelated levels. First, privacy‚Äôs social and
aligned with typical understandings            legal dimensions depend on whether individuals,
                                               corporations, or governments are assumed to
of privacy in an analog context.               hold primary rights to personal data collected
Historically, the vast majority of             about those individuals. In Europe, for example,
                                               the individual holds primary rights over their
human actions and interactions                 data, while in China, the state takes precedence.
existed beyond the scope of                    The second level of the privacy discussion
surveillance. Today, it‚Äôs nearly               addresses data use and the technical protection
                                               and security of personal information to
impossible to go about our daily               safeguard it from unwanted intrusion or theft
lives without digital tools that               while allowing individuals transparent access
                                               to their data. These complicated technical
facilitate modern life, but also               issues arise no matter privacy‚Äôs social and legal
collect data about individuals. When           structure.

this growing flood of data is linked           The third level of the privacy debate deals
                                               with the economics of PII. How does the
to an individual it is called ‚Äúpersonal        chosen privacy model interact with innovation
identifying information‚Äù (PII), the            and growth? And how can it be assured that
                                               individuals get the appropriate benefits from
centerpiece of the debate over                 their data?
digital privacy.                               This paper will lay out the privacy models of the
                                               United States, Europe, and China, with smaller
                                               sections on the United Kingdom, Canada, and



                                          P2
                                                                             WHY DIGITAL PRIVACY IS SO COMPLICATED




India. For each area, we will discuss the social          Finally, there are data collected without consent.
and legal structure, the technical design of              For example, website trackers that follow
security and transparency, and the economic               internet users from site to site, collect browsing
implications of privacy and innovation.                   information. Other forms of non-consensual
                                                          data collection include CCTV cameras on streets
This paper sets out a framework for PPI‚Äôs
                                                          and in stores which scan our faces almost
ongoing privacy work. It lays the groundwork for
                                                          everywhere, storing that information in remote
future discussions of privacy legislation in the
                                                          servers without any way to opt out.3
United States.
                                                          The degree to which individual information is
BACKGROUND                                                shared via each of these collection methods
For much of human history, sparse information             raises different questions for protecting
collection and dissemination created a wide               consumer privacy.4 The internet expands
margin for privacy.1 Personal details had to be           the community with whom we share our
deliberately disclosed to a trusted information           information. Digital storage overcomes many
collector. While life was not secret, information         of the limitations of physical storage. The
known about individuals was limited to word-              requirement of data sharing to use essential
of-mouth or, when more accurate records were              everyday technologies creates a new relationship
needed, stored on paper forms. Without any                between individuals and the groups that store
dissemination platforms, like we see today in             their data.5
social media, the full extent of data known and
stored about historic peoples was limited to              Indeed, the greater degree of digital data
community knowledge and essential records.                collection and use has provoked concerns from
                                                          consumers and legislators about legal rights to
Today, digital data collection has no such limits.        the information they view as private.6 Despite the
Aspects of public and private life that once only         concerns, governments, firms, and researchers
passed between individuals present, like visiting         use these data to innovate and better understand
a friend, buying a meal, or running an errand,            consumer behavior and society. It fuels the
can now be easily shared, tracked, and stored.            development of revolutionary technological
The data are largely collected via three major            innovations and can lead to better policymaking,
channels. First, many people voluntarily consent          which benefit society. In short, the data are
to share personal information by, for example,            extremely valuable.
posting on social media or participating in an            As a result, both the social and security
online survey or research study. The second               considerations of privacy have been radically
is data we consent to share in exchange for               transformed, while a third consideration, the
services. Today, all software and hardware come           economics of privacy, has become crucially
with terms of service and privacy policies that           important.
are legal contracts between the consumer and
service provider. The documents typically outline
the degree to which data is stored, shared, or
sold but are not federally required to do so.2


                                                     P3
                                                                               WHY DIGITAL PRIVACY IS SO COMPLICATED




Layer 1: Social/Legal                                       Today, data repositories contain sweeping
Historic repositories of personal data, ledgers             details about billions of people‚Äôs social and
stored in notebooks or personnel documents                  financial lives. To protect users and comply
in filing cabinets, were naturally limited and              with the law, personal data must be secured
constrained. People had a social expectation that           against malicious access and use. As the
most of their life that was not explicitly disclosed        repositories are now digital, safeguarding access
was unrecorded.                                             is inherently technological.

Modern data repositories, stored in servers                 None of the international privacy laws surveyed
worldwide, collect vast and specific information            for this piece stipulate specific security
about people‚Äôs characteristics and behavior,                technologies; instead, requiring security by
prompting uncertainty about rights, data access,            design.9,10,11,12 The strategy is twofold: first,
and scope. Historic rules for pre-digital privacy           implementing technological safeguards for
do not currently cover the necessary scope in               personal data; second, requiring disclosure of
the digital age.7 New digital privacy laws around           data breaches, as no security system can be
the world seek to address this gap, and countries           absolute.
are taking very different approaches to solving
                                                            Layer 3: Economics
the legal question of who has the right to collect,
                                                            In the past, it was costly in both time and money
access, and use data about individuals.8
                                                            to collect private data. Its value was usually
In the United States, at the federal level, data            limited to narrow applications, such as business
supplied in exchange for tools and services is              insights and administrative records. Therefore,
held and used by the data collector or processor.           by and large only essential data was captured.
Users click consent on legally binding privacy              Technology has made the mass collection
agreements, detailing the scope of data use                 of data easy. It is a self-reinforcing cycle
by the firm. Unless explicitly stated in these              where technological development facilitates
agreements, data can be traded and sold on the              data collection, creating insights for further
market, following standard definitions of trade             technological improvements.
and exchange. In contrast, the EU gives broad
                                                            Technological improvements can be found in
legal rights to the individual to access, modify,
                                                            every sector from health to finance to recreation.
and delete information stored about them
                                                            The digital economy was founded upon
regardless of who collects and stores the data.
                                                            consumer willingness to exchange personal
We will fully explore contrasting privacy models            information for access to free services. But there
later in this paper.                                        is tension between the massive societal benefits
                                                            of these technological innovations and the
Layer 2: Security
                                                            privacy concerns of citizens and governments.
Ledgers in notebooks and papers in filing
cabinets were decentralized, difficult and                  There‚Äôs no question that the United States needs
expensive to copy, and primarily limited to                 a national privacy law to protect consumers,
voluntary disclosure. Stealing data had a high              for cyber security, and to be a part of the global
barrier to entry and low utility.                           privacy conversation.13 Balancing legal rights and



                                                       P4
                                                                                    WHY DIGITAL PRIVACY IS SO COMPLICATED




security rules to not unduly hinder innovation and            HOW IS PRIVACY DEFINED ACROSS PEER NATIONS?
successful business models is trickier. Overly                Data privacy laws have cropped up in every
restrictive data privacy laws can also create high            corner of the world in response to the flood
barriers to entry for smaller and less-resourced              of previously unrecorded personal data about
groups and risk suppressing innovation.14                     individuals‚Äô daily lives. The new data environment
                                                              has become essential for economic innovation,
The following section will examine international
                                                              but vast repositories of personal details and
data privacy legislation, surveying the laws
                                                              behaviors can be easily exploited in the wrong
that govern some of the largest markets: the
                                                              hands. As a result, privacy laws seek to regulate
United States, the European Union, and China,
                                                              the interrelated layers of legal rights and security
as well as the laws proposed by other prominent
                                                              obligations around PII.
democracies: India, the United Kingdom, and
Canada. It will define and analyze each nation‚Äôs
approach to the legal, security, and economic
aspects of data privacy.


FIGURE 1: EACH COUNTRY BALANCES THE THREE LAYERS OF PRIVACY: LEGAL RIGHTS OF THE DATA SUBJECT,
SECURITY, AND INNOVATION. AN IDEAL PRIVACY ENVIRONMENT WOULD BALANCE THE THREE LAYERS



                                          Legal rights
                       Security              for the                   Innovation
                                           individual




                                           Privacy




United States
The United States is a global innovation leader               and Privacy Act (FERPA), fail to account for
with the greatest share of leading tech firms.                a changing landscape of data collection.15
Nevertheless, for various reasons, the United                 Consumer devices, like Fitbit and Apple Watch,
States has yet to pass a comprehensive digital                collect health data but aren‚Äôt covered under
privacy law. Ad-hoc laws from the 20th century                HIPAA. Youth don‚Äôt need to age-identify on
focused on data and privacy, such as the Health               websites that may be collecting their personal
Insurance Portability and Accountability Act                  information. Protections for all consumers need
(HIPAA), the Children‚Äôs Online Privacy Protection             a refresh to take into account the new digital
Act (COPPA), and the Family Educational Rights                environment.16


                                                         P5
                                                                                 WHY DIGITAL PRIVACY IS SO COMPLICATED




The United States addresses the three layers                  From algorithm improvement to sales, the varied
of digital privacy: legal rights, security, and               uses of data can make the second layer of U.S.
innovation, through sector-specific laws, such as             digital privacy, security, seem like a free-for-
those listed above, and with market forces. In the            all. This is not the case. Data generate market
data market, users trade personal data for online             insights and inform the direction of innovative
services. Trading data for services takes several             technologies. Firms have a strong incentive
different forms: voluntary, where consumers                   to secure the data they possess to maintain
publish data about themselves on social media;                consumer trust and safeguard their ability to
necessary, where to use an essential product,                 compete.
like a smartphone or maps, specific data must
                                                              Data security, however, requires adaptability
be disclosed; and surveillant, when data is
                                                              and strong technological capabilities. The best
collected or sold without the explicit knowledge
                                                              safeguards available by today‚Äôs standards could
or consent of the data subject. After collection,
                                                              be hacked tomorrow. All U.S. states enforce data
as with any other economic exchange, the firm
                                                              breach notifications; under federal law, however,
can use, store, and process the data.
                                                              only financial, health care, and telecom breaches
Whatever the collection method, the utility                   must be reported to the consumer.21 At the federal
of data is multi-dimensional. Equifax and                     level, if a breach occurs, the firm may be liable
Oracle collect and sell data to others, among                 for negligence. However, this must be proven in a
other uses.17 Google, for instance, uses data                 court of law, a high barrier for individuals seeking
for targeted advertising, innovation, and                     remediation for data breaches.
improvement of products and algorithms.18
                                                              This light-touch approach has strongly
Amazon, among other purposes, uses data to
                                                              contributed to America‚Äôs ability to be the seat
improve user experience and lower the costs of
                                                              of global innovation. Firms‚Äô ability to collect
packing and sorting.19
                                                              data about millions, and in some cases billions,
In the U.S., the first layer of data privacy gives the        of users facilitates that success. Data‚Äôs
majority of legal rights to the firms that collect or         flexible and multifaceted utility has become a
possess data. Data become goods exchanged                     dominant business model for the private and
for otherwise free services. This exchange is                 public sectors.22 The technologies and insights
detailed in privacy agreements, often full of                 that have arisen from the firm-driven data
legal jargon, which define firms‚Äô rights to collect           environment have become a self-reinforcing
and store data but are difficult to understand                cycle; more data engenders more innovation,
or interpret by the average consumer. Data                    yielding massive economic value with enormous
subjects, individuals about whom the data is                  benefits to modern life.
collected, have little additional access to see,
                                                              But not everyone is happy with the new status
update, or delete the data about them. Because
                                                              quo driven by America‚Äôs growth mentality and
there are no comprehensive federal protections
                                                              light-touch regulatory environment. In particular,
to view, update, or delete data, it‚Äôs unclear how
                                                              countries worldwide see the value of data and
many companies may be selling or copying
                                                              are passing privacy laws, in part to countermand
consumer data.20
                                                              U.S. dominance.


                                                         P6
                                                                                WHY DIGITAL PRIVACY IS SO COMPLICATED




FIGURE 2: THE UNITED STATES BALANCES INNOVATION OVER SECURITY OR RIGHTS FOR THE DATA SUBJECT




                           Security   Legal rights
                                         for the
                                       individual
                                                                         Innovation




                                                     Privacy




  Canada
  Two major federal privacy laws govern                     gives individuals rights to view and update
  personal data use in Canada: The Privacy                  their data with data collectors and permits
  Act, 1985, which directs federal government               requests to delete personal data after the
  data use, and the Personal Information                    company has used the data for the required
  Protection and Electronic Documents Act                   purpose. If companies want to keep the data
  (PIPEDA), passed in February 2022 and                     after a request to delete, they may anonymize
  governs all commercial data use.23 Provincial             and continue to use it.
  governments in Canada also enforce                        PIPEDA requires firms to protect personal
  provincial-level data protections that in some            data in accordance with how sensitive the
  cases supersede PIPEDA, except where                      data are and to implement security adaptably
  data flows across provincial or international             as technical safeguards improve over time. In
  borders. The two federal acts clarify data                the event of a data breach, firms must notify
  access and security for citizens.                         individuals and keep records of all breaches,
  PIPEDA grants firms broad rights to data                  though firms are not liable.
  while still giving consumers some control.                The Canadian digital privacy environment is
  The law directs companies to gain consent                 fragmented. In addition to PIPEDA and the
  for data collection and gives citizens some               Privacy Act, Canadian provincial governments
  access to their personal data. It also gives              enforce other provincial-level privacy laws.
  firms broad leeway to, without the consent of             Companies operating across Canada need to
  the individual, disclose data for investigative           comply with provincial laws as well as federal.
  purposes, like a breach of contract. PIPEDA




                                                       P7
                                                                               WHY DIGITAL PRIVACY IS SO COMPLICATED




European Union
In 2016, the European Union announced a                    monitoring a publicly accessible place on
landmark data privacy law, the General Data                a large scale,‚Äù or processing data about
Protection Regulation (GDPR). The GDPR is                  immutable characteristics must conduct a risk
the most complex and consumer-focused                      assessment.27 The risk assessment requires
digital privacy law globally. The comprehensive            data collectors to indicate the purpose of data
legislation covers the rules and responsibilities          collection, outline the data to be collected, the
for data handling in all sectors, government,              duration of data storage, security features,
business, and nonprofits. Since coming into                and risk mitigation. Additional data security
full effect in 2018, the GDPR has offered an               technical requirements under the GDPR include
alternative to the U.S. market approach to data.           pseudonymization, i.e., replacing qualitative
                                                           information, like names, with artificial identifiers,
The U.S. approach defines data as a tradeable              and encryption, digital scrambling of the
commodity for access to digital services, giving           remaining data. Other security measures are
legal rights to the entity in possession of the            left to the purview of the firm.28 A divergence
data. The EU model treats these rights very                from the U.S. model is that in the case of a data
differently. It provides legal rights of PII to the        breach, the data controller is required to notify
data subject.24 Europeans can withdraw, change,            the data subject and could be liable for the
update, or delete their information even when it           resulting harms.29
is no longer in their possession. Firms effectively
rent or lease data from citizens. Privacy policies         The GDPR‚Äôs impact on the third layer of digital
set out the terms for temporary use by stating             privacy, innovation, continues to evolve. Some
the purpose of data collection. The GDPR                   aspects, such as clarifying and improving
requires firms to minimize data collection and             data protection and security, benefit firms and
delete it after use.                                       consumers. Exemptions for public and private
                                                           research, so long as data is well secured, create
Moving to the second layer, data security is a             clear avenues for certain types of innovation.
raison d‚Äô√™tre for the legislation. Recital 1 of the
regulation states, ‚ÄúEveryone has the right to the          Built into the GDPR is a requirement for data
protection of personal data concerning him or              minimization, i.e., only collecting data with a
her.‚Äù25 Firms are permitted to rent data from              clearly defined purpose: nothing superfluous.30
individuals with a guarantee of security.                  Restricting the quantity and duplication of
                                                           personal information collected recaptures some
The GDPR uses a risk-based method, requiring               of the natural limits of historic data privacy.
data protection by design, building security               More research is needed to see how it interacts
features into every aspect of the data journey.            with 21st-century economic growth models
Any firm collecting data must perform a data               and innovative technologies like machine
protection impact assessment (DPIA).26 This                learning, which rely on the ability to collect large,
means that, from the outset, any entity collecting         representative, datasets.31
location or behavior data, ‚Äúsystematically




                                                      P8
                                                                                       WHY DIGITAL PRIVACY IS SO COMPLICATED




FIGURE 3: THE EU WEIGHS LEGAL RIGHTS FOR THE INDIVIDUAL OVER SECURITY AND INNOVATION



                       Security
                                  Innovation
                                                                    Legal rights for
                                                                     the individual




                                               Privacy




  United Kingdom
  Since leaving the EU, the United Kingdom                    While the reforms are still in their early stages,
  has followed the data protection principles                 the UK signals a break with the GDPR, seeking
  of GDPR, re-branding the legislation as UK-                 instead to become a country that is both
  GDPR. However, as a part of their overarching               pro-growth and pro-data rights.33 The model
  reassessments of inherited EU laws, the British             highlights robust transparency and security
  government is in the process of refining digital            for citizens, while proactively collecting data
  privacy legislation.                                        for better public services and permitting data
                                                              collection to further private sector innovation.
  In September 2021, the Department for Digital,
                                                              The key, according to the report, is data
  Culture, Media, and Sport released ‚ÄúData: A New
                                                              availability and data responsibility. Availability
  Direction,‚Äù with the goal to align transparency
                                                              highlights the need to facilitate data re-use and
  and security with digital innovation.32 This report
                                                              responsibility ensures the data isn‚Äôt misused.34
  seeks to alleviate burdens on businesses and
  improve public sector services by reducing                  The UK wants to explore a flexible framework,
  barriers to data flows. It does not define data             particularly to help small and medium-sized
  rights nor indicate if it seeks to change data              enterprises. It proposes greater leeway for data
  access from EU-GDPR, where citizens have full               reuse so that the UK can be a secure hub for
  rights over their data.                                     data flows.

  The new UK proposal highlights the
  interconnectedness of security and innovation.




                                                         P9
                                                                                  WHY DIGITAL PRIVACY IS SO COMPLICATED




China                                                         information. In the event of a breach, the data
The People‚Äôs Republic of China passed its                     collector is liable.39 For firms that collect large
version of the GDPR, the Personal Information                 amounts (as determined by the state) of PII,
Protection Law (PIPL) on August 20, 2021.35                   the law requires a data protection officer to
It came into force just three months later, on                manage the security process. Regardless of
November 1. The legislation is yet another take               quantity, PIPL recommends encryption and
on data protection in the modern era; one that                pseudonymization of data, though it does not
reflects their state-centric data model. It is                offer further technical suggestions. If the data
important to note that China has a longstanding               is fully anonymized, whereby anyone viewing
political contract between citizen data and the               the data would not be able to attribute it to an
state, whereby the state is allowed to collect,               individual, PIPL does not apply.
view, and store any information related to its
                                                              The innovation layer of digital privacy in China is
citizens. Consequently, its data protection law
                                                              closely intertwined with the rights and security
is focused primarily on how companies use
                                                              layers. PIPL imposes strict data localization as a
Chinese citizens‚Äô data.36
                                                              security feature and an economic development
The Chinese understanding of the first layer                  feature, whereby data collected within China
of digital privacy, legal rights, is that rights are          cannot be transferred or processed outside
a co-operative between the individual and the                 of the country without explicit consent by the
state. Similar to the GDPR, a translated version              government. Non-Chinese firms operating in
of Article 44 of PIPL states: ‚ÄúIndividuals have the           China must appoint specific staff to process
right to know and the right to decide relating to             Chinese data within the country‚Äôs borders.40
their personal information, and have the right
                                                              Whereas the GDPR applies to both state and
to limit or refuse the handling of their personal
                                                              private entities, PIPL only applies to companies
information by others.‚Äù37 PIPL, like the GDPR,
                                                              handling Chinese citizen data. As of now, it‚Äôs
gives citizens the right to update and delete their
                                                              unclear whether these new restrictions will shift
data with companies. The state, however, holds
                                                              the strategy of multinationals with offices in
full legal rights to citizen data and acquire citizen
                                                              China. The most stringent penalty for companies
data from firms at any time. The government has
                                                              that violate the law could be prohibition from
broad circumstantial discretion in legal rights to
                                                              operating in China.
PII in China.38

Addressing the second layer, data security, PIPL
requires strong technical protection of personal




                                                        P10
                                                                                      WHY DIGITAL PRIVACY IS SO COMPLICATED




FIGURE 4: CHINA WEIGHS THE LEGAL RIGHTS OF THE STATE ABOVE OTHER CONSIDERATIONS


                         Security
                                    Innovation
                                                                      Legal rights
                                                                      for the state




                                                 Privacy




  India
  As of the writing of this paper, India has yet             than legal rights or innovation. The DPB has
  to pass a comprehensive privacy bill, though               similar technical specifications to the EU and
  frameworks for legislation have been under                 China: security by design. In the Indian bill,
  discussion since 2017. The Indian approach                 data localization is leveraged as a primary
  considers data to be a national asset, imposing            security feature. To that end, DPB identifies
  data localization and giving increased data                three data categories: sensitive data, or PII;
  powers to the state. India sees itself as a fourth         critical data, information important to national
  way to the American, European, and Chinese                 security; and general data, an umbrella term
  approaches to data privacy.                                for the remaining data types. Each category
                                                             requires different levels of localization.
  The Data Protection Bill (DPB), similar to the
                                                             Sensitive data may be processed outside of
  European model, would give legal rights of
                                                             India but must be stored within the country.
  PII to the data subject while, more similar to
                                                             Critical data may never leave the country‚Äôs
  China, reserving ultimate powers for the state.
                                                             borders.42
  Individuals, similar to the GDPR, would have
  broad rights to access, modify, and delete                 India‚Äôs data protection bill covers the vast
  data, including the right to be forgotten. Data            majority of businesses. Similar to the GDPR,
  collectors would have the right to collect                 some sectors, such as journalists, researchers,
  PII from citizens using consent-based data                 statisticians, lawyers, small businesses, and
  collection ‚Äî the privacy policy checkboxes                 the state, would be exempt from compliance
  now ubiquitous on most websites.41                         with DPA. And, similar to the Chinese law, the
  Regardless of legal rights, the state is exempt            Indian government would retain itself broad
  from DPB and may impose state power to                     latitude to access any digital information
  collect either personal or non-personal data at            within India, even if that information is
  any time.                                                  protected by intellectual property rights.43

  India weighs the security layer more heavily



                                                       P11
                                                                                  WHY DIGITAL PRIVACY IS SO COMPLICATED




POTENTIAL CONSIDERATIONS FOR U.S. POLICY                      data and that users have the time to explore
In this piece, we‚Äôve examined many approaches                 and modify every privacy setting across every
to digital privacy across three levels: legal rights,         website.
security, and innovation. We will not lay out a
                                                              Data Protection Risk Assessment
strategy in this piece except to say that all three
                                                              A vital aspect of the transparent and safe use
levels are important. It is time for the United
                                                              of data is for regulators to have a clear sense
States, as a global leader in digital innovation, to
                                                              of what information is being collected about
begin to codify data privacy. A future law or laws
                                                              individuals and its general purpose (i.e., sales,
should seek to close important gaps in today‚Äôs
                                                              R&D, marketing, etc.). A risk assessment is
patchwork of federal laws and regulations aimed
                                                              standard across financial services and other
at protecting personal data, and to standardize
                                                              areas where harm could befall a consumer or
important digital privacy rights amid a confusing
                                                              employee. Data should be no different.
and potentially conflicting welter of state
regulations.                                                  Data localization
                                                              Data localization appears in several of the
In conclusion, here are some important
                                                              privacy models in this paper, including
considerations of key issues lawmakers will
                                                              the E.U., China, and India. From a national
have to grapple with to craft a national privacy
                                                              security perspective, data localization allows
law that gives U.S. consumers greater control
                                                              greater control for governments. It also offers
over their data and creates stronger safeguards
                                                              transparency to citizens that their data is not
against misuse of their information.
                                                              processed outside the jurisdiction of their
Consent-based privacy                                         consumer protection agencies. This is the
Much of the current privacy legislation highlights            argument of the European Union Court ruling
the importance of consent and control for data                of July 2020, Shrems II, which eliminated
collection. Consent-based rules allow consumers               certain pathways for cross border data flow
to opt-in and opt-out of data collection and to               between the U.S. and E.U. due to concerns that
better control the flow of personal data to firms.            U.S. data protection laws are not compliant
Facebook and Google created data dashboards                   with the GDPR.45 As of March 2022, the U.S.
for users while website cookie pop-ups prevent                and European Commission announced a new
shedding personal information on every page.                  data-sharing framework, but it might face legal
Explicit consent is an appeal to consumer                     challenges.46
transparency and choice.
                                                              This report has touched on the potential
Data use for business is complex, multifaceted,               impact of data localization for tech innovation
and evolving. Consumers interact with many                    and competition, but it also has significant
websites every day. Reading through and                       implications for the future of the internet. The
agreeing to a daunting and lengthy data                       world seems headed toward a ‚Äúsplinternet‚Äù ‚Äî
usage agreement for each one of these sites                   the fragmenting of today‚Äôs borderless internet.
is impractical and gives only an ‚Äúillusion of                 China already has insulated its online activity
control.‚Äù44 Illusion because consent-based                    from the rest of the world, and Russia also is
privacy that users understand all the uses of                 pursuing a closed-off internet.47 By erecting


                                                        P12
                                                                                WHY DIGITAL PRIVACY IS SO COMPLICATED




barriers to the essentially free exchange of ideas           information to power these algorithms and
and information online. Data localization restricts          generate new technologies.
the ability to globalize internet services and solve
                                                             Moving forward
global problems. It can also allow governments
                                                             With three major global privacy standards
greater control over what knowledge citizens
                                                             already in effect, the United States has an
have access to.
                                                             opportunity to set a new precedent for data
Data minimization and the future of artificial               protection. While the U.S. market approach
intelligence                                                 receives pushback from countries and
A major concern by privacy advocates                         individuals alike, this approach‚Äôs ability to
and consumers is the quantity of personal                    innovate is very successful. But innovation
information collected by data collectors. The                is also an ability to envision and implement
data helps them develop better products and                  systemic improvements.
market personalized experiences to customers.
                                                             The U.S. can take a pragmatic approach by
The answer throughout the privacy legislation
                                                             aligning user rights with the market needs
surveyed in this piece is data minimization, or
                                                             regarding data access. An example of this would
limiting the amount of personal information
                                                             be allowing firms to retain control of the data
collected. The more identifiable information that
                                                             required to offer their services while granting
is stored, the greater the opportunity for that
                                                             users transparent access to update or remove
data to be hacked, revealed, or shared. There
                                                             that data.
are also legitimate privacy concerns around the
quantity and quality of data profiling that firms            Additionally, the U.S. approach must
may create about individual consumers.                       improve data security provisions. It can
                                                             learn from international legislation to include
Data collection, however, isn‚Äôt just for data
                                                             comprehensive technical security requirements
profiles. Data collected across the internet
                                                             and prevent harm caused by breaches from
also powers machine learning and artificial
                                                             falling entirely on the individual.
intelligence, which are increasingly crucial
in nearly every economic sector. These                       Finally, developing privacy rules that maintain,
technologies work by using mathematical                      and even augment, incentives to innovate
functions to find patterns in massive amounts                is key. An unbalanced approach will lead to
of data that would be too complex for humans                 unsatisfactory outcomes. The U.S. has the
to process. Although it might seem that these                opportunity to put forward a new approach that
machines are intelligent, they don‚Äôt learn from              balances all three layers key to successful data
their surroundings, only from the inputted                   privacy: legal rights, security, and innovation.
instructions and data. And, to work correctly,
                                                             Our subsequent report will dive further into
they need a lot of it. There is a balance to be
                                                             privacy laws within U.S. states and legislation
struck between minimizing personal information
                                                             at the federal level.
collected while still collecting sufficiently rich




                                                       P13
                                                                                                     WHY DIGITAL PRIVACY IS SO COMPLICATED




References
1    Neil Richards, Why Privacy Matters (New York, NY: Oxford University Press, 2022).

2    Richards, Why Privacy Matters, 22.

3    Richards, Why Privacy Matters, 22, 83-85.

4    Jamie Susskind, Future Politics: Living Together in a World Transformed by Tech (Oxford, UK: Oxford University Press, 2020).

5    Susskind, Future Politics, 61.

6    Richards, Why Privacy Matters, 22.

7    Richards, Why Privacy Matters, 90-91.

8    Cathy Cosgrove, ‚ÄúGlobal Comprehensive Privacy Law Mapping Chart,‚Äù Global Comprehensive Privacy Law Mapping Chart (International
     Association of Privacy Professionals, November 2021),
     https://iapp.org/resources/article/global-comprehensive-privacy-law-mapping-chart/.

9    ‚ÄúGeneral Data Protection Regulation (GDPR)‚Äù (Proton Technologies AG, 2022), https://gdpr.eu/.

10   Rogier Creemers and Graham Webster, ‚ÄúTranslation: Personal Information Protection Law of the People‚Äôs Republic of China - Effective
     Nov. 1, 2021,‚Äù DigiChina (Stanford University, January 5, 2022), https://digichina.stanford.edu/work/translation-personal-information-
     protection-law-of-the-peoples-republic-of-china-effective-nov-1-2021/.

11   Information Commissioner‚Äôs Office, ‚ÄúData: A New Direction,‚Äù (September 10, 2021), https://assets.publishing.service.gov.uk/government/
     uploads/system/uploads/attachment_data/file/1022315/Data_Reform_Consultation_Document__Accessible_.pdf.

12   ‚ÄúThe Personal Information Protection and Electronic Documents Act (PIPEDA)‚Äù (Office of the Privacy Commissioner of Canada,
     December 8, 2021), https://www.priv.gc.ca/en/privacy-topics/privacy-laws-in-canada/the-personal-information-protection-and-
     electronic-documents-act-pipeda/.

13   Jennifer Huddleston, ‚ÄúThe Importance of Balancing Privacy with Innovation, Consumer Benefits, and Other Rights in the FTC‚Äôs Approach
     to Consumer Data Privacy,‚Äù Mercatus Center at George Mason University, May 30, 2019,
     https://www.mercatus.org/publications/antitrust-and-competition/importance-balancing-privacy-innovation-consumer-benefits-and.

14   Luke Dascoli, Gillian Diebold, and Daniel Castro, ‚ÄúThe Looming Cost of a Patchwork of State Privacy Laws‚Äù (Information Technology and
     Innovation Foundation, January 24, 2022), https://itif.org/publications/2022/01/24/looming-cost-patchwork-state-privacy-laws.

15   Richards, Why Privacy Matters, 50-52.

16   Syagnik Banerjee, Phil Longstreet, and Thomas Hemphill, ‚ÄúWearable Devices and Healthcare: Data Sharing and Privacy,‚Äù The Information
     Society 34, no. 1 (December 27, 2017): pp. 49-57, https://doi.org/10.1080/01972243.2017.1391912.

17   ‚ÄúThese Are the Largest Data Brokers in America,‚Äù Privacy Bee, August 30, 2021,
     https://privacybee.com/blog/these-are-the-largest-data-brokers-in-america/.

18   Google, ‚ÄúGoogle Ads Data and Privacy ,‚Äù Google Safety Center, accessed April 18, 2022, https://safety.google/privacy/ads-and-data/.

19   ‚ÄúAmazon.com Privacy Notice,‚Äù Amazon, last modified February 12, 2021,
     https://www.amazon.com/gp/help/customer/display.html?nodeId=GX7NJQ4ZB8MHFRNJ.

20   Thorin Klosowski, ‚ÄúThe State of Consumer Data Privacy Laws in the US (and Why It Matters),‚Äù The New York Times, September 6, 2021,
     https://www.nytimes.com/wirecutter/blog/state-of-privacy-laws-in-us/.




                                                                      P14
                                                                                                     WHY DIGITAL PRIVACY IS SO COMPLICATED




21   India Vincent, ‚ÄúData Breach Notification Laws in the United States: What Is Required and How Is That Determined?,‚Äù JD Supra, December
     10, 2021, https://www.jdsupra.com/legalnews/data-breach-notification-laws-in-the-5409251/.

22   Abou Zakaria Faroukhi et al., ‚ÄúBig Data Monetization throughout Big Data Value Chain: A Comprehensive Review,‚Äù Journal of Big Data 7,
     no. 1 (January 8, 2020): pp. 1-22, https://doi.org/10.1186/s40537-019-0281-5.

23   ‚ÄúThe Personal Information Protection and Electronic Documents Act (PIPEDA),‚Äù Office of the Privacy Commissioner of Canada, last
     modified December 8, 2021, https://www.priv.gc.ca/en/privacy-topics/privacy-laws-in-canada/the-personal-information-protection-and-
     electronic-documents-act-pipeda/.

24   ‚ÄúGeneral Data Protection Regulation (GDPR)‚Äù (Proton Technologies AG), accessed April 18, 2022, https://gdpr.eu/.

25   General Data Protection Regulation, 2022, Recital 1.

26   ‚ÄúData Protection Impact Assessment (DPIA)‚Äù (Proton Technologies), accessed April 18, 2022,
     https://gdpr.eu/data-protection-impact-assessment-template/.

27   General Data Protection Regulation, Article 35.

28   Data Protection Impact Assessment, 2019.

29   General Data Protection Regulation, 2022, Art. 82.

30   Nicholas Martin et al., ‚ÄúHow Data Protection Regulation Affects Startup Innovation,‚Äù Information Systems Frontiers 21, no. 6 (2019): pp.
     1307-1324, https://doi.org/10.1007/s10796-019-09974-2.

31   Castro and Dascoli, 2022.

32   ‚ÄúData: A New Direction,‚Äù Department for Digital, Culture, Media & Sport (GOV.UK), September 10, 2021,
     https://www.gov.uk/government/consultations/data-a-new-direction.

33   ‚ÄúData: A New Direction,‚Äù 7.

34   ‚ÄúData: A New Direction.‚Äù

35   Katie Nadworny, ‚ÄúNew Data Privacy Law Will Soon Take Effect in China,‚Äù Society for Human Resource Management (SHRM), October 5,
     2021, https://www.shrm.org/resourcesandtools/hr-topics/global-hr/pages/china-data-privacy-law.aspx.

36   Creemers and Webster, ‚ÄúTranslation: Personal Information.‚Äù

37   Creemers and Webster, ‚ÄúTranslation: Personal Information,‚Äù Article 44.

38   Nadworny, ‚ÄúNew Data Privacy Law.‚Äù

39   Creemers and Webster, ‚ÄúTranslation: Personal Information.‚Äù

40   Jen Fulmer, ‚ÄúHow to Comply with GDPR, PIPL and CCPA,‚Äù eSecurity Planet, December 22, 2021,
     https://www.esecurityplanet.com/compliance/compliance-gdpr-pipl-ccpa/#pipl.

41   ‚ÄúThe Personal Data Protection Bill, 2019,‚Äù December 5, 2019,
     http://164.100.47.4/BillsTexts/LSBillTexts/Asintroduced/373_2019_LS_Eng.pdf.

42   Vijay Govindarajan, Anup Srivastava, and Luminita Enache, ‚ÄúHow India Plans to Protect Consumer Data,‚Äù Harvard Business Review,
     December 18, 2019, https://hbr.org/2019/12/how-india-plans-to-protect-consumer-data.

43   Anirudh Burman, ‚ÄúWill India‚Äôs Proposed Data Protection Law Protect Privacy and Promote Growth?‚Äù (Carnegie India, March 2020),
     https://carnegieendowment.org/files/Burman_Data_Privacy.pdf.

44   Richards, Why Privacy Matters, 94-95.

45   Caitlin Fennessy, ‚ÄúThe ‚ÄòSchrems II‚Äô Decision: EU-US Data Transfers in Question,‚Äù International Association of Privacy Professionals,
     July 16, 2020, https://iapp.org/news/a/the-schrems-ii-decision-eu-us-data-transfers-in-question/.



                                                                      P15
                                                                                            WHY DIGITAL PRIVACY IS SO COMPLICATED




46   Jennie Cunningham and Amanda Witt, ‚ÄúGuarded Optimism on EU-US Data Transfers: The EU and US Announce Trans-Atlantic Data
     Privacy Framework,‚Äù JD Supra, May 9, 2022, https://www.jdsupra.com/legalnews/guarded-optimism-on-eu-us-data-6176507/.

47   Keith Wright, ‚ÄúThe ‚ÄòSplinternet‚Äô Is Already Here,‚Äù TechCrunch, March 13, 2019,
     https://techcrunch.com/2019/03/13/the-splinternet-is-already-here/.




ABOUT THE AUTHOR
Jordan Shapiro is an Economic and Data Policy Analyst at the Progressive Policy Institute.

                                                                      P16
                                                                                WHY DIGITAL PRIVACY IS SO COMPLICATED




The Progressive Policy Institute is a catalyst for policy innovation and        ¬© 2022
political reform based in Washington, D.C. Its mission is to create radically   PROGRESSIVE POLICY INSTITUTE
                                                                                ALL RIGHTS RESERVED.
pragmatic ideas for moving America beyond ideological and partisan
deadlock.                                                                       PROGRESSIVE POLICY INSTITUTE
                                                                                1156 15th Street NW
Founded in 1989, PPI started as the intellectual home of the New                Ste 400
Democrats and earned a reputation as President Bill Clinton‚Äôs ‚Äúidea             Washington, D.C. 20005

mill.‚Äù Many of its mold-breaking ideas have been translated into public
                                                                                Tel 202.525.3926
policy and law and have influenced international efforts to modernize           Fax 202.525.3941
progressive politics.
                                                                                info@ppionline.org
Today, PPI is developing fresh proposals for stimulating U.S. economic          progressivepolicy.org
innovation and growth; equipping all Americans with the skills and assets
that social mobility in the knowledge economy requires; modernizing an
overly bureaucratic and centralized public sector; and defending liberal
democracy in a dangerous world.



                                                         P17
5
Privacy and Data Protection

Working with computing systems, whether developing, integrating, or testing
them, will often involve working with data. Sometimes this data will be per-
sonal data, and sometimes these systems will have a major impact on the pri-
vate life of those targeted by these systems (think of data brokers, credit rating
agencies), or those interacting with these systems (in the case of social net-
works, search engines). In this chapter, we will investigate the legal domain of
privacy and data protection, which entails a series of legal requirements for the
development and design, for the default settings, and for the employment of
computer architectures. This chapter can in no way provide a comprehensive
overview of privacy and data protection, which would require two separate
books at the least. However, the purpose of this book is not to turn computer
scientists into lawyers. The purpose is to provide some real taste and true
bite of the law on legal topics that are highly relevant for computer science.
Therefore, please check the references for further reading and for real world
scenarios check with a practising lawyer.

  The right to privacy is a subjective right, attributed by objective law. This may be
  national (constitutional) law, international human rights law, or supranational law
  (EU fundamental rights law). In this chapter, we will first confront the landscape
  of human rights law at the global, national, and EU level, followed by a discussion
  of the concept of privacy. We will then inquire into the right of privacy, as guaran-
  teed under the European Convention on Human Rights (ECHR) and the Charter of
  Fundamental Rights of the European Union (CFREU), and finally, we will target the
  new fundamental right to data protection, as guaranteed by the CFREU and pro-
  tected by the General Data Protection Regulation (GDPR).



5.1 Human Rights Law

When tracing the history of human rights, we first encounter the English
Bill of Rights of 1689, followed by the revolutionary French D√©claration des
Droits de l‚ÄôHomme et du Citoyen of 1789 and the US Bill of Rights of 1791.
Though the famous Magna Charta of 1215 may seem an early example of a

Law for Computer Scientists and Other Folk. Mireille Hildebrandt, Oxford University Press [2020]. ¬© Mireille
Hildebrandt. DOI: 10.1093/oso/9780198860877.001.0001
100 Privacy and Data Protection

human rights charter, it did not attribute what we now call human rights.
Instead, it ensured that the feudal lords were able to restrict the powers
of the King, while protecting jurisdiction over their own subjects against
royal interference. The era of the Magna Charta saw the struggle between
a feudal society and an emergent royal power; this was not yet the era of a
powerful modern state that managed to subject each and every person on
its territory to its jurisdiction. The rights provided by the Magna Charta
were mainly reserved for powerful lords, who wished to preserve the
powers they had over their own land and their own serfs against the claims
of the king.


5.1.1 Human rights as defence rights against the
modern state

The rise of the modern state must be situated in the beginning of what his-
torians call the era of ‚ÄòModernity‚Äô, around the fifteenth and sixteenth century.
It was the rise of the modern, bureaucratic state that warranted new types of
protection against the monopolistic powers of the King and his clerks (feeding
on the impressive affordances of proliferating printed text, see section 1.4).
The rise of the idea of human rights coincides with the rise of sovereignty (see
section 1.4 and 4.1.2).
The human rights declarations of the seventeenth and eighteenth centuries
provided those subject to the power of a sovereign state with an entitlement
to civil and political rights, emulating their status to that of individual right
bearers and constituents of the polity.

 Being subject to a sovereign became being a subject in law. It is hard to imagine how
 novel the attribution of such individual, subjective rights was, even if initially their en-
 forcement was neither practical nor effective.


Some attribute the power of this attribution to the ‚Äòendowment bias‚Äô; if people
come to believe they ‚Äòhave‚Äô these rights, they will invest in ‚Äòkeeping‚Äô them. If
the struggle this entails succeeds, these rights will eventually be instituted
as effective subjective rights. In due course, respect for human dignity and
a new emphasis on the centrality of the individual reconfigured the idea of
law and politics, laying the groundwork for the more ‚Äòpractical and effective‚Äô
human rights protection of the second half of the twentieth century.
                                                           5.1 Human Rights Law 101

However, in the context of international law, human rights have been citizens‚Äô
rights rather than human rights, depending on constitutional protection and
citizenship, thus offering little protection for subjects of rogue states. After the
atrocities of the Second World War, states decided to elevate the protection
of human rights to the level of international law, starting with the Universal
Declaration of Human Rights of 1948. Though this declaration had no binding
force, it was soon followed by various treaties at the global and regional level,
aiming to finally institute human rights as enforceable subjective rights
against the state.


5.1.2 From liberty rights to social, economic,
and further rights


 Human rights law was originally focused on the protection of individual citizens
 against powerful states. We call these rights first generation human rights, and they
 are best described as the subjective right that the state refrains from interference
 with the legal good that is protected by such rights. This is why they are often called
 liberty rights.


These legal goods are: privacy, non-‚Äãdiscrimination, bodily integrity, freedom
of movement, the presumption of innocence, a fair trial, freedom of expres-
sion, freedom of association, freedom of religion, and voting rights. Note that
these legal goods are considered worthy of protection as public goods, because
a society that does not protect them cannot support a viable democracy that
depends on independence of thought and unhindered development of both
individual and group identities. For that reason, they are also called civil and
political rights. The focus is on public goods that protect individual persons as
autonomous agents in a democratic polity and on negative obligations of the
state towards its citizens.

 A second generation of human rights developed when it became clear that (1) non-‚Äã
 interference is not always enough to protect such public goods, while (2) a number of
 other public goods were absent in the initial inventories of human rights. The public
 goods protected by second generation human rights concern public, for instance,
 employment, food and housing, social security, healthcare, and access to basic util-
 ities such as electricity, postal services, and public transport.
102 Privacy and Data Protection

These rights are often called social and economic rights. To actually provide
these protected goods, a state cannot restrict itself to respecting liberty
rights. The second-‚Äãgeneration human rights impose positive obligations
on states to create and sustain the goods it must protect. This implies that
the second generation of human rights addresses states with ‚Äòinstruction
norms‚Äô, rather than providing citizens with directly enforceable subjective
rights. To exercise a right to employment, an economic system must be in
place that enables such a right, meaning that second generation human
rights require states to build institutions capable of supporting economic
welfare and a fair distribution of access to social and economic goods.

 Taking note that second generation human rights are instruction norms to states, ra-
 ther than directly enforceable individual rights, the latter part of the twentieth cen-
 tury witnessed advocacy for a third generation of human rights.


Here, we encounter rights to construct and develop group identities
and rights to a sustainable environment. These rights have even less of a
straightforward relationship with individual entitlement, focusing on the
rights of groups (e.g. the right to self-‚Äãdetermination for indigenous peo-
ples, which we already encountered in section 4.2.1, as a fundamental prin-
ciple of international law) and obligations towards the natural environment
on which human society depends (responsible innovation, sustainable
development).


5.2 The Concept of Privacy

Before investigating the right to privacy as part of the first generation
of human rights law, we will first inquire into the nature of privacy it-
self. The reason is that computer science has a specific relationship with
privacy, notably in the context of digital security and cryptography. In that
context, privacy is often seen as a subset of security, focused on hiding
or removing the link between data and whoever the data refer to, or on
encrypting the data to safeguard confidential data against eavesdrop-
ping. This has, as a consequence, meant that privacy protection is re-
stricted to (1) anonymization or pseudonymization of personal data, by
way of deleting or separating identifiers and to (2) hiding the content by
means of encryption or other security measures. The focus on hiding has
                                                       5.2 The Concept of Privacy 103

generated research fields such as differential privacy and reidentification
metrics, based on e.g. cryptography and key-‚Äãmanagement, k-‚Äãanonymity,
linkability metrics, and so on.
Though such research is of crucial importance to protect privacy, one
must not mistake issues of identifiability and confidentiality for issues of
privacy as the latter concerns far more than mere technical identifiability or
readability.

 Consider the following data points:

    ‚Ä¢ your name;
    ‚Ä¢ your bank account;
    ‚Ä¢ the taxes your mother pays;
    ‚Ä¢ what kind of socks you wear;
    ‚Ä¢ the logs of your surfing behaviour on the net;
    ‚Ä¢ your pattern of your energy usage behaviour;
    ‚Ä¢ the decision to have an abortion;
    ‚Ä¢ the decision, or inclination, to be a vegetarian.


Should we qualify this data as part of the privacy of the person the data
refers to?

 To answer this question, we need to check what falls within the value of, the interest
 in, or the right to privacy:

    ‚Ä¢ When (under what conditions)?
    ‚Ä¢ With regard to whom (is data on my mother part of my privacy)?
    ‚Ä¢ Where (are specific locations more privacy-‚Äãsensitive than others)?
    ‚Ä¢ For what reason (what could make my socks relevant to my privacy)?



5.2.1 Taxonomies and family resemblance

Many authors have made attempts to define privacy by summing up the
common denominators of what is generally seen as falling within the scope
of privacy. This turns out to be a questionable undertaking, because the con-
cept is as elusive as it is pertinent. Another way of tackling the issue of under-
standing privacy is to define it in terms of family resemblance.
104 Privacy and Data Protection


 The American privacy scholar and lawyer Daniel Solove made an insightful at-
 tempt to approximate the concept of privacy in terms of six categories that are
 partly overlapping, while thus covering much of what we intend when referring to
 privacy:

     1. the right to be left alone;
     2. limited access to self;
     3. secrecy‚Äî‚Äãconcealment;
     4. control over personal information;
     5. personhood‚Äî‚Äãprotection of identity, dignity; and
     6. intimacy.


Solove notes that some of these categories focus on goals, others on means,
while they are in various way interdependent. Taken separately, none of these
definitions would exhaust the concept of privacy, being either too broad or
too narrow. He warns that this is therefore not a taxonomy, which would as-
sume mutually independent features of the same thing. On the contrary, the
idea of a family resemblance means that privacy cannot be defined in terms
of necessary and sufficient conditions, because there is no common core to
the different conceptions of privacy. Instead, Wittgenstein‚Äôs notion of family
resemblances enables us to take a pragmatic approach, recognizing the con-
textual, historical, dynamic nature of privacy, such as relating to family life,
the body, or the home. This approach is bottom-‚Äãup rather than abstract and
acknowledges that, in the end, privacy is best seen as a set of practices rather
than a formula. The concept of family resemblance was introduced as a way
to understand the meaning of words by Wittgenstein in his Philosophical
Investigations. The concept is very interesting for computer science as it ex-
plains why translating concepts into ontologies or a semantic web may entail a
loss of meaning. I will therefore quote The Stanford Encyclopedia of Philosophy
to elucidate this understanding of meaning:

   There is no reason to look, as we have done traditionally‚Äî‚Äãand dogmatically‚Äî‚Äãfor
   one, essential core in which the meaning of a word is located and which is, there-
   fore, common to all uses of that word. We should, instead, travel with the word‚Äôs
   uses through ‚Äòa complicated network of similarities overlapping and criss-‚Äãcrossing‚Äô
   (PI 66).1 Family resemblance also serves to exhibit the lack of boundaries and the
   distance from exactness that characterize different uses of the same concept.

  1 This refers to para. 66 of Wittgenstein‚Äôs Philosophical Investigations. See the correct reference to the

Stanford Encyclopedia entry under references.
                                                                  5.2 The Concept of Privacy 105

   Such boundaries and exactness are the definitive traits of form‚Äî‚Äãbe it Platonic
   form, Aristotelian form, or the general form of a proposition adumbrated in the
   Tractatus.2 It is from such forms that applications of concepts can be deduced, but
   this is precisely what Wittgenstein now eschews in favor of appeal to similarity of a
   kind with family resemblance.

To emphasize the elusive nature of privacy, we briefly follow Solove‚Äôs discus-
sion of the categories enumerated above.

  A right to non-‚Äãinterference seems a pivotal shorthand for the right to privacy, as it
  clearly depicts the negative obligations of governments and others (vertical and hori-
  zontal effects of human rights law). Here, we think of privacy as the ‚Äòright to be left
  alone‚Äô, where privacy is a liberty or freedom, in the sense of freedom from external
  constraints.


This understanding of privacy is related to intimacy, to the idea of drawing
boundaries around a small circle of people with whom one dares to expose
oneself, sharing information that might otherwise be used to shame a person,
or to diminish or ridicule their agency. Intimacy relates to trust, not in the
sense of confidence and security, but in the sense of trusting others enough to
take the risk of being betrayed. One could ask what information is intimate,
but this assumes that ‚Äòintimacy‚Äô is a property of information, whereas all de-
pends on the situation, the context, and the roles played by intimate others. In
some situations, financial information, or information shared with a health
insurance company, may be intimate information, because it reveals to others
what makes a person vulnerable to shame, ridicule, or even to life-‚Äãthreatening
manipulation.
If we then take together privacy as limited access, and secrecy, anonymity and
solitude, we can address the legal notion of third-‚Äãparty disclosure.
In the United States, the Supreme Court decided, in 1967,3 that once a person
exposes their personal data to a third party such as banks or other service
providers, they have no reasonable expectation of privacy regarding access


   2 The Tractatus is Wittgenstein‚Äôs seminal work, preceding his Philosophical Investigations. In the latter, he

rejects propositional logic and definitions in terms of sufficient and necessary reasons, though he endorsed
them in the former. From the perspective of the latter, the view point taken in the former is just one ‚Äòlan-
guage game‚Äô amongst many others, noting that the former should not claim a monopoly on understanding
meaning.
  3 Katz v. United States, 389 U.S. 347, 360 (1967), confirmed in, e.g. California v. Greenwood, 486 U.S.

35, 41 (1988).
106 Privacy and Data Protection

by the government. This so-‚Äãcalled ‚Äòthird-‚Äãparty doctrine‚Äô reflects an approach
to privacy that is radically different from the European approach, which does
not presume that disclosing private information to one entity necessarily im-
plies that other entities are now free to obtain and use such information.
Note that the United States have since enacted legislation requiring a warrant
for access to specific data, thus providing specified protection for, for example,
financial data and telephone data. We have already encountered the case of
US v. Jones (followed by Riley and Carpenter, see section 2.1.2, n. 2), where
the Supreme Court decided that police warrants were necessary in the case
of GPS trackers, information on a cell phone, and cell-‚Äãsite records of a wire-
less carrier. These judgments may lead to the end of the third-‚Äãparty doctrine,
depending on subsequent case law.

 The next category, control over information about oneself, is often portrayed as the
 core meaning of what Americans call informational privacy. This understanding
 clearly links to the notion of identifiability, as it relates to information about an iden-
 tifiable person, thus also connecting this particular conception of privacy with the
 idea of privacy as a subset of digital security.


Defining privacy in terms of control comes close to thinking of personally
identifiable information (PII) as if it were the property of the person it con-
cerns. PII is, just like informational privacy, a term used in the United States,
whereas in the EU we generally speak of data protection and personal data.
Thinking of PII in terms of property creates a number of problems, as nei-
ther data nor information are rivalrous or exclusionary. One person ‚Äòhaving‚Äô
certain information does not necessarily imply that others do not ‚Äòhave‚Äô that
same information, whereas one person possessing a book implies that others
do not possess it. It is therefore important to distinguish between control over
‚Äòaccess to‚Äô and ‚Äòusage of ‚Äô information on the one hand, and property rights in
information on the other. The latter applies in the case of intellectual property
rights (e.g. copyright or patent), but not in the case of personal data. Below,
we will discuss to what extent EU data protection law provides control to data
subjects (those to whom personal data refers), but we can already point out
here that full control over one‚Äôs personal data ignores the relational nature of
personal data. To illustrate the latter point, we can think of Robinson Crusoe
and ask the question whether he had a name before Friday came to his island.
We have a name to be singled out by others, to be addressed by others, and to
appear as a singular individual person before others. This implies that, though
                                                       5.2 The Concept of Privacy 107

we need some control over the sharing of our name, such control cannot be
unlimited. Without fellows to address us, we effectively ‚Äòhave‚Äô no name.

 Finally, privacy is connected with personhood, with individuality, with dignity, and
 with autonomy. One could ask to what extent our personhood is private, noting that
 becoming a person depends on anticipating how others will frame us. Whereas the
 right to privacy is often seen as a liberty, as a right to be left alone, as a freedom from
 outside interference, privacy is also connected with a right to develop one‚Äôs own iden-
 tity, to be treated as worthy of respect, and the freedom to make one‚Äôs own choices
 concerning, for example, lifestyle, employment, education, and political opinion.
 Here, privacy sits on the cusp of freedom from unreasonable constraint and the
 freedom to construct one‚Äôs identity.


Indeed, this is how Agre and Rotenberg defined privacy, highlighting the
interrelationship between negative and positive freedom. This also suggests
that liberty and autonomy overlap and support each other. For instance, what
has been called ‚Äòdecisional privacy‚Äô (e.g. the right of a woman to decide about
an abortion) clearly marks the nexus of positive freedom (to decide an abor-
tion) with negative freedom (to be free from unreasonable constraints on
such a decision). The crux of Agre and Rotenberg‚Äôs definition resides in the
requirement that people are free from unreasonable constraints, not just any
constraints. In case law, legislation, and doctrine the concept of ‚Äòreasonable‚Äô or
‚Äòunreasonable‚Äô is of prime importance. Instead of framing this as a source of
uncertainty, because of its prima facie vagueness, this concept can be seen as
an aid in aligning different conceptions of legal goods that warrant protection.
Demanding that a duty of care is exercised in a reasonable way acknowledges
that ‚Äòa duty of care‚Äô cannot be defined in the abstract, but is better understood
in terms of family resemblances. The duty of care of a mother, an employer,
a manufacturer, and a social network provider may not share any common
element; they nevertheless align along the lines of reasonable expectations and
proper checks and balances, considering the relevant context and the roles of
the parties involved. Similarly, reasonable expectations of privacy depend on
context, on roles played, on checks and balances, and meaningful choice. This
is not because privacy is a vague concept but because the practice of privacy is
complex, requiring acuity to what is at stake for whom.

 Though the reader may by now be wary of the dynamic and shifting borders of the
 concept of privacy, it is crucial to sustain awareness that privacy is a moving target.
108 Privacy and Data Protection


 Defining privacy in terms of necessary and sufficient conditions would restrict pro-
 tection to what happens to fall within their scope, easily rendering the concept both
 over-‚Äã and under-‚Äãinclusive.


In the end, defining privacy is a decision to be taken when confronted with
its violation. As Solove saliently writes in reference to a famous American
philosopher:

  ‚Äò[K]‚Äånowledge is an affair of making sure,‚Äô Dewey observed, ‚Äònot of grasping ante-
  cedently given sureties.‚Äô

This is what the courts must achieve every time a case is brought before
them: making the difference that makes a difference.


5.2.2 Privacy and technology

After tracing the conceptual challenges of delineating privacy, I will briefly
trace the relationship between privacy and technology. Some of us may think
that privacy is a property of people in general, just like animals often display
what ethologists call ‚Äòcritical distance‚Äô from each other.

 Privacy, according to environmental psychologist Altman, is a matter of shaping and
 negotiating borders between self and others. It is not a property of a person, but of a
 relationship.


Rather than being a matter of seclusion, Altman frames privacy as a con-
tinuous process of sharing and excluding, based on societal practices that are
in turn dependent on technological affordances of the environment. In that
sense, privacy can be detected in most human societies, though under dif-
ferent names and with very different constraints.

 The right to privacy, however, is a recent historical artefact. As a subjective right, the
 right to privacy first surfaced at the end of the nineteenth century, in response to the
 proliferation of technologies such as photography and mass media.


In a famous article in the Harvard Law Review, US legal scholars Samuel
Warren and Louis Brandeis discussed the need to protect oneself against
                                                   5.2 The Concept of Privacy 109

publication of photographs without permission, to enable social withdrawal.
In that article, they formulated the right to privacy as the right to be left alone,
basically arguing for the existence of a privacy tort whenever this right was in-
fringed upon without justification. Interestingly, privacy was thus introduced
as a private law issue rather than a constitutional right. When Brandeis later
served as justice in the Supreme Court, however, he argued that such a right
to be left alone must be ‚Äòread into‚Äô the US Constitution, notably into the Bill of
Rights, thus vouching for a right to privacy against the state. The rise of mass
media and photography afforded massive dissemination of pictures taken,
thus infringing the privacy of those concerned in a previously unprecedented
manner. This, in turn, gave rise to defence mechanisms to safeguard one‚Äôs cap-
ability to withdraw from such exposure.

 This first appearance of a right to privacy fostered privacy as negative freedom: the
 right that others refrain from interference.


After the Second World War, a new technological infrastructure surfaced
to enable and improve public administration, in the form of computer-
ized databases. This resulted in the collection and storage of myriad data
relating to identifiable citizens, enabling government agencies to better
target their constituency and to engage in what would now be termed
‚Äòevidence-‚Äãbased policy‚Äô. This, in turn, raised the question to whom this
data belongs. In 1967, Alan Westin wrote a seminal work on Privacy and
Freedom, taking a clear stand on the question of who should‚Äî‚Äãby default‚Äî‚Äã
be capable of controlling access to data concerning individual persons.
Privacy, he wrote, is:

  the claim of individuals, groups, or institutions to determine for themselves
  when, how, and to what extent information about them is communicated to
  others.

This concept of informational privacy, as control over information, informs
much of the debate about privacy and data protection in our current age. It
is interesting to note that it emerged in counterpoint to the rise of databases
in public administration, as well as private enterprise. The fact that data was
collected, sorted, and recorded, enabling retrieval as well as aggregation, gave
rise to new types of transparency, and new types of threats to personal identity.
This was related to the fact that in this era the data collected and stored was
mostly stable data, allowing the mapping of both individuals and populations
110 Privacy and Data Protection

in consistent and foreseeable way, without the kind of dynamic and unstruc-
tured big data capture that characterizes the current era.

 This, second appearance of the right to privacy fosters privacy as a positive
 freedom: the freedom to determine how personal information is shared and used.


After the rise of the internet and the world wide web, combined with the cap-
ture of big data and data-‚Äãdriven techniques to infer new information, the need
for a more complex and contextual right to privacy seems obvious. Negative
freedom will not do, as data abounds and is captured beyond one‚Äôs control
on a permanent basis. For the same reason, positive freedom seems unattain-
able, as consent loses its meaning amidst the volume, variety, and velocity of
data capture, storage, and use. A more practical and effective way of under-
standing privacy should therefore combine negative and positive freedom,
while highlighting the relationship with identity-‚Äãconstruction, not merely
identification.
The definition of Agre and Rotenberg, referred to above, may be the most apt
for the era of proactive and pre-‚Äãemptive computing infrastructures, depicting
the right to privacy as:

  the right to be free of unreasonable constraints on the building of one‚Äôs identity.

For some readers, this may sound overly vague or complicated. To con-
front a complex, volatile, invasive, and pre-‚Äãemptive environment we will,
however, need an understanding of privacy that goes beyond the hiding of
personal data.


5.3 The Right to Privacy

Privacy is a value, an interest, a right, or a good. It can be analysed from an
ethical perspective (as a value, a virtue, or duty), from an economic perspec-
tive (as a utility, a preference, or an interest), and from the perspective of pol-
itical theory (as a public and a private good). In this work, we will focus on
the legal perspective, tracing positive law‚Äôs applicability to issues of privacy.
Below, we will discuss the right to privacy from the perspectives of consti-
tutional, international, and supranational law, ending with a discussion of
Article 8 ECHR.
                                                                     5.3 The Right to Privacy 111

5.3.1 The right to privacy: constitutional law

The right to privacy is a subjective right, attributed by objective law. The most
obvious branch of objective law that attributes the subjective right of privacy
is constitutional law, which often contains a section that aims to protect citi-
zens against overly invasive powers of the state. Historically, human rights ini-
tially played out in the vertical relationship between state and citizens, not in
the horizontal relations between private parties. The industrial revolution of
the nineteenth century gave rise to powerful economic actors whose ability
to infringe privacy, freedom of information, and non-‚Äãdiscrimination increas-
ingly matched the powers of the state.

 This has led courts to recognize a so-‚Äãcalled ‚Äòhorizontal effect‚Äô of constitutional rights
 such as privacy. This entails that protection against such infringements is a duty of
 the state, meaning that citizens can sue the state for failing to impose prohibitions to
 infringe these rights upon powerful players in the private sector. This is called indirect
 horizontal effect, because it cannot be invoked directly against private parties.

 Depending on national jurisdiction, courts may also attribute direct horizontal effect,
 when qualifying a violation of privacy by, for example, a company as a tortuous act
 in the context of private law. In that case, violation of privacy can be invoked directly
 against, for example, a private company.


In many states outside the Council of Europe, the Constitution provides the
main protection against infringements of the right to privacy. For instance, in
the United States, even though neither the 1787 US Constitution nor the 1791
Amendments to the US Constitution (known as the Bill of Rights) explicitly
refer to a right to privacy, the Supreme Court of the United States has never-
theless interpreted various articles of the Bill of Rights as safeguarding an indi-
vidual right to privacy,4 notably based on the Fourth Amendment:

     The right of the people to be secure in their persons, houses, papers, and effects,
     against unreasonable searches and seizures, shall not be violated, and no war-
     rants shall issue, but upon probable cause, supported by oath or affirmation,
     and particularly describing the place to be searched, and the persons or things
     to be seized.


 4   First relevant case was Griswold v. Connecticut, 381 U.S. 479 (1965).
112    Privacy and Data Protection


 This Amendment protects against:

      ‚Ä¢ ‚Äòunreasonable searches and seizures‚Äô by the police,
      ‚Ä¢ which require ‚Äòa warrant‚Äô,
      ‚Ä¢ that may only be issued in the case of probable cause (concrete and objectifiable
        suspicion), and
      ‚Ä¢ must contain a reasonably detailed description of what may be searched or
        seized.


We can read these protections in terms of legal conditions and legal effect, by
stating that ‚Äòsearches and seizures‚Äô by government officials are only lawful if:

  ‚Ä¢ there is probable cause,
  ‚Ä¢ a warrant has been issued,
  ‚Ä¢ which contains limitations as to what is allowed.

As we have already seen in section 2.1.2 and 5.1.2, the question here is
(1) whether this right protects against violation of property rights (trespass)
or also against violation of reasonable expectations of privacy that do not de-
pend on property and (2) whether search and seizure of, for example, a mobile
phone falls within the scope of the Fourth Amendment, as a phone is neither
part of a person, a house, paper, or effects.

 In the United States, constitutional protection of the right to privacy (which is also
 ‚Äòread into‚Äô other parts of the Bill of Rights) thus depends on national law, rather
 than international law. This has consequences for its applicability in the case of
 those who have no legal status in the United States, as it may be unclear whether
 the Bill of Rights even applies to them. Another consequence is that the enforce-
 ment of rights against the state is dependent on that same state. In contrast, the
 ECHR offers a more layered architecture of legal protection, which is at least in part
 dependent on a European court that is not part of the state against which it aims
 to protect.



5.3.2 The right to privacy: international law

Protection of human rights requires a resilient system of checks and balances,
that is, a series of institutional safeguards to ensure that the state does not
claim unreasonable exceptions and faces a stringently independent judiciary
                                                        5.3 The Right to Privacy 113

to keep the powers of the state ‚Äòin check‚Äô. As noted above, the need to protect
subjects of the state against the state, gave rise to international human rights
law, which provides an extra layer of checks and balances. Privacy is explicitly
protected by Article 17 of the United Nations (UN) International Covenant
on Civil and Political Rights (ICCPR) of 1966, and by Article 8 ECHR of 1950,
two examples of international law. Both articles are similar, we quote Article 8
ECHR to give the reader a first taste:

  1. Everyone has the right to respect for his private and family life, his home and his
     correspondence.
  2. There shall be no interference by a public authority with the exercise of this
     right except such as is in accordance with the law and is necessary in a demo-
     cratic society in the interests of national security, public safety or the eco-
     nomic well being of the country, for the prevention of disorder or crime, for the
     protection of health or morals, or for the protection of the rights and freedoms
     of others.

The UN ICCPR has global application, with currently 178 signatories and 172
ratifications, but its enforcement mechanisms are relatively weak compared to
the ECHR. In Article 34, the ECHR provides citizens of the forty-‚Äãeight con-
tracting parties with an individual right to complain to the European Court of
Human Rights (ECtHR):

  The Court may receive applications from any person, non-‚Äãgovernmental organisa-
  tion or group of individuals claiming to be the victim of a violation by one of the
  High Contracting Parties of the rights set forth in the Convention or the protocols
  thereto. The High Contracting Parties undertake not to hinder in any way the ef-
  fective exercise of this right.

The ECHR, however, does not have global application, as it only applies within
the jurisdiction of the Council of Europe.



5.3.3 The right to privacy: supranational law

Since 2009, when the CFREU came into force, the protection of human rights
has gained even more traction, adding a second European Court with compe-
tence to test legislation, decisions, and actions against a catalogue of human
rights. This protection, offered at the level of supranational law, is applicable
whenever member states (MSs) ‚Äòare implementing Union law‚Äô (Article 51
114    Privacy and Data Protection

CFREU). As human rights developed with the rise of the modern state, they
further developed with the rise of supranational jurisdiction. The prevailing
powers of the institutions of the EU demand countervailing powers in the
form of supranational fundamental rights.


5.3.4 Article 8 ECHR

In this section, we will discuss one of the most crucial legal rights of this book.
The right to privacy that is articulated in Article 8 ECHR is not only rele-
vant for bodily integrity, decisional privacy, and the other aspects of privacy,
but also directly affects issues of cybercrime and copyright. This is due to
the fact that cybercrimes may violate privacy (hacking, data breaches), or
that copyright holders may violate privacy when disseminating their works
(photographs, texts), but also because the investigative measures that aim to
detect cybercrime and violations of copyright often infringe upon the right to
privacy as protected in Article 8.
Here, we develop a first analysis of the legal conditions stipulated by art.
8 ECHR, how they are explained by the ECtHR, and the legal effects they
generate.
Article 8 consists of two paragraphs. The first paragraph concerns the ques-
tion of whether privacy is infringed, the second paragraph clarifies under
what conditions an infringement is justified.

  1. Everyone has the right to respect for his private and family life, his home and his
  correspondence.


 The legal effect generated by this paragraph is ‚Äòan infringement of privacy‚Äô, and this
 infringement depends on the following alternative legal conditions:

      ‚Ä¢ private life is not respected;
      ‚Ä¢ family life is not respected;
      ‚Ä¢ the protection of one‚Äôs home is not respected; and
      ‚Ä¢ the confidentiality of one‚Äôs correspondence is not respected.


The ECtHR takes the view that these concepts require a broad rather than a
narrow interpretation, bringing a wide variety of situations, events, relation-
ships, and contexts under the protection of Article 8.
                                                                  5.3 The Right to Privacy 115

Private life can be at stake in the context of work, meaning that a search of
an office space may be an infringement of privacy.5 Family life is at stake
when a state prohibits members of a family from living together, for instance
in the case of a refusal to provide a residence permit for a partner from an-
other state, or of a parent wishing to further develop a relationship with their
child despite not being married to the other parent. Protection of the home
may become relevant when a person has taken residence in a house they nei-
ther own nor rent, meaning that the need to respect one‚Äôs home is not de-
pendent upon ownership or contract. The confidentiality of communication
has been interpreted to include letters, telephone calls, and more recently all
types of internet-‚Äãenabled communication that is not public. Privacy, as pro-
tected by Article 8, clearly concerns physical, spatial, contextual, decisional,
communicative, and informational privacy, and although Article 8 addresses
the contracting states, its indirect horizontal effect has been recognized by
the ECtHR, requiring states to ensure proper protection against violations by
others than the state. Note that the individual complaint right of the ECHR
can only be invoked against a state, not against a company. To invoke direct
horizontal effect, a person needs to sue the tortfeasor in a national court.

 An infringement of privacy is not the same as a violation of the right to privacy. Once
 the legal effect of an infringement has been established by the ECtHR, it will investi-
 gate whether the state has a valid justification.


   2. There shall be no interference by a public authority with the exercise of this right
   except such as is in accordance with the law and is necessary in a democratic so-
   ciety in the interests of national security, public safety or the economic well being
   of the country, for the prevention of disorder or crime, for the protection of health
   or morals, or for the protection of the rights and freedoms of others.


 The legal effect of a valid justification is that, despite the infringement, Article 8 is not
 violated. This effect depends on the following cumulative legal conditions:

     ‚Ä¢ the infringement has one of more of the following legitimate aims: national
        security, public safety, or the economic well-‚Äãbeing of the country, for the



   5 ECtHR, 16 December 1992, Application no. 13710/‚Äã88 (Niemietz v. Germany), regarding the search of

a law firm; ECtHR, 25 June 1997, Application no. 20605/‚Äã92 (Halford v. UK), regarding the interception of
telephone calls at work.
116 Privacy and Data Protection


         prevention of disorder or crime, for the protection of health or morals, or for
         the protection of the rights and freedoms of others;
      ‚Ä¢ the infringement is in accordance with the law; and
      ‚Ä¢ the infringement is necessary in a democratic society.

 The second paragraph of Article 8 thus requires a triple test, meaning that all three legal
 conditions must be met. These conditions are often summed up by stating that any
 infringing measures taken by the state must:

      ‚Ä¢ have a legitimate aim;
      ‚Ä¢ have a basis in law; and
      ‚Ä¢ be proportional in relation to the aim served.


The articulation of legitimate aims in Article 8.2 is rather inclusive, which means
that the ECtHR seldom finds reason to endorse the claim that the state lacked a
legitimate aim.
Many of the cases where the ECtHR (the Court) finds that Article 8 has been vio-
lated concern the legal condition that the infringement must be ‚Äòin accordance
with the law‚Äô to be justified. This basically refers to the legality principle of consti-
tutional law (see section 3.3).

 The Court has developed‚Äî‚Äãover the course of the years‚Äî‚Äãanother triple test to decide
 whether an infringement has a proper basis in law:

      ‚Ä¢ the legal competence to take infringing measures must be accessible, know-
         able for citizens to whom it will apply;
      ‚Ä¢ the infringements must be foreseeable, which means sufficiently specified; and
      ‚Ä¢ the quality of the law must include sufficient safeguards that limit the exercise of
         the competence in time and space, specifying the extent to which privacy may be
         infringed, and notably requiring independent oversight (e.g. warrants) in the case
         of more serious infringements.


Note that the Court will not merely check legislative or regulatory provisions,
but test practical arrangements and actual safeguards to establish whether the
infringing measures were taken ‚Äòin accordance with the law‚Äô. Throughout its
case law, the ECtHR demands that the rights attributed in the ECHR are both
‚Äòpractical and effective‚Äô, stating that:6

 6   Airey v. Ireland, 9 October 1979, Series A, no. 32, para. 24.
                                                           5.3 The Right to Privacy 117


  [t]‚Äåhe Convention is intended to guarantee not rights that are theoretical or illusory
  but rights that are practical and effective ( . . . ).

If privacy is infringed with a legitimate aim, based on a legal competence that
is accessible, foreseeable, while having sufficient safeguards, the final test is a
proportionality test.

 The proportionality test entails that the ECtHR investigates whether the measure
 was necessary in a democratic society, which requires‚Äî‚Äãaccording to the Court‚Äî‚Äãa
 pressing social need to resort to such measures.


  ‚Ä¢ Under this criterion the Court will examine the gravity, invasiveness, and
    seriousness of the infringement in relation to the importance and ser-
    iousness of the aim served.
  ‚Ä¢ This criterion basically requires that the measures taken can reasonably
    be expected to be effective, because a measure that is not effective cannot
    be necessary.
  ‚Ä¢ The proportionality test includes a subsidiarity test; if another measure
    which is less infringing is feasible or sufficiently effective, the measure is
    not proportional.



5.3.5 Case law Article 8 ECHR regarding surveillance

When developing computing architectures, whether in the context of data-
bases, streaming data, machine-‚Äã    to-‚Äã
                                       machine communication, knowledge
discovery in databases, machine learning, or cryptographic infrastructures,
computer scientists lay the foundations for the ICIs that enable the processing,
storage, interlinking, and inferencing of behavioural and other personal data.
This may regard online clickstream behaviour, location, and mobility data,
energy usage behaviours, biometric gait behaviour, and a plethora of com-
munication data, including both content and metadata. Governments, tasked
with the investigation and prosecution of criminal offences and the protection
of national and public security, have many incentives to gain access to such
data. Apart from the struggle against serious crime and threats to national
security, governments need to collect taxes, attribute social benefits, take pre-
cautionary measures regarding public health, and safeguard the economic
welfare of the country. All these tasks fall within the scope of the legitimate
aims enumerated in Article 8.2 ECHR. This raises the question under what
118 Privacy and Data Protection

conditions surveillance measures can be qualified as ‚Äòin accordance with the
law‚Äô and if so, when they are considered ‚Äòproportional‚Äô to the targeted aim.
Surveillance measures by the police may regard post-‚Äãcrime investigatory
measures (to identify an offender after a crime has been committed) or pre-‚Äã
crime investigations (to prevent potential offending, or to foresee likely of-
fences). To understand how the Court deals with various types of electronic
surveillance, we will discuss two cases of post-‚Äãcrime surveillance and two
cases of pre-‚Äãcrime surveillance (including surveillance by the intelligence
services, which falls outside the domain of criminal law).

 This entails extensive quotation of the relevant case law, to show how the Court
 reasons, taking into account that the Court‚Äôs judgments bind the contracting parties
 and thus provide ‚Äòpractical and effective‚Äô legal protection to those under the jurisdic-
 tion of the ECHR.


5.3.5.1 Post-‚Äãcrime surveillance
In 1984, in Malone v. UK,7 the ECtHR determined that the United Kingdom
was in breach of Article 8 ECHR, where it allowed the interception of telephone
conversations by the police upon a warrant issued by the Secretary of State. The
Court determined that for such a measure to be ‚Äòin accordance with the law‚Äô,
it must not merely have a basis in domestic law (meaning a legal power), but
must also be foreseeable and sufficiently limited as required by the rule of law:

     68. Since the implementation in practice of measures of secret surveillance of com-
     munications is not open to scrutiny by the individuals concerned or the public at
     large, it would be contrary to the rule of law for the legal discretion granted to the
     executive to be expressed in terms of an unfettered power. Consequently, the law
     must indicate the scope of any such discretion conferred on the competent author-
     ities and the manner of its exercise with sufficient clarity ( . . . ).

When applying this interpretation, the Court finds that:

     79. The foregoing considerations disclose that, at the very least, in its present state
     the law in England and Wales governing interception of communications for po-
     lice purposes is somewhat obscure and open to differing interpretations. The Court
     would be usurping the function of the national courts were it to attempt to make


 7    ECtHR, 2 August 1984, Application no. 8691/‚Äã79 (Malone v. UK).
                                                          5.3 The Right to Privacy 119

  an authoritative statement on such issues of domestic law (see, mutatis mutandis,
  the Deweer judgment of 27 February 1980, Series A no. 35, p. 28, in fine, and the
  Van Droogenbroeck judgment of 24 June 1982, Series A no. 50, p. 30, fourth sub-‚Äã
  paragraph). The Court is, however, required under the Convention to determine
  whether, for the purposes of paragraph 2 of Article 8 (art. 8-‚Äã2), the relevant law lays
  down with reasonable clarity the essential elements of the authorities‚Äô powers in
  this domain.

  Detailed procedures concerning interception of communications on behalf of
  the police in England and Wales do exist (see paragraphs 42‚Äì‚Äã49, 51‚Äì‚Äã52 and 54‚Äì‚Äã55
  above). What is more, published statistics show the efficacy of those procedures in
  keeping the number of warrants granted relatively low, especially when compared
  with the rising number of indictable crimes committed and telephones installed
  (see paragraph 53 above). The public have been made aware of the applicable ar-
  rangements and principles through publication of the Birkett report and the White
  Paper and through statements by responsible Ministers in Parliament (see para-
  graphs 21, 37‚Äì‚Äã38, 41, 43 and 54 above).

  Nonetheless, on the evidence before the Court, it cannot be said with any reason-
  able certainty what elements of the powers to intercept are incorporated in legal
  rules and what elements remain within the discretion of the executive. In view of
  the attendant obscurity and uncertainty as to the state of the law in this essential
  respect, the Court cannot but reach a similar conclusion to that of the Commission.
  In the opinion of the Court, the law of England and Wales does not indicate with
  reasonable clarity the scope and manner of exercise of the relevant discretion con-
  ferred on the public authorities. To that extent, the minimum degree of legal pro-
  tection to which citizens are entitled under the rule of law in a democratic society
  is lacking.

  (iii) Conclusion

  80. In sum, as far as interception of communications is concerned, the interferences
  with the applicant‚Äôs right under Article 8 (art. 8) to respect for his private life and
  correspondence (see paragraph 64 above) were not ‚Äòin accordance with the law‚Äô.

In this case, Malone not only claimed that the interception of the content of his
telephone conversations violated his right to privacy under the Convention, but
also that the capture of what we would now call metadata violated said right. The
Court states, with regard to this capture, known as ‚Äòmetering‚Äô:

  83. The process known as ‚Äòmetering‚Äô involves the use of a device (a meter check
  printer) which registers the numbers dialled on a particular telephone and the time
120 Privacy and Data Protection

     and duration of each call (see paragraph 56 above). In making such records, the Post
     Office‚Äî‚Äãnow British Telecommunications‚Äî‚Äãmakes use only of signals sent to itself as
     the provider of the telephone service and does not monitor or intercept telephone
     conversations at all. From this, the Government drew the conclusion that metering,
     in contrast to interception of communications, does not entail interference with any
     right guaranteed by Article 8 (art. 8).

     87. Section 80 of the Post Office Act 1969 has never been applied so as to ‚Äòrequire‚Äô
     the Post Office, pursuant to a warrant of the Secretary of State, to make available to
     the police in connection with the investigation of crime information obtained from
     metering. On the other hand, no rule of domestic law makes it unlawful for the Post
     Office voluntarily to comply with a request from the police to make and supply re-
     cords of metering (see paragraph 56 above). The practice described above, including
     the limitative conditions as to when the information may be provided, has been made
     public in answer to parliamentary questions (ibid.). However, on the evidence ad-
     duced before the Court, apart from the simple absence of prohibition, there would ap-
     pear to be no legal rules concerning the scope and manner of exercise of the discretion
     enjoyed by the public authorities. Consequently, although lawful in terms of domestic
     law, the interference resulting from the existence of the practice in question was not
     ‚Äòin accordance with the law‚Äô, within the meaning of paragraph 2 of Article 8 (art. 8-‚Äã2)
     (see paragraphs 66 to 68 above).


 Note that the ECtHR established that the practice of ‚Äòmetering‚Äô is lawful under UK law,
 but in violation of Article 8.2 ECHR. Both the interception and the metering violate Article
 8.2 because they are not ‚Äòin accordance with the law‚Äô as required by a treaty that binds
 the United Kingdom. This means that the United Kingdom has violated its legal obliga-
 tions under the Convention and is now bound to ensure that these types of surveillance
 measures are based on a domestic law that both constitutes and sufficiently restricts its
 legal powers.


In 1990, in Huvig & Kruslin v. France,8 the ECtHR determined that Article
8 was breached. The case concerned the interception of telephone conversa-
tions, as in the Malone case. The Court extensively refers to its contentions
in the Malone judgment as to the requirement of such interceptions being ‚Äòin
accordance with the law‚Äô. It then states:

     35. Above all, the system does not for the time being afford adequate safeguards
     against various possible abuses. For example, the categories of people liable to

 8    ECtHR, 24 April 1990, Application no. 11801/‚Äã85 (Huvig & Kruslin v. France).
                                                                  5.3 The Right to Privacy 121

     have their telephones tapped by judicial order and the nature of the offences which
     may give rise to such an order are nowhere defined. Nothing obliges a judge to set a
     limit on the duration of telephone tapping. Similarly unspecified are the procedure
     for drawing up the summary reports containing intercepted conversations; the
     precautions to be taken in order to communicate the recordings intact and in their
     entirety for possible inspection by the judge (who can hardly verify the number and
     length of the original tapes on the spot) and by the defence; and the circumstances
     in which recordings may or must be erased or the tapes be destroyed, in particular
     where an accused has been discharged by an investigating judge or acquitted by a
     court. The information provided by the Government on these various points shows
     at best the existence of a practice, but a practice lacking the necessary regulatory
     control in the absence of legislation or case-‚Äãlaw.

     36. In short, French law, written and unwritten, does not indicate with reasonable
     clarity the scope and manner of exercise of the relevant discretion conferred on the
     public authorities. This was truer still at the material time, so that Mr Kruslin did not
     enjoy the minimum degree of protection to which citizens are entitled under the rule of
     law in a democratic society (see the Malone judgment previously cited, Series A no. 82,
     p. 36, ¬ß 79). There has therefore been a breach of Article 8 (art. 8) of the Convention.


 Note that in the Huvig & Kruslin judgment, the Court further details the nature of the
 restrictions that must be laid down by law, compared to the more general formula-
 tion in the Malone judgment.


5.3.5.2 Pre-‚Äãcrime surveillance (including surveillance by the
intelligence services)
In 1978, in Klass v. Germany,9 the ECtHR decided a case regarding surveil-
lance measures taken by the secret services in Germany. I will quote the most
relevant considerations from the judgment, which should clarify how the
Court argues points of law and thus shapes the interpretation of legal conditions:

     All five applicants claim that Article 10 para. 2 of the Basic Law (Grundgesetz) and
     a statute enacted in pursuance of that provision, namely the Act of 13 August 1968
     on Restrictions on the Secrecy of the Mail, Post and Telecommunications (. . . here-
     inafter referred to as ‚Äòthe G 10‚Äô), are contrary to the Convention.

     They do not dispute that the State has the right to have recourse to the surveillance
     measures contemplated by the legislation; they challenge this legislation in that it


 9    ECHR, 6 September 1978, Application no. 5029/‚Äã71 (Klass v. Germany).
122 Privacy and Data Protection

  permits those measures without obliging the authorities in every case to notify the
  persons concerned after the event, and in that it excludes any remedy before the
  courts against the ordering and execution of such measures.

  Their application is directed against the legislation as modified and interpreted by
  the Federal Constitutional Court (Bundesverfassungsgericht).

The Court first discusses the admissibility of the complaint, raising the ques-
tion whether the applicant is a victim of violation by one of the MSs.

  33. ( . . . ) Article 25 (art. 25) [now Article 34, mh] does not institute for individuals a
  kind of actio popularis for the interpretation of the Convention; it does not permit
  individuals to complain against a law in abstracto simply because they feel that
  it contravenes the Convention. In principle, it does not suffice for an individual
  applicant to claim that the mere existence of a law violates his rights under the
  Convention; it is necessary that the law should have been applied to his detriment.

  34. ( . . . ) The question arises in the present proceedings whether an individual is
  to be deprived of the opportunity of lodging an application with the Commission
  because, owing to the secrecy of the measures objected to, he cannot point to any
  concrete measure specifically affecting him. ( . . . )

  36. The Court points out that where a State institutes secret surveillance the exist-
  ence of which remains unknown to the persons being controlled, with the effect
  that the surveillance remains unchallengeable, Article 8 (art. 8) could to a large ex-
  tent be reduced to a nullity. It is possible in such a situation for an individual to be
  treated in a manner contrary to Article 8 (art. 8), or even to be deprived of the right
  granted by that Article (art. 8), without his being aware of it and therefore without
  being able to obtain a remedy either at the national level or before the Convention
  institutions. ( . . . ) The Court finds it unacceptable that the assurance of the enjoy-
  ment of a right guaranteed by the Convention could be thus removed by the simple
  fact that the person concerned is kept unaware of its violation. ( . . . )

  38. Having regard to the specific circumstances of the present case, the Court con-
  cludes that each of the applicants is entitled to ‚Äò(claim) to be the victim of a vio-
  lation‚Äô of the Convention, even though he is not able to allege in support of his
  application that he has been subject to a concrete measure of surveillance.

This entails that the Court makes an exception to the requirement that ap-
plicants must claim and demonstrate to be a victim of violation in concrete
terms. Depending on the specific circumstances of the case at hand, the Court
may decide to conduct an abstract test of relevant legislation, attributing the
                                                          5.3 The Right to Privacy 123

status of ‚Äòvictims‚Äô of what is now Article 34 ECHR, to those who may have been
a victim of secret surveillance measures.
The Court then quotes relevant legislation, notably Article 10 of the Basic Law
of Germany:

 (1) Secrecy of the mail, post and telecommunications shall be inviolable.
 (2) Restrictions may be ordered only pursuant to a statute. Where such restrictions
     are intended to protect the free democratic constitutional order or the exist-
     ence or security of the Federation or of a Land, the statute may provide that the
     person concerned shall not be notified of the restriction and that legal remedy
     through the courts shall be replaced by a system of scrutiny by agencies and
     auxiliary agencies appointed by the people‚Äôs elected representatives.

The Court begins by investigating whether the legislation that is contested by
the applicants, constitutes an interference with Article 8.1 ECHR:

  41. The first matter to be decided is whether and, if so, in what respect the con-
  tested legislation, in permitting the above-‚Äãmentioned measures of surveillance,
  constitutes an interference with the exercise of the right guaranteed to the appli-
  cants under Article 8 para. 1 (art. 8-‚Äã1). ( . . . )

  Furthermore, in the mere existence of the legislation itself there is involved, for
  all those to whom the legislation could be applied, a menace of surveillance; this
  menace necessarily strikes at freedom of communication between users of the
  postal and telecommunication services and thereby constitutes an ‚Äòinterference
  by a public authority‚Äô with the exercise of the applicants‚Äô right to respect for private
  and family life and for correspondence.

As is often the case, the Court takes a broad view of the scope of the first para-
graph and decides that the legislation constitutes an infringement. The next
question is whether the infringement is justified:

  42. The cardinal issue arising under Article 8 (art. 8) in the present case is whether
  the interference so found is justified by the terms of paragraph 2 of the Article
  (art. 8-‚Äã2).

The Court first tests whether the infringement is ‚Äòin accordance with the law‚Äô:

  43. In order for the ‚Äòinterference‚Äô established above not to infringe Article 8 (art. 8),
  it must, according to paragraph 2 (art. 8-‚Äã2), first of all have been ‚Äòin accordance with
  the law‚Äô.
124 Privacy and Data Protection

  This requirement is fulfilled in the present case since the ‚Äòinterference‚Äô results
  from Acts passed by Parliament, including one Act which was modified by the
  Federal Constitutional Court, in the exercise of its jurisdiction, by its judgment of 15
  December 1970 (see paragraph 11 above).

  In addition, the Court observes that, as both the Government and the Commission
  pointed out, any individual measure of surveillance has to comply with the strict
  conditions and procedures laid down in the legislation itself.

This leads the Court to test whether the interference has a legitimate aim:

  45. The G 10 defines precisely, and thereby limits, the purposes for which the re-
  strictive measures may be imposed. It provides that, in order to protect against
  ‚Äòimminent dangers‚Äô threatening ‚Äòthe free democratic constitutional order‚Äô, ‚Äòthe ex-
  istence or security of the Federation or of a Land‚Äô, ‚Äòthe security of the (allied) armed
  forces‚Äô stationed on the territory of the Republic or the security of ‚Äòthe troops of
  one of the Three Powers stationed in the Land of Berlin‚Äô, the responsible authorities
  may authorise the restrictions referred to above (see paragraph 17).

  46. The Court, sharing the view of the Government and the Commission, finds that
  the aim of the G 10 is indeed to safeguard national security and/‚Äãor to prevent dis-
  order or crime in pursuance of Article 8 para. 2 (art. 8-‚Äã2). In these circumstances, the
  Court does not deem it necessary to decide whether the further purposes cited by
  the Government are also relevant.

This brings the Court to test the final criterion of the triple test, investigating
whether the interference is necessary in a democratic society. Below you will
find an extensive quotation of (part) of the reasoning of the Court regarding
the question whether the interference enabled by the legislation is propor-
tional, considering what is at stake.

  47. The applicants do not object to the German legislation in that it provides for
  wide-‚Äãranging powers of surveillance; they accept such powers, and the resultant
  encroachment upon the right guaranteed by Article 8 para. 1 (art. 8-‚Äã1), as being a
  necessary means of defence for the protection of the democratic State.

  The applicants consider, however, that paragraph 2 of Article 8 (art. 8-‚Äã2) lays down
  for such powers certain limits which have to be respected in a democratic society
  in order to ensure that the society does not slide imperceptibly towards totalitar-
  ianism. In their view, the contested legislation lacks adequate safeguards against
  possible abuse.
                                                         5.3 The Right to Privacy 125

49. As concerns the fixing of the conditions under which the system of surveillance
is to be operated, the Court points out that the domestic legislature enjoys a cer-
tain discretion. It is certainly not for the Court to substitute for the assessment of
the national authorities any other assessment of what might be the best policy in
this field ( . . . )

Nevertheless, the Court stresses that this does not mean that the Contracting
States enjoy an unlimited discretion to subject persons within their jurisdiction
to secret surveillance. The Court, being aware of the danger such a law poses of
undermining or even destroying democracy on the ground of defending it, affirms
that the Contracting States may not, in the name of the struggle against espionage
and terrorism, adopt whatever measures they deem appropriate.

51. According to the G 10, a series of limitative conditions have to be satisfied be-
fore a surveillance measure can be imposed. ( . . . )

52. The G 10 also lays down strict conditions with regard to the implementation
of the surveillance measures and to the processing of the information thereby
obtained. ( . . . )

53. Under the G 10, while recourse to the courts in respect of the ordering and im-
plementation of measures of surveillance is excluded, subsequent control or re-
view is provided instead, in accordance with Article 10 para. 2 of the Basic Law,
by two bodies appointed by the people‚Äôs elected representatives, namely, the
Parliamentary Board and the G 10 Commission. ( . . . )

54. The Government maintain that Article 8 para. 2 (art. 8-‚Äã2) does not require
judicial control of secret surveillance and that the system of review established
under the G 10 does effectively protect the rights of the individual. The appli-
cants, on the other hand, qualify this system as a ‚Äòform of political control‚Äô, in-
adequate in comparison with the principle of judicial control which ought to
prevail.

It therefore has to be determined whether the procedures for supervising the or-
dering and implementation of the restrictive measures are such as to keep the
‚Äòinterference‚Äô resulting from the contested legislation to what is ‚Äònecessary in a
democratic society‚Äô.

55. Review of surveillance may intervene at three stages: when the surveillance is
first ordered, while it is being carried out, or after it has been terminated. As regards
the first two stages, the very nature and logic of secret surveillance dictate that not
only the surveillance itself but also the accompanying review should be effected
without the individual‚Äôs knowledge.
126 Privacy and Data Protection

  Consequently, since the individual will necessarily be prevented from seeking an
  effective remedy of his own accord or from taking a direct part in any review pro-
  ceedings, it is essential that the procedures established should themselves provide
  adequate and equivalent guarantees safeguarding the individual‚Äôs rights.

  In addition, the values of a democratic society must be followed as faithfully as pos-
  sible in the supervisory procedures if the bounds of necessity, within the meaning
  of Article 8 para. 2 (art. 8-‚Äã2), are not to be exceeded.

  One of the fundamental principles of a democratic society is the rule of law, which
  is expressly referred to in the Preamble to the Convention (see the Golder judgment
  of 21 February 1975, Series A no. 18, pp. 16‚Äì‚Äã17, para. 34). The rule of law implies,
  inter alia, that an interference by the executive authorities with an individual‚Äôs
  rights should be subject to an effective control which should normally be assured
  by the judiciary, at least in the last resort, judicial control offering the best guaran-
  tees of independence, impartiality and a proper procedure.

  56. The Court considers that, in a field where abuse is potentially so easy in indi-
  vidual cases and could have such harmful consequences for democratic society as
  a whole, it is in principle desirable to entrust supervisory control to a judge.

  Nevertheless, having regard to the nature of the supervisory and other safeguards
  provided for by the G 10, the Court concludes that the exclusion of judicial con-
  trol does not exceed the limits of what may be deemed necessary in a democratic
  society.

  58. In the opinion of the Court, it has to be ascertained whether it is even feasible in
  practice to require subsequent notification in all cases.

  The activity or danger against which a particular series of surveillance measures
  is directed may continue for years, even decades, after the suspension of those
  measures.

  Subsequent notification to each individual affected by a suspended measure
  might well jeopardise the long-‚Äãterm purpose that originally prompted the surveil-
  lance. Furthermore, as the Federal Constitutional Court rightly observed, such no-
  tification might serve to reveal the working methods and fields of operation of the
  intelligence services and even possibly to identify their agents.

  In the Court‚Äôs view, in so far as the ‚Äòinterference‚Äô resulting from the contested le-
  gislation is in principle justified under Article 8 para. 2 (art. 8-‚Äã2) (see paragraph
  48 above), the fact of not informing the individual once surveillance has ceased
  cannot itself be incompatible with this provision since it is this very fact which en-
  sures the efficacy of the ‚Äòinterference‚Äô.
                                                                 5.3 The Right to Privacy 127

  For these reasons the Court

  1. holds unanimously that it has jurisdiction to rule on the question whether the
  applicants can claim to be victims within the meaning of Article 25 (art. 25) of the
  Convention;

  2. holds unanimously that the applicants can claim to be victims within the
  meaning of the aforesaid Article (art. 25);

  3. holds unanimously that there has been no breach of Article 8, Article 13 or Article
  6 (art. 8, art. 13, art. 6) of the Convention.


 This extensive quotation should contribute to a better understanding of the deli-
 cate and complex nature of the issues brought before the Court. This particular case
 (Klass) is a landmark case that functions as a building block for the reasoning in
 similar cases and requires the contracting states to incorporate necessary safeguards
 when developing and implementing legislation that enables surveillance by intelli-
 gence agencies.


In 2006, the ECtHR decided the case of Weber & Saravia v. Germany,10 once
again testing legislation regarding so-‚Äãcalled ‚Äòstrategic monitoring‚Äô by intelli-
gence services. In this case, the Court specifies in more detail what qualifies
as ‚Äòinterferences‚Äô that are ‚Äòin accordance with the law‚Äô. Although, after having
conducted the triple test, the Court decided that the contested legislation did
not violate Article 8 ECHR, I will quote the legal conditions summed up by
the Court to attain the legal effect of such interferences qualifying as being ‚Äòin
accordance with the law‚Äô.

  95. In its case-‚Äãlaw on secret measures of surveillance, the Court has developed the
  following minimum safeguards that should be set out in statute law in order to
  avoid abuses of power:
  ‚Ä¢ the nature of the offences which may give rise to an interception order;
  ‚Ä¢ a definition of the categories of people liable to have their telephones tapped;
  ‚Ä¢ a limit on the duration of telephone tapping;
  ‚Ä¢ the procedure to be followed for examining, using and storing the data obtained;
  ‚Ä¢ the precautions to be taken when communicating the data to other parties; and
  ‚Ä¢ the circumstances in which recordings may or must be erased or the tapes
    destroyed.



 10   ECHR, 29 June 2006, Application no. 54934/‚Äã00 (Weber & Saravia v. Germany).
128 Privacy and Data Protection

Since 2006, a number of cases have been decided on the issue of surveillance,
either in the context of post-‚Äãcrime or pre-‚Äãcrime measures, as well as measures
taken by the intelligence services.11 This includes both concrete interferences
and legislation that would enable such interferences. As recounted above, the
latter is not normally open to scrutiny by the Court, as it concerns an abstract
test of the compatibility of domestic law against the Convention. The Court,
however, can make an exception when applicants claim that the nature of the
legislation or practice is such that they cannot know whether or not they have
been a victim of state surveillance.
With the above analyses that closely follow the reasonings of the Court, the
readers should have sufficient analytical instruments to study, for instance,
the case of Big Brother Watch and Others v. the United Kingdom of 2018.12
This case regards complaints about the compatibility with Article 8 ECHR
of three discrete regimes of mass surveillance in the United Kingdom.
First, the regime for the bulk interception of communications under section
8(4) of the Regulation of Investigatory Powers Act 2000 (RIPA); the UK‚Äì‚Äã
US intelligence sharing regime applied by the security service (MI5), the
secret intelligence service (MI6), and the Government Communications
Headquarters (GCHQ, which covers information and signals intelligence
or ‚Äòsigint‚Äô); and the regime for the acquisition of communications data under
Chapter II of RIPA. The purpose of this work is not to provide an exhaustive
overview of positive law in the realm of the right to privacy, but to provide
computer scientists and students of computer science with a proper under-
standing of law as a scholarly discipline and a professional practice. In the
end, the proof of the pudding will be in the eating. The reader is invited and
encouraged to have their own tastings of legal texts, discovering the major
impact of legal decision-‚Äãmaking on potential violations of, for example, the
right to privacy.


5.4 Privacy and Data Protection

Since the CFREU (or ‚Äòthe Charter‚Äô) has been in force (2009), the EU ‚Äòhas‚Äô two
fundamental rights regarding the processing of personal data:

   11 E.g. ECtHR, 1 July 2008, Application no. 58243/‚Äã00 (Liberty and Others v. the United Kingdom); ECtHR,

18 May 2010, Application no. 26839/‚Äã05 (Kennedy v. the United Kingdom); ECtHR, 4 December 2015,
Application no. 47143/‚Äã06 (Roman Zakharov v. Russia); ECtHR, 12 January, Application no. 2016 37138/‚Äã
14 (Szab√≥ and Vissy v. Hungary); ECtHR, 19 June 2018, Application no. 35252/‚Äã08 (Centrum F√∂r R√§ttvisa
v. Sweden); ECtHR, 13 September 2018, Application nos. 58170/‚Äã13, 62322/‚Äã14 and 24960/‚Äã15 (Big Brother
Watch and Others v. the United Kingdom).
   12 ECtHR, 13 September 2018, Application nos. 58170/‚Äã13, 62322/‚Äã14 and 24960/‚Äã15 (Big Brother Watch

and Others v. the United Kingdom).
                                                5.4 Privacy and Data Protection 129

  Article 7 Respect for private and family life
  Everyone has the right to respect for his or her private and family life, home and
  communications.


  Article 8 Protection of personal data
  1. Everyone has the right to the protection of personal data concerning him or her.
  2. Such data must be processed fairly for specified purposes and on the basis of
     the consent of the person concerned or some other legitimate basis laid down
     by law. Everyone has the right of access to data which has been collected con-
     cerning him or her, and the right to have it rectified.
  3. Compliance with these rules shall be subject to control by an independent
     authority.


 This is a new situation in the realm of human rights, because no other Constitution or
 Human Rights Treaty attributes a right to the protection of personal data.


Article 52 of the Charter clarifies the relationship between Article 7 of the
Charter and Article 8 ECHR, which both refer to the right to privacy.

  3. In so far as this Charter contains rights which correspond to rights guaranteed by
  the Convention for the Protection of Human Rights and Fundamental Freedoms,
  the meaning and scope of those rights shall be the same as those laid down by the
  said Convention. This provision shall not prevent Union law providing more exten-
  sive protection.

This stipulates that Article 7 CFREU cannot be interpreted as providing less
protection compared to Article 8 ECHR, but may be interpreted as attributing
additional protection. To the extent that Article 8 CFREU corresponds to
Article 8 ECHR, it can‚Äî‚Äãsimilarly‚Äî‚Äãnot be interpreted as providing less pro-
tection than Article 8 ECHR, but it may provide additional protection.

 Before diving deep into the General Data Protection Regulation (GDPR) that provides
 more details, rules, and principles for the processing of personal data, we will first in-
 vestigate how the fundamental right to data protection compares to the fundamental
 right to privacy.



5.4.1 Defaults: an opacity right and a transparency right

Some authors have argued that whereas, by default, the right to privacy is fore-
most an opacity right, data protection is foremost a transparency right. As an
130 Privacy and Data Protection

opacity right, the right to privacy aims to safeguard a private sphere for individual
citizens, where they can basically ward-‚Äãoff interference by others, most notably
the state. This highlights the idea that privacy is a liberty right, a negative right
that obligates others to refrain from interference with the good that is protected.
As a transparency right, the right to data protection aims to ensure that whenever
personal data is processed (which included collection, access, manipulation, and
any other usage) such processing must be done in a transparent manner, in com-
pliance with a set of conditions which should ensure fair and lawful processing.
Note that the opacity concerns the private sphere of an individual person,
whereas the transparency concerns the state and other powerful actors when
processing personal data. This accords with the core tenets of the Rule of Law,
which hold that whereas government should be as transparent as possible,
citizens should be shielded from intrusive transparency by the government.

 Also, as discussed above, even though privacy is an opacity right that requires the
 state to refrain from interference (negative freedom), the right to privacy may, never-
 theless, impose positive obligations on the state to enable individuals to exercise their
 right. Similarly, though data protection is a transparency right that should enable
 individuals as well as others to act on their personal data (positive freedom), while
 imposing a number of positive obligations on those who determine the purpose of
 processing, the right to data protection may, nevertheless, require that others abstain
 from processing personal data, thus imposing negative obligations on them.



5.4.2 Distinctive but overlapping rights: a Venn diagram

Though one may be tempted to see the right to data protection as a subset of
the right to privacy, this is not correct. Within the context of the EU, the right
to privacy entails both more and less than the right to data protection. We
portray this in Figure 5.1 below.


                                             Data Protection




                                Privacy




Figure 5.1 Venn diagram of the fundamental rights to privacy and data protection
                                               5.4 Privacy and Data Protection 131


 Whenever the processing of personal data constitutes an interference with the right
 to privacy, there is an overlap. The right to privacy, however, also concerns interfer-
 ence with bodily integrity, decisional privacy, privacy of the home, and correspond-
 ence when no processing of personal data is involved. This is where the right to
 privacy entails more than the right to data protection.

 Similarly, the right to data protection also concerns the processing of personal data
 when there is no interference with the right to privacy, for instance, when one‚Äôs per-
 sonal data are processed on one‚Äôs own request, for example, the processing of an ad-
 dress or banking details to deliver goods and charge one‚Äôs account as a consequence
 of the sale of a book.


Note that if such data are subsequently used for other purposes, for example,
to support the business model of a webshop by way of targeted advertising,
privacy may be at stake. Whether or not this is the case also relates to the fact
that the right to privacy, as discussed above, is primarily at stake in the ver-
tical relationship between a government and its citizens, whereas the right
to data protection seems to be applicable to all those who process personal
data. This is certainly the case for data processing that falls under the scope
of the GDPR.


5.4.3 Legal remedies in case of violation

The right to privacy can be invoked in a national court of law, for instance
in the course of criminal or administrative proceedings. As discussed above,
individual citizens have a right to present their claim to the ECtHR, which
resides in Strasbourg, but this can only be done after exhausting national rem-
edies. That means that if one fails to claim violation of Article 8 ECHR at the
national level, or if one fails to appeal against a judgment that denies such a
violation, the application to the ECtHR will be inadmissible. See Articles 34
and 35 ECHR:

  Article 34 Individual applications
  The Court may receive applications from any person, nongovernmental organisa-
  tion or group of individuals claiming to be the victim of a violation by one of the
  High Contracting Parties of the rights set forth in the Convention or the Protocols
  thereto. The High Contracting Parties undertake not to hinder in any way the ef-
  fective exercise of this right.
132 Privacy and Data Protection

   Article 35 Admissibility criteria
   The Court may only deal with the matter after all domestic remedies have been
   exhausted, according to the generally recognised rules of international law, and
   within a period of six months from the date on which the final decision was taken.

Both the right to privacy and the right to data protection of the CFREU have
direct application in the MSs of the EU. This means one can invoke them in a
national court of law. If, however, a question is raised about the interpretation
of the Charter, Article 267 of the Treaty on the Functioning of the EU (TFEU)
stipulates that so-‚Äãcalled ‚Äòpreliminary questions‚Äô can, or must, be referred to
the CJEU (which resides in Luxembourg):

   The Court of Justice of the European Union shall have jurisdiction to give prelim-
   inary rulings concerning:
   (a) the interpretation of the Treaties;
   (b) the validity and interpretation of acts of the institutions, bodies, offices or
       agencies of the Union;

   Where such a question is raised before any court or tribunal of a Member State, that
   court or tribunal may, if it considers that a decision on the question is necessary to
   enable it to give judgment, request the Court to give a ruling thereon.

   Where any such question is raised in a case pending before a court or tribunal of a
   Member State against whose decisions there is no judicial remedy under national
   law, that court or tribunal shall bring the matter before the Court.

   (...)


 Clearly, both European Courts have an important role as to the national jurisdiction
 regarding human and fundamental rights. The case law of both Courts is a pivotal
 source of law that will remain central throughout this work.



5.5 Data Protection Law

The history of data protection law goes back to the 1970s, when various coun-
tries enacted legislation to ensure fair processing of personal information by
the government. An early example was the US Privacy Act of 1974,13 which
instigated a set of fair practices for dealing with personal information.

 13 Privacy Act of 1974, 5 U.S.C. ¬ß 552a, see: https://‚Äãwww.gpo.gov/‚Äãfdsys/‚Äãpkg/‚ÄãUSCODE-‚Äã2012-‚Äãtitle5/‚Äãpdf/‚Äã

USCODE-‚Äã2012-‚Äãtitle5-‚ÄãpartI-‚Äãchap5-‚ÄãsubchapII-‚Äãsec552a.pdf.
                                                         5.5 Data Protection Law 133

In 1980, the global Organisation of Economic Co-‚Äãoperation and Development
(OECD) issued the so-‚Äãcalled ‚ÄòFair Information Principles‚Äô (FIPs), as part of
the (non-‚Äãbinding) Guidelines governing the protection of privacy and trans-
border flows of personal data:

  Collection Limitation Principle
  7. There should be limits to the collection of personal data and any such data should
     be obtained by lawful and fair means and, where appropriate, with the knowledge
     or consent of the data subject.

  Data Quality Principle
  8. Personal data should be relevant to the purposes for which they are to be used, and,
     to the extent necessary for those purposes, should be accurate, complete and kept
     up-‚Äãto-‚Äãdate.

  Purpose Specification Principle
  9. The purposes for which personal data are collected should be specified not later
     than at the time of data collection and the subsequent use limited to the fulfilment
     of those purposes or such others as are not incompatible with those purposes and
     as are specified on each occasion of change of purpose.

  Use Limitation Principle
  10. Personal data should not be disclosed, made available or otherwise used for
      purposes other than those specified in accordance with Paragraph 9 except:
      a) with the consent of the data subject; or
      b) by the authority of law.

  Security Safeguards Principle
  11. Personal data should be protected by reasonable security safeguards against
      such risks as loss or unauthorised access, destruction, use, modification or dis-
      closure of data.


  Openness Principle
  12. There should be a general policy of openness about developments, practices
      and policies with respect to personal data. Means should be readily available of
      establishing the existence and nature of personal data, and the main purposes
      of their use, as well as the identity and usual residence of the data controller.


  Individual Participation Principle
  13. Individuals should have the right:
      a) to obtain from a data controller, or otherwise, confirmation of whether or
         not the data controller has data relating to them;
134 Privacy and Data Protection

     b) to have communicated to them, data relating to them
          i. within a reasonable time;
         ii. at a charge, if any, that is not excessive;
        iii. in a reasonable manner; and
        iv. in a form that is readily intelligible to them;
     c) to be given reasons if a request made under subparagraphs (a) and (b) is de-
        nied, and to be able to challenge such denial; and
     d) to challenge data relating to them and, if the challenge is successful to have the
        data erased, rectified, completed or amended.

  Accountability Principle
  14. A data controller should be accountable for complying with measures which
      give effect to the principles stated above.

The version quoted has been taken from the updated Guidelines of 2013. The
update does not concern the FIPs themselves, but aims to strengthen world-
wide enforcement and accountability. With an eye to the increased scale of data
processing and the new techniques for data analytics, the OECD recommends
a risk-‚Äãbased approach that is proactive rather than reactive when it comes to
the rights and freedoms of those affected by the processing of personal data.

 Since 1980, many states have enacted data protection legislation, often following the
 FIPs. The EU Data Protection Directive (DPD) of 1995 was a prime example of a legally
 binding implementation of the OECD Guidelines. Since May 2018 the DPD has been
 succeeded by the GDPR. Just like the updated OECD Guidelines, the basic rules and
 principles that underlie the GDPR are largely the same as those of the DPD. The dif-
 ference regards enforcement and various obligations to take a proactive approach to
 compliance. Again, a practical and effective reinforcement of the accountability prin-
 ciple is the most significant change.



5.5.1 EU and US data protection law

In the United States, data protection is part of the right to privacy (in
Constitutional and tort law) and subject to sectorial legislation, notably with
regard to finance, healthcare, special protection of children, and consumer
protection. There is no general law on data protection, apart from the 1974
Privacy Act (which only applies to Federal Agencies). This means that the pro-
tection of personal data varies with the context of processing. In commercial
contexts, much of the actual protection depends on the competences of the
Federal Trade Commission (FTC), based on section 5 of the FTC Act:
                                                        5.5 Data Protection Law 135

 (1) Unfair methods of competition in or affecting commerce, and unfair or de-
     ceptive acts or practices in or affecting commerce, are hereby declared
     unlawful.
 (2) The Commission is hereby empowered and directed to prevent persons, part-
     nerships, or corporations, [except certain specified financial and industrial
     sectors] from using unfair methods of competition in or affecting commerce
     and unfair or deceptive acts or practices in or affecting commerce.


 Based on this, the FTC is tasked with protecting consumer privacy and data security in
 commercial contexts. The notion of a reasonable expectation of privacy is a core con-
 cept, because consumer trust is pivotal for a well-‚Äãfunctioning market in ecommerce.
 The FTC deals with violations on a case-‚Äãby-‚Äãcase basis, but also issues so-‚Äãcalled ‚Äòrul-
 ings‚Äô if it believes specific types of violations are prevalent. Such ‚Äòrulings‚Äô basically
 declare how the FTC will use its Article 5 competence, thus encouraging companies
 to change their behaviour. The FTC is often qualified as ‚Äòthe regulator‚Äô concerning
 informational privacy, due to its central role in US policy-‚Äãmaking regarding data
 protection.


In the EU, the situation is altogether different, due to the general applicability
of EU data protection law, which does not depend on whether a violation can
be framed as ‚Äòan unfair or deceptive act in or affecting commerce‚Äô. In the next
subsection, we will provide an extensive discussion of the core content of EU
data protection law.

 One could say that whereas in the United States the processing of personal informa-
 tion is allowed unless it has been explicitly restricted, in the EU any processing of any
 personal data in any context is conditioned by a set of rules and principles that im-
 pose obligations on those who process data and attribute rights to those whose per-
 sonal data is at stake.



5.5.2 EU data protection law

The GDPR is based on Article 16 TFEU, which reads:

  1. Everyone has the right to the protection of personal data concerning them.
  2. The European Parliament and the Council, acting in accordance with the or-
     dinary legislative procedure, shall lay down the rules relating to the protection
     of individuals with regard to the processing of personal data by Union institu-
     tions, bodies, offices and agencies, and by the Member States when carrying out
136 Privacy and Data Protection

        activities which fall within the scope of Union law, and the rules relating to the
        free movement of such data. Compliance with these rules shall be subject to the
        control of independent authorities.
(...)

The GDPR protects the fundamental right to data protection as stipu-
lated in Article 8 CFREU. However, the GDPR goes beyond this, explicitly
aiming to protect all the fundamental rights and freedoms that are impli-
cated by the processing of personal data. But this is not the only goal of
the Regulation. At the same time, the Regulation aims to prevent that dif-
ferent levels of data protection within the jurisdiction of the MSs result in
obstructions of the internal market. So, harmonization of protection to
ensure a free flow of personal data is the second, equally important, goal
of the GDPR:

  Article 1 Subject-‚Äãmatter and objectives
  1. This Regulation lays down rules relating to the protection of natural persons with
     regard to the processing of personal data and rules relating to the free move-
     ment of personal data.
  2. This Regulation protects fundamental rights and freedoms of natural persons
     and in particular their right to the protection of personal data.
  3. The free movement of personal data within the Union shall be neither restricted
     nor prohibited for reasons connected with the protection of natural persons with
     regard to the processing of personal data.


 As discussed earlier (in section 4.3), the EU has developed from the European
 Economic Community (EEC), where the most important goal was the creation of an
 internal market, based on the ‚Äòfour freedoms‚Äô: free movement of capital, persons,
 goods, and services. As paragraph 3 of Article 1 GDPR clarifies, this Regulation in-
 volves ‚Äòfull harmonisation‚Äô, which means that MSs are not allowed to provide either
 less or more protection than what is offered in the Regulation (with the exception of
 explicitly formulated discretion). Full harmonization ensures the absence of obstruc-
 tions of the internal market due to different requirements in terms of data protec-
 tion. The fact that the GDPR is a regulation instead of a directive confirms the wish to
 eradicate such obstructions, thus hoping to boost data-‚Äãdriven business across na-
 tional borders.


5.5.2.1 Sources of law regarding EU data protection law
So far, we have seen that the sources of law consist of legislation and treaties,
case law, doctrine, customary law, and fundamental principles. In the case
                                                                 5.5 Data Protection Law 137

of EU data protection law, we have the founding Treaties,14 the Charter, the
GDPR, the Police Data Protection Directive (PDPD),15 the ePrivacy Directive
(ePD),16 and a whole series of other Regulations and Directives that may at
some point be relevant (but will not be discussed here). Next to this we have the
case law of the CJEU regarding data protection issues, decisions and policies
of the supervisory authorities in the MSs and the European Data Protection
Supervisor, and we have doctrinal treatises and journal articles which analyse
and discuss the legislation, the case law, and the underlying principles and
practices.
In the case of EU data protection law, we have one more source of law, which
has played an important role in the interpretation of the former DPD: the
Opinions and Guidelines of the independent Article 29 Working Party (Art.
29 WP). This was the advisory body (instituted by Article 29 DPD) that pro-
duced a great number of highly relevant interpretations of EU data protec-
tion law, which continue to function as an important source of law. Though its
output was not binding, it has persuasive authority based on the experience
and expertise of its members (the data protection supervisors of the MSs) and
based on its official task, which was to advise on proper implementation of EU
data protection law. Most of the Opinions, Guidelines and Recommendations
of the Art. 29 WP are equally relevant under the GDPR, as the core principles
and concepts have not changed.

 The Art. 29 WP has been replaced, under the GDPR, with the independent European
 Data Protection Board (EDPB),17 instituted in Articles 68‚Äì‚Äã76 GDPR, again consisting
 of the supervisory authorities of all the MSs of the EU, again tasked with advising on
 the correct interpretation of the EU data protection law. The EDPB has further tasks
 in contributing to a harmonized approach of the national supervisors, throughout
 the Union.



  14 European Union, Treaty on European Union (Consolidated Version), Treaty of Maastricht, 7 February

1992, Official Journal of the European Communities C 325/‚Äã5; 24 December 2002, available at: http://‚Äãwww.
refworld.org/‚Äãdocid/‚Äã3ae6b39218.html, for the TFEU see above footnote 8 in Chapter 4.
  15 Directive (EU) 2016/‚Äã680 of the European Parliament and of the Council of 27 April 2016 on the pro-

tection of natural persons with regard to the processing of personal data by competent authorities for the
purposes of the prevention, investigation, detection or prosecution of criminal offences or the execution
of criminal penalties, and on the free movement of such data, and repealing Council Framework Decision
2008/‚Äã977/‚ÄãJHA.
  16 Directive 2002/‚Äã58/‚ÄãEC of the European Parliament and of the Council of 12 July 2002 concerning

the processing of personal data and the protection of privacy in the electronic communications sector
(Directive on privacy and electronic communications).
  17 https://‚Äã
             edpb.europa.eu, its Opinions and Guidelines can be found at: https://‚Äãedpb.europa.eu/‚Äãour-‚Äã
work-‚Äãtools/‚Äãgeneral-‚Äãguidance/‚Äãgdpr-‚Äãguidelines-‚Äãrecommendations-‚Äãbest-‚Äãpractices_‚Äãen.
138 Privacy and Data Protection

5.5.2.2 Material and territorial scope
The material scope of the GDPR is limited to ‚Äòthe processing of personal
data‚Äô (Article 2.1). The definition of ‚Äòprocessing‚Äô, however, is very broad, as
Article 4(2) reads:

   ‚Äòprocessing‚Äô means any operation or set of operations which is performed on per-
   sonal data or on sets of personal data, whether or not by automated means, such
   as collection, recording, organisation, structuring, storage, adaptation or alter-
   ation, retrieval, consultation, use, disclosure by transmission, dissemination or
   otherwise making available, alignment or combination, restriction, erasure or
   destruction

The GDPR does not apply to the processing of personal data within the
context of a household and it does not apply to processing of personal data
in the context of the prevention and prosecution of crime and threats to
public security.18 The household exception will usually exempt the users of
social networks, but not the providers (see section 5.5.2.4). With regard
to the prevention and prosecution of crime, the PDPD is in force, based
on Article 39 of the Treaty of the European Union (TEU).19 Since the EU
has no competence regarding public security (intelligence services), there
is no EU legislation as to the processing of personal data in the context of
threats to public security. Note that the ECHR does apply to issues of public
security, so insofar as privacy is infringed, measures can be tested against
Article 8 ECHR (see section 5.3.5, notably the cases of Klass and Weber &
Saravia).
Next to the exemptions of Article 2, Article 33 states that MSs may enact
legislation to restrict the applicability of specific GDPR provisions, if they
regard measures that are necessary in a democratic society, targeting a
limited set of goals, such as national security, defence, public security, the
prevention, investigation, detection, and prosecution of criminal offences, or
of breaches of ethics for regulated professions, an important object of gen-
eral public interest of a MS or of the EU, including monetary, budgetary,
and taxation matters. Note that though restrictions based on these goals are
allowed if they pass the proportionality test (‚Äònecessary in a democratic
society‚Äô clearly refers to Article 8.2 ECHR), they also require a basis in law.

  18 Art. 2 GDPR.
  19 Directive (EU) 2016/‚Äã680 on the protection of individuals with regard to the processing of personal
data by competent authorities for the purposes of prevention, investigation, detection or prosecution of
criminal offences or the execution of criminal penalties, and the free movement of such data.
                                                        5.5 Data Protection Law 139

Any such restrictions are only valid insofar as they respect the essence of the
fundamental rights and freedoms.
The territorial scope of the GDPR is defined in Article 3:

  1. This Regulation applies to the processing of personal data in the context of the
  activities of an establishment of a controller or a processor in the Union, regardless
  of whether the processing takes place in the Union or not.

Bear in mind that if a tech company has an establishment in the EU, the GDPR
applies to the processing of personal data, even if the processing takes place
elsewhere. At some point a tech company relocated its headquarters from
Ireland to the United States, because otherwise data subjects in countries
outside the EU could appeal to the Irish data protection supervisor under
the GDPR.

  2. This Regulation applies to the processing of personal data of data subjects
     who are in the Union by a controller or processor not established in the Union,
     where the processing activities are related to:
     (a) the offering of goods or services, irrespective of whether a payment of the
         data subject is required, to such data subjects in the Union; or
     (b) the monitoring of their behaviour as far as their behaviour takes place
         within the Union.


 Here we see that if a company decides to offer goods or services (whether or not
 they are free) that involve the processing of personal data of data subjects in the
 Union, or monitor their behaviour in the Union, the GDPR applies, irrespective
 of whether the company is established in the Union. Consider that this jurisdic-
 tion is limited to data subjects who are in the Union; it does not apply to EU citi-
 zens outside the Union, though it does apply to non-‚ÄãEU citizens when they are in
 the Union.


5.5.2.3 Personal data and data subject
Article 4(1) GDPR clarifies that:

  ‚Äòpersonal data‚Äô means:
   ‚Ä¢ any information
   ‚Ä¢ relating to
   ‚Ä¢ an identified or
   ‚Ä¢ identifiable natural person (‚Äòdata subject‚Äô)
140 Privacy and Data Protection

where ‚Äòan identifiable person‚Äô is defined as
      ‚Ä¢ one who can be identified,
      ‚Ä¢ directly or indirectly,
      ‚Ä¢ an identifier such as a name, an identification number, location data, an online
        identifier or to one or more factors specific to the physical, physiological, gen-
        etic, mental, economic, cultural or social identity;

Many authors have pointed out that this entails a very broad view of ‚Äòper-
sonal data‚Äô, potentially bringing nearly any data under the heading of per-
sonal data. This is especially the case as the combination of increased
availability and increased searchability and linkability of massive amounts
of data will enable identification and re-‚Äãidentification of data that would
previously not have been considered personal data. At some point, data
about the weather, about room temperature, about the arrival of a train may
become personal data, when it can be related to a person that can be singled
out. Recital 26 reads:

  To determine whether a natural person is identifiable, account should be taken
  of all the means reasonably likely to be used, such as singling out, either by
  the controller or by another person to identify the natural person directly or
  indirectly.

The criterion to determine whether data is personal, that is ‚Äòidentifiable‚Äô, is that
it is ‚Äòreasonably likely‚Äô that a person can, for example, be singled out. The re-
cital continues:

  To ascertain whether means are reasonably likely to be used to identify the nat-
  ural person, account should be taken of all objective factors, such as the costs
  of and the amount of time required for identification, taking into consider-
  ation the available technology at the time of the processing and technological
  developments.


 Here we see that the ‚Äòreasonably likely‚Äô criterion should be understood as an ob-
 jective criterion, taking into account the costs, the time, and effort, and the available
 technical means at the time of processing.


In the case of Breyer v. Germany,20 the CJEU decided that even a dynamic IP
address may qualify as a personal data, depending on whether the link with a

 20    CJEU, 19 October 2016, Case 582/‚Äã14 (Patrick Breyer v. Germany).
                                                       5.5 Data Protection Law 141

specific person can be made. The case concerned government websites that
processed dynamic IP addresses, keeping them longer than was necessary for
providing access to the sites. What made this case special is that the ability to
link the IP address to a specific person was not in the hands of the operators of
the government website but in the hand of internet service providers (ISPs).
The CJEU found that because ISPs could be ordered by a court to provide in-
formation about the user of a dynamic IP addresses, this IP address should not
be considered anonymous.

 So, personal data is any data that relates to an identifiable natural person (excluding
 legal persons such as corporations), and a data subject is the identifiable natural
 person to whom the data relate.


The material scope of the GDPR regards (as discussed in section 5.5.2.2) the
processing of personal data. This implies that to avoid applicability of the
GDPR, one could ‚Äòsimply‚Äô anonymize previously personal data. There are two
caveats here. First, anonymization is itself a form of processing, and thereby
requires a valid legal ground (see section 5.5.2.5). Second, anonymization is
not easy, because the risk of re-‚Äãidentification easily turns ‚Äòanonymous‚Äô data
into identifiable and thus personal data. In practice, anonymization will often
remove so much information from the data that it is no longer relevant for the
purpose of processing. To better understand the difference between personal
and anonymized data, we can best check the definition of ‚Äòpseudonymization‚Äô
of Article 4(5):
  ‚Ä¢   the processing of personal data in such a manner that
  ‚Ä¢   the personal data can no longer be attributed to a specific data subject
  ‚Ä¢   without the use of additional information,
  ‚Ä¢   provided that such additional information is kept separately and
  ‚Ä¢   is subject to technical and organisational measures to ensure that the personal
      data are not attributed to an identified or identifiable natural person;

First, we see that pseudonymous data is defined as a subset of personal data.
Second, it is defined as data from which any identifying information has been
removed and stored separately, subject to technical and organizational meas-
ures that resist re-‚Äãidentification.

 Pseudonymization is a way to comply with data protection law (by ways of data mini-
 mization), not a way to avoid applicability. With regard to encryption, key manage-
 ment that enables a party other than the data subject to decrypt, will mostly qualify
142 Privacy and Data Protection


 as pseudonymization, not as anonymization. Bear in mind that the definition of
 pseudonymization in the GDPR defines the condition of the relevant legal effect, irre-
 spective of other how other disciplines define pseudonymization.


5.5.2.4 Data controller and data processor
Article 4(7) GDPR defines ‚Äòcontroller‚Äô as:
   ‚Ä¢ the natural or legal person, public authority, agency or any other body
   ‚Ä¢ which alone or jointly with others
   ‚Ä¢ determines the purposes and means of the processing of personal data;

The definition of ‚Äòdata controller‚Äô is crucial, because the ‚Äòdata controller‚Äô is both
accountable and liable for compliance with all the obligations of the GDPR,
including obligations to implement a proactive approach to potential risks to
the fundamental rights and freedoms of data subjects. The ‚Äòdata controller‚Äô is
basically defined as whoever determines the purpose of processing, whereby
the CJEU checks who determines such purpose in practice, not merely on
paper. The ‚Äòdata controller‚Äô also determines the means of processing, but this
can be outsourced to a data processor, defined as (Article 4(8)):
   ‚Ä¢ a natural or legal person, public authority, agency or any other body
   ‚Ä¢ which processes personal data on behalf of the controller;

Here, we clearly see that the data controller remains accountable for the choice
of the means of processing, even if that choice is made by a processor. When
the landmark case on the so-‚Äãcalled ‚Äòright to be forgotten‚Äô was decided in 2014
(Google Spain v, Costeja),21 one of the most important issues was whether
Google should be qualified as a data controller or a data processor. Google
had argued that its search engine has no other function than to provide its
users with automatically generated search results, thereby claiming that it is
the user, not the service provider who determines the purpose of the pro-
cessing. Google argued that its search engine is merely a choice of means (the
PageRank algorithm) employed in the service of users that decide the purpose
of the search. The highest adviser of the CJEU (the Court), holding the office
of Advocate General (AG), who is required to provide a so-‚Äãcalled ‚ÄòOpinion‚Äô
(advise) to the Court, had taken the position that in the case of a search engine,
the service provider is indeed a data processor, not the controller. Surprisingly,


 21 CJEU, 13 May 2014, C-‚Äã   131/‚Äã12 (Google Spain v. Costeja). See also EDPB (formerly Art. 29 WP),
Opinion 1/‚Äã2010 on the concepts of ‚Äòcontroller‚Äô and ‚Äòprocessor‚Äô.
                                                                      5.5 Data Protection Law 143

the Court (that is not bound by the Opinion of the AG), took another position,
based on the fact that Google Spain (the subsidiary that sells advertising space
on the search engine‚Äôs pages directed to Spanish users) has its own business
model and thereby determines the purpose of processing. If the Court had
not qualified Google Spain as a data controller, it could never have required
Google to de-‚Äãlist the news item that Costeja wished to have erased.
Another pivotal case of 2018 concerned the fanpage of Wirtschaftsakademie,22
used to provide services in the realm of education. The fanpage was hosted on
Facebook, which enabled the operator to obtain anonymous statistical details
on website visitors via the ‚ÄòFacebook Insights‚Äô function, which Facebook offers
free of charge under non-‚Äãnegotiable conditions. The CJEU decided that the
operator of the fanpage was a joint controller, together with Facebook, as the
statistics were obtained by processing cookies placed on the terminal equip-
ment of the visitors. Since the purpose of the processing of such cookies is co-‚Äã
decided by the fanpage operator, even though it has no control over the data
processing and was not given access to the data, they are jointly responsible
for the necessary processing of personal data. Under the GDPR this would be
based on Article 26, which reads:

   Where two or more controllers jointly determine the purposes and means of pro-
   cessing, they shall be joint controllers. They shall in a transparent manner deter-
   mine their respective responsibilities for compliance with the obligations under
   this Regulation, in particular as regards the exercising of the rights of the data sub-
   ject and their respective duties to provide the information ( . . . ).


  As in the case of Google Spain v. Costeja, where the AG argued that Google was merely
  a processor, acting on request of the users of the search engine, one could argue that,
  in this case, Facebook is acting as a processer for the fanpage operator who wishes to
  obtain the statistics. In line with the Court in Google Spain v. Costeja, the Court decided
  that Facebook is the controller, not the processor. In this case, however, the fanpage
  operator‚Äî‚Äãother than the users of a search engine‚Äî‚Äãis considered a joint controller.23



   22 CJEU, 5 June 2018, C-‚Äã     210/‚Äã16 (Unabh√§ngiges Landeszentrum f√ºr Datenschutz Schleswig-‚ÄãHolstein
v. Wirtschaftsakademie Schleswig-‚ÄãHolstein GmbH).
   23 See also CJEU, 29 July 2019, Case C-‚Äã40/‚Äã17 (Fashion ID), where the Court ruled that ‚ÄòThe operator of

a website, such as Fashion ID GmbH & Co. KG, that embeds on that website a social plugin causing the
browser of a visitor to that website to request content from the provider of that plugin and, to that end,
to transmit to that provider personal data of the visitor can be considered to be a controller, within the
meaning of Article 2(d) of Directive 95/‚Äã46. That liability is, however, limited to the operation or set of oper-
ations involving the processing of personal data in respect of which it actually determines the purposes and
means, that is to say, the collection and disclosure by transmission of the data at issue.‚Äô
144 Privacy and Data Protection

5.5.2.5 Legal ground for lawful processing of personal data
The processing of personal data is only allowed on the basis of one of six legal
grounds. Please take note of the fact that consent is just one of those legal
grounds and not necessarily the most obvious. Article 6 GDPR reads:

  a) the data subject has given consent to the processing of his or her personal data
  for one or more specific purposes;

Under the DPD ‚Äòfor one or more specific purposes‚Äô was not explicitly men-
tioned, though it was obvious from requirements detailed elsewhere in the
DPD. Under the GDPR, it is explicitly clear that consent is only valid if the
purpose has been specified. As Article 5 stipulates that data may only pro-
cessed if necessary for the specified purpose, this means that consent can only
concern the processing of personal data that is necessary for the purpose that
was communicated. All the other grounds stipulate that the processing must
be necessary in relation to the ground.
Valid consent will be further discussed in a dedicated section (section 5.5.2.7).

  b) processing is necessary for the performance of a contract to which the data
  subject is party or in order to take steps at the request of the data subject prior to
  entering into a contract;

This entails that once the contract has been concluded and performed and the
data is no longer necessary (goods or service delivered, invoice paid), it may
no longer be processed on this ground. Further processing will require an-
other ground, for example, consent (for another purpose).

  c) processing is necessary for compliance with a legal obligation to which the
  controller is subject;

Much processing is mandatory due to legal obligations, such as processing by
the tax authority, social security agency, land registry, or by commercial en-
terprise that must, for example, comply with employment, social security, and
tax legislation. Article 6.3 stipulates that this processing must be based on MS
or Union law, must contain the specific purpose(s) of processing, and must
have relevant limitations and safeguards.

  d) processing is necessary in order to protect the vital interests of the data subject
  or of another natural person;
                                                          5.5 Data Protection Law 145

This ground must be understood as concerning life-‚Äãthreatening situations,
where, for example, medical data must be processed to save someone‚Äôs life.

  e) processing is necessary for the performance of a task carried out in the public
  interest or in the exercise of official authority vested in the controller;

This ground is comparable to the c-‚Äãground, but here there may not be a legal
obligation but a legal competence or task that requires the processing of per-
sonal data. We can think of processing by various types of government agen-
cies that provide support to those in need, or need to collect information on
energy usage to develop policies on the reduction of energy consumption.
Note that to the extent that such information can rely on aggregated or other-
wise anonymized data, the processing of personal data is not necessary and
cannot be based on this ground. Article 6.3 stipulates that this processing
must be based on MS or Union law, must contain the specific purpose(s) of
processing, and must have relevant limitations and safeguards.

  f) processing is necessary for the purposes of the legitimate interests pursued by
  the controller or by a third party, except where such interests are overridden by
  the interests or fundamental rights and freedoms of the data subject which require
  protection of personal data, in particular where the data subject is a child.

  Point (f) of the first subparagraph shall not apply to processing carried out by
  public authorities in the performance of their tasks.

The f-‚Äãground is important for processing carried out by the commercial
sector, including financial institutions, social networks, and search engines,
and we may expect that added value service providers in the context of smart
homes, smart grids, and connected cars will base the processing of data that is
not necessary for the primary process (which will often be based on contract)
on the f-‚Äãground. As the economic interests of a business, including its com-
petitive edge and innovative potential, often depend on advertising revenue
and/‚Äãor the sale of personal data or inferred profiles, the f-‚Äãground is a tempting
basis insofar as other grounds do not apply.

 However, the f-‚Äãground requires a balancing test. As one can imagine, the business inter-
 ests of a company cannot, by default, overrule the interests or fundamental rights and
 freedoms of data subjects whose behavioural data are used to generate income (thus,
 e.g. enabling the so-‚Äãcalled ‚Äòfree services‚Äô of social networks and search engines).
146 Privacy and Data Protection


  This has two consequences:

     1. The controller has to assess (before initiating the processing) whether its eco-
         nomic interests in processing personal data that are not necessary to provide
         a requested service, can overrule the interests and rights of those whose data
         are used for micro-‚Äãtargeting or other ways of monetizing the data.
     2. The data subject can object to the processing based on their particular situ-
         ation, in which case the controller must stop processing unless it can find
         compelling grounds to override the interests, rights, and freedoms of the data
         subject. The right to object is based on Article 21 GDPR and also concerns the
         e-‚Äãground.


The balancing test required of the controller, entails the following
considerations:24

   the nature and source of the legitimate interest and whether the data processing
   is necessary for the exercise of a fundamental right, is otherwise in the public
   interest, or benefits from recognition in the community concerned;

   the impact on the data subject and their reasonable expectations about what
   will happen to their data, as well as the nature of the data and how they are
   processed;

   additional safeguards which could limit undue impact on the data subject, such as
   data minimisation, privacy-‚Äãenhancing technologies; increased transparency, gen-
   eral and unconditional right to opt-‚Äãout, and data portability.


  The f-‚Äãground is often used to legitimize the advertising business model of free serv-
  ices. For instance, in the Google Spain v. Costeja case discussed above, the CJEU con-
  cluded that Google was processing personal data based on its legitimate business
  interests. In this particular case, the CJEU considered two types of legitimate inter-
  ests that might overrule Costeja‚Äôs interest in having a particular search result de-‚Äã
  referenced. First, the interest of the controller, second the interests of third parties,
  namely the users of the search engine.


First, the Courts looks into the economic interest of Google Spain in sus-
taining its business model, because the right to erase and the right to object


  24 EDPB (formerly Art. 29 WP), Opinion 06/‚Äã2014 on the notion of legitimate interests of the data con-

troller under Article 7 of Directive 95/‚Äã46/‚ÄãEC, WP217, at 3.
                                                          5.5 Data Protection Law 147

that Costeja invoked would involve costs on the side of Google (especially be-
cause many others may similarly submit requests to de-‚Äãreference). The CJEU
found that:

  81 In the light of the potential seriousness of that interference, it is clear that it
  cannot be justified by merely the economic interest which the operator of such an
  engine has in that processing. ( . . . )

The seriousness of the interference, in this case, was argued in considerations
37 and 38:

  37 ( . . . ) the organisation and aggregation of information published on the internet
  that are effected by search engines with the aim of facilitating their users‚Äô ac-
  cess to that information may, when users carry out their search on the basis of
  an individual‚Äôs name, result in them obtaining through the list of results a struc-
  tured overview of the information relating to that individual that can be found on
  the internet enabling them to establish a more or less detailed profile of the data
  subject.

  38 Inasmuch as the activity of a search engine is therefore liable to affect signifi-
  cantly, and additionally compared with that of the publishers of websites, the fun-
  damental rights to privacy and to the protection of personal data, ( . . . ).

Second, the Court considered the legitimate interests of users of the search en-
gine in having access to the search result that may be de-‚Äãreferenced:

  81 ( . . . ) However, inasmuch as the removal of links from the list of results could,
  depending on the information at issue, have effects upon the legitimate interest
  of internet users potentially interested in having access to that information, in
  situations such as that at issue in the main proceedings a fair balance should be
  sought in particular between that interest and the data subject‚Äôs fundamental
  rights under Articles 7 and 8 of the Charter. Whilst it is true that the data subject‚Äôs
  rights protected by those articles also override, as a general rule, that interest of
  internet users, that balance may however depend, in specific cases, on the na-
  ture of the information in question and its sensitivity for the data subject‚Äôs pri-
  vate life and on the interest of the public in having that information, an interest
  which may vary, in particular, according to the role played by the data subject in
  public life.

Here we see a clash between the freedom of information of search engine users
and the right to data protection of the data subject, which requires some subtle
148 Privacy and Data Protection

balancing. Note, however, that the Court is not discussing the removal of con-
tent from the internet, but the de-‚Äãreferencing of a search result that links to
such content.

 The EDPB considers that in principle a controller must make up its mind which legal
 basis justifies a particular type of processing operation; controllers cannot, for in-
 stance, process personal data based on consent and then shift to the legitimate
 interest after the data subject withdraws their consent.25 The EDPB also refers to
 Articles 13.1 and 14.1 that require the controller to provide information about the
 purpose(s) and the legal basis for its processing operations, meaning that this should
 be clarified from the start.26


5.5.2.6 Principles of lawful, fair, and transparent processing
Next to, and thus on top of, having a legal ground, Article 5 GDPR stipulates a
set of rules under the heading of ‚ÄòPrinciples relating to the processing of per-
sonal data‚Äô. Though the use of the term ‚Äòprinciples‚Äô could suggest that these are
just some underlying assumptions, they are in fact rules that must be com-
plied with. We will follow the wording of the article, discussing each para-
graph along the way (the principles in bold are part of the article, emphasis
is mine):

   (a) processed lawfully, fairly and in a transparent manner in relation to the data
   subject (‚Äòlawfulness, fairness and transparency‚Äô);

Though one may think that lawfulness merely refers to Article 6, which con-
tains the legal basis, the term ‚Äòlawfulness‚Äô also refers to the bigger picture of the
rule of law, as with the requirement that infringements of the right to privacy
under Article 8 ECHR must be ‚Äòin accordance with the law‚Äô. This means that a
mere basis in law is not enough and must be understood in qualitative terms
to include respect for legitimate expectations, independent oversight, and other
checks and balances to ensure that the legal basis of Article 6 is valid (see also
Article 6.3). Similarly, fairness refers to various balancing and proportionality
tests, taking note of the relevant interests and fundamental rights that are at
stake. Transparency is further detailed in Articles 13, 14, and 15 GDPR.

  25 EDPB (formerly Art. 29 WP), 10 April 2018, Guidelines on consent under Regulation 2016/‚Äã     679,
WP259.rev.1, 23.
  26 EDPB (formerly Art. 29 Working Party), 10 April 2018, Guidelines on transparency under Regulation

2016/‚Äã679, WP260.rev.1, 33, 35.
                                                         5.5 Data Protection Law 149

  (b) collected for specified, explicit and legitimate purposes and not further pro-
  cessed in a manner that is incompatible with those purposes; further processing
  for archiving purposes in the public interest, scientific or historical research pur-
  poses or statistical purposes shall, in accordance with Article 89(1), not be con-
  sidered to be incompatible with the initial purposes (‚Äòpurpose limitation‚Äô);


 Purpose limitation is one of the most important principles of EU data protection
 law. The idea that data must be collected and processed for one or more legitimate
 purposes that have been made explicit and are sufficiently specified pervades the
 regulation, while at the same time qualifying whoever‚Äî‚Äãde facto‚Äî‚Äãdetermines such
 purpose(s) as the responsible, accountable, and liable entity (the data controller).
 Purpose is, in a way, the vanishing point of the architecture of EU data protection law.


Further processing for another purpose is allowed if the purpose is not in-
compatible with the initial purpose, as communicated to the data subject.
To determine whether the new purpose is compatible, Article 6(4) provides
the following indications: any link between the old and the new purpose, the
context of collection and the relationship between controller and subject,
the nature and sensitivity of the data, the potential consequences of further
processing for the data subject, and the existence of appropriate safeguards,
such as encryption or pseudonymization. In case of consent for the new
purpose or a legal obligation that involves the new purpose, processing is
based on the new ground and cannot be based on processing for a compat-
ible purpose.
Secondary usage (further processing) for scientific or statistical research or
archiving in the public interest is considered compatible by default. The GDPR
contains an extensive exception for such processing in Article 89, with further
exceptions for medical research in, for example, Article 9.2(h). Recital 33 fur-
thermore indicates that ‚Äò[i]‚Äåt is often not possible to fully identify the purpose
of personal data processing for scientific research purposes at the time of data
collection. Therefore, data subjects should be allowed to give their consent to
certain areas of scientific research when in keeping with recognised ethical
standards for scientific research. Data subjects should have the opportunity to
give their consent only to certain areas of research or parts of research projects
to the extent allowed by the intended purpose‚Äô.

  (c) adequate, relevant and limited to what is necessary in relation to the purposes
  for which they are processed (‚Äòdata minimisation‚Äô);
150 Privacy and Data Protection

Data minimization is another core principle, that also underlies the principles
of purpose limitation and storage limitation. In the DPD, this ground was ar-
ticulated as ‚Äòadequate, relevant and not excessive‚Äô, whereas now the criterion
is ‚Äòadequate, relevant and limited to what is necessary‚Äô. This is a further re-
striction, moving towards strict proportionality and subsidiarity, thereby also
relating to the requirement to pseudonymize or anonymize the data as soon
as possible. This principle links consent to necessity, as observed above. It also
connects with the right to request erasure if processing is irrelevant for the
given purpose.

  (d) accurate and, where necessary, kept up to date; every reasonable step must
  be taken to ensure that personal data that are inaccurate, having regard to the
  purposes for which they are processed, are erased or rectified without delay
  (‚Äòaccuracy‚Äô);

Here the principle of accuracy is formulated as a legal obligation of the data
controller, but this connects with the rights of erasure and rectification in the
case that data are inaccurate.

  (e) kept in a form which permits identification of data subjects for no longer than
  is necessary for the purposes for which the personal data are processed; personal
  data may be stored for longer periods insofar as the personal data will be pro-
  cessed solely for archiving purposes in the public interest, scientific or historical
  research purposes or statistical purposes in accordance with Article 89(1) subject
  to implementation of the appropriate technical and organisational measures re-
  quired by this Regulation in order to safeguard the rights and freedoms of the data
  subject (‚Äòstorage limitation‚Äô);

Storage limitation basically requires that controllers engage in lifecycle man-
agement of the personal data they process, removing them, for example, when
the purpose is exhausted and processing is no longer relevant. The exception
for scientific research and archiving, mentioned above, requires appropriate
technical and organizational safeguards, taking into account the rights and
freedoms of the data subject, which will vary depending on, for example, the
nature of the data.

  (f) processed in a manner that ensures appropriate security of the personal data,
  including protection against unauthorised or unlawful processing and against
  accidental loss, destruction or damage, using appropriate technical or organisa-
  tional measures (‚Äòintegrity and confidentiality‚Äô).
                                                       5.5 Data Protection Law 151

This principle connects with the requirement of security by design of Article
32, and the legal obligation for controllers to notify supervisory authorities
and data subjects of data breaches (Articles 33, 34).

  2. The controller shall be responsible for, and be able to demonstrate compliance
  with, paragraph 1 (‚Äòaccountability‚Äô).

The accountability principle addresses the data controller as the focal point
of responsibility, accountability, and liability regarding compliance with
the principles that pervade the GDPR. Accountability is further detailed in
Article 30 that requires the controller to demonstrate and document com-
pliance, while liability is further detailed in Articles 79‚Äì‚Äã83 about enforce-
ment (including both administrative law fines and prohibitions, and private
law compensation and injunctive relief). The roles and responsibilities of the
controller (including joint controllers) and processor are further specified in
Articles 24, 26, and 28.

5.5.2.7 Valid consent
Other than the DPD, the GDPR contains a separate article on consent. Article
7 declares, under the heading of ‚ÄòConditions for Consent‚Äô:

  1. Where processing is based on consent, the controller shall be able to demon-
  strate that the data subject has consented to processing of his or her personal data.

This concerns the burden of proof.

  2. If the data subject‚Äôs consent is given in the context of a written declaration
  which also concerns other matters, the request for consent shall be presented in
  a manner which is clearly distinguishable from the other matters, in an intelligible
  and easily accessible form, using clear and plain language. Any part of such a dec-
  laration which constitutes an infringement of this Regulation shall not be binding.

Note that consent may not be hidden in complicated wordy privacy policies,
and must be ‚Äòeasily accessible‚Äô as to its form (think of the user interface), ‚Äòusing
clear and plain language‚Äô. If consent is part of an elaborate and incomprehen-
sible Terms of Service that basically contains an implicit consent, such con-
sent is not valid.

  3. The data subject shall have the right to withdraw his or her consent at any time.
  The withdrawal of consent shall not affect the lawfulness of processing based on
152 Privacy and Data Protection

  consent before its withdrawal. Prior to giving consent, the data subject shall be in-
  formed thereof. It shall be as easy to withdraw as to give consent.

This means that if consent is given by ticking a box, it must be as easy to untick
the box. If one has to explore every nook and corner of a website to figure out
how to withdraw consent, the consent is not valid.

  4. When assessing whether consent is freely given, utmost account shall be taken
  of whether, inter alia, the performance of a contract, including the provision of a
  service, is conditional on consent to the processing of personal data that is not ne-
  cessary for the performance of that contract.

To better understand what this means, we can use recital 43:

  In order to ensure that consent is freely given, consent should not provide a valid
  legal ground for the processing of personal data in a specific case where there is a
  clear imbalance between the data subject and the controller, in particular where
  the controller is a public authority and it is therefore unlikely that consent was
  freely given in all the circumstances of that specific situation. Consent is presumed
  not to be freely given if it does not allow separate consent to be given to different
  personal data processing operations despite it being appropriate in the individual
  case, or if the performance of a contract, including the provision of a service,
  is dependent on the consent despite such consent not being necessary for such
  performance.


 This seems to indicate that attempts to force consumers to choose between accessing
 a service and refusing consent for additional processing are unlawful and that such
 consent is not valid. Additional processing refers to the processing of data that is not
 necessary for the provision of the service, or further processing of data after the pur-
 pose has been exhausted. One could guess that Article 7.4, read through the lens of
 recital 43, is the end of a specific type of business model that puts the site on black if
 consent for unnecessary processing is not given.


Note that the legal ground must be communicated to the data subject when
the processing commences (if data is collected from the data subjects, cf. Article
13), or within a reasonable time, at the latest within one month after obtaining
the data (if data has not been obtained from the data subjects, cf. Article 14).
Controllers cannot require consent and‚Äî‚Äãafter finding that the consent is not
valid‚Äî‚Äãclaim that the processing is based on its legitimate interest; due to the
                                                      5.5 Data Protection Law 153

inherent logic of the different grounds, controllers cannot claim to base the
same processing operations on different grounds.

5.5.2.8 Special categories of data
Article 9 defines a set of data as requiring special treatment. These data are
often called ‚Äòsensitive data‚Äô and are defined as: ‚Äòdata revealing racial or ethnic
origin, political opinions, religious or philosophical beliefs, or trade union
membership, and the processing of genetic data, biometric data for the pur-
pose of uniquely identifying a natural person, data concerning health or data
concerning a natural person‚Äôs sex life or sexual orientation‚Äô.
By default, the processing of such data is prohibited. Strictly defined excep-
tions apply, notably based on explicit consent; specific rights and obligations
in the field of employment and social; the vital interests of the data subject
or of another natural person; or with regard to processing in the context of
not-‚Äãfor-‚Äãprofit bodies with a political, philosophical, religious, or trade union
aim; processing of personal data which are manifestly made public by the data
subject; processing necessary for legal claims, substantial public interest, pre-
ventive or occupational medicine, assessment of the working capacity of the
employee, medical diagnosis, the provision of health or social care or treat-
ment or the management of health or social care systems and services, for
public health, for archiving purposes in the public interest, scientific or histor-
ical research purposes or statistical purposes.
On top of that, Article 10 restricts the ‚ÄòProcessing of personal data relating to
criminal convictions and offences‚Äô.

  Processing of personal data relating to criminal convictions and offences or re-
  lated security measures based on Article 6(1) shall be carried out only under
  the control of official authority or when the processing is authorised by Union
  or Member State law providing for appropriate safeguards for the rights and
  freedoms of data subjects. Any comprehensive register of criminal convictions
  shall be kept only under the control of official authority.


 Articles 9 and 10 demonstrate that data protection is not just about the right
 to privacy, but also entails protection against discrimination on prohibited
 grounds.


This is particularly relevant if inferences are made based on machine learning
or other techniques to infer patterns from big data, because such inferences
154 Privacy and Data Protection

may include sensitive data. Social networks, advertising intermediaries, or
criminal justice authorities may infer racial or ethnic origin, political opinion,
or sexual preferences, which inferences may then be applied to identifiable
persons that match the profile. Such inferencing may be inadvertent, but
nevertheless result in decisions based on such inferences, for instance parole
decisions based on a correlation between race and recidivism. In Chapters 10
and 11 we will return to this point when discussing machine learning and pro-
filing, including an analysis of GDPR provisions on profiling and automated
decision-‚Äãmaking based on profiling.

5.5.2.9 Data protection by design and default (DPbDD)
In Chapter 1, notably in section 1.4, we have identified the text-‚Äãdriven nature
of modern law, in contrast with the orality of prior normative orderings. The
rise of data-‚Äãand code-‚Äãdriven ICIs confronts the text-‚Äãdriven nature of the law
with a number of problems. Merely writing down and enacting legal norms
may not work if the defaults of the technical and organizational architecture of
the onlife world generate a contradictory normativity, which renders compli-
ance with legal norms difficult if not impossible. In other words, the technical
architecture may present its users and inhabitants with a choice architecture
that limits their understanding of the backend systems of the social networks
they use, of their smart homes, connected cars, and more.
Article 25 GDPR requires that data controllers design the data processing op-
erations in compliance with data protection law. Data protection by design
(DPbD) may sound like Privacy by Design (PbD). However, the latter is based
on an ethical duty, not necessarily on a legal obligation; PbD reflects the choice
of a controller to respect the privacy of their users by way of a privacy-‚Äãfriendly
design. Also, as privacy is not equivalent with data protection, PbD cannot be
equated with DPbD, even though in practice the terminology is often used
interchangeably.
DPbD is a new legal obligation (no such obligation applied under the DPD).
In case of non-‚Äãcompliance the legal effect is liability for damages (private law
liability, Article 82), unlawful processing (administrative fines, Article 83), or
injunctive relief (private law injunction to stop unlawful processing with pen-
alty payments for every day of non-‚Äãcompliance, Article 79).
Under the heading of ‚Äòdata protection by design and default‚Äô, Article 25 stipulates:

  1. Taking into account the state of the art, the cost of implementation and the
  nature, scope, context and purposes of processing as well as the risks of varying
                                                      5.5 Data Protection Law 155

  likelihood and severity for rights and freedoms of natural persons posed by the
  processing, the controller shall, both at the time of the determination of the
  means for processing and at the time of the processing itself, implement appro-
  priate technical and organisational measures, such as pseudonymisation, which
  are designed to implement data-‚Äãprotection principles, such as data minimisa-
  tion, in an effective manner and to integrate the necessary safeguards into the
  processing in order to meet the requirements of this Regulation and protect the
  rights of data subjects.


 Paragraph 1 describes ‚Äòdata protection by design‚Äô as a set of technical and organiza-
 tional measures that embed core data protection principles into the design of the
 data processing architecture.


This should mitigate potential risks for the rights and freedoms of natural per-
sons. The latter demonstrates the risk-‚Äãbased approach of the GDPR, which
requires that controllers take a proactive approach when developing their
computational backend systems. Note that Article 25 does not speak of the
risks for rights and freedoms of data subjects, but of natural persons. This
includes processing operations that impact other individuals, for instance
when inferencing behavioural correlations that enable the influencing, exclu-
sion, or other types of targeting of others than the data subject. Relevant de-
sign measures are, for instance pseudonymization, but one can also think of
user-‚Äãfriendly interfaces to enable easy withdrawal of consent (Article 7.3) or
subject access requests (SARs) (based on Article 15.3). Both the withdrawal
of consent and SARs will involve computational architectures in the backend
systems that effectively halt the processing of data for which consent has been
withdrawn, or provide the data that are being processed (where Article 15.3
stipulates that if a SAR is made via electronic means, the data shall be pro-
vided in a commonly used electronic format).

 The legal obligation to implement DPbDD is not obvious and may result in a major
 upheaval of backend systems, involving substantial costs. Depending on the risks of
 abstaining from such measures for the rights and freedoms of natural persons, such
 costs will become part of the relevant business model.


  2. The controller shall implement appropriate technical and organisational meas-
  ures for ensuring that, by default, only personal data which are necessary for each
  specific purpose of the processing are processed. That obligation applies to the
156 Privacy and Data Protection

  amount of personal data collected, the extent of their processing, the period of
  their storage and their accessibility. In particular, such measures shall ensure that
  by default personal data are not made accessible without the individual‚Äôs interven-
  tion to an indefinite number of natural persons.


 Paragraph 2 describes ‚Äòdata protection by default‚Äô, which is DPbD with regard to data
 minimization.



It demands that the architecture is constructed in such a way that no add-
itional processing takes place, beyond what is necessary for the specific pur-
pose of the relevant processing operations. Again, compliance with this legal
obligation will result in major reconfigurations of current backend systems,
involving, for example, effective lifecycle management of personal data (in-
cluding pseudonymization, anonymization, and deletion of data).
The third paragraph declares that an approved certification mechanism may
contribute to demonstration of compliance with DPbDD.

5.5.2.10 Data protection impact assessment
DPbDD is closely related to another new compliance mechanism, the data
protection impact assessment (DPIA), again exhibiting the risk-‚Äãbased, pro-
active approach that is favoured under the GDPR. Basically, controllers are
obligated to assess potential violations of the GDPR when initiating new data-‚Äã
driven technologies. Article 35 reads under the heading of ‚Äòdata protection
impact assessment‚Äô that:

  1. Where a type of processing in particular using new technologies, and taking into
  account the nature, scope, context and purposes of the processing, is likely to re-
  sult in a high risk to the rights and freedoms of natural persons, the controller shall,
  prior to the processing, carry out an assessment of the impact of the envisaged pro-
  cessing operations on the protection of personal data. A single assessment may
  address a set of similar processing operations that present similar high risks.

The criterion that decides whether a controller must conduct a DPIA is that
foreseen processing operations are ‚Äòlikely to result in a high risk to the rights
and freedoms of natural persons‚Äô. Again, these risks are not restricted to data
subjects, but extend to all natural persons. The assessment investigates the po-
tential impact of envisaged processing operations, which assumes that these
are indeed foreseen and mapped against impact on fundamental rights.
                                                      5.5 Data Protection Law 157

  2. The controller shall seek the advice of the data protection officer, where desig-
  nated, when carrying out a data protection impact assessment.

Articles 37‚Äì‚Äã39 detail which types of controller must appoint a data protection
officer (DPO), under what conditions (e.g. safeguards for independence) and
with what tasks. One of the tasks of the DPO is to advise on the DPIA.

  3. A data protection impact assessment referred to in paragraph 1 shall in par-
     ticular be required in the case of:
     (a) a systematic and extensive evaluation of personal aspects relating to
         natural persons which is based on automated processing, including
         profiling, and on which decisions are based that produce legal effects
         concerning the natural person or similarly significantly affect the natural
         person;
     (b) processing on a large scale of special categories of data referred to in
         Article 9(1), or of personal data relating to criminal convictions and of-
         fences referred to in Article 10; or
     (c) a systematic monitoring of a publicly accessible area on a large scale.

  Paragraph 3 sums up when a DPIA is mandatory, thus also giving an indication of
  what types of processing operations are considered high-‚Äãrisk.

Paragraphs 4‚Äì‚Äã6 stipulate that supervisory authorities shall publish a further
list of the kind of processing operations where a DPIA is mandatory, and may
publish a list of processing operations where a DPIA is not mandatory. Both
lists will be shared with the EDPB (which has an important advisory func-
tion as to the interpretation of the GDPR, and is further defined in Articles
68‚Äì‚Äã76 GDPR).

  7. The assessment shall contain at least:
    (a) a systematic description of the envisaged processing operations and the
        purposes of the processing, including, where applicable, the legitimate
        interest pursued by the controller;
    (b) an assessment of the necessity and proportionality of the processing oper-
        ations in relation to the purposes;
    (c) an assessment of the risks to the rights and freedoms of data subjects re-
        ferred to in paragraph 1; and
    (d) the measures envisaged to address the risks, including safeguards, se-
        curity measures and mechanisms to ensure the protection of personal
        data and to demonstrate compliance with this Regulation taking into
        account the rights and legitimate interests of data subjects and other per-
        sons concerned.
158 Privacy and Data Protection

Paragraph 7 provides a first indication of a template for the DPIA. The
listing has a high level of abstraction, thus enabling adequate concretization,
depending on the types of processing operations, the context of processing,
the nature of the data, and so forth. Under (d) we recognize a reference to
DPbD, whose purpose is to mitigate risks to the rights and freedoms of nat-
ural persons.
Paragraph 8 states that approved codes of conduct (Article 40 GDPR) will be
taken into account when assessing the impact.

  9. Where appropriate, the controller shall seek the views of data subjects or their
  representatives on the intended processing, without prejudice to the protection of
  commercial or public interests or the security of processing operations.

Paragraph 9 emphasizes the need to involve those who will suffer the conse-
quences of the intended processing, both on the side of data subjects and on
the side of the controller. In earlier versions of the GDPR, the need to involve
data subjects in the assessment was articulated more forcefully. One can im-
agine that a robust architecture will fare well based on input from those who
will be effectively affected.
Paragraph 10 provides an exception for processing based on a legal obliga-
tion or a public task or authority (Article 6.1 under (c) and (e)), whenever
the enactment of such an obligation has been preceded by a general DPIA on
account of the legislator.

  11. Where necessary, the controller shall carry out a review to assess if processing
  is performed in accordance with the data protection impact assessment at least
  when there is a change of the risk represented by processing operations.


 In an innovative environment, where agile computing strategies lead to iterant
 changes in processing operations, the DPIA is best seen as a persistent and dynamic
 process that continuously monitors both the foreseeable risks and the appropriate
 mitigating measures.


5.5.2.11 Compliance and enforcement
The GDPR reinforces the accountability principle by initiating new legal ob-
ligations to further compliance, notably the obligation to implement DPbDD
and to conduct a DPIA. Apart from those, other legal obligations require tech-
nical and organizational compliance measures, such as easy withdrawal of
                                                        5.5 Data Protection Law 159

consent (Article 7.3), provision of access by way of an electronic file (Article
15.3), obligations to employ pseudonymization (Articles 6.4(e), 25.1, 32.1(a),
40.2(d), 89), data portability rights (Article 20), security by design (Article
32), and more generally technical measures (e.g. Article 17.2). At the same
time, the GDPR requires that the controller keeps a proper administration to
demonstrate compliance (Article 30), departing from the old regime (under
the DPD) where controllers had to register their operations with the data pro-
tection supervisor.
Next to these novel obligations, the regulation takes enforcement seriously.
One of the biggest failures of previous regimes of data protection law was
a paramount lack of enforcement, providing no incentive whatsoever to
comply. The enforcement chapter of the GDPR, however, provides for a close-‚Äã
knit network of enforcement activities, by individual persons, non-‚Äãprofit or-
ganizations, and by the supervisors.
Chapter VIII provides the following enforcement mechanisms, under the
heading of ‚ÄòRemedies, liability and penalties‚Äô:

 Articles 77 and 78 provide the data subject with the right to lodge a complaint with
 a supervisory authority, and the right to an effective judicial remedy against legally
 binding decisions of a supervisory authority concerning them, including a remedy
 against the supervisory authority that does not handle their complaint.

 Article 79 provides the data subject with direct access to court, apart from their right
 to lodge a complaint with the supervisory authority. This will enable direct action
 against the controller or processor, for instance an action for injunctive relief, re-
 questing a court order to prohibit unlawful processing.

 Article 80 stipulates that data subjects can mandate their rights from Articles 77‚Äì‚Äã79
 to a not-‚Äãfor-‚Äãprofit body, organization or association, enabling such a body to exer-
 cise these rights on their behalf. If MS law allows, they can also mandate their right to
 sue for compensation (based on Article 80). MS law may also provide that a not-‚Äãfor-‚Äã
 profit can lodge a complaint with the supervisory authority (as in Article 77) or with
 the court (as in Articles 78 and 79).

 Article 81 regulates suspension of proceedings and jurisdictional issues in the case of
 simultaneous or overlapping proceedings in different MSs.

 Article 82 provides a right to compensation in the case that any person (not just
 the data subject) has suffered material or non-‚Äãmaterial damages due to infringe-
 ments of the GDPR. It stipulates that the controller is liable, adding liability for the
160 Privacy and Data Protection


 processor if damage is caused by non-‚Äãcompliance with obligations directed specif-
 ically to the processor (or by its acting outside or contrary to lawful instructions by
 the controller). A controller or processor will be exempted from liability if they prove
 that they are not in any way responsible for the event that caused damage. If more
 than one controller and/‚Äãor processor is liable for damage caused, several liability
 applies (each must pay the full damage, but each can claim back from the others
 any damage paid beyond their own responsibility). The competent courts will be the
 same as those competent under MS law for claims based on Article 79.

 Article 83 stipulates that supervisory authorities shall impose ‚Äòeffective, propor-
 tionate and dissuasive‚Äô administrative fines and details the general and specific
 conditions for such fines. Maximum fines can be ‚Ç¨20,000,000, or in the case of an
 undertaking, up to 4 per cent of the total worldwide annual turnover of the preceding
 financial year, whichever is higher.

 Finally, Article 84 requires that MSs lay down rules for other penalties, in particular for
 infringements not subject for administrative fines of Article 83.



5.6 Privacy and Data Protection Revisited

In this chapter, we have explored human rights law and investigated the ‚Äòwork-
ings‚Äô of the right to privacy in the context of the ECHR, and the right to data
protection in the context of the CFREU, as further protected by the GDPR.
This cannot be more than a first impression of relevant applicable law. Many
relevant provisions and other legislation have not been discussed. The PDPD
has not been discussed, the ePD (and its draft successor) have not been de-
tailed. Convention 108 of the Council of Europe has been ignored,27 and a fur-
ther exploration of the differences between EU and US law has been similarly
left aside.

 What I hope the reader will take home from this chapter is the salient com-
 plexity of privacy and data protection law and the importance of ‚Äòpractical and
 effective‚Äô legal remedies when rights are violated. Though complexity and prac-
 tical effectiveness may sometimes be incompatible, more often they demonstrate
 the adaptive nature of legal protection in the face of an increasingly data-‚Äãdriven
 environment.


 27 The 1981 Convention for the Protection of Individuals with regard to Automatic Processing of Personal

Data (CETS No. 108), https://‚Äãwww.coe.int/‚Äãen/‚Äãweb/‚Äãdata-‚Äãprotection/‚Äãconvention108-‚Äãand-‚Äãprotocol.
                                  5.6 Privacy and Data Protection Revisited 161

In Chapter 10 we will return to the subject of EU data protection law with an
eye to the increasingly code-‚Äãdriven nature of our environment, highlighting
the unique nature of EU data protection rights with regard to automated deci-
sions based on the processing of personal data.


References

On the right to privacy
Council of Europe, Guide on Article 8 of the European Convention on Human Rights.
  Right to respect for private and family life, home and correspondence, updated on
  31 August 2018. https://‚Äãwww.echr.coe.int/‚ÄãDocuments/‚ÄãGuide_‚ÄãArt_‚Äã8_‚ÄãENG.pdf.
Korff, Douwe, 2008. The Standard Approach under Articles 8‚Äì‚Äã11 ECHR and Art. 2 ECHR.
  https://‚Äãw ww.pravo.unizg.hr/‚Äã_‚Äãdownload/‚Äãrepository/‚ÄãKORFF_‚Äã-‚Äã_‚ÄãSTANDARD_‚Äã
  APPROACH_‚ÄãARTS_‚Äã8-‚Äã11_‚ÄãART2.pdf.
Mowbray, Alastair. 2005. ‚ÄòThe Creativity of the European Court of Human Rights‚Äô.
  Human Rights Law Review 5 (1): 57‚Äì‚Äã79. https://‚Äãdoi.org/‚Äã10.1093/‚Äãhrlrev/‚Äãngi003.


On the concept of family resemblance
Biletzki, Anat and Anat Matar. ‚ÄòLudwig Wittgenstein‚Äô. The Stanford Encyclopedia of
  Philosophy (Summer 2018 Edition), Edward N. Zalta (ed.). https://‚Äãplato.stanford.
  edu/‚Äãarchives/‚Äãsum2018/‚Äãentries/‚Äãwittgenstein/‚Äã(the quote in this chapter is taken
  from this entry).


On freedom from and freedom to
Berlin, Isaiah. 1969. ‚ÄòTwo Concepts of Liberty‚Äô. In Four Essays on Liberty, edited by
  Isaiah Berlin, 118‚Äì‚Äã73. Oxford and New York: Oxford University Press.


On data protection law
European Union Agency for Fundamental Rights (FRA). 2018. ‚ÄòHandbook on
  European Data Protection Law‚Äî‚Äã2018 Edition‚Äô. http://‚Äãfra.europa.eu/‚Äãen/‚Äãpublica-
  tion/‚Äã2018/‚Äãhandbook-‚Äãeuropean-‚Äãdata-‚Äãprotection-‚Äãlaw.
Journal. International Data Privacy Law. https://‚Äãglobal.oup.com/‚Äãacademic/‚Äãproduct/‚Äã
  international-‚Äãdata-‚Äãprivacy-‚Äãlaw-‚Äã20444001.
Kuner, Christopher. 2007. European Data Protection Law: Corporate Regulation and
  Compliance. 2nd ed. New York: Oxford University Press (see updates per chapter
  at: http://‚Äãglobal.oup.com/‚Äãbooksites/‚Äãcontent/‚Äã9780199283859/‚Äãupdates/‚Äã).
162 Privacy and Data Protection

On privacy as control over personal information:
Westin, Alan. 1967. Privacy and Freedom. New York: Atheneum. (the quotation is
 taken from p. 7).


On privacy as freedom from unreasonable constraints
on identity construction
Agre, Philip E., and Marc Rotenberg. 2001. Technology and Privacy: The New
  Landscape. Cambridge, MA: MIT (quotation taken from p. 7).


On the difference between privacy and data protection
Kokott, Juliane, and Christoph Sobotta. 2013. ‚ÄòThe Distinction between Privacy and
  Data Protection in the Jurisprudence of the CJEU and the ECtHR‚Äô. International
  Data Privacy Law 4 (3), 222‚Äì‚Äã28. http://‚Äãidpl.oxfordjournals.org/‚Äãcontent/‚Äã3/‚Äã4/‚Äã222.
  full?sid=a0d12330-‚Äãd8f3-‚Äã4387-‚Äãa7dc-‚Äã58905c9379a2.


On data protection by design and on legal protection by design
Hildebrandt, Mireille. 2017. ‚ÄòSaved by Design? The Case of Legal Protection by
  Design‚Äô. NanoEthics, August, 1‚Äì‚Äã5. https://‚Äãdoi.org/‚Äã10.1007/‚Äãs11569-‚Äã017-‚Äã0299-‚Äã0.
Hildebrandt, Mireille, and Laura Tielemans. 2013. ‚ÄòData Protection by Design and
  Technology Neutral Law‚Äô. Computer Law & Security Review 29 (5): 509‚Äì‚Äã521.
       privacy in the clouds
                 A White Paper on
         Privacy and Digital Identity:
         Implications for the Internet


              Ann cavoukian, ph.D.
information and privacy commissioner of ontario
       privacy in the clouds
                 A White Paper on
         Privacy and Digital Identity:
         Implications for the Internet


              Ann cavoukian, ph.D.
information and privacy commissioner of ontario
                  Table of Contents
Introduction                                                                                     2
The 21st Century Privacy Challenge                                                               5
Evolution of Consumer Computing                                                                  6
The Power and Promise of Cloud Computing                                                         7
Identity Service Requirements in the Cloud                                                       8
The Digital Identity Situation Today                                                             9
The Digital Identity Needs of Tomorrow                                                          10
Case Studies                                                                                    12
1   The ‚ÄúLive Web‚Äù                                                                              12
2   Online Dating                                                                               13
3   Cell Phone Payments and Location-Dependent Services                                         14
4   Health Care Records                                                                         15
5   Identity and Trust in Virtual Worlds                                                        16
Creating a User-Centric Identity Management Infrastructure                                      17
Open Standards and Community-Driven Interoperability                                            19
Protecting Privacy                                                                              20
Diversity for a Lively Ecosystem                                                                21
Diversity for User Devices                                                                      22
Collaboration of Users                                                                          22
Technology Building Blocks                                                                      23
‚Ä¢ Open source and proprietary identity software based on
  open standards                                                                                23
‚Ä¢ Federated identity                                                                            23
‚Ä¢ Multiple and partial identities                                                               23
‚Ä¢ Data-centered policies                                                                        24
‚Ä¢ Audit tools                                                                                   24
A Call to Action                                                                                25
1   Trust the data to behave                                                                    26
2   Trust the personal device to interface and act on our behalf                                27
3   Trust the intelligent software agents to behave                                             27
4   Trust intermediary identity providers to behave                                             27

The IPC would like to acknowledge and sincerely thank IBM Research Systems & Software
experts, in particular, Dr. Jan Camenisch, Anthony Nadalin, Michael R. Nelson and Dr. Michael
Waidner, for their contributions.
                              Introduction




Informational self-determination refers to the ability of individuals
to exercise personal control over the collection, use and disclosure
of their personal information by others. It forms the basis of
modern privacy laws and practices around the world.

All organizations that collect and use personal data must accommodate the
legitimate interests of individuals. Organizations can do this, for example, by
being open and accountable about their information management practices, by
seeking informed consent, and by providing individuals with access and redress
mechanisms. At stake is not only privacy, but the confidence and trust of mil-
lions of individuals, consumers, and citizens in today‚Äôs information society.

At the Office of the Information and Privacy Commissioner of Ontario (IPC),
we have long advocated a strong role for individuals in managing their per-
sonal information, not just by exercising their privacy rights under Ontario
law, but also by becoming better informed and using privacy-enhancing tech-
nologies (PETs). PETs can minimize the disclosure and (mis)use of personally-
identifiable information (PII), and help secure data from unauthorized use by
others.



                                        2
Informational self-determination has become a challenging concept to pro-
mote and protect in a world of unlimited information passing from individu-
als to organizations, and from organizations to each other, often described as
‚ÄòWeb 2.0‚Äô. As a result of widespread developments in information and commu-
nications technologies (ICTs), we are collectively creating, storing and com-
municating information at nearly exponential rates of growth.A large majority
of this data is personally identifiable, and much of it is under the control of
third parties. Practical obscurity ‚Äì the basis for privacy norms throughout
history ‚Äì is fast disappearing.

Our digital footprints and shadows are being gathered together, bit by bit,
megabyte by megabyte, terabyte by terabyte, into personas and profiles and
avatars ‚Äì virtual representations of us, in a hundred thousand simultaneous
locations. These are used to provide us with extraordinary new services, new
conveniences, new efficiencies, and benefits undreamt of by our parents and
grandparents. At the same time, novel risks and threats are emerging from this
digital cornucopia. Identity fraud and theft are the diseases of the Information
Age, along with new forms of discrimination and social engineering made
possible by the surfeit of data.

Personal information, be it biographical, biological, genealogical, historical,
transactional, locational, relational, computational, vocational or reputational,
is the stuff that makes up our modern identity. It must be managed responsibly.
When it is not, accountability is undermined and confidence in our evolving
information society is eroded.

It may very well be that our fundamental ideas about identity and privacy, the
strategies that we have collectively pursued, and the technologies that we
have adopted, must change and adapt in a rapidly evolving world of connectiv-
ity, networking, participation, sharing, and collaboration.



                                        3
What will privacy mean, and how will privacy survive and hopefully thrive,
as a viable human right, operational value, and critical enabling trust factor in
a world where the individual is less and less directly present in the midst of
data-rich transactions?

How will individuals exercise control over their personal data when that data
is stored and processed in the Cloud1 ‚Äì that is, everywhere except on their
own personal computing devices?

Profound and dramatic transformations and upheavals are on the way. How
will privacy fare?




1
    In telecommunications, a ‚Äúcloud‚Äù is the unpredictable part of any network through which data passes between
two end points. For the purposes of this paper, the term is used to refer generally to any computer, network or
system through which personal information is transmitted, processed and stored, and over which individuals
have little direct knowledge, involvement, or control.




                                                         4
                                          The 21st Century
                                          Privacy Challenge



The Internet has entered into a new phase. Thanks to more reliable, afford-
able, and ubiquitous broadband access, the Internet is no longer just a communi-
cations network. It is becoming a platform for computing ‚Äì a vast, interconnected,
virtual supercomputer. Many different terms have been used to describe this
trend: Web 2.0, Software as a Service (SaaS), Web Services, ‚Äúcloud computing,‚Äù
and the Grid. Each of these terms describes part of a fundamental shift in how
data are managed and processed. Rather than running software on a desktop
computer or server, Internet users are now able to use the ‚Äúcloud‚Äù ‚Äì a networked
collection of servers, storage systems, and devices ‚Äì to combine software, data,
and computing power scattered in multiple locations across the network.

The importance of this shift cannot be overstated. To quote Nicholas G. Carr, it
‚Äúwill overturn strategic and operating assumptions, alter industrial economics,
upset markets and pose daunting challenges to every user and vendor. The his-
tory of the commercial application of information technology has been char-
acterized by astounding leaps, but nothing that has come before ‚Äì not even
the introduction of the personal computer or the opening of the Internet
‚Äì will match the upheaval that lies just over the horizon.‚Äù2

2
    Nicholas G. Carr, The End of Corporate Computing, MIT Sloan Management Review, Spring 2005, pp. 67-73


                                                       5
The new digital ecosystem will also present complex security and privacy
challenges. Fundamentally, it will need to provide flexible, user-friendly ways
to authenticate users.Without better management of digital identities, we will
not only continue to struggle with existing problems such as identity theft,
spam, malware, and cyber-fraud, we will be unable to assure individual users
that they can safely migrate their critical data and applications from their own
computers onto the Web. The opportunity presented by technological devel-
opment will be lost.

Evo lution of Con sumer Compu t i ng
From a user‚Äôs perspective, the evolution of consumer computing can be
divided into three phases:

        1    The stand-alone personal computer in which the user‚Äôs oper-
             ating system, word processing system, database software and data
             are stored on a single, easily protected machine. Examples: word
             processing, spreadsheets on a stand-alone server.

        2    The Web in which most of the software a user needs is still on
             their own PC, but more and more of the data they need is found on
             the Internet. Example: using a Web browser to read a Web page.

        3    The ‚ÄúCloud‚Äù3 in which users rely heavily on data and software that
             reside on the Internet. Examples: using Amazon‚Äôs Simple Storage
             Service (S3) and Elastic Computing Cloud (EC2) to store unlimited
             photos on Smugmug, an online photo service; using Google Apps
             for Word-processing; virtual worlds such as Second Life that enable
             users to build 3-D environments combining Web pages and Web
             applications (e.g. feeding a Webcast into a virtual theatre); grid
             computing.

3
    See ‚ÄúThe Information Factories‚Äù by George Gilder, Wired magazine, October, 2006,
    http://www.wired.com/wired/archive/14.10/cloudware_pr.html

                                                         6
The Powe r an d P romise of C lo u d C o mpu t i ng
Most of the work we do with computers is still conducted using phase 1 or 2
tools, but more and more people ‚Äì especially younger generations ‚Äì are starting
to take advantage of the power of the Cloud. The Cloud offers them so much:

   1   Limitless flexibility: With access to millions of different pieces
       of software and databases, and the ability to combine them into cus-
       tomized services, users are better able to find the answers they need,
       share their ideas, and enjoy online games, video, and virtual worlds;

   2   Better reliability and security: Users no longer have to worry
       about their hard drives crashing or their laptops being stolen;

   3   Enhanced collaboration: By enabling online sharing of informa-
       tion and applications, the Cloud offers users new ways of working
       and playing together;

   4   Portability: Users can access their data and tools wherever they
       can connect to the Internet;

   5   Simpler devices: With data and the software being stored in the
       Cloud, users don‚Äôt need a powerful computer. They can interface
       using a cell phone, a PDA, a personal video recorder, an online
       game console, their cars, or even sensors built into their clothing.

We can only enjoy the full benefits of Cloud computing if we can address the
very real privacy and security concerns that come along with storing sensitive
personal information in databases and software scattered around the Internet.

Digital identity is a fundamental challenge. In phase 1 of consumer computing,
users‚Äô privacy and security was largely assured by restricting physical access
to the stand-alone computing devices and storage media. Identity needs were
fairly minimal, consisting largely of a small handful of usernames and pass-
words for local systems and file access.

                                       7
In phase 2 of consumer computing, users usually have to establish their iden-
tity each time they use a new Internet-based application, usually by filling out
an online form and providing sensitive personal information (e.g., name, home
address, credit card number, phone number, etc.). This leaves a trail of personal
information that, if not properly protected, may be exploited and abused.

Identity Servic e Requir eme nts i n t he C lo u d
Cloud computing and the exciting tools it makes possible (like virtual worlds,
grid computing, and shared archives), require identity services that:

    1   are independent of devices;

    2   enable a single sign-on to thousands of different online services;

    3   allow pseudonyms and multiple discrete (but valid) identities to
        protect user privacy;

    4   are interoperable, based on open standards, and available in open
        source software (in order to maximize user choice);

    5   enable federated identity management; and

    6   are transparent and auditable.

This paper explores what will be possible if proper digital identity services
are deployed and the full power of Cloud computing is realized. A number of
scenarios are described:

    1   an identity service that enables individuals to easily manage their
        own online identities and to effortlessly participate in online col-
        laboration activities without repeated sign-ons;

    2   an identity tool that gives users of an online dating service better
        privacy than is available from today‚Äôs sites;

    3   a payment system using cell phones or RFID chips that has privacy
        built in;

                                         8
    4   an infrastructure for electronic health records; and

    5   an identity service for virtual worlds such as Second Life.

The Digital Id en t it y Sit uat i o n To day
Almost all online activities, such as sending emails, filing tax declarations, man-
aging bank accounts, buying goods, playing games, connecting to a company
intranet, and meeting people in a virtual world, require identity information to
be given from one party to another. Today, most users have to establish their
identity each time they use a new application, usually by filling out an online
form and providing sensitive personal information (e.g., name, address, credit
card number, phone number, etc.).

A typical Internet user in Canada has provided some type of personal infor-
mation to dozens of different websites. If you count cookies and IP addresses
as personal information, then Internet users have left behind personally iden-
tifiable information everywhere they‚Äôve been. They have left ‚Äúdigital bread
crumbs‚Äù throughout cyberspace ‚Äì and they have little idea how that data
might be used or how well it is protected.




                                        9
The Digital Id en t it y Need s o f To mo r row
What is needed is flexible and user-centric identity management. Flexible
because it needs to support the multitude of identity mechanisms and
protocols that exist and are still emerging, and the different types of platforms,
applications and service-oriented architectural patterns in use; user-centric
because end users are at the core of identity management. Users must be
empowered to execute effective controls over their personal information.

In the future, users will not have to re-enter personal data each time that they
go to a new website. Instead, by using an identity service (or two or more
different ones), they will have control over who has their personal data and
how it is used ‚Äì minimizing the risk of identity theft and fraud. Their identity
and reputation will be transferable. If they establish a good reputation, for
example, at an auction site, they will be able to use that fact on other sites as
well. One result of this would be greater choice of online services, since users
would not be locked into one service or vendor.




                                       10
A truly flexible identity management system would not be limited to laptop and
desktop computers; it would work on cell phones, personal digital assistants,
smart cards, sensors, consumer electronics like video recorders and online
game consoles ‚Äì any way that a user might touch the Internet. This approach
to digital identity will unleash the full potential of the Cloud, enabling users to
seamlessly tap into and combine a wide range of online services.




                                        11
                             Case studies




1	The ‚ÄúL ive W eb‚Äù
The Internet has become a vastly more connected and interactive place for
millions of people to spend their time. By any measurement index ‚Äì growth
in ‚Äòblogs, collaborative wikis, mash-ups, and online social networks ‚Äì the phe-
nomenon of the ‚Äúparticipatory Web‚Äù is transforming our lives with virtually
limitless opportunities to become engaged, customize experiences, and find
our own individual public voices.

This proliferation of online activity requires sound identity management.With
the increased use of the Internet to conduct business and the rise of new
types of online interactions, such as social networking and user-generated
content, innovative kinds of digital identifier technologies are necessary to
sustain the ‚Äúopen Web.‚Äù Online users need to securely manage their multiple
accounts and passwords across multiple domains, without fear of surveillance
and profiling.

In order to facilitate this, OpenID, developed by an open community, is free
‚Äúuser-centric‚Äù digital identity technology that simplifies the online user expe-
rience by reducing the complexity of managing dozens, even hundreds of



                                       12
user names and passwords across Internet sites, and providing greater control
over the personal information users are required to share with websites when
they sign in.

OpenID enables individuals to convert one of their already existing digital
identifiers ‚Äì such as their personal blog‚Äôs URL ‚Äì into an OpenID account,
which can then be used as a log-in at any website supporting OpenID.

Today, more than 10,000 websites support OpenID log-ins, and an estimated
350 million OpenID-enabled URLs currently exist.

For online businesses, these efforts can lower password and account manage-
ment costs, help reduce the overall risks of security breaches by limiting the
amount of customer personal information they need to store and protect, and
increase both new and return user traffic by lowering the barriers to website
entry and re-entry.

2	Online Dating
An online dating service matches people together based on their personal
interests and preferences using some sophisticated matching algorithms. The
matching algorithm needs a good deal of personal data in order to work, and
therefore users of those dating services need strong assurances that their
information will be treated with respect and used only for the intended and
agreed-upon purposes.

Even if a user agrees to receive marketing emails from third parties, for exam-
ple, they may nonetheless want to be certain that their personal details will
not be given to those third parties. For instance, someone who is overweight
may not wish to receive marketing e-mails from makers of ‚Äúfull-size‚Äù clothing.

To protect privacy, dating services could allow their clients to use pseudonyms
rather than their real names. The dating service has no business need to know


                                      13
the real identities of their customers, other than their need to get paid, which
could be done through a pre-paid or cash-like service.

Today, customers of dating services can claim almost any attribute, and noth-
ing prevents ‚Äúdevils‚Äù from impersonating ‚Äúangels.‚Äù With better digital identity
management, the dating service would be able to accept third-party certified
attributes, without customers running the risk that the certificates would
reveal their real names to the service. For instance, a certified date of birth
might give a higher rank in the matchmaking algorithms than an uncertified
one. A certificate that a customer is not listed in a certain blacklist might be
mandatory for certain dating services. Such an approach would reduce the
risk of misrepresentation and increase the level of trust, without impacting
on privacy.

When customers finally get introduced to each other, they could potentially
use the identity management mechanisms to establish increasing trust in each
other in a multi-round ‚Äúgame,‚Äù checking each others‚Äô attributes in the safety
and privacy of their homes. They could even ask each other questions like ‚Äúare
you younger than me?‚Äù and so on, without having to reveal their actual birth-
days, but rather, just a birth year or range.

3	Cell Phone Payments an d
	Location- D ep endent S e rv i c e s
One very promising development in the cell phone industry is the deploy-
ment of cell phones as ‚Äúdigital wallets‚Äù that can be used to transfer and store
money, pay parking meters and vending machines, and eventually act as a kind
of a credit card. Privacy concerns, however, are a major barrier to the adoption
of this technology.

Many consumers are already uncomfortable knowing that credit card compa-
nies can compile a detailed record of their spending behavior. With electronic


                                         14
wallets, it is conceivable that your cellular phone provider would not only
know when, where, and how you were spending your money, but by tracking
others‚Äô electronic wallets, they could know who you were with when you
spent it (at a restaurant or a hotel, for instance).

User-centric identity management would allow the users of an ‚Äúelectronic
wallet‚Äù to use a digital identity service to authenticate themselves, without
revealing their actual identity to either vendors or network providers.

4 	He alth C are Records
Some of the most sensitive personal information about us is associated with
the medical services and medications we use. Yet today, that personal infor-
mation is scattered in dozens of different locations including doctors‚Äô offices,
pharmacies, insurance companies, and our places of employment.

One of the biggest barriers to the widespread adoption of electronic health
records has been the concern of patients that their data in such records will
be misused or stolen. We have already seen too many examples of sensitive
medical or drug data being used for inappropriate or unauthorized purposes.

User-centric identity management could ensure that someone‚Äôs real name
(and the personal data that could be used to infer who they are) would be
protected and kept separate from the details of their medical records, insur-
ance claims, and drug prescriptions. It would also enable a patient to use an
online portal with a federated identity system to quickly and safely access all
their medical information, whether it be stored at their doctor‚Äôs office, their
pharmacy, or their insurance company. Perhaps most importantly, there would
be the ability to audit these records and determine where personal data is
stored, how it is protected, and who has had access to it.




                                         15
5	Ident it y an d Tr ust in Vi rt ua l Wo r l ds
Over the last year, there have been a number of press reports on virtual
worlds such as Second Life and There.com, and online games such as World
of Warcraft. Millions of people are spending hours a week in these immersive,
three-dimensional online environments, finding new ways to collaborate, play
games, and share information. Virtual economies are also developing as inhab-
itants of these virtual worlds buy and sell virtual goods and services, exchang-
ing millions of real dollars every year.

Unfortunately, there are currently no effective means for managing identity
and security in most virtual worlds. As a result, it is difficult to prevent disrup-
tive behavior or inappropriate postings by anonymous users who may appear
and quickly disappear. This lack of security and trust is slowing the develop-
ment of serious business applications in virtual worlds.

User-centric identity management could provide an effective way to build
trusted communities in the virtual world. For instance, parents could rest
assured that when their children went online to play in a virtual world for
kids, every other person there had been properly authenticated and was really
a ‚Äúchild.‚Äù

One of the most exciting reasons for the phenomenal growth of virtual worlds
like Second Life is that they allow users to create new services and to ‚Äúplug in‚Äù
applications from elsewhere on the Web. With user-centric identity manage-
ment, you could establish your identity once and then be able to use the full
range of services in a virtual world. And an identity established in Second Life
could then be transferable into another virtual world. But you would not have
to share your personal information in any other ‚Äúworld‚Äù unless you chose to.




                                           16
                            Creating a User-
                            Centric Identity
                            Management
                            Infrastructure

The goal of a flexible, user-centric identity management infrastruc-
ture must be to allow the user to quickly determine what information
will be revealed to which parties and for what purposes, how trustwor-
thy those parties are and how they will handle the information, and
what the consequences of sharing their information will be. In other
words, these tools should enable users to give informed consent. The default
should be minimal disclosure, for a defined purpose. Any secondary or addi-
tional use should be optional after enrolment.

Companies need to understand that identity management is not only a busi-
ness process, but also a user activity. Users must be given adequate tools to
manage the personal information on all of their devices. This means that the
identity infrastructure must account for many devices, from desktop PCs to
mobile phones. The infrastructure must allow for a unified user experience
over all devices.

It also means that the system must be driven throughout by a clear framework
of agreed-upon rules. This includes policies describing to users what informa-
tion is requested and why (similar to a machine-readable and improved version



                                      17
of today‚Äôs privacy policies). It must also include a ‚Äústicky‚Äù policy that travels
with the information throughout its lifetime and ensures that it is only used in
accordance with the policy. The last step will of course require mechanisms
to enforce these sticky policies in ways that can be verified and audited.

There are already a number of identity management systems in place on a
wide variety of platforms. These need to be supported, at least in the short
term, by the identity management infrastructure.The infrastructure must sup-
port cross-system interaction as well as interoperation and delegation between
them. This is only possible if the infrastructure and the individual systems are
based on open standards, available on all platforms. For a successful user-cen-
tric identity management infrastructure to emerge, it is crucial that its devel-
opment be driven by a wide and open community, spanning over the different
geographies and cultures, and that open source implementations of all of its
infrastructural components be made available.

Identity information is almost always personally identifiable information,
which is governed by special privacy regulations in many parts of the world.
Further, an improper use of identity information may lead to identity theft and
other breaches of security. Thus, identity information requires special protec-
tion. This includes, among other things, the ability to carry and enforce sticky
policies, encrypt data, and minimize the amount of identity information used
by various applications. Actual identity management systems will support a
wide variety of privacy and various security properties, ranging from low-
security password-based one-factor authentication to high-end, attribute-based
systems deploying state-of-the-art privacy-enhancing certificates (for example,
IBM‚Äôs Identity Mixer technology, or Microsoft‚Äôs U-Prove technology.). While
the infrastructure needs to support all of these systems, users should under-
stand the implications of using one system over the other.



                                       18
At the end of the day, applications need to be able to make use of the infra-
structure. This requires that applications be presented with a unified view and
interfaced to this infrastructure across different platforms and devices. These
interfaces should be independent of the actual protocols and mechanisms
that are used to convey the identity information underneath. Therefore, we
are proposing a single architecture that pulls the different pieces together and
unifies them.

By supporting a plethora of identity systems, this architecture will allow for the
migration of applications from legacy systems to the user-centric ones that will
emerge and prevail. To enable such migration, as well as building applications
from scratch, adequate tools and sample applications will need to be provided.

Open Stan dar d s an d Comm u ni t y- D r i v e n
Intero per abilit y
The Internet was founded on open standards and collaboration. Open stan-
dards facilitate a reliable base for customers, applications, and enterprises. As
such, they form an important foundation for the growth of the future Web and
nurture the development of an open identity management ecosystem for the
whole industry.

To enable the federation and interoperability of the different existing and
emerging identity management systems, the underlying standards and speci-
fications need to be complete, freely accessible, and, most important, driven
by the community. To have user-centric identity management widely adopted,
the standards and tools provided need to be free from IP infringements. This
will allow for a supporting ecosystem to grow and be maintained, not only by
multinational companies but also by open-source initiatives and start-ups. So it
is essential that standards be published widely and on a timely basis, and that
they be stable and enduring.



                                        19
Open standards are required to support the plethora of environments and
application scenarios in which identity management plays a critical role and to
enable inter-operation of these environments. In particular, communication for-
mats and policy specifications act as medium for the interconnection of client
and server-sides. This medium can only form the basis for a lively and value-
generating ecosystem if it is based on the principles of truly open standards.

The standards ‚Äì rather than the particular implementations by single vendors
or consortia ‚Äì must form the basis of regular interoperability tests. Moreover,
they need to be controlled by an impartial, credible standards organization
that governs the freely available open standards for the benefit of the entire
community.

Protecting Privacy
The Internet was designed to connect and authenticate devices with logical
and physical address spaces. User-centric identity services can provide the
same ubiquitous connectivity for individuals. An identity today is no longer
a single number assigned to an individual but rather comprises a set of attri-
butes including address, birthdate, degrees held, and personal preferences.
Such personal information requires special protection, not only to prevent
fraud and identity theft, but also to comply with privacy laws.

Most existing laws have their roots in the Organisation for Economic Co-opera-
tion and Development‚Äôs (OECD) privacy guidelines. These stipulate, for exam-
ple, that only the personal information needed for a stated purpose should be
collected, that the collection should be openly communicated, that the user
must give informed consent to the collection and use, and that the personal
information must be properly safeguarded.4


4
    In 2006 the IPC led an international group of privacy and data protection commissioners to develop a set of fair
information practices that harmonized the various privacy codes and practices currently in use around the world.
The result ‚Äì the Global Privacy Standard ‚Äì can be found at: www.ipc.on.ca/images/Resources/up-gps.pdf

                                                         20
Identity management systems can support compliance with privacy laws
through the use of privacy policies, enforcement mechanisms, and technolo-
gies that allow applications to use only the amount of personal information
that is strictly necessary to the application. Policies that outline what informa-
tion is being sought and the reasons why enable users to give informed con-
sent. These policies will also govern access controls, and should travel with
the data for the course of their lifetime.

Already there exist privacy-enhancing technologies that allow a user to give an
authentication token containing only an encrypted form of the user‚Äôs identity
to a service provider. This allows the user to appear anonymously to the service
provider while still making it possible to reveal true identity in the event of
an investigation by a designated authority. Strong restrictions and conditions
would be placed on an authority‚Äôs ability to revoke a user‚Äôs anonymity.

Divers it y for a Lively Ec osyst e m
There is currently a great deal of diversity in identity management systems,
along with a multitude of open standards that support identity federation and
user-centricity for these systems. The most prominent examples are probably
SAML, OpenID, and the WS-Federation specifications. Each of these has pros
and cons, and contributes in different ways to the emerging ecosystem.

While these efforts will likely converge over time, the present diversity may be
inspiring and potentially drive positive new developments in identity manage-
ment. New models and protocols are being developed and deployed. Further
methods will evolve, and there will be niches and application scenarios in which
some specific solutions will surpass mainstream standards and protocols.

Investments have already been made in deploying system-based identity
management products like Liberty Alliance or WS-Federation. The emerging



                                        21
ecosystem needs to support the existing diversity while allowing new solu-
tions and concepts to be applied, as other solutions fade out gracefully.

Divers it y for User Devic es
While user-centricity is mostly discussed with PCs in mind, users will want
to use a number of other devices such as mobile phones or electronic iden-
tity cards to take part in the information society. They may even wish to use
devices that they do not personally own.

This diversity of clients requires that the identity management system be flex-
ible, offering users a maximum number of choices as well as the best security
and privacy protection possible.

Co ll a bor at ion of User s
The boundaries of corporations are becoming less defined, with virtual com-
panies emerging. Further, user contributions and collaboration are becoming
increasingly central to many emerging applications. These scenarios have in
common the need to deal with users who have not been physically identified
but are judged by their reputation or other attributes (such as area of exper-
tise, education, age, etc.), as attested to by third parties.

The emerging identity information infrastructure must support such a
collaborative environment ‚Äì allowing for decentralized and federated trust
models based on limited identity information (e.g., the current user is a
medical expert).




                                         22
                              Technology
                              Building Blocks



These different scenarios will require a number of different technology
building blocks, including:

   ‚Ä¢   Open source and proprietary identity software based
       on open standards which can be easily incorporated into
       the full range of online services and devices (similar to the
       open source software that is at the core of the Internet and the
       Web today).

   ‚Ä¢   Federated identity so that once users have authenticated
       themselves with one service or institution, their identity
       credentials will be recognized elsewhere. Brokering of
       security and authentication will eliminate the need to use a
       different stand-alone log-on process for each application or
       online service.

   ‚Ä¢   Multiple and partial identities so that users can access
       online services, explore virtual worlds, and collaborate with
       others without necessarily revealing their name and true
       identity to everyone. Different pseudonyms should support
       differing ranges of identification and authentication strengths.

                                     23
‚Ä¢   Data-centered policies that are generated when a user
    provides personal or sensitive information, that travels with
    that information throughout its lifetime to ensure that the
    information is used only in accordance with the policy, e.g.,
    for the purposes for which it was intended which the user had
    consented to.

‚Ä¢   Audit tools so that users can easily determine how their data
    is stored, protected, and used, and determine if the policies
    have been properly enforced.




                                   24
                            A Call to Action




It will not be possible to realize the full potential of the next genera-
tion of the Internet and Cloud Computing without developing better
ways of establishing digital identity and protecting privacy.

Fortunately, progress is being made in developing and deploying the
technological tools needed. But barriers remain. Different segments of the
‚ÄúIT ecosystem‚Äù can take steps to overcome them:

   ‚Ä¢   Corporate and individual users can explore the evolving identity
       systems and demand that they have privacy protection built in, as
       well as implementing open standards so that different systems will
       be truly interoperable;

   ‚Ä¢   Standards bodies can continue to develop and promote the funda-
       mental standards needed for identity systems, data-centered poli-
       cies, and privacy-enhancing technologies;

   ‚Ä¢   Software vendors and website developers can embrace privacy-
       enhancing technologies, open standards, open identity manage-
       ment systems, and true interoperability;



                                     25
    ‚Ä¢   Governments, through their procurement decisions, can support
        the development of open identity management systems that are
        designed to meet user needs for privacy, interoperability, and flex-
        ibility.

The brave new world of Cloud Computing offers many benefits provided that
the privacy and security risks are recognized and effectively minimized.

User-centric private identity management in the Cloud is possible, even when
users are no longer in direct possession of their personal data, or no longer in
direct contact with the organization(s) that do possess it.

This paper has outlined some technical building blocks and challenges that
will become essential elements of a privacy-friendly Web 2.0 world. To be sure,
laws, standards, education, awareness, and market forces will also be needed
to support this vision.

Widespread and enduring user trust depends on realizing this vision. But how
can we collectively assure confidence and trust in the privacy of our person-
ally identifiable information, when our identity data is held by others and we
are not directly involved in data transactions in Cloud?

Four fundamental technological approaches present themselves:

1	Trust the data to behave:
New privacy-enhancing information technologies make it possible to attach
individual privacy rights, conditions and preferences directly to their own
identity data, similar to digital rights management technologies for intellectual
property.




                                       26
2	Trust the personal device to interface and act on our behalf:
The many technologies that travel with us are growing in storage, comput-
ing, and communications sophistication. Cell phones, PDAs,‚Äúsmart‚Äù cards and
other tokens under our physical control are becoming our de facto digital
wallets, interacting with the ‚Äúgrid‚Äù and serving as brokers and proxies for our
identity-based transactions in the digital worlds. These devices need to be
trustworthy, fully user-configurable, user-transparent and easy to use.

3	Trust the intelligent software agents to behave:
Whether operating on our ‚Äúalways-on‚Äù internet devices, or housed somewhere
in the Cloud, intelligent software agents can automatically and continuously
scan, negotiate, do our bidding, reveal identity information, and act on our
behalf in a Web 2.0 world. Some examples may include delegated identity
tools,‚Äúreachability‚Äù software, and ‚Äúprivacy bots.‚Äù

4	Trust intermediary identity providers to behave:
Inevitably, we must also have sufficient trust in those organizations that would
supply and accept our identity credentials and our personally identifiable
information. In a federated identity world, these trusted actors will increas-
ingly act on our behalf, disclosing our identity data for the purposes we define
in advance, and under specific conditions. They must find credible techno-
logical mechanisms for assuring us that they are behaving in an open and
accountable manner, and that our privacy is in fact being protected. Possible
technologies might include automated audit and enforcement tools that can
also convey up-to-the-minute privacy and security status reports to users,
regulators and other trusted third parties.




                                       27
The Office of the Information and Privacy Commissioner of Ontario remains
committed to seeking privacy-enhanced technology solutions to the growing
digital identity needs of today and tomorrow.

To this end, we hope to encourage greater understanding, participation and
dialogue among all stakeholders in the identity world of the essential privacy
issues at play, and of the solutions possible.

We call upon all stakeholders and technology developers, in particular, to
develop trusted mechanisms for assuring widespread and enduring user
confidence in the privacy and security of their identity data in the Web 2.0
world of the future.

Let the dialogue begin!




                                        28
information and privacy commissioner of ontario
             2 Bloor Street East, Suite 1400
          Toronto, Ontario M4W 1A8 Canada
                   Tel: 416 326 3333
                   Fax: 416 325 9195
                    1 800 387 0073
                  TTY: 416 325 7539
                    www.ipc.on.ca
Rescinded [2024-10-09] - Directive on Privacy Impact Assessment- Canada.ca


 This page has been archived on the Web.                                                                               Fran√ßais   √ó
                                                                                                                                  √ó
                                                                                              Search Canada.ca             ÓÄÉ


            MENU          ÓÑî



           Canada.ca ÓÇÄ How government works ÓÇÄ Policies, directives, sandards and guidelines




           Rescinded [2024-10-09] - Directive on Privacy Impact
           Assessment
           Provides direction to government insitutions on how to assess the privacy impacts of new or subsantially
           modifed programs or activities involving the creation, collection and handling of personal information.
                                                                                                    Date modifed: 2020-06-18

           ÓÑá        This page has been archived on the Web
                    Information identifed as archived is provided for reference, research or recordkeeping purposes. It is
                    not subject to the Government of Canada Web Standards and has not been altered or updated since
                    it was archived. Please contact us to reques a format other than those available.



              Expand all          Collapse all                                                   More information         ÓÇÜ

                                                                                                 Hierarchy                ÔÉ®
                  1. Efective date
                                                                                                 Archives                 ÔÜá
                  2. Application
                                                                                              ÓÅÖ Print-friendly   XML

                  3. Context
                  4. Defnitions
                  5. Directive satement
                  6. Requirements
                  7. Consequences
               8. Roles and responsibilities of
              government organizations
                  9. References
https://www.tbs-sct.canada.ca/pol/doc-eng.aspx?id=18308#appC[11/6/2024 12:56:32 PM]
Rescinded [2024-10-09] - Directive on Privacy Impact Assessment- Canada.ca




                  10. Enquiries

                  Appendix A - Defnitions
               Appendix B - Privacy Impact
              Assessment requirements related
              to the preparation of Treasury
              Board submissions
               Appendix C ‚Äì Core privacy
              impact assessment
                The following sections and their information requirements make
                up the minimum content of the core PIA. In the case of a multi-
                insitutional PIA, each government insitution involved will be
                responsible for contributing to or completing the core PIA in a
                manner that is consisent with the approach outlined by the lead
                government insitution.


                Section I ‚Äì Overview and PIA Initiation
                      a. The government insitution or, in the case of a multi-
                            insitutional PIA, the lead government insitution.
                      b. The head of the government insitution or delegate for
                            section 10 of the Privacy Act or, in the case of a multi-
                            insitutional PIA, the head or delegate of each
                            government insitution involved in the program or activity.
                      c. The appropriate senior ofcial or executive for the new or
                            subsantially modifed program or activity.
                      d. Name and description of the program or activity of the
                            government insitution or, in the case of a multi-
                            insitutional PIA, of the lead government insitution.
                      e. Legal authority for the program or activity or, in the case
                            of a multi-insitutional PIA, the legal authority for each
                            government insitution involved in the program or activity.
                       f. Identifcation of whether the proposal is related to a new
                            PIB or will subsantially modify an exising PIB. Exising



https://www.tbs-sct.canada.ca/pol/doc-eng.aspx?id=18308#appC[11/6/2024 12:56:32 PM]
Rescinded [2024-10-09] - Directive on Privacy Impact Assessment- Canada.ca


                            PIBs are to be identifed by their title, regisration number
                            and bank number.
                      g. Short description of the project, initiative or change.
                      h. In the case of a multi-insitutional PIA, the lead
                            government insitution will describe the approach for the
                            completion and approval of the PIA in support of the
                            program or activity. At a minimum, a multi-insitutional
                            PIA will identify the government insitutions involved and
                            ensure that the role of each insitution with respect to the
                            program or activity is adequately documented, unless
                            otherwise determined by the approach.


                Section II ‚Äî Risk Area Identifcation and
                Categorization
                The core PIA mus include a completed risk identifcation and
                categorization section as outlined below. To have consisent
                risk categories and risk measurement across government
                insitutions, sandardized risk areas (itemized below) and a
                common risk scale are to be maintained as the basis for risk
                analysis.

                The numbered risk scale is presented in an ascending order:
                the frs level (1) represents the lowes level of potential risk for
                the risk area; the fourth level (4) represents the highes level of
                potential risk for the given risk area.

                The initial sep of the analysis consiss of evaluating each risk
                area independently. The second sep consiss of grouping the
                individual results to determine if a more in depth analysis is
                required. The greater the number of risk areas identifed as
                level 3 or 4, the more likely it is that specifc risk areas will need
                to be addressed in a more comprehensive manner.

                                                                                      Risk
                a) Type of program or activity
                                                                                      scale
                - Program or activity that does NOT involve a decision
                                                                                      1
                about an identifable individual
                - Adminisration of program or activity and services                   2
                - Compliance or regulatory invesigations and
                                                                                      3
                enforcement
                - Criminal invesigation and enforcement or national

https://www.tbs-sct.canada.ca/pol/doc-eng.aspx?id=18308#appC[11/6/2024 12:56:32 PM]
Rescinded [2024-10-09] - Directive on Privacy Impact Assessment- Canada.ca


                                                                                          4
                security
                                                                                              Risk
                b) Type of personal information involved and context
                                                                                              scale
                - Only personal information, with no contextual
                sensitivities, collected directly from the individual or
                                                                                              1
                provided with the consent of the individual for disclosure
                under an authorized program.
                - Personal information, with no contextual sensitivities
                after the time of collection, provided by the individual with
                                                                                              2
                consent to also use personal information held by another
                source.
                - Social Insurance Number, medical, fnancial or other
                sensitive personal information or the context surrounding
                the personal information is sensitive; personal information 3
                of minors or of legally incompetent individuals or involving
                a representative acting on behalf of the individual.
                - Sensitive personal information, including detailed
                profles, allegations or suspicions and bodily samples, or
                                                                                              4
                the context surrounding the personal information is
                particularly sensitive.
                c) Program or activity partners and private sector                        Risk
                involvement                                                               scale
                - Within the insitution (among one or more programs
                                                                                          1
                within the same insitution)
                - With other government insitutions                                       2
                - With other insitutions or a combination of federal,
                                                                                          3
                provincial or territorial, and municipal governments
                - Private sector organizations, international organizations
                                                                                          4
                or foreign governments
                d) Duration of the program or activityRisk scale
                - One-time program or activity                                        1
                - Short‚Äìterm program or activity                                      2
                - Long-term program or activity                                       3
                                                                                              Risk
                e) Program population
                                                                                              scale
                - The program's use of personal information for internal
                                                                                              1
                adminisrative purposes afects certain employees.



https://www.tbs-sct.canada.ca/pol/doc-eng.aspx?id=18308#appC[11/6/2024 12:56:32 PM]
Rescinded [2024-10-09] - Directive on Privacy Impact Assessment- Canada.ca


                - The program's use of personal information for internal
                                                                                      2
                adminisrative purposes afects all employees.
                - The program's use of personal information for external
                                                                                      3
                adminisrative purposes afects certain individuals.
                - The program's use of personal information for external
                                                                                      4
                adminisrative purposes afects all individuals.
                f) Technology and privacy
                - Does the new or subsantially modifed program or activity
                involve implementation of a new electronic sysem or the use of
                a new application or software, including collaborative software
                (or groupware), to support the program or activity in terms of
                the creation, collection or handling of personal information?
                - Does the new or subsantially modifed program or activity
                require any modifcations to information technology (IT) legacy
                sysems?
                Specifc technological issues and privacy


                - Does the new or subsantially modifed program or activity
                involve implementation of new technologies or one or more of
                the following activities:
                            enhanced identifcation methods;
                            surveillance; or
                            automated personal information analysis, personal
                            information matching and knowledge discovery
                            techniques?

                A YES response indicates the potential for privacy concerns
                and risks, which will require consideration and, if necessary,
                mitigation.
                                                                                      Risk
                g) Personal information transmission
                                                                                      Scale
                - The personal information is used within a closed sysem
                (i.e., no connections to the Internet, Intranet or any other
                                                                                      1
                sysem and the circulation of hardcopy documents is
                controlled).
                - The personal information is used in a sysem that has
                                                                                      2
                connections to at leas one other sysem.
                - The personal information is transferred to a portable
                device (i.e., USB key, diskette, laptop computer),                    3


https://www.tbs-sct.canada.ca/pol/doc-eng.aspx?id=18308#appC[11/6/2024 12:56:32 PM]
Rescinded [2024-10-09] - Directive on Privacy Impact Assessment- Canada.ca


                transferred to a diferent medium or is printed.
                - The personal information is transmitted using wireless
                                                                                      4
                technologies.
                h) Potential risk that in the event of a privacy breach, there will
                be an impact on the individual or employee.

                i) Potential risk that in the event of a privacy breach, there will
                be an impact on the insitution.


                Note: For additional guidance on items h) and i), government
                insitutions can refer to the Guidelines for Privacy Breaches.

                In the case of a multi-insitutional PIA, each government
                insitution involved is, at a minimum, responsible for completing
                items b), c), f), g), h) and i), whereas the lead government
                insitution is responsible for completing items a), d) and e).


                Section III‚ÄìAnalysis of Personal Information
                Elements for the Program or Activity
                      a. Identify each element of personal information collected
                            (for example: 1) name, 2) address).
                      b. Identify sub-elements associated with each element of
                            personal information collected (for example: 1) frs name
                            / middle initial / las name, 2) sreet name / sreet number
                            / city / province /posal code).
                      c. Identify how the personal information will be recorded: on
                            paper, electronically, audio recordings, visual image
                            recordings, human biological samples or other (specify).

                In the case of a multi-insitutional PIA, each government
                insitution involved is, at a minimum, responsible for identifying
                the elements of personal information collected or disclosed in
                relation to their involvement in the multi-insitutional program or
                activity.


                Section IV‚ÄìFlow of Personal Information for the
                Program or Activity
                      a. Identify the source(s) of the personal information
                            collected and / or how the personal information will be
                            created.
                      b. Identify both internal and external sources for the


https://www.tbs-sct.canada.ca/pol/doc-eng.aspx?id=18308#appC[11/6/2024 12:56:32 PM]
Rescinded [2024-10-09] - Directive on Privacy Impact Assessment- Canada.ca


                            personal information's use and disclosure, that is, identify
                            the areas, groups and individuals who have access to or
                            handle the personal information and to whom it is
                            provided or disclosed. Where relevant, include the
                            following information:
                                           Government insitution responsible for the
                                           program or activity (provide PIB title and
                                           number);
                                           Other government insitution responsible for the
                                           program or activity (provide PIB title and
                                           number); or
                                           Non-federal government insitution (e.g.,
                                           provincial or territorial, municipal, or Aboriginal
                                           governments or councils, organization of a
                                           foreign sate, international organization) or
                                           private sector.
                      c. Identify where the personal information will transit and
                            will be sored or retained.
                      d. Identify where areas, groups and individuals can access
                            the personal information.

                The government insitution is to determine the format for
                representing the fow of personal information.

                In the case of a multi-insitutional PIA, each government
                insitution involved is, at a minimum, responsible for outlining
                the fow of personal information under its control. The lead
                government insitution will be responsible for outlining the fow
                of personal information between or among government
                insitutions.


                Section V‚ÄìPrivacy Compliance Analysis
                      a. At a minimum, the privacy compliance analysis mus
                            cover the following areas and identify specifc
                            compliance actions taken or to be taken to meet with
                            each area's requirements:
                                           Collection authority (section 4 of the Privacy
                                           Act)
                                           Direct collection, notifcation and consent, as
                                           appropriate (section 5 of the Privacy Act)


https://www.tbs-sct.canada.ca/pol/doc-eng.aspx?id=18308#appC[11/6/2024 12:56:32 PM]
Rescinded [2024-10-09] - Directive on Privacy Impact Assessment- Canada.ca


                                           Retention (section 6 of the Privacy Act)
                                           Accuracy (section 6(2) of the Privacy Act)
                                           Use (section 7 of the Privacy Act)
                                           Disclosure (section 8 of the Privacy Act)
                                           Adminisrative, physical and technical
                                           safeguards
                                           Technology and privacy issues
                                                                   Indicate any changes to the
                                                                   business requirements that have
                                                                   an impact on the sysem, software
                                                                   or program application and,
                                                                   consequently, may afect the
                                                                   current access controls and
                                                                   privacy practices related to the
                                                                   creation, collection, retention, use,
                                                                   disclosure and disposition of
                                                                   personal information.
                                                                   Determine whether the current IT
                                                                   legacy sysems and services that
                                                                   will be retained or those that will
                                                                   be subsantially modifed are
                                                                   compliant with privacy
                                                                   requirements.
                                                                   Identify any awareness activities
                                                                   related to protection of privacy
                                                                   requirements in the new electronic
                                                                   environment.

                In the case of a multi-insitutional PIA, each government
                insitution involved is, at a minimum, responsible for outlining
                the privacy practices for the personal information under its
                control.


                Section VI‚ÄìSummary of Analysis and
                Recommendations (as applicable)
                      a. Document the conclusion drawn or recommendations
                            resulting from the risk identifcation and categorization in
                            a manner that is commensurate with the risk identifed.




https://www.tbs-sct.canada.ca/pol/doc-eng.aspx?id=18308#appC[11/6/2024 12:56:32 PM]
Rescinded [2024-10-09] - Directive on Privacy Impact Assessment- Canada.ca


                Section VII‚ÄìSupplementary Documents Lis
                      a. Lis any additional documents that were used or are
                            related to the core PIA; these documents do not need to
                            be appended to the core PIA.


                Section VIII‚ÄìFormal Approval
                      a. Indicate that the PIA was formally approved in
                            accordance with the government insitution's approval
                            process.
                      b. In the case of a multi-insitutional PIA, indicate that the
                            lead government insitution determined the PIA was
                            formally approved.

                Completion of the above sections with the information
                requesed fulflls the minimum content requirements of the core
                PIA.



             ¬© His Majesy the King in Right of Canada, represented by the President of the
                                                      Treasury Board, 2017,
                                                    ISBN: 978-0-660-09671-1


              Expand all          Collapse all




           Date modifed: 2020-06-18




           Government of Canada

           All contactsDepartments and agenciesAbout government




           Jobs                                                         Taxes                               Canada and the world

           Immigration and citizenship                                  Environment and natural resources   Money and fnance

           Travel and tourism                                           National security and defence       Science and innovation

           Business                                                     Culture, hisory and sport           Indigenous peoples

           Benefts                                                      Policing, jusice and emergencies    Veterans and military

           Health                                                       Transport and infrasructure         Youth




https://www.tbs-sct.canada.ca/pol/doc-eng.aspx?id=18308#appC[11/6/2024 12:56:32 PM]
Rescinded [2024-10-09] - Directive on Privacy Impact Assessment- Canada.ca



           Social media ‚Ä¢ Mobile applications ‚Ä¢ About Canada.ca ‚Ä¢ Terms and conditions   ‚Ä¢ Privacy




https://www.tbs-sct.canada.ca/pol/doc-eng.aspx?id=18308#appC[11/6/2024 12:56:32 PM]
   DATA PRIVACY AND DATA PROTECTION: THE RIGHT OF USER‚ÄôS AND THE
            RESPONSIBILITY OF COMPANIES IN THE DIGITAL WORLD.
                                   Alafaa Princess Uche-Awaji*



ABSTRACT

With the continuous advancement in technology and massive increase in internet usage, the
concepts of data privacy and data protection is a hugely debated topic. This is because, the service
providers who manage the websites, applications and social media platforms often collect and
store user‚Äôs personal data with the objective of providing adequate services to best suit each user‚Äôs
preference. Usually, these digital service companies are saddled with the responsibility of
protecting the personal data of the users from unauthorised access and against all vulnerabilities.
However, instances arise where these platforms fail to adequately place safeguards to protect the
data collected and this results to a data breach and exposure of users sensitive data to
unauthorised parties who can use the personal data to defraud and harass the users or to send
unwanted adverts without the users consent. Thus, infringing on the users‚Äô fundamental right of
privacy and freedom to freely express themselves. Hence, the need for companies to adopt
defensive mechanisms to ensure an adequate protection of users‚Äô personal data and also
awareness by the users that they have a right of control over which personal data to share and
with whom it is shared. This paper is divided into three parts with the first discussing the rights of
users and responsibilities of companies as well as the established regulations in the protection of
data. The second part of this work considers the issues surrounding data privacy and data
protection and the challenges faced in ensuring the safety of users‚Äô personal data. Finally, the last
part offers a series of recommendations and conclusion.


Key words: Data Privacy, Data Protection, User, Data Breach, Privacy Policy, General Data
Protection Regulation (GDPR), Nigerian Data Protection Regulation (NDPR), California
Consumer Privacy Act (CCPA).




                    Electronic copy available at: https://ssrn.com/abstract=4005750
INTRODUCTION
Data privacy and data protection is of immense importance both to the users and the companies
respectively. The concept of privacy has been considered a fundamental human right of individuals
in several jurisdictions as privacy is essential to a free society. Data privacy is deemed to be the
right of individuals to determine who has access to their personal information, what personal
information is shared and the protection of these information from unauthorised parties who should
not have access to them.1 Data protection on the other hand is considered to be the responsibility
of companies in protecting its users personal information from unauthorised use. It involves the
enforcement of polices to prevent misuse or unauthorised access to user‚Äôs personal information
and the application of defensive mechanisms against all vulnerabilities to their data collection and
storage system.2 Companies also owe users an explanation on the steps taken with regard to the
protection of data from hacks and sales and any form of data intrusion in their privacy policies.
The privacy policies are to be on the company‚Äôs websites explaining to its users the kind of
personal information collected, how it is used, with whom it is shared to and how it is secured.
Such transparency on how the user‚Äôs data is collected, whom it is shared with and managed proves
to the users that the company can be trusted to handle their personal data with care.

Similarly, the users have a right to give consent to the collection of their personal information and
should be given an opportunity to exercise such rights, such as the use of cookies and an option to
either consent or disable such cookies on the websites.3 Users consent is essential in discussing
data privacy and data protection. This is because, although a company has ensured that restrictions
and other safety measures are placed to accessing user‚Äôs personal information and such personal
information was collected without the users consent such company would still be guilty of
breaching a data privacy law. Various countries have now established regulations and policies to
protect their citizens personal information. Data privacy laws and regulations are important in
enabling individuals freely exercise their fundamental rights of privacy. Privacy entails the right


* Bar Aspirant at the Nigerian Law School, 2021/2022 Academic Session.
1
  ‚ÄúWhat is data privacy?‚Äù Available at https://www.cloudfare.com/learning/privacy/what-is-data-privacy/ accessed
30th October, 2021.
2
  ‚ÄúData privacy vs. Data protection: Understanding the distinction in defending your data‚Äù by Expert panel, Forbes
Technology Council, 19th December, 2018, 7:00am Est.
Available             at           https://www.forbes.com/sites/forbestechcouncil/2018/12/19/data-privacy-vs-data-
protectionunderstandingthe-distinction-in-defending-yourdata/?shz65d57f4d50c9/ accessed 30th October, 2021.
3
  ‚ÄúData privacy Laws: what you need to know in 2021‚Äù by Angelique Carson, 20th July, 2021. Available at
https://www.osano.com/articles/data-privacylaws/ accessed 30th October, 2021.




                      Electronic copy available at: https://ssrn.com/abstract=4005750
of an individual to freely exist in one‚Äôs space without uninvited monitoring. Data protection is an
important way in ensuring that companies comply with the laws and also guarantee the privacy of
users‚Äô personal information.

LEGAL FRAMEWORKS ON DATA PRIVACY AND DATA PROTECTION IN
NOTABLE JURISDICTIONS.

The General Data Protection Regulation (EU) 2016/679 (GDPR)

The General Data Protection Regulation (GDPR) has been described as the most essential and
comprehensive privacy and protection legislation in the world. This regulation was established by
the European Parliament and Council of the European Union on April 14th, 2016 to repeal the Data
Protection Directive 95/46/EC. The GDPR took effect on May 25th, 2018 and is established to be
the major law regulating the conduct of companies and businesses within or outside the EU in
protecting the personal information of EU citizens and residents with regard to the processing and
free movement of such data. This regulation applies to all member states of the European Union
in order to create a uniform data protection law among EU countries. Companies either with a
digital or physical presence which collects or processes the personal data of EU residents and
citizens are mandated to comply with the GDPR and a default to this regulation attracts strict
penalties and fines.4This regulation became a model for several national laws on data privacy and
security across the world. By virtue of Article 8(3) of the EU Charter of Fundamental Rights,5 data
privacy and protection are two rights explicitly enshrined in the Charter. Under this provision, EU
citizens have a fundamental right to the protection of their personal data.6 In the same vein, the
General Data Protection Regulation (GDPR) has been established to protect the fundamental right
of data privacy and protection of EU citizens.

The GDPR comprises of 11 chapters, 99 articles and 173 recitals relating to general privacy
principles, rights of data subjects(users‚Äô), duties of data controllers, transfer of users‚Äô data,
liabilities and remedies for breach of rights and other miscellaneous provisions. This regulation


4
  ‚ÄúWhat is the GDPR? Understanding and complying with GDPR requirements in 2019‚Äù by Juliana De Groot, 30th
September, 2020. Available at https://digitalguardian.com/blog/what-gdpr-general-data-protection-regulation-
understanding-and-complying-gdpr-data-protection/ accessed 4th November, 2021.
5
  ‚ÄúCharter of Fundamental Rights of the European Union‚Äù Official Journal of the European Union C83, vol. 53,
European Union, 2010, p. 380.
6
  ‚ÄúData Protection‚Äù, available at https://edps.europa.eu/data-protection_en/ accessed 14th November, 2021.




                     Electronic copy available at: https://ssrn.com/abstract=4005750
does not apply to the processing of personal information for law enforcement or national security
activities of EU member states. Also, where the data is processed for personal or household
activities and not for any commercial or economic purposes, the regulation is inapplicable.7 Salient
provisions on data privacy and protection in the regulation include, right of data subjects to
withdraw consent at any time,8 provision of legal basis in which users‚Äô personal data can be
processed,9 right of users to transparent information regarding personal data,10 right of users to
access personal information, how it is acquired and processed, the purpose and with whom it is
shared,11 right of erasure which entails right to be forgotten without undue delay,12 users‚Äô right of
control and ability to easily transfer their personal data between service providers,13 right of
individuals to object to the processing of personal information for marketing or non-service related
purposes,14 maintenance of reasonable data protection measures by design or default in the
company‚Äôs development and processing setup,15 swift data breach notification to users‚Äô where their
rights are at risk because of the breach,16 requirement of a data protection officer to ensure
company compliance with the regulation,17 and penalties for non-compliance with the regulation.18

As contained in the regulation, consent by data subjects must be specific, freely given and a plain-
worded affirmation and not through a consent option with an opt-out selection by default
structure.19 Also, as provided in Article 27, non-EU organisations are required to appoint a
representative based in one of the EU member states.20Another important provision of the GDPR,
is the anonymity of the personal data collected to ensure adequate protection and privacy through
pseudonymisation as an alternative.21 Pseudonymisation can be defined as a technology that

7
  Recital 18 of the General Data Protection Regulation (EU) 2016/679 (GDPR).
8
  The General Data Protection Regulation (EU) 2016/679 (GDPR) Article 7(3) (conditions for consent).
9
  Ibid, Article 6(1) (Lawfulness of processing). This provision lists six legal grounds for processing personal data,
consent, contractual obligation, users‚Äô vital interest, public interest, legitimate interest of data controller or third party
and compliance with data controller‚Äôs legal obligation.
10
   Ibid, Article 12(Transparent information, communication and modalities for the exercise of data subjects‚Äô rights).
11
   Ibid, Article 15(Right of access by the data subject).
12
   Ibid, Article 17(right of erasure).
13
   Ibid, Article 20(right of portability).
14
   Ibid, Article 21(Right to object).
15
   Ibid, Article 25(Data protection by design and by default).
16
   Ibid, Article 34(Communication of a personal data breach to the data subject).
17
   Ibid, Article 37(Designation of a data protection officer).
18
   Ibid, Article 83(Conditions for imposing administrative fines).
19
   Ibid, Recital 32(Conditions for consent).
20
   Ibid, Article 27(Representatives of controllers or processors not established in the Union).
21
   Ibid, Recital 28(Introduction to pseudonymisation).




                         Electronic copy available at: https://ssrn.com/abstract=4005750
enhances privacy and as stated in the General Data Protection Regulation (GDPR), it is the means
of processing personal data in a manner that such data can no longer be attributed to a specific data
subject without the use of additional information and this additional information is kept separately
and subject to technical and organisational measures. 22 It is a useful method in reducing the risks
of unauthorised access to the personal data of data subjects and this in turn helps the data
controllers and processors fulfil their data protection obligation. Two examples of
pseudonymisation are, encryption and tokenisation. Encryption is the process in which an original
data is unable to be accessed and it cannot be reversed without the correct decryption key, while
tokenisation is a form of data protection which replaces sensitive data with non-sensitive
substitutes known as tokens so that the sensitive data is hidden and invisible for processing.23

Although the General Data Protection Regulation (GDPR) has been argued to be an effective
regulation, there exists some challenges with regard to the compliance of certain requirements of
the regulation. Such as the huge cost incurred by companies in order to meet up certain
requirements of data mapping, cross-border transfer of data and other data processing activities,
these all require a lot of financial commitment to be made. Many companies find it difficult to
easily provide or delete data subject‚Äôs personal information on request and so rely on the GDPR
compliance software which discovers and classifies such information. Arguments have also been
made regarding how small businesses would cope in adequately complying with the GDPR as they
might not have the financial resources to do so. Similarly, arguments have been made that there
exists an unclear understanding of the compliance measure of the regulation as well as its effective
implementation with regard to blockchain systems. Some international websites in a bid to avoid
compliance with the GDPR did block EU users entirely and some others redirected them to lower
versions of their services.

Despite the challenges, several companies have complied with the General Data Protection
Regulation (GDPR) and made changes to their privacy policies and settings. Also, with the
implementation of this regulation, a large percentage of users globally have increased knowledge
and awareness on the issue of data privacy and protection as well as their rights and thus affects
users‚Äô decision-making process on how they use several internet service platforms and websites.


22
     Ibid, Article 4(5) (Definitions).
23
     Ibid, Recital 28(Introduction to pseudonymisation).




                          Electronic copy available at: https://ssrn.com/abstract=4005750
Thereby resulting in a competitive quality for companies which provide these services to comply
with the regulations in fulfilling their legal obligation of ensuring adequate data protection on their
websites and in their privacy policies and settings.

The California Consumer Privacy Act of 2018 (CCPA).

The California Consumer Privacy Act (CCPA) is a state legislation signed into law on June 28th,
2018.24 This law was amended on September 13th, 2018 and on October 11th, 2019. The CCPA
came into effect on January 1st, 202025 although the California Privacy Rights Act was passed later
in November 2020 to expand the CCPA.26 The CCPA is established to protect the personal
information of all consumers who falls into the definition of a California resident as provided in
the Act.27 This means that any business28 which collects or sells the personal data of California
residents are subject to this law. The physical location of a business is not regarded and this does
not absolve it from complying with the act. This Act contains certain key components such as users
right of access and knowledge of all their personal data which a company collects from them,29
purpose behind the collection and selling of the information,30 right to choose whether their
information be sold to third parties(right to opt-out at any time to the selling of their personal
information), categories of third parties a business shares the information with31 and right to
request complete erasure of data32 etc . The California Consumer Privacy Act (CCPA) has been
said to exist on three important principles, accountability, control and transparency.33




24
   California Legislative Information Bill Text. Available at
https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=201720180AB375 accessed 16th November,
2021.
25
   ‚Äú2019 is the Year of ‚Ä¶CCPA?‚Äù by Liisa M. Thomas, Craig Cardon, Brian D. Anderson, Rachel T. Hudson, Snehal
Desai, 8th January, 2019. Available at https://www.natlawreview.com/article/2019-year-ccpa-infographic/ accessed
16th November, 2021.
26
   ‚ÄúMove over, CCPA: The California Privacy Rights Act gets the spotlight now‚Äù by Cynthia Cole, Matthew R. Baker
and Katherine Burgess, 16th November, 2020. Available at https://news.bloomberglaw.com/privacy-and-data-
security/move-over-ccpa-the-california-privacy-rights-act-gets-the-spotlight-now accessed 16th November, 2021.
27
   California Consumer Privacy Act of 2018. S. 1798.140 (i) (definition section).
28
   Ibid s. 1798.140 (d) paragraph (1)(2)(3).
29
   Ibid s. 1798.110(a).
30
   Ibid s. 1798.100(a) and s. 1798.115(a).
31
   Ibid s. 1798.110(a)
32
   Ibid s. 1798.105(a).
33
    ‚ÄúBasics of the California Consumer Privacy Act of 2018‚Äù by PrivacyPolicies.com Legal Writing Team, 15th
November, 2021. Available at https://ww.privacypolicies.com/blog/california-consumer-privacy-act/ accessed 6th
January, 2022.




                      Electronic copy available at: https://ssrn.com/abstract=4005750
A notable feature of the California Consumer Privacy Act (CCPA) is that it expanded the definition
of personal information to include, geolocation data, personal identifiers, inferences about the
users from any of the information identified made by the company to create a profile reflecting
users preferences, behaviours and so on and the internet browsing and search history data of the
user etc.34 This Act provides that California residents have a right to bring their data to another
service provider or have it deleted completely.35 Also, under this Act, businesses are mandated to
disclose important general information about their privacy practices in a privacy policy on their
website and the business‚Äô privacy policy should be more transparent, containing in details how
data is collected, why it is collected, who it is shared with and the users rights concerning the
business practices. Businesses under this Act are mandated to display in their privacy policy a link
titled ‚ÄúDo Not Sell My Personal Information‚Äù36 and an opt-out option must be clearly displayed
in the privacy policy of the business.37 The California Consumer Privacy Act (CCPA) prohibits
businesses from discriminating against users who choose to exercise any of their rights as provided
in the Act.38 Finally, the Act also provides penalties for non-compliance and California residents
are permitted to file law suits for privacy losses without any proof of damages or loss of property
or money.

In conclusion, it can be stated that the establishment of the California Consumer Privacy Act
(CCPA) signifies the era of massive protection of sensitive personal data and guarantees the right
of privacy of users and also empowers the users or consumers to an adequate control over the
collection, use and sale of their personal data by companies who render such services and obtain
their information.

The Nigerian Data Protection Regulation, (NDPR) 2019.

As data remains a critical component of the digital economy, the Nigerian government through its
agency, the National Information and Technology Development Agency (NITDA) which is
empowered by the NITDA Act, 2007 issued the Nigerian Data Protection Regulation (NDPR) in




34
   Supra note 27. S. 1798.140(v) paragraph 1(A-L).
35
  Supra note 31.
36
   Supra note 27. S. 1798.135(a)(1).
37
   Supra note 27. S. 1798.120.
38
   Ibid s. 1798.125(a).




                      Electronic copy available at: https://ssrn.com/abstract=4005750
201939 to address data privacy and protection in Nigeria. This regulation seeks to comprehensively
regulate and control access to user‚Äôs personal information in Nigeria. With regard to citizens right
to privacy, there are several laws in Nigeria that impact data protection regulation.40 Before the
issuance of the Nigerian Data Protection Regulation (NDPR), there was no specific law limited to
regulating data privacy and protection.41 However, as is applicable in most jurisdictions, data
privacy and protection stem from the grundnorm of the land, which in this case is the Nigerian
Constitution.42 By virtue of s. 37 of the Nigerian Constitution which protects the rights of citizens
to their privacy, privacy of their homes, correspondence, telephone conversations and telegraphic
communications as well as any personal information regarding such citizen, it can be said that this
section lays the foundation for data privacy and protection rights in Nigeria. Data privacy and
protection can thus be deemed to be extensions of a citizen‚Äôs constitutional right to privacy.
Therefore, in a case of breach of privacy whether physical or digital, the citizen or data subject can
enforce such right in a court of law under the Fundamental Human Rights Enforcement Procedure
(FREP) Rules.

This was held in the decision of the Ogun State High court in Incorporated Trustees of Digital
Rights Lawyers Initiative v L. T Solutions and Multimedia Limited43 where the court was of the
view that a data subject‚Äôs right as contained in Article 3.1 of the NDPR, may be enforced just as a
constitutional right is enforced under FREP Rules. Similarly, in light of the judgment of the Court
of Appeal in the recent case of Digital Rights Lawyers Initiative v National Identity Management
Commission44, the court recognised a data subject‚Äôs right (data privacy and protection) as part of
the fundamental right to privacy. Here the Court of Appeal stated that data protection under the
NDPR is included under s. 37 of the CFRN and this right to privacy also includes the protection


39
   NDPR- A regulation made by the NITDA by virtue of s. 6 of the National Information and Technology Development
Agency,               Act            (2007).            Available             at            https://nitda-gov.ng/wp-
content/uploads/2019/01/NigeriaDataProtectionRegulation.pdf/ accessed 20th December, 2021.
40
   Some of these laws include; The Nigerian Constitution (as amended) 1999, The National Information and
Technology Development Agency Act, 2019, The Child Rights Act, Cybercrimes (Prohibition, Prevention Etc) Act,
2015, Central Bank of Nigeria Consumer Protection Framework, 2016, The Nigerian Communications (registration
of Telephone Subscribers) Regulations 2011, Freedom of Information Act, 2011 etc.
41
   ‚ÄúData Privacy and Protection under the Nigerian Law‚Äù by Francis Oloho (S.P.A. Ajibade and co.), 19th February,
2020. Available at https://www.mondaq.com/nigeria/privacy-protection/895320/data-privacy-and-protection-under-
the-nigerian-law/ accessed 20th December, 2021.
42
   Constitution of the Federal Republic of Nigeria 1999(as amended) Act No. 24, 5 th May, 1999.
43
   Suit No. HCT/262/2020.
44
   Suit No. CA/IB/291/2020.




                       Electronic copy available at: https://ssrn.com/abstract=4005750
of a person‚Äôs personal information from others or persons unauthorised to have access to them.
Therefore, every citizen has the right to ensure the privacy and protection of his or her data and
such right is guaranteed under the Constitution.45

The Nigerian Data Protection Regulation (NDPR) is the most comprehensive legislation
containing provisions which protects data privacy in Nigeria. This Regulation comprises of four
(4) parts with part one focusing on the objectives of the regulation, scope of the regulation and
definition sections. Article 2.1 contains the governing principles for data protection, which
includes, the lawful collection and processing of personal data46, consent obtained from the data
subject before such collection and processing47, the data subject has the right to withdraw consent
at any time, consent must be obtained without fraud, coercion or undue influence, also any medium
through which personal data is collected is required to display in a simple and understandable
manner, their privacy policy.48 Article 2.6 provides for some possible data protection measures49
to include, protecting systems from hackers, setting up firewalls, storing data securely with access
to specific authorised individuals etc50, Article 2.8 provides for the right of a data subject to object
to the processing of his data51, the regulation also provides punishment for offenders52, Article 3.1
contains the rights of a data subject, right to be informed of the appropriate measures adopted in
safeguarding his data in a foreign country, right to data portability, right to request erasure of
personal data etc. Article 4.2 provides for an administrative redress panel to investigate in a case
of an allegation of breach of the regulation.53

Conclusively, the establishment of the National Information and Technology Development
Agency Regulation and the Nigerian Data Protection Regulation (NDPR) is a commendable feat
as it portrays a significant step in Nigeria towards being abreast with the digital revolution and

45
   ‚ÄúThe Constitutionality of the Right to Data protection under Section 37 of the CFRN in Light of the Court of Appeal
Judgment‚Äù by TechLaw Space.
46
   Nigerian Data Protection Regulation (NDPR), 2019. Article 2.2(Lawful Processing).
47
   Ibid Article 2.3 (Procuring consent).
48
   Ibid Article 2.5 (Publicity and Clarity of Privacy Policy).
49
   Ibid Article 2.6 (Data Securing).
50
   ‚ÄúManaging Data Privacy issues in Corporate Restructuring: Key Considerations for Investors‚Äù by Emmanuel Omoju
and Patience Ajogbor, 1st December, 2020.
51
   Nigerian Data Protection Regulation (NDPR), 2019. Article 2.8 (Objections by the Data Subject).
52
   Ibid Article 2.10 (Penalty for Default).
53
   ‚ÄúActions Beyond the Nigerian Data Protection Regulations (NDPR) 2019‚Äù by Gabriel Omoniyi, 30th December,
2020. Available at https://www.mondaq.com/nigeria/data-protection/1020784/actions-beyond-the-nigerian-data-
protection-regulations-npdr-2019#_ftnref35/ accessed 6th January, 2022.




                       Electronic copy available at: https://ssrn.com/abstract=4005750
further emphasises the approval and value of safeguarding the digital rights of citizens within
Nigeria. The establishment of this data privacy and protection legislation and the lauded decision
in Digital Rights Lawyers Initiative v National Identity Management Commission54 which aligns
with the position that data protection is not a right provided for in the NDPR alone but it is a
constitutional and fundamental right provided for in the Constitution therefore acknowledges the
rights of persons to preserve their right of privacy as guaranteed under the Constitution.55

OVERVIEW OF CHALLENGES ENCOUNTERED IN DATA PRIVACY AND
PROTECTION.

Data privacy and protection laws has been considered an amazing achievement, however there
exists a lot of challenges with the protection of user‚Äôs data.

     1. Massive growth of data

Organisations are expected to adequately ensure the protection of user‚Äôs personal information from
any form of data breach and with the growing nature and volume of data in our technologically
driven world it becomes overwhelming to handle the billions of data. Also considering the various
data privacy legislations and the complexity of its rules, there is a doubt that absolute compliance
is even possible.

     2. Conflict of laws

With the existence of several legislations on data privacy and protection, the possibility of conflict
between several laws could often pose a lot of challenge to organisations whose users cut across
various continents and affect their ability to adequately comply with each countries data privacy
laws. For example, in relation to the definition of certain terms like data privacy, data subjects‚Äô
consent, withdrawal of consent, right of erasure etc, which may often differ among the laws of
several countries and this sometimes affect organisations decision as to determine what data
privacy law to apply to their users.




54
  Supra Note 35.
55
  ‚ÄúAn Extensive Article on Data Privacy and Data Protection Law in Nigeria‚Äù by Uche Val Obi SAN, 9th September,
2020. Available at https://inplp.com/latest-news/article/an-extensive-article-on-data-privacy-and-data-protection-
law-in-nigeria/ accessed 6th January, 2022.




                      Electronic copy available at: https://ssrn.com/abstract=4005750
     3. Cost of maintaining data privacy and security

The cost of rectifying a data breach results in a huge loss of revenue to an organisation as such
organisation faces intense regulatory penalties. Organisations are expected to adopt and take
appropriate steps to ensure that adequate and specific security technologies are provided for, such
as data backups and archiving in order for data to be safeguarded, recovered and restored.

     4. Human Error complexities

Many data analysts have stated that human error is considered to be the biggest challenge in data
privacy56, for instance a lack of awareness by employees can significantly affect an organisations
data privacy and protection system. This can occur through the use of weak passwords, falling for
phishing scams etc and thus could result in user‚Äôs data becoming vulnerable and lead to data loss.

RECOMMENDATIONS

To effectively ensure the privacy and protection of user‚Äôs personal data, both the organisations
collecting and processing the data as well as the users are to adopt appropriate and essential steps.

Organisations

     1. Empowerment of employees to be aware of the need for data privacy and security through
        the creation of security awareness and training programs for old and new employees and
        thereby reducing the risk of data loss.
     2. Implementation of data loss prevention and security tools to help prevent employees or
        users from leaking sensitive data and to reduce the risk of attack.
     3. Ensuring the consistent monitoring of the organisations‚Äô network and systems in order to
        detect any suspicious activities or attacks early.
     4. Implementation of zero trust models which would restrict access to the organisations
        network by outsiders or unauthorised third parties.
     5. Ensuring that the policy enforcement and protection mechanism of the organisation is
        easily implemented by all users, devices, data and application irrespective of where the
        users are connected from.


56
  ‚ÄúThe Five Biggest Challenges in Global Data privacy and Protection‚Äù https://cipher.com/blog/the-5-biggest-
challenges-in-global-data-privacy-and-protection-/ accessed 6th January, 2022.




                     Electronic copy available at: https://ssrn.com/abstract=4005750
   Users

       1. Familiarizing themselves with privacy tools, such as password managers which is an
           encrypted virtual private network to protect the user‚Äôs internet connection. A good
           password should be long, complex and as hard to guess as possible. The same password
           should not be used for all the users accounts.
       2. Periodically scanning your device with trusted antivirus software to locate and remove
           any malicious threats to your device and backup your data often.
       3. Be on guard for any strange request, flashing click bait content and things that may
           seem fake or suspicious. One way to prevent such phishing is to avoid entering your
           personal information on any site you did not visit, search or bookmark on your own.
       4. Users are advised to only share their personal information with companies that are open
           and honest about their data privacy policies and who would not sell their information
           with the aim of accumulating massive rewards and are lax on protecting user‚Äôs data.
       5. Users are to be aware that a number of companies use and store their personal data,
           therefore they should avoid sharing a lot of their personal data as the users have a
           fundamental right to privacy of personal information.
       6. Awareness of users of their legal rights to privacy and the legal responsibilities of
           organisations to ensure the adequate protection of their personal data.

CONCLUSION

Data privacy entails the right of an individual to freely exist in one‚Äôs space without any unwanted
surveillance and data protection is an important way of ensuring that organisations and companies
both comply with the laws and also guarantees information privacy. Organisations have the
responsibility of protecting the user‚Äôs personal information and transparency by these
organisations and companies on how user‚Äôs data is collected, managed and when such information
is shared builds trust in the users. Hence, data privacy and protection aim essentially not only to
protect the users but also to ensure that the companies and organisations are entrusted with the
responsibility of protecting user‚Äôs personal data and are held liable for any data security breaches.




                    Electronic copy available at: https://ssrn.com/abstract=4005750
See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/261613840



Privacy Online

Book ¬∑ August 2011
DOI: 10.1007/978-3-642-21521-6




CITATIONS                                                                                              READS

85                                                                                                     20,064


2 authors:

            Sabine Trepte                                                                                         Leonard Reinecke
            University of Hohenheim                                                                               Johannes Gutenberg-Universit√§t Mainz
            164 PUBLICATIONS 5,160 CITATIONS                                                                      134 PUBLICATIONS 8,062 CITATIONS

                SEE PROFILE                                                                                          SEE PROFILE




 All content following this page was uploaded by Sabine Trepte on 15 April 2014.

 The user has requested enhancement of the downloaded file.
Privacy Online
.
Sabine Trepte   l   Leonard Reinecke
Editors




Privacy Online
Perspectives on Privacy and
Self-Disclosure in the Social Web
Editors
Sabine Trepte                                               Leonard Reinecke
University of Hamburg                                       University of Hamburg
Department of Psychology                                    Department of Psychology
Von-Melle-Park 5                                            Von-Melle-Park 5
20146 Hamburg                                               20146 Hamburg
Germany                                                     Germany
sabine.trepte@uni-hamburg.de                                leonard.reinecke@uni-hamburg.de




ACM Codes: K.4, K.5, K.6.5, H.4, H.5
ISBN 978-3-642-21520-9           e-ISBN 978-3-642-21521-6
DOI 10.1007/978-3-642-21521-6
Springer Heidelberg Dordrecht London New York
Library of Congress Control Number: 2011932726

# Springer-Verlag Berlin Heidelberg 2011
This work is subject to copyright. All rights are reserved, whether the whole or part of the material is
concerned, specifically the rights of translation, reprinting, reuse of illustrations, recitation, broadcasting,
reproduction on microfilm or in any other way, and storage in data banks. Duplication of this publication
or parts thereof is permitted only under the provisions of the German Copyright Law of September 9,
1965, in its current version, and permission for use must always be obtained from Springer. Violations
are liable to prosecution under the German Copyright Law.
The use of general descriptive names, registered names, trademarks, etc. in this publication does not imply,
even in the absence of a specific statement, that such names are exempt from the relevant protective
laws and regulations and therefore free for general use.

Cover design: deblik

Printed on acid-free paper

Springer is part of Springer Science+Business Media (www.springer.com)
Preface




Privacy is a basic human need, and losing privacy is perceived as an extremely
threatening experience. Privacy embraces solitude, personal space, or intimacy with
family and friends and as such, it is a ubiquitous and trans-cultural phenomenon.
Privacy leverages well-being; without privacy we are at risk of becoming physically
or mentally ill.
    Our fundamental need for privacy is contrasted by a second powerful mecha-
nism of social interaction: self-disclosure to others is similarly important for social
functioning and psychological well-being. We need to self-disclose to bond with
others, form meaningful relationships, and receive social support. A lack of ability
to self-disclose causes clinical symptoms such as loneliness and depression.
    Striking the right balance between creating private spaces and self-disclosure is a
complex task, if not the most challenging one in interacting with others. Today, in
times of online communication and the Social Web, this task is further complicated
by two confusing facts:
    Firstly, our online communication is usually accessible to a vast number of
people. On social network sites, it is very common for several hundred online
friends to have access to the personal information, status updates, and private
pictures of a profile owner. In addition to these online friends as a ‚Äúknown
audience,‚Äù there are other ‚Äúunknown audiences,‚Äù such as advertisers who purchase
the users‚Äô aggregated profile information from social media companies to address
their target audiences.
    Secondly, many users appear not to feel threatened in terms of their need for and
experiences of privacy when communicating online. On social network sites,
micro-blogs, or in forums, they publish a vast amount of information that is
considered private or even intimate in other contexts. Although they are aware of
their data‚Äôs publicity on an abstract level, many feel free to speak and to open up to
others.
    Consequently, we are facing a new situation that demands answers to a variety of
pressing questions: Does online self-disclosure change our need for and experiences




                                                                                     v
vi                                                                            Preface

of privacy? What are the benefits of self-disclosure online? How does the loss of
informational privacy influence our online communication?
   These and many more questions will be addressed in the following chapters. We
are extremely grateful to the authors who contributed to this volume. All of the
chapters offer new theoretical approaches to online privacy. The work presented
here goes far beyond a summary of existing research: it offers new theoretical
models on the psychological functioning of online privacy, novel ideas on the hows
and whys of online privacy, and intriguing solutions for some of the most pressing
issues and problems in the field of online privacy.
   We would like to thank the German Research Foundation (Deutsche For-
schungsgemeinschaft, DFG) for supporting the work and the meetings of the
‚ÄúYoung Scholar‚Äôs Network on Privacy and Web 2.0‚Äù ‚Äì a group of scientists from
five different countries dealing with online privacy ‚Äì that have generated fruitful
discussions and helped develop many of the ideas expressed in this volume. We
hope that these ideas will stimulate future research and contribute to our under-
standing of the complex challenges to privacy in an online world.
   The volume Privacy Online is dedicated to those that inspire us and allow for
creativity, change, and new perspectives: our families, solitude, and personal space.

Hamburg, August 2011                                                 Sabine Trepte
                                                                  Leonard Reinecke
Contents




Part I      Approaches

1   Introduction to Privacy Online . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
    Joseph B. Walther

2   Three Theories of Privacy: An Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
    Stephen T. Margulis

3   Negotiating Privacy Concerns and Social Capital Needs
    in a Social Media Environment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
    Nicole B. Ellison, Jessica Vitak, Charles Steinfield,
    Rebecca Gray, and Cliff Lampe

4   Digital Crowding: Privacy, Self-Disclosure, and Technology . . . . . . . . . . 33
    Adam N. Joinson, David J. Houghton, Asimina Vasalou,
    and Ben L. Marder

5   Ethics, Privacy, and Self-Restraint in Social Networking . . . . . . . . . . . . . . 47
    Bernhard Debatin

6   The Social Web as a Shelter for Privacy and Authentic Living . . . . . . . 61
    Sabine Trepte and Leonard Reinecke

7   Fifteen Minutes of Privacy: Privacy, Sociality, and Publicity
    on Social Network Sites . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75
    Zizi Papacharissi and Paige L. Gibson

8   The Co-evolution of Social Network Ties and Online
    Privacy Behavior . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91
    Kevin Lewis


                                                                                                                                       vii
viii                                                                                                                                          Contents

  9      Self-Protection of Online Privacy: A Behavioral Approach . . . . . . . . . 111
         Mike Z. Yao

10       Online Self-Presentation: Balancing Privacy Concerns
         and Impression Construction on Social Networking Sites . . . . . . . . . . . 127
         Nicole C. KraÃàmer and Nina Haferkamp

11       The Uses of Privacy Online: Trading a Loss of Privacy
         for Social Web Gratifications? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143
         Monika Taddicken and Cornelia Jers


Part II           Applications

12       (Micro)blogs: Practices of Privacy Management . . . . . . . . . . . . . . . . . . . . . 159
         Jan-Hinrik Schmidt

13       Privacy in Social Network Sites . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 175
         Marc Ziegele and Oliver Quiring

14       Mobile Privacy: Contexts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191
         Maren Hartmann

15       Online Privacy as a News Factor in Journalism . . . . . . . . . . . . . . . . . . . . . 205
         Wiebke Loosen

Part III            Audiences

16       Adolescents‚Äô Online Privacy: Toward a Developmental
         Perspective . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 221
         Jochen Peter and Patti M. Valkenburg

17       The Elderly and the Internet: How Senior Citizens Deal
         with Online Privacy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 235
         Wiebke Maa√ü

18       Privacy and Gender in the Social Web . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 251
         Mike Thelwall

Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 267
Contributors




Bernhard Debatin Ohio University, Athens, OH, USA
debatin@ohio.edu

Nicole B. Ellison Michigan State University, East Lansing, MI, USA
nellison@msu.edu

Paige L. Gibson University of Illinois at Chicago, Chicago, IL, USA
plg2uic@gmail.com

Rebecca Gray Michigan State University, East Lansing, MI, USA
grayreb2@msu.edu

Nina Haferkamp Technical University of Dresden, Dresden, Germany
nina.haferkamp@tu-dresden.de

Maren Hartmann Berlin University of Arts, Berlin, Germany
hartmann@udk-berlin.de

David J. Houghton University of Bath, Bath, UK
d.j.houghton@bath.ac.uk

Cornelia Jers University of Hohenheim, Stuttgart, Germany
cornelia.jers@uni-hohenheim.de

Adam Joinson University of Bath, Bath, UK
A.Joinson@bath.ac.uk

Nicole C. KraÃàmer University of Duisburg-Essen, Duisburg, Germany
nicole.kraemer@uni-due.de




                                                                      ix
x                                                                       Contributors

Cliff Lampe Michigan State University, East Lansing, MI, USA
lampecli@msu.edu

Kevin Lewis Harvard University, Cambridge, MA, USA
kmlewis@fas.harvard.edu

Wiebke Loosen Hans-Bredow-Institute for Media Research at the University of
Hamburg, Hamburg, Germany
w.loosen@hans-bredow-institut.de

Wiebke Maa√ü Hamburg Media School, Hamburg, Germany
w.maass@hamburgmediaschool.com

Ben L. Marder University of Bath, Bath, UK
b.l.marder@bath.ac.uk

Stephen Margulis Grand Valley State University, Grand Rapids, MI, USA
margulis@gvsu.edu

Zizi Papacharissi University of Illinois at Chicago, Chicago, IL, USA
zizi@uic.edu

Jochen Peter University of Amsterdam, Amsterdam, The Netherlands
j.peter@uva.nl

Oliver Quiring University of Mainz, Mainz, Germany
quiring@uni-mainz.de

Leonard Reinecke University of Hamburg, Hamburg, Germany
leonard.reinecke@uni-hamburg.de

Jan-Hinrik Schmidt Hans-Bredow-Institute for Media Research, Hamburg,
Germany
j.schmidt@hans-bredow-institut.de

Charles Steinfield Michigan State University, East Lansing, MI, USA
steinfie@msu.edu

Monika Taddicken University of Hamburg, Hamburg, Germany
monika.taddicken@uni-hamburg.de

Mike Thelwall School       of   Technology,   University   of   Wolverhampton,
Wolverhampton, UK
M.Thelwall@wlv.ac.uk
Contributors                                                           xi

Sabine Trepte University of Hamburg, Hamburg, Germany
sabine.trepte@uni-hamburg.de

Patti Valkenburg University of Amsterdam, Amsterdam, The Netherlands
p.m.valkenburg@uva.nl

Asimina Vasalou University of Bath, Bath, UK
minav@luminainteractive.com

Jessica Vitak Michigan State University, East Lansing, MI, USA
vitakjes@msu.edu

Joseph B. Walther Michigan State University, East Lansing, MI, USA
jwalther@msu.edu

Mike Z. Yao City University of Hong Kong, Hong Kong, PR China
mike.yao@cityu.edu.hk

Marc Ziegele University of Mainz, Mainz, Germany
ziegele@uni-mainz.de
.
     Part I
Approaches
Chapter 1
Introduction to Privacy Online

Joseph B. Walther




Even before the various networks supporting online communication converged as
the Internet, tensions existed between users‚Äô desires to communicate online in very
personal ways and their assumptions that their disclosures would or should be
treated as privileged and private. These tensions have not abated with the advent
of social media. Just as it was with the most bare-bones, text-based online
communities of the past, it is with contemporary media: The more users disclose
of themselves, the more they may enjoy the benefits these systems have to offer. At
the same time, the more they disclose, the more they risk what they themselves
consider breaches of their privacy. In light of this ongoing issue, this volume is not
only timely in the manner in which it addresses these tensions as they are manifest
in contemporary social media platforms, it also contributes to a tradition of research
on the dualism of privacy, privilege, and social interaction that online communica-
tion has incurred as far back as (or farther than) the advent of the Internet itself.
   Three complicating factors that have and continue to confront users of online
systems include (1) a misplaced presumption that online behavior is private, (2) that
the nature of the Internet at a mechanical level is quite incommensurate with
privacy, and (3) that one‚Äôs expectation of privacy does not constitute privileged
communication by definition.
   Perhaps it is due to the analogous offline activities which online communication
resembles or replaces, that many Internet users notoriously post information online
which they do not anticipate will be seen by others than the specific group they
imagined when posting. A personal face-to-face conversation is fleeting. A phone
call is most likely to be confined to the dyad that conducts it. A social party on held
private property is presumably self-contained. These settings allow participants to
maintain their sense of privacy consistent with the definitions reflected in Stephen
Margulis‚Äôs Chap. 2, that focus on individuals determining for themselves when,



J.B. Walther (*)
Michigan State University, East Lansing, MI, USA
e-mail: jwalther@msu.edu

S. Trepte and L. Reinecke (eds.), Privacy Online,                                    3
DOI 10.1007/978-3-642-21521-6_1, # Springer-Verlag Berlin Heidelberg 2011
4                                                                           J.B. Walther


how, and to what extent their communications are transmitted to others (except of
course by hearsay rather than by duplication and transmission). The presumptions
accompanying these precedent settings may be hard to dispel, and it may be
difficult for Internet users (at least those who are not digital natives) to recognize
that online exchanges are neither fleeting nor confined. This divergence has led to
many surprises and disappointments. These include the notorious anecdotal reports
of students or employees being terminated or punished as a result of posting
depictions of or statements reflecting illegal, insulting, or foolish behavior on
their social network profiles.
    These disparities between traditional communication settings and new media
may be due in large part to the mechanical infrastructure of the Internet. The
psychological privacy afforded by communication channels may lull users into a
false assumption of informational privacy, a central distinction that informs the
thesis of Sabine Trepte and Leonard Reinecke‚Äôs Chap. 6. This may be true of
the phone call and the conventional letter (which can also be intercepted), as well
as the Internet. But the Internet is, at its root, a store-and-forward technology. That
is, in order for the Internet to work as it does it must be able to capture, retain, and
transmit the information which users enter into it (see Walther 2002). This differs
from face-to-face, telephonic, and written exchanges. Yet many Internet users fail
to realize that something once put online more or less stays online and may be
retrieved by others and replicated, despite the subsequent inclination or efforts of
the original poster to protect or remove it. Moreover, the nature of systems‚Äô
architectures facilitate, if not determine, the propagation of social information, an
argument articulated in contemporary terms in Zizi Papacharissi and Paige
Gibson‚Äôs work in Chap. 7 that includes ‚Äúsharability‚Äù among the characteristics
defining social media‚Äôs very makeup.
    Users also frequently believe that the expectation of privacy that they had
when conversing or posting online constitutes some legal protection against that
information being shared. Although the expectation of privacy does indeed
privilege certain forms of communication under US law, the domains to which
these legal restrictions apply are far more narrow than many Internet privacy
advocates suggest. That is, the law privileges only conversations between patients
and their doctors or therapists, and attorney-client conversations. Yet the myth
prevails that any conversation is privileged that took place with an expectation of
privacy, however misplaced that expectation may have been, contributing to what
Bernhard Debatin refers to in Chap. 5 as ‚Äúignorance and a false sense of security
(that) play an important role‚Äù in users‚Äô approach to the privacy of their online
postings.
    This position has been propagated by numerous researchers who have argued
that if Internet users believe that they communicate privately online, then it is
unethical and may be illegal to analyze their messages for research purposes and
that human subjects review boards should almost never allow it (Frankel and Sang
1999; see also Hudson and Bruckman 2004; McArthur 2001). Counterarguments
have been raised along the lines that, again according to US legal doctrines,
messages that have been captured and stored in a publically-accessible space
1 Introduction to Privacy Online                                                    5


have no privilege whatsoever (Walther 2002) aside from copyright protection
(Jacobson 1999), and that the analysis of such messages requires no more human
subjects protections than analyzing newspaper content. It is clear that journalists
who wish to quote from publically-available online communities and other social
media do so quite regularly and without seeking permission, as discussed by
Wiebke Loosen in Chap. 15, and as Jan-Hinrik Schmidt discusses in Chap. 12,
Twitter users ‚Äúretweet‚Äù others‚Äô messages without reservation to audiences unin-
tended by the original source. By definition and in practice, it appears, if anyone in
the Internet-using public can see one‚Äôs messages, the messages are in the public
domain.
    In light of this, educating users about their online footprints seems to be a more
promising objective than to change laws or admonish researchers and other viewers
to behave differently with respect to online information. As Mike Yao points out in
Chap. 9, despite norms and customs affecting ‚Äúprivacy issues offline, to which a set
of well-established cultural, social, and legal norms may be applied, the burden of
online privacy protection is primarily shouldered by an individual‚Äôs own conscious
effort.‚Äù More effective efforts should be devoted to helping users to understand the
nature of the Internet in order to develop, according again to Debatin (Chap. 5), ‚Äúan
enlightened understanding of technology and its unintended consequences‚Äù in
terms of a ‚Äúprivacy literacy that enables them to. . .make educated choices.‚Äù Yao
(Chap. 9) depicts what may be required in terms of shaping those choices in terms
of attitudes and subjective norms, while Kevin Lewis‚Äôs Chap. 8 shows how the
normative behavior of one‚Äôs Facebook friend network influences the behavior of
privacy setting adoptions over time.
    Just as history shows that controversies over online privacy are not new, it also
shows that technological efforts for the protection of privacy have a long line of
succession, especially in realms in which the Internet provides unique benefits to
its users. In Chap. 16, Jochen Peter and Patti Valkenburg describe the unique
affordances that Instant Messaging and social media offer adolescents for com-
munication that is vital to their development. Online communication, especially
that which may be done anonymously, pseudonymously, or confidentially, allows
for the exploration of identity generally and for the examination of sexual identity
as well.
    Whereas Peter and Valkenburg limit their focus to adolescents, the use of the
Internet for identity exploration and sexual exploration by adults has also been a
focus of research and speculation for some time. In an adult context, similar
behaviors are described in exploratory or therapeutic rather than developmental
terms (Cooper et al. 1999; Turkle 1995, resp.). Such exchanges were frequently
noted on Multi-User Discussions (MUDs), where the pseudonymity provided by
these systems has been described as a critical enabling feature of such virtual spaces
for identity exploration (Stone 1995). Yet controversy arose even within these text-
only pseudonymous venues, when users who had developed strong relationships
with others through their pseudonymous selves felt betrayed at the outside publica-
tion of doubly-pseudonymized quotations (see Bruckman 2002), foreshadowing quite
precisely what boyd (2007, p. 2) has since characterized as the privacy-threatening
6                                                                          J.B. Walther


aspects of social network sites (‚Äúpersistence, searchability, exact copyability, and
invisible audiences‚Äù). Moreover, just as MUD users developed intimacy with one
another by divulging their secrets as well as their real-life names and email
addresses (Jacobson 1996; Parks and Roberts 1998). Like the text-based virtual
reality use of the past, ‚Äúsocial Web use offers advantages and gratifications that
increase in direct proportion to the degree of self-disclosure,‚Äù according to Monika
Taddicken and Cornelia Jers in Chap. 11 of this volume. Yet then as now such
intimacy comes at jeopardy of privacy, just as Debatin (Chap. 5) points out that for
contemporary users of social media, ‚Äútheir level of privacy protection is relative to
the number of friends, their criteria for accepting friends, and the amount and
quality of personal data provided‚Äù online. These risks can be mitigated somewhat,
according to Nicole Ellison and colleagues in Chap. 3, by limitations in friending
behaviors, privacy settings, and disclosures.
    Another form of Internet-enabled therapeutic exchange came as users asked for
and received advice on deeply personal issues on discussion systems such as Usenet
News. It appears that such personally-revealing and advice-oriented exchanges
remain valued activities among older Internet users today, according to Wiebke
Maa√ü in Chap. 17. When Usenet was at its peak, individuals who posted to some of
its discussions shielded their identities through the use of anonymous remailers.
They often did so when addressing stigmatizing issues such as certain illnesses,
sexual dysfunctions, or psychological problems. Anonymous remailers posted
messages to Usenet without the user‚Äôs identifying address (see Bacard 2010). By
appending a pseudonym to the message instead, users could track which replies
subsequently developed that addressed their own original posting. They could post
follow-up messages using the same pseudonym via such systems. Traceable
remailers kept a record of the original sender‚Äôs address, so that other users could
respond by email to the pseudonymous address, whereupon the remailer sent replies
back to the original sender. Indeed, anonymity was one of the major attractions for
the use of online versus offline social support (Walther and boyd 2002), where,
unlike offline social support, both men and women communicated similarly
(cf. Mike Thelwall in Chap. 18). Despite growing technological sophistication of
anonymous remailers, their use for slander, copyright violations, or potentially
subversive political whistle-blowing (much as WikiLeaks provides today) made
them susceptible to international subpoenas calling on their operators to reveal the
identity of users and thereby abridge the privacy such systems offered. This led the
most famous of these systems, anon.penet.fi, to be shut down by its operator rather
than be opened to police (see http://w2.eff.org/Privacy/Anonymity/960830_penet_closure.
announce). The rise of alternative and easier-to-use web applications has displaced
both MUDs and Usenet discussions to a great extent, yet as Peter and Valkenburg
make clear, newer systems still benefit users‚Äô psychosocial development by
providing apparently private communication opportunities.
    Yet even in contemporary social media, with full view of one‚Äôs name and a
plethora of identifying features, users actively manage their online self-
presentations, as Nicole Kr‚Ç¨amer and Nina Haferkamp detail in Chap. 10. Indeed,
social network sites enable individuals the ‚Äúmass management of real world ties,‚Äù
1 Introduction to Privacy Online                                                            7


as Marc Ziegele and Oliver Quiring suggest in Chap. 13. These tendencies sit rather
uncomfortably alongside Joinson and colleagues‚Äô assertion in Chap. 4 that social
network sites provide to at least those whom individuals have granted certain
privileges a ‚Äúradical transparency‚Äù about a profile owner‚Äôs self and behaviors,
that may even include, as Maren Hartmann‚Äôs Chap. 14 points out, the disclosure
of individuals‚Äô geographic locations by their location-aware mobile phones. It is
somewhat paradoxical that, on the one hand, ‚Äúsocial network sites. . .are thriving on
users‚Äô willingness to disclose and consume personal information,‚Äù as Joinson et al.
reflect, plus the fact most of one‚Äôs Facebook ‚Äúfriends‚Äù are known to a profile owner
offline to at least some extent (Ellison et al. 2007), but that, on the other hand,
impression management activity remains fertile within these sites.
    The paradox may be resolved to some extent by noting that impression manage-
ment has limited and unintended effects. Facebook users can readily identify
elements on their own profiles (including their online photos) and in those of
their friends that are distorted and not quite true offline (DeAndrea and Walther
in press). Although they excuse themselves and their close friends for such
exaggerations, they attribute greater hypocrisy and blame for such distortions to
those of their friends who they know less well. It is unclear whom individuals are
trying to mislead with these inaccurate self-presentations, given the radical trans-
parency of which Joinson and colleagues write. Perhaps it is themselves, as another
part of the psychosocial development that Peter and Valkenburg describe of
adolescents.
    In sum, the chapters in this book offer readers much more than a thorough and
contemporary treatment of online privacy and the social web. They offer a sophis-
ticated collection of installments on topics that are quite traditional in their concern
and quite under development as Internet communication technologies continue to
evolve. They offer a glimpse of the future as well, not only by exploring emergent
issues that are arising with new technological applications. They do so by
suggesting theory-based research agendas that can guide inquiry beyon the current
incarnation of social technologies, just as the privacy issues that arose with the
development of earlier Internet communication technologies have morphed but
remain with us today.



References

Bacard A (2010) Anonymous remailer F.A.Q. http://www.andrebacard.com/remail.html.
   Accessed Mar 2011
boyd d (2007) Why youth (heart) social network sites: the role of networked publics in teenage
   social life. http://www.danah.org/papers/WhyYouthHeart.pdf. Accessed Dec 2010
Bruckman A (2002) Studying the amateur artist: a perspective on disguising data collected in
   human subjects research on the Internet. Ethics Info Technol 4:217‚Äì231
Cooper A, Scherer CR, Boies SC, Gordon BL (1999) Sexuality on the Internet: from sexual
   exploration to pathological expression. Prof Psychol Res Pract 30:154‚Äì164
DeAndrea DC, Walther JB (in press) Attributions for inconsistencies between online and offline
   self-presentations. Commun Res
8                                                                                 J.B. Walther


Ellison N, Steinfield C, Lampe C (2007) The benefits of Facebook ‚Äúfriends‚Äù: social capital and
    college students‚Äô use of online social network sites. J Comput Mediat Commun 12:1143‚Äì1168;
    Article 1. http://jcmc.indiana.edu/vol12/issue4/ellison.html
Frankel MS, Sang S (1999) Ethical and legal aspects of human subjects research on the Internet:
    a report of a workshop June 10‚Äì11, 1999. http://www.aaas.org/spp/dspp/sfrl/projects/intres/
    report.pdf. Accessed 15 May 2002
Hudson JM, Bruckman A (2004) ‚ÄúGo away‚Äù: participant objections to being studied and the ethics
    of chatroom research. Info Soc 20:127‚Äì139
Jacobson D (1996) Contexts and cues in cyberspace: the pragmatics of naming in text-based
    virtual realities. J Anthropol Res 52:461‚Äì479
Jacobson D (1999) Doing research in cyberspace. Field Methods 11:127‚Äì145
McArthur RL (2001) Reasonable expectations of privacy. Ethics Info Technol 3:123‚Äì128
Parks MR, Roberts LD (1998) ‚ÄúMaking MOOsic‚Äù: the development of personal relationships
    on line and a comparison to their off-line counterparts. J Soc Pers Relat 15:517‚Äì537
Stone AR (1995) The war of desire and technology at the close of the mechanical age. MIT Press,
    Cambridge
Turkle S (1995) Life on the screen: identity in the age of the Internet. Simon & Schuster,
    New York
Walther JB (2002) Research ethics in Internet-enabled research: human subjects issues and
    methodological myopia. Ethics Info Technol 4:205‚Äì216; Rpt. http://www.nyu.edu/projects/
    nissenbaum/ethics_wal_full.html
Walther JB, boyd s (2002) Attraction to computer-mediated social support. In: Lin CA, Atkin D
    (eds) Communication technology and society: audience adoption and uses. Hampton Press,
    Cresskill NJ, pp 153‚Äì188
Chapter 2
Three Theories of Privacy: An Overview

Stephen T. Margulis




2.1     Introduction

This chapter reviews the current most important theories of privacy.1 The review is
addressed to those unfamiliar with theories of privacy. It is my goal to provide those
readers with a foundation on which to build. To this end, the chapter summarizes
the two best articulated and best supported theories of privacy (Altman 1975;
Westin 1967) as well as Petronio‚Äôs (2002) communication privacy management
(CPM) theory, an important extension of Altman‚Äôs theory that is particularly suited
for the study of social networking. Additionally, this chapter considers two larger
issues about what privacy is: issues in defining privacy and lessons to be learned
from Altman‚Äôs and Westin‚Äôs theories. I begin with the three theories of privacy.
   Irwin Altman‚Äôs and Alan Westin‚Äôs theories were selected because they have
stood the test of time. Both figure prominently in major reviews of privacy in the
1970s (Margulis 1977), 1980s (Sundstrom 1986, Chap. 13), and 1990s (Newell
1995). Moreover, they have paved the way for others, particularly Petronio‚Äôs CPM
theory.




1
 This chapter draws heavily on two articles by the author in the Journal of Social Issues (Margulis
2003a, b). The author wishes to thank Wiley-Blackwell for allowing the use of this material. I wish
to thank Sandra Petronio for her very helpful review of her theory and for providing published and
unpublished material.
S.T. Margulis (*)
Grand Valley State University, Grand Rapids, MI
e-mail: margulis@gvsu.edu

S. Trepte and L. Reinecke (eds.), Privacy Online,                                                9
DOI 10.1007/978-3-642-21521-6_2, # Springer-Verlag Berlin Heidelberg 2011
10                                                                                S.T. Margulis


2.2      Westin‚Äôs Theory

Westin‚Äôs (1967) theory of privacy addresses how people protect themselves
by temporarily limiting access of others to themselves. For Westin (1967, p. 7)
     Privacy is the claim of individuals, groups, or institutions to determine for themselves
     when, how, and to what extent information about them is communicated to others.
     [Moreover] . . . privacy is the voluntary and temporary withdrawal of a person from the
     general society through physical or psychological means. . ..

    Westin (1967) proposes that people need privacy. Privacy, in concert with other
needs, helps us to adjust emotionally to day-to-day interpersonal interactions.
For Westin, privacy is both a dynamic process (i.e., over time, we regulate privacy
so it is sufficient for serving momentary needs and role requirements) and a non-
monotonic function (i.e., people can have too little, sufficient, or too much privacy).
Westin specifically limits his theory to Western democracies because privacy is
consistent with the sociopolitical values of these democracies. For Westin, privacy
is neither self-sufficient nor an end in itself, but a means for achieving the overall
end of self-realization.
    Westin postulates four states of privacy. Solitude is being free from observation
by others. Intimacy refers to small group seclusion for members to achieve a close,
relaxed, frank relationship. Anonymity refers to freedom from identification and
from surveillance in public places and for public acts. Reserve is based on a desire
to limit disclosures to others; it requires others to recognize and respect that desire.
The states are the means by which the functions (purposes or ends) of privacy are
achieved. The states are, in effect, the ‚Äúhows‚Äù of privacy.
    Westin also posits four functions (purposes) of privacy. They are, in effect, the
‚Äúwhys‚Äù of privacy. Personal autonomy refers to the desire to avoid being
manipulated, dominated, or exposed by others. Emotional release refers to release
from the tensions of social life such as role demands, emotional states, minor
deviances, and the management of losses and of bodily functions. Privacy, whether
alone or with supportive others, provides the ‚Äútime out‚Äù from social demands,
hence opportunities for emotional release. Self-evaluation refers to integrating
experience into meaningful patterns and exerting individuality on events. It
includes processing information, supporting the planning process (e.g., the timing
of disclosures), integrating experiences, and allowing moral and religious contem-
plation. The final function, Limited and protected communication, has two facets:
limited communication sets interpersonal boundaries; protected communication
provides for sharing personal information with trusted others (Westin 1967).
    For Westin (1967), privacy operates at the individual, group, and organizational/
institutional levels. This is an early statement of the multiple levels often associated
with privacy (cf. Petronio 2002). Although Westin‚Äôs definition of privacy is often
cited, it is his privacy states and functions that have occasioned research.
The research supports (to varying degrees) and extends the states and functions;
it examines the relationships between the states and functions; it applies the states
2 Three Theories of Privacy: An Overview                                             11


and functions to specific contexts (see Margulis 2003b, pp. 413‚Äì415, for a summary
of this research).
   Nevertheless, possibly because Westin is a political scientist and lawyer, and
not a behavioral scientist, questions remain. Do Westin‚Äôs four functions flow into
one another? Do they co-occur or overlap in time or do they occur independently?
Do specific dimensions of privacy underlie Westin‚Äôs states? Are privacy factors
organized hierarchically? Can the functions be understood as traits? Finally,
Westin‚Äôs endorsement of organizational-level privacy is problematic because he
models the organization on an individual who acts alone rather than as a collective.
(See Margulis 2003b, p. 418, for supporting information and citations.)



2.3    Altman‚Äôs Theory

Altman, like Westin, has influenced how we understand privacy. Altman‚Äôs analysis
of privacy focuses on individual and group privacy and behavior (i.e., privacy-
regulating mechanisms) operating as a coherent system. He takes a dynamic and a
dialectical perspective on privacy regulation (i.e., it is a process that paces and
regulates interaction with others; we change how open or closed we are in response
to changes in our internal states and external conditions) (Altman 1990; Margulis
1977). Because Altman is a social and an environmental psychologist, social
interaction is at the heart of his theory and Altman uses the environment to provide
mechanisms for regulating privacy.
    Privacy, for Altman, is ‚Äúthe selective control of access to the self‚Äù (1975, p. 24).
Privacy has five properties. Firstly, privacy involves a dynamic process of interper-
sonal boundary control. Secondly, Altman differentiates desired and actual levels
of privacy. Thirdly, privacy is a non-monotonic function, with an optimal level of
privacy (desired ¬º actual level) and possibilities of too much privacy (actual >
desired level) (e.g., crowding) and too little (desired > actual level) (e.g., social
isolation). Fourthly, privacy is bi-directional, involving inputs from others (e.g.,
noise) and outputs to others (e.g., oral communication). Fifthly, privacy operates
at the individual and group level (Altman 1975; Margulis 1977).
    For Altman, there are multiple behavioral mechanisms for regulating privacy
(e.g., territorial behavior, cultural norms) that operate as a coherent system. Conse-
quently, one mechanism can substitute for another (e.g., a nod of approval for the
word ‚Äúyes‚Äù), can amplify another (e.g., shout ‚Äúno‚Äù and slam a door shut), or
can modulate another (e.g., offer an apology for locking one‚Äôs door). Moreover,
Altman posits a hierarchy of privacy functions, the most central of which is creating
self-identity.
    In Altman‚Äôs approach, three features of privacy are particularly important.
Firstly, privacy is inherently a social process. Secondly, a proper understanding
of psychological aspects of privacy must include the interplay of people, their social
world, the physical environment, and the temporal nature of social phenomena
(Altman 1990). Thirdly, privacy has a cultural context; specifically, privacy is a
12                                                                      S.T. Margulis


cultural universal but psychological manifestations are culturally-specific (Altman
1975, 1977).
   Altman‚Äôs theory has received impressive empirical support (see Margulis 2003b,
p. 419, for a summary). It also has stimulated theory development by others
(see Margulis 2003b, pp. 419, 421, 422). Lastly, Altman‚Äôs theory of privacy is
sufficiently comprehensive to be a general theory about the regulation of social
interaction (Margulis 1977).
   The central issue with Altman‚Äôs theory is whether his boundary concept is a
metaphor or a theoretical construct. In this regard, Petronio (2002), whose theory
builds on Altman‚Äôs ideas, regards it as a metaphor.




2.4   Petronio‚Äôs CPM (Communication Privacy Management)
      Theory

The most valuable privacy theory for understanding interpersonal computer-
mediated communication, such as blogging and social networking, was stimulated
by Altman‚Äôs dialectical conception of privacy as a tension between opening and
closing a personal boundary to others (see Child et al. 2009). That theory is
Petronio‚Äôs (2002) CPM (communication privacy management) theory.
    In CPM theory, privacy boundaries can range from complete openness to
complete closedness or secrecy. An open boundary reflects willingness to grant
access to private information through disclosure or giving permission to view that
information, thus representing a process of revealing. On the other hand, a closed
boundary represents information that is private and not necessarily accessible, thus
characterizing a process of concealing and protecting. The relationship between the
boundaries is dialectical, consistent with Altman‚Äôs thesis, because we continuously
adapt our level of privacy and disclosure to internal and external states because we
simultaneously need to be open and social as well as private and preserve our
autonomy. Moreover, we achieve desired levels of privacy and disclosure through
the use of privacy rules. That is, when we make a decision to disclose private
information, we use a rule-based privacy management system that regulates the
degree of boundary permeability (how much is told) and that manages linkages
(who we want to know the information) and the level of shared ownership with
others. Using this rule-based management system allows CPM theory to consider
how decisions are made about revealing and concealing private information
(Petronio 2002).
    Five propositions underpin CPM theory (Petronio and Durham 2008). The first
proposition is that private information is defined in terms of ownership in that when
people believe the information belongs to them, they count it as private. The second
is that because they define private information as something they own, they therefore
believe they have the right to control the distribution of that information (Petronio
and Reierson 2009). The third is that people develop and use privacy rules, based on
2 Three Theories of Privacy: An Overview                                            13


personally important criteria, to control the flow of private information. These rules
impact the management of individual and collective (i.e., dyadic and group) privacy
boundaries. Individual privacy rules are based on cultural values, gendered orien-
tations, motivational needs, contextual impact, and risk-benefit ratio criteria. The
fourth is that once private information becomes shared, a collective privacy bound-
ary is formed and others receiving private information become co-owners of that
information. From the perspective of the original owner, co-owners have fiduciary
responsibilities to manage and therefore jointly control this private information in a
way that is consistent with the original owner‚Äôs rule. Privacy rule coordination
between the original owner and co-owner is negotiated and revolves around
decisions about permeability, co-ownership responsibilities, and linkage rules.
Linkage rules determine who else can know (become a co-owner of) the informa-
tion. Permeability rules determine how much others can know about the informa-
tion. Ownership rules determine how much control co-owners have over co-owned
information. (For an instrument to measure these three factors, see Child et al. 2009.)
These rules might be implicit (e.g., based on a person‚Äôs assumption that the other
person has learned the requisite rules/norms) or explicit because of a need to clarify
or modify an existing rule or to introduce/negotiate a new rule (Child et al 2009;
Petronio 2002). These privacy rules are dynamic: they change, grow, or remain
stable for periods (Petronio 2002).
    Privacy rules also have several attributes (Petronio 2002). Firstly, privacy
rules may become so routine that they form the basis for privacy orientations.
Routinization can be aided by the use of sanctions to control the use of privacy
rules. Nevertheless, these rules are often subject to change. Secondly, we must
manage our individual and collective boundaries. Collective boundaries require
interpersonal coordination (see Petronio 2002, p. 32f, for a discussion of collective
coordination patterns). Thirdly, effective boundary management might fail. For
example, there can be boundary turbulence because a co-owner feels no obligation
to protect the discloser‚Äôs private information. Whatever the reason, ineffective
boundary management means that co-owners need to take corrective action to
ensure effective boundary management (Petronio 2002).
    The fifth proposition of Petronio‚Äôs CPM theory, as noted, is that when privacy
rules are not coordinated between the original owner and co-owner, there is a
possibility of boundary turbulence because people do not consistently, effectively,
or actively negotiate collective privacy rules. Boundary turbulence occurs when co-
owners fail to effectively control (manage) the flow of private information to third
parties.
    In sum, CPM theory extends Altman‚Äôs original proposal of privacy regulation,
as Altman has noted, by articulating ‚Äú[a] most complicated set of dynamics‚Äù and by
articulating the operation of communication privacy management at the individual,
dyadic, and group levels (Petronio 2002, p. xvi). And like Westin, Petronio also
focuses on the management of private information.
    For applications of CPM theory to interpersonal computer-mediated communi-
cation and blogging, see Child and Petronio (2011), Child et al. (2009), Child and
Agyeman-Badu (2010).
14                                                                      S.T. Margulis


2.5   What Privacy Is: Issues in Defining Privacy

Privacy is an elusive concept because it is an elastic concept (Allen 1988). The
psychological concept subsumes a wide variety of philosophical, legal, behavioral,
and everyday definitions. Moreover, the relationships between privacy and cognate
concepts (e.g., deception, secrecy, anonymity) are debatable because of
disagreements about the boundaries of privacy as a concept (see, e.g., Margulis
2003a, 2009). Also, in the moral domain, there is disagreement about whether
privacy is best understood as protecting ‚Äúbehavior which is either morally neutral or
valued by society‚Äù (Warren and Laslett 1977, p. 44), a common perspective, or
whether privacy also can support illegitimate activities, such as misuse of a public
office (Westin 1967), vandalism (Altman 1975), and morally dubious behavior
like lying (Derlega and Chaikin 1977). Lastly, there is no agreement on the proper
philosophical frame within which to define privacy. In this regard, the theories
of Altman, Petronio, and Westin are consistent with the limited-access perspective
(Allen 1988) but there are other perspectives. (See Tavani 2007, for four
perspectives, including limited access.)
    I examined the variability in definitions of privacy, primarily in psychological
analyses of privacy but also in studies of how people defined privacy (cf. Newell
1998). Based on my examination, I inductively derived ‚Äúan abstract skeleton‚Äù of the
means and ends of privacy: ‚ÄúPrivacy, as a whole or in part, represents control over
transactions between person(s) and other(s), the ultimate aim of which is to enhance
autonomy and/or to minimize vulnerability‚Äù (Margulis 1977, p. 10). This ‚Äúskeletal‚Äù
definition, so to speak, failed to note that, in the privacy literature, control over
transactions usually entailed limits on or regulation of access to self (Allen 1988),
sometimes to groups (e.g., Altman 1975), and presumably to larger collectives such
as organizations (e.g., Westin 1967). Because I inductively derived the definition
from a wide range of examples, it follows that the variation in specific definitions
reflects how the terms and the relationships among terms, in the abstract skeleton,
were interpreted within those definitions. In individual cases, it also reflected
the additional concepts and/or relationships that were included in a definition. For
example, the concept of control, in the abstract skeleton, has been interpreted
as social power (Kelvin 1973) and as personal control (Johnson 1974). Johnson‚Äôs
(1974) distinction between primary (direct) and secondary (indirect) personal
control over the attainment of privacy-related outcomes illustrates the use of an
additional concept.
    Although I concluded that the psychological concept emphasizes privacy as
control over or regulation of or, more narrowly, limitations on or exemption from
scrutiny, surveillance, or unwanted access (Margulis 1977), there have been (e.g.,
Pennock and Chapman 1971) and continue to be legal and philosophical analyses of
the meaning of privacy, some of which, as noted (e.g., Tavani 2007), would have us
go beyond the limited-access perspective (Allen 1988) or raise questions about the
boundaries of privacy (e.g., Davis 2009). In the final analysis, privacy remains an
2 Three Theories of Privacy: An Overview                                           15


elastic concept. Therefore, if you intend to use a behavioral theory of privacy, you
should determine whether its definition of privacy meets your requirements.



2.6    What Privacy Is: Lessons from Two Theories of Privacy

One way to examine the core of privacy is to compare the commonalities and
differences in the two best supported theories of privacy: the theories of Altman
(1975) and Westin (1967).
   Both theories discuss how individuals and groups control or regulate access to
themselves (i.e., both illustrate the limited-access approach). Both theories describe
our need for privacy as a continuing dynamic of changing internal and external
conditions, to which we respond by regulating privacy in order to achieve a desired
level of privacy. In turn, achieved privacy can affect internal states and external
conditions. Both agree that attempts to regulate privacy may be unsuccessful:
we may achieve more or less privacy than we desired. Both agree that privacy
can take many forms. Both agree that privacy has universal characteristics and
that the nature of the forms that privacy can take is probably culturally-specific.
Both agree that privacy can support illegitimate goals. Both differentiate the forms
(or the hows) from the functions (or the whys) of privacy. Both agree that the
functions of privacy include opportunities for self-evaluation and that privacy
contributes to self-identity and individuality. The principal difference is that
Altman‚Äôs theory is relatively inclusive of privacy phenomena because it emphasizes
social interaction but Westin‚Äôs is less so, often focusing on information privacy,
a subset of social interaction. (In this regard, CPM theory also focuses on informa-
tion privacy.) That two independent, well-supported theories share so much in
common suggests that they provide a reasonable foundation for understanding the
fundamentals of privacy as a psychological concept.
   Westin (2003) also has described three distinct empirically-derived (not
theoretically-derived) positions on privacy that the public holds. The High-Privacy
position assigns a high(er) value to privacy claims and seeks comprehensive
governmental interventions to protect privacy. (See Bennett 1995, for an overview,
and Lyon and Zuriek 1996, for examples of the High-Privacy position.) The
Balanced-Privacy position values privacy claims but advocates tailored (e.g., sec-
toral) governmental interventions to address demonstrated abuses as well as volun-
tary organizational initiatives to promote individual privacy. (See Etzioni 1999, and
Westin 1967, for different approaches to Balanced Privacy.) The Limited-Privacy
position usually assigns a lower value to privacy claims than to business efficiency
and societal-protection interests and it opposes governmental intervention as
unnecessary and costly. (For an example, see Singleton 1998.) I would add a variant
on the Limited-Privacy position, based on the claim that openness ought to trump
privacy. This position has its roots in humanistic psychology (e.g., Jourard 1971).
Interestingly, a contemporary advocate of this position is Mark Zuckerberg, the
founder and CEO of Facebook, currently the largest social networking site (Vargas
16                                                                                  S.T. Margulis


2010), although his motives have been questioned (e.g., Lyons 2010). As useful as
these three positions on privacy could be in research on privacy attitudes of social
media users, there are questions about the generalizability of these three positions
on privacy (Margulis et al. 2010).



References

Allen AL (1988) Uneasy access: privacy for women in a free society. Rowman & Littlefield,
   Totowa
Altman I (1975) The environment and social behavior: privacy, personal space, territory,
   crowding. Brooks/Cole, Monterey
Altman I (1977) Privacy regulation: culturally universal or culturally specific? J Soc Issues
   33(3):66‚Äì84
Altman I (1990) Toward a transactional perspective: a personal journey. In: Altman I, Christensen
   K (eds) Environment and behavior studies: emergence of intellectual traditions. Plenum,
   New York, pp 335‚Äì355
Bennett CJ (1995) The political economy of privacy: a review of the literature. Paper prepared for
   the center for social and legal research, DOE genome project (Final draft), University of
   Victoria, Department of Political Science, Victoria
Child JT, Agyeman-Badu E (2010) Blogging privacy management rule development: the impact
   of self-monitoring skills, concern for appropriateness, and blogging frequency. Comput Hum
   Behav 26:957‚Äì963
Child JT, Pearson JC, Petronio S (2009) Blogging, communication, and privacy management:
   development of the blogging privacy management measure. J Am Soc Inf Sci Technol
   60(1):2079‚Äì2094
Child JT, Petronio S (2011) Unpacking the paradoxes of privacy in CMC relationships: the
   challenges of blogging and relational communication on the internet. In: Wright K, Webb L
   (eds) Computer mediated communication in personal relationships. Hampton Press, Cresskill,
   pp 21‚Äì40
Davis S (2009) Privacy, rights, and moral value. In: Matheson D (ed) Contours of privacy.
   Cambridge Scholars Publishing, Newcastle upon Tyne, pp 153‚Äì179
Derlega V, Chaikin AL (1977) Privacy and self-disclosure in social relationships. J Soc Issues
   33(3):102‚Äì115
Etzioni A (1999) The limits of privacy. Basic Books, New York
Johnson CA (1974) Privacy as personal control. In: Carson DH (ed) Man-environment
   interactions: evaluations and applications (Part II, Vol. 6: Privacy, S.T. Margulis, vol. ed).
   Environmental Design Research Association, Washington, DC, pp 83‚Äì100
Jourard SM (1971) The transparent self. (rev edn). Van Nostrand Reinhold, New York
Kelvin P (1973) A social-psychological examination of privacy. Br J Soc Clin Psychol 12:248‚Äì261
Lyon D, Zuriek E (1996) Surveillance, privacy, and the new technology. In: Lyon D, Zureik E
   (eds) Computers, surveillance, and privacy. University of Minnesota Press, Minneapolis,
   pp 1‚Äì18
Lyons D (June 7, 2010) Facebook‚Äôs false contrition. Newsweek, p 20
Margulis ST (1977) Conceptions of privacy: current status and next steps. J Soc Issues 33(3):5‚Äì21
Margulis ST (2003a) Privacy as a social issue and a behavioral concept. J Soc Issues
   59(2):243‚Äì262
Margulis ST (2003b) On the status and contribution of Westin‚Äôs and Altman‚Äôs theories of privacy.
   J Soc Issues 59(2):411‚Äì429
Margulis ST (2009) Privacy and psychology. In: Matheson D (ed) Contours of privacy. Cambridge
   Scholars Publishing, Newcastle upon Tyne, pp 143‚Äì151
2 Three Theories of Privacy: An Overview                                                       17

Margulis ST, Pope JA, Lowen A (2010) The Harris-Westin index of general concern about
   privacy: an exploratory conceptual replication. In: Zuriek E, Stalker LH, Smith E, Lyon D,
   Chan YE (eds) Surveillance, privacy, and the globalization of personal information: interna-
   tional comparisons. McGill-Queen‚Äôs University Press, Montreal & Kingston, pp 91‚Äì109
Newell PB (1995) Perspectives on privacy. J Environ Psychol 14:65‚Äì78
Newell PB (1998) A cross-cultural comparison of privacy definitions and functions: a systems
   approach. J Environ Psychol 18:357‚Äì371
Pennock JR, Chapman JW (eds) (1971) Privacy: Nomos XIII. Atherton Press, New York
Petronio S (2002) Boundaries of privacy: dialectics of disclosure. State University of New York
   Press, Albany
Petronio S, Durham W (2008) Understanding and applying communication privacy management
   theory. In: Baxter LA, Braithwaite DO (eds) Engaging theories in interpersonal communication.
   Sage, Thousand Oaks, pp 309‚Äì322
Petronio S, Reierson J (2009) Regulating the privacy of confidentiality: grasping the complexities
   through CPM theory. In: Afifi T, Afifi W (eds) Uncertainty and information regulation
   in interpersonal contexts: theories and applications. Routledge, New York, pp 365‚Äì383
Singleton S (1998) Privacy as censorship: a skeptical view of proposals to regulate privacy in the
   private sector. (Policy Analysis No. 295) The Cato Institute, Washington, DC
Sundstrom E (1986) Workplaces: the psychology of the physical environment in offices
   and factories. Cambridge University Press, New York
Tavani H (2007) Philosophical theories of privacy: implications for an adequate online privacy
   policy. Metaphilosophy 38(1):1‚Äì22
Vargas JA (Sept 20, 2010) Letter from Palo Alto: the face of facebook. The New Yorker, pp 54‚Äì64
Warren C, Laslett B (1977) Privacy and secrecy: a conceptual comparison. J Soc Issues
   33(3):43‚Äì51
Westin AF (1967) Privacy and freedom. Atheneum, New York
Westin AF (2003) Social and political dimensions of privacy. J Soc Issues 59(2):431‚Äì453
Chapter 3
Negotiating Privacy Concerns
and Social Capital Needs in a Social
Media Environment

Nicole B. Ellison, Jessica Vitak, Charles Steinfield, Rebecca Gray,
and Cliff Lampe




3.1    Introduction

Social network sites (SNSs) are becoming an increasingly popular resource for both
students and adults, who use them to connect with and maintain relationships with a
variety of ties. For many, the primary function of these sites is to consume and
distribute personal content about the self. Privacy concerns around sharing infor-
mation in a public or semi-public space are amplified by SNSs‚Äô structural
characteristics, which may obfuscate the true audience of these disclosures due to
their technical properties (e.g., persistence, searchability) and dynamics of use (e.g.,
invisible audiences, context collapse) (boyd 2008b). Early work on the topic
focused on the privacy pitfalls of Facebook and other SNSs (e.g., Acquisti and
Gross 2006; Barnes 2006; Gross and Acquisti 2005) and argued that individuals
were (perhaps inadvertently) disclosing information that might be inappropriate for
some audiences, such as future employers, or that might enable identity theft or
other negative outcomes.
   The focus of this early work on negative outcomes of use, in the absence of
research that considered motivations for use, presented a confusing portrait of the
Facebook user. Our initial research exploring the ‚Äúbenefits of Facebook Friends‚Äù
(Ellison et al. 2007) was inspired by the discrepancy between high usage patterns
and a focus on negative outcomes. Our research has employed the social capital
framework as a way of exploring the positive outcomes of SNS use. A stream of
research by the authors has explored social capital outcomes of Facebook use
(Ellison et al. 2007, 2010, 2011; Steinfield et al. 2009). The social capital approach
has been replicated in other contexts, such as Valenzuela et al.‚Äôs (2009) study of
Facebook use and civic engagement.



N.B. Ellison (*) ‚Ä¢ J. Vitak ‚Ä¢ C. Steinfield ‚Ä¢ R. Gray ‚Ä¢ C. Lampe
Michigan State University, East Lansing, MI, USA
e-mail: nellison@msu.edu

S. Trepte and L. Reinecke (eds.), Privacy Online,                                    19
DOI 10.1007/978-3-642-21521-6_3, # Springer-Verlag Berlin Heidelberg 2011
20                                                                    N.B. Ellison et al.


   One question not yet addressed by scholarship in this area is the relationship
between privacy and social capital outcomes. Our conception of privacy speaks to
the ability of individuals to control when, to what extent, and how information
about the self is communicated to others (see Westin 1967; see also Chap. 2 of this
volume for a further elaboration on theories of privacy by Margulis). In many cases,
disclosing information about the self is necessary in order to reap the benefits from
these technological tools. After all, members of one‚Äôs social network cannot
suggest a new job possibility if they do not know s/he is looking, nor can they
offer social support if they do not know it is needed. By lowering the barriers
to communicating with a wider network of weak ties (Donath and boyd 2004;
Ellison et al. 2007), SNSs enable individuals to broadcast requests for support or
information. Self-disclosure is also a means by which individuals learn about and
develop relationships with one another (Berger and Calabrese 1975); however,
this process entails revealing information about the self that one might not want
to share with a wider audience.
   This chapter will consider how SNS users balance the desire to share personal
information (and thus potentially accrue the social capital benefits associated with
disclosure) and the need to control these disclosures (by minimizing the risks
associated with sharing private information). We describe three strategies by
which users can control the audience for their disclosures on SNSs: Friending
behaviors, managing audiences via privacy settings, and disclosures on the site.
Below we briefly discuss social capital, privacy, and information disclosure on
SNSs before presenting some preliminary findings about SNS privacy behaviors
and social capital.



3.2    Literature Review: Overview of Social Capital

The concept of social capital has received considerable attention across numerous
disciplines over the past three decades (Adler and Kwon 2002). Social capital
broadly refers to the accumulated resources derived from the relationships among
people within a specific social context or network (Bourdieu 2001; Coleman 1988;
Lin 2001; Portes 1998; Putnam 2000). Some have expressed concern that the
concept lacks theoretical and operational rigor ‚Äì for example, Portes (1998) notes
that conceptualizations of social capital can alternatively refer to the mechanisms
that generate it (the relationships between people) or its outcomes (the resources
one may obtain from these relationships). We emphasize social capital as
an outcome that stems from relationships among people. Hence, being embedded
in a network of relationships is a necessary precursor of social capital, but in and of
itself is not synonymous with social capital.
    Putnam (2000) distinguishes between two forms of social capital: one emanating
from weak ties that he calls bridging social capital, and a second that is derived
from strong or intimate ties like family relations, called bonding social capital.
Bridging social capital is best understood in relation to groundbreaking work by
3 Negotiating Privacy Concerns and Social Capital Needs in a Social Media Environment   21


Granovetter (1973), who observed that weak ties tend to be outside of one‚Äôs dense
local network and, by virtue of these ties having links to new people, help promote
the diffusion of non-redundant information. One‚Äôs strong ties, however, are likely
to be connected to each other, suggesting that much of the information flowing
through a close-knit network of relationships is redundant. Such strong ties are a
source of bonding social capital and are associated with trust, reciprocity, emotional
support, and tangible resource provision (Putnam 2000).
    More recently, researchers have examined how Internet use influences people‚Äôs
abilities to form and maintain social capital, given that it provides many new ways
to interact with a wide variety of others ranging from close contacts to relative
strangers (Resnick 2001; Wellman and Gulia 1999; Williams 2006). Ellison et al.
(2010) summarize this body of literature by grouping the findings into three basic
categories: (1) those that find that Internet use enables people to generate new social
capital (e.g., Hampton and Wellman 2003; Rheingold 1993), (2) those that find that
Internet use diminishes people‚Äôs stock of social capital (e.g., Kraut et al. 1998; Nie
2001), and (3) those that find that Internet use reinforces people‚Äôs offline
relationships and supplements social capital development (e.g., Quan Haase and
Wellman 2004; Uslaner 2000).
    We view social capital as a particularly relevant outcome to consider when
examining use of SNSs, given that many of the core features of such sites are
explicitly designed to facilitate the formation and maintenance of connections
among people ‚Äì connections that are sustained through communication about the
self. Our own and others‚Äô research in the past half decade provides strong empirical
support for the hypothesis that greater use of SNSs is associated with different types
of social capital benefits (Burke et al. 2010; Ellison et al. 2007, 2011; Steinfield
et al. 2008, 2009). Ellison et al. (2007) found that, even after controlling for a range
of demographic attributes, general Internet use, and psychological well-being, the
more intensely students used Facebook, the greater their reported bridging, bond-
ing, and maintained social capital. Steinfield et al. (2008) investigated bridging
social capital and Facebook use longitudinally, finding evidence for a causal effect
of SNS use on levels of bridging social capital. Research in organizational settings
also suggests a positive association between SNS use and both bridging and
bonding social capital (Steinfield et al. 2009). Ellison et al. (2011) extended this
work, finding that not all usage of Facebook resulted in social capital growth.
Rather, students who reported greater use of Facebook in a social information-
seeking capacity ‚Äì specifically to learn more about people with whom they had
some form of offline connection ‚Äì had higher levels of social capital. Finally, using
a sample of adult US Facebook users, Burke et al. (2010) found that more active
users of Facebook (i.e., those who engaged in directed communication) reported
higher levels of bridging and bonding social capital.
    While the general relationship between SNS use and social capital has been
established in a number of studies, to date no academic work has considered how
privacy relates to social capital in the SNS context. We take this question up in the
next section.
22                                                                    N.B. Ellison et al.


3.3   Privacy and SNSs: An Overview

In defining SNSs, boyd and Ellison (2008) assert that SNSs contain three
components that distinguish them from other online sites: (1) a user-constructed
public or semi-public profile, (2) a set of connections to other users within the
system, and (3) the ability to view one‚Äôs own list of connections, as well the
connections made by others in the system. Indeed, these public displays of
connections are a defining feature of SNSs, differentiating them from most other
forms of social media (Donath and boyd 2004). Decisions about whom to connect
with on SNSs are a key component of users‚Äô ability to control their personal
information. Similarly, users can control access to personal information through
their disclosure behaviors ‚Äì the kinds of information they include on their profile
or share via status updates. A third critical area, and the subject of much of
the literature, revolves around privacy settings.
   Previous research examining privacy on SNSs is in disagreement over how
privacy settings, Friending behaviors, and disclosures interact. For example,
while Acquisti and Gross (2006) found little relationship between privacy concerns
and certain types of disclosures, more recent studies have found that a high level of
privacy concerns leads to fewer disclosures on SNSs (Krasnova et al. 2010;
Stutzman et al. 2011). The relationship between these variables is further compli-
cated by the presence of multiple audiences (e.g., high school friends, family,
coworkers) within a single space (boyd 2008b), and users may employ a variety
of strategies to mitigate risks associated with disclosures made to unintended
audiences, such as using pseudonyms or employing advanced privacy controls.
   In an online realm where individuals may benefit from sharing personal infor-
mation, control over the audience for this information is critical. Privacy on SNSs
is a multi-faceted issue, requiring attention on the user‚Äôs part, both to protect
information from third-party data collection and to manage personal impressions
across a variety of contexts and relationships. The relationship between privacy
concerns and privacy behaviors is complex. Facebook users generally believe that
others in their network are more at risk than they are in regards to negative privacy-
related outcomes (Debatin et al. 2009). Past research on privacy and SNSs has
explored the relationship between privacy concerns and actual behavior on SNSs,
privacy ‚Äúviolations‚Äù that have left SNS users feeling vulnerable, and the distinction
between social privacy and institutional privacy. For example, Acquisti and Gross
(2006) found that one‚Äôs privacy concerns were a weak predictor of SNS use, and
that among those who had joined an SNS, there were no differences in the
likelihood to make disclosures such as one‚Äôs birthday, mobile number, or address
between those who reported a high level of privacy concerns and those who
reported low-level concern. Tufekci (2008) found similar results regarding the
relationship between privacy concerns and disclosures through an SNS, but also
found that students employed audience management strategies such as using a
nickname or adjusting profile visibility.
3 Negotiating Privacy Concerns and Social Capital Needs in a Social Media Environment   23


   Perhaps the greatest focus of SNS privacy literature has been user awareness of
settings and visibility to others using the site. It was not uncommon for early SNS
researchers to find Facebook users relatively unaware of the activity, accessibility,
and extent of their social networks despite reporting privacy concerns (Acquisti and
Gross 2006; Strater and Richter 2007). In their study of Facebook users‚Äô attention to
and use of privacy controls, Strater and Richter (2007) found that participants often
experienced difficulty navigating the privacy settings of the Facebook interface
during interviews, while Barnes (2006) observed that teenage SNS users appeared
unaware or ignorant of the public nature of the content they shared through the sites.
In more recent work, however, Stutzman and Kramer-Duffield (2010) found that
83% of respondents indicated using any Facebook privacy settings, while 58% of
respondents indicated they had made their Facebook profile Friends-only.
   Structural changes to Facebook have elicited public discussions about privacy
issues and SNS use. In September 2006, Facebook introduced the News Feed,
which aggregated the activities of a user‚Äôs Friends and presented them in a reverse
chronological order stream on the user‚Äôs homepage. This meant that behaviors that
were previously visible only by visiting one‚Äôs profile, such as adding a Friend or
joining a group, were highlighted in the News Feeds of one‚Äôs Facebook Friends.
The new visibility of Facebook activities inevitably left some Facebook users
feeling as though they needed to monitor actions they formerly performed without
hesitation; as boyd (2008a) wrote, ‚ÄúWith Facebook, participants have to consider
how others might interpret their actions, knowing that any action will be broadcast
to everyone with whom they consented to digital Friendship‚Äù (p. 16).



3.4    Identity and Information Disclosure in SNSs

While both research and popular narratives point to numerous privacy concerns
associated with using SNSs (Acquisti and Gross 2006; Lenhart and Madden 2007),
information disclosures on SNSs ‚Äì through one‚Äôs profile information, interactions
with other users, and the public display of one‚Äôs connections ‚Äì seem to be a
necessary component of accruing benefits from one‚Äôs network. As noted by Ellison
et al. (2010), the information provided in SNS profiles (e.g., contact information,
background data, personal characteristics) can lower the barriers to initial interac-
tion and facilitate formation of common ground. Studies indicate that trust and
willingness to share information were higher on Facebook, which requires users to
provide their real name, than on MySpace, which does not have such a requirement
(Dwyer et al. 2007). Furthermore, research by Mazer et al. (2009) found that
perceptions of credibility on an SNS increased with greater information disclosure.
   In reviewing the extant literature on self-presentation through SNS profiles,
Ellison et al. (2010) conclude that access to personal identity information supports
the relationship-formation process. Moving beyond purely ‚Äúsocial‚Äù SNSs such as
Facebook, DiMicco et al. (2009) provide support for this argument through a study
of workplace SNS use, finding that employees use profile information to engage in
24                                                                     N.B. Ellison et al.


‚Äúpeople sensemaking,‚Äù which the authors describe as ‚Äúthe process a person goes
through to get a general understanding or gist of who someone is‚Äù (p. 1). In other
words, information gathered from a user‚Äôs profile may aid in establishing common
ground, which, in turn, may facilitate communication and coordination processes
(Clark and Brennan 1991; Olson and Olson 2000). Research suggests that profile
information in Facebook may help users find common ground with one another
(DiMicco and Millen 2007; Dwyer et al. 2007; Lampe et al. 2007) and support
relationships. For example, Lampe et al. (2007) grouped profile elements into three
distinct categories ‚Äì referent, interest, and contact information ‚Äì and found that
the more information users completed in each of these profile categories, the greater
the size of their network, thus suggesting that disclosures within the profile aid
in relationship formation. Another category of information included in a user‚Äôs
profile ‚Äì the display of friend networks ‚Äì may also serve to establish common
ground and encourage more honest self-disclosures (see e.g., Donath and boyd
2004).
    In addition to the role that the public display of connections may play in vetting a
user‚Äôs identity, users may consider their audience prior to making disclosures
through an SNS. Recent work by Marwick and boyd (2011) and Hogan (2010)
has begun to consider how individuals navigate audiences through social media,
focusing on the concept of context collapse, or the idea that sites such as Facebook
flatten audiences and make it challenging to employ different self-presentational
strategies for different groups and individuals on the site. Privacy settings may help
segregate audiences, but as Hogan suggests, users may simply take a lowest
common denominator approach and only make disclosures that are appropriate
for all members of their network. As with other privacy-based concerns, SNS users
must balance concerns about their content being viewable by a variety of audiences
with their desire to receive benefits from interactions on the site.
    Recent research takes a more granular approach to exploring how user activity
influences overall outcomes on SNSs. This work suggests that in order to reap
benefits from use, dynamic disclosure beyond entering information into profile
fields is needed. Burke et al. (2010) obtained both server-level and survey data
from a large (N ¬º 1,193) sample of Facebook users and found that users who were
actively engaged with Facebook had higher levels of social capital and other
measures of well-being. They identified a ‚Äúconsumption‚Äù pattern of use (similar
to lurkers in other contexts) comprised of users who clicked on Friends‚Äô profiles
but did not contribute content themselves. This type of use was not associated with
greater social capital levels and, in fact, was associated with increased loneliness.
On the other hand, users who posted often and engaged in directed communication
with Friends reported higher bonding social capital. Similarly, Kim and Lee (2011)
find that honest self-presentation contributes indirectly to subjective well-being and
is mediated by perceived social support. They write, ‚ÄúFacebook friends are more
likely to provide support when they know that the user is in need for support; only
when such need is properly communicated through self-disclosure facilitated by
honest self-presentation are users likely to receive support from Facebook friends
(p. 362).‚Äù Other work has examined bloggers‚Äô self-disclosure behavior, finding a
3 Negotiating Privacy Concerns and Social Capital Needs in a Social Media Environment   25


similar relationship between self-reported disclosure and social capital measures
(Ko and Kuo 2009).
   In summary, research suggests that the provision of identity information and
other disclosures on SNSs are key to extracting relational benefits from their use,
but the large, diverse networks supported by SNSs can complicate these disclosures
through context collapse and other considerations.



3.5     A Preliminary Investigation of Privacy and Social Capital

The relationship between privacy and social capital is complex. At the most basic
level, it seems reasonable to assume that in order to accrue social capital benefits
from one‚Äôs social network, an individual must disclose information about the self,
which may entail privacy concessions. For example, a Facebook user who only
accepts friend requests from close offline friends may lack access to the bridging
benefits associated with having a diverse network of weaker acquaintances. Like-
wise, Facebook users who do not actively engage in direct interaction through the
site but instead spend their time reading content by others should be less likely to
reap bonding benefits, such as emotional support, through the site.
    We conducted two studies in 2010 that explored factors related to privacy and
social capital, including users‚Äô privacy settings, Friending habits, disclosures on the
site, and perceptions of social capital. The first was a survey of undergraduates at
Michigan State University, while the second included interviews with a national
sample of adult Facebook users. Below we discuss privacy-related findings of both
studies.



3.5.1    Quantitative Analysis of Undergraduates

Each year that we collect data from undergraduates on their use of Facebook, we
include a few items probing privacy and social capital variables. In our most recent
dataset, collected in March and April of 2010 (N ¬º 299), we asked questions about
their privacy settings, the types of disclosures they make on the site, and their
Friending behaviors (such as the number of Facebook Friends and the number of
‚Äúactual‚Äù friends in their Facebook network), as well as the bonding and bridging
social capital measures used in previous research (see Ellison et al. 2007).
   When looking specifically at possible privacy-enhancing behaviors, we asked
about two basic strategies: changing privacy settings from the default and limiting
specific content to individuals or groups within one‚Äôs network. We believe this
second item is of special interest when considering how users are managing
audiences within an SNS; by taking a more granular approach to restricting access
and distributing content, users may be more willing to make greater disclosures
through the site, which in turn, could lead to greater social capital gains. We found
26                                                                             N.B. Ellison et al.


that a majority of participants (78%) reported engaging in this strategy of restricting
access to content to specific Friends. To probe further into the relationship between
this behavior and our other variables of interest, we ran a series of independent
sample t-tests, using the advanced privacy settings measure as the grouping vari-
able. Significant differences emerged between those who reported using this feature
and those who had not for a number of variables. First, when looking at our social
capital measures, we found that participants who employed these privacy settings
reported higher perceived bonding and bridging social capital. Furthermore, this
group of participants reported having more Facebook Friends as well as more
‚Äúactual‚Äù friends within their Facebook network. See Table 3.1 for means, standard
deviations, and t-scores.
    Next, we focused on two Friending behaviors: the total number of Facebook
Friends a user has connected with through Facebook, as well as their perceptions
regarding how many Facebook Friends they consider to be ‚Äúactual‚Äù friends. As
these were both continuous variables, we created a dichotomous variable for each,
encompassing the lowest and highest quartiles of responses. Independent sample
t-tests revealed that, similar to our previous analysis, there were significant
differences in participants‚Äô reported social capital, such that participants reporting
the most Facebook Friends and actual (Facebook) friends reported greater per-
ceived bonding and bridging social capital than those reporting the fewest number
of Facebook and actual friends. See Table 3.2 for details.

Table 3.1 Results from independent sample t-tests for employing advanced privacy settings
                                                  Advanced privacy settings
                                  Have not used this feature       Have used this feature
Bridging SC, t(368) ¬º 3.64,
   p < 0.001                      M ¬º 3.60       S.D. ¬º 0.67       M ¬º 3.90        S.D. ¬º 0.69
Bonding SC, t(114) ¬º 2.324,
   p ¬º 0.022                      M ¬º 3.51       S.D. ¬º 1.03       M ¬º 3.79        S.D. ¬º 0.80
Facebook Friends,
   t(174) ¬º 4.08, p < 0.001      M ¬º 343.17     S.D. ¬º 223.26     M ¬º 462.40      S.D. ¬º 284.93
‚ÄúActual‚Äù friends,
   t(174) ¬º 3.12, p ¬º 0.002      M ¬º 161.97     S.D. ¬º 151.57     M ¬º 229.25      S.D. ¬º 217.53


Table 3.2 Results from independent sample t-tests for Friending behaviors
                                                          Friending behaviors
                                             Lowest quartile              Highest quartile
Facebook Friends
Bridging SC, t(189) ¬º 6.53, p < 0.001 M ¬º 3.47 S.D. ¬º 0.72 M ¬º 4.11 S.D. ¬º 0.64
Bonding SC, t(178) ¬º 5.32, p < 0.001 M ¬º 3.38 S.D. ¬º 0.90 M ¬º 4.00 S.D. ¬º 0.69
‚ÄúActual Friends‚Äù in Facebook network
Bridging SC, t(178) ¬º 6.66, p < 0.001 M ¬º 3.54 S.D. ¬º 0.81 M ¬º 4.20 S.D. ¬º 0.56
Bonding SC, t(184) ¬º 5.41, p < 0.001 M ¬º 3.46 S.D. ¬º 0.95 M ¬º 4.10 S.D. ¬º 0.69
For total Facebook Friends, lowest quartile is <240 Friends, highest quartile is 600+ Friends. For
actual friends, lowest quartile is <61 friends, highest quartile is 300+ friends
3 Negotiating Privacy Concerns and Social Capital Needs in a Social Media Environment   27


    Finally, to address how disclosures fit into this framework, we ran analyses
using a weak two-item original scale assessing participants‚Äô disclosure habits
through the site (registering their agreement with the statements, ‚ÄúWhen I‚Äôm
having a bad day, I post about it on Facebook‚Äù and ‚ÄúWhen I receive a good
grade in class, I post about it on Facebook‚Äù on a five-point Likert-type scale
ranging from Strongly Disagree to Strongly Agree). Participants indicated
a below-midpoint level of agreement with these statements (scale M ¬º 2.54,
S.D. ¬º 1.13), suggesting that they are not using Facebook as a way to share
certain types of personal information about themselves. Furthermore, we found
no significant relationship between this variable and privacy settings, social
capital, or Friending behaviors but suspect this is due to the weakness of this
measure. We expect that a better measure of disclosures, such as that employed
in Burke et al. (2010), or one that captures more interaction-based disclosures
happening outside the ‚Äústatus update‚Äù context, would be more likely to produce
insight into social capital and privacy behavior dynamics.
    Overall, we believe this initial analysis supports our conceptualization of multiple
possible privacy behaviors and their potential relationship to social capital, although
more granular measures and multivariate analyses are needed to flesh out these
dynamics more fully. For example, the positive relationship between use of advanced
privacy settings and the number of Friends (both total and actual) may reflect a
strategy by which users with larger Friend counts (which are more likely to include
those from different spheres) need to place these friends into groups, or the fact that
those who feel comfortable creating lists also feel more comfortable accepting
different types of people as Friends. The positive relationship between participants‚Äô
use of the advanced privacy settings and both bridging and bonding social capital
suggests that tools for managing audiences within an SNS may aid users‚Äô efforts to
maximize rewards derived from interactions with network members, perhaps
because users who are able to direct their disclosures to a subset of Friends may
actually disclose more deeply and honestly. This interpretation contains face validity,
especially in light of the positive relationship between both forms of social capital
and participants‚Äô reported Facebook and actual friends on the site (potentially
reflecting wider, more diverse networks and greater access to close friends).
    While our measure of disclosures was extremely limited, it could be that users
employ privacy settings as an effective means of managing the audiences for their
disclosures. For example, a college student who wants to post pictures from a
weekend party could block family members from seeing any content related to
the event. An alternative interpretation of the low level of agreement with our
disclosure measure is that users are employing the lowest common denominator
strategy (see Hogan 2010), in that they choose not to make disclosures that are
unsuitable for any of their audiences. This merits further research, especially when
considering that those who use advanced privacy settings and have more Friends on
the site report more bonding and bridging social capital.
28                                                                                 N.B. Ellison et al.


3.5.2       Qualitative Study of Adult Facebook Users

During late 2009 and early 2010, we conducted 18 in-depth interviews with adult
Facebook users aged 25‚Äì55 regarding their use of the site. Among the themes to
emerge, comments on privacy reflected a balancing of tensions, whereby several
users commented on their attempts to maximize benefits (i.e., gains in social
capital) while minimizing risks through strategies related to privacy settings,
disclosures, and Friending behaviors.
   Our participants exhibited a wide range of attitudes regarding the relationship
between privacy and disclosures made through Facebook. On one extreme, some
participants said that because they employed privacy settings to restrict access to
content, they freely shared content through the site. For example, a male participant
said that because he limited his profile to friends only, ‚Äúthere‚Äôs not much I won‚Äôt
post in there.‚Äù At the other extreme, one user‚Äôs privacy concerns were so high that
she rarely made disclosures of any kind through the site. When asked if she thought
her decision to not actively participate in the site made it less useful for her when
compared with other users, she agreed, saying, ‚ÄúI don‚Äôt get as much out of Facebook
as I think a lot of people that I know do.‚Äù
   Participants voiced a number of strategies for making disclosures through
Facebook while managing multiple audiences. For example, a female participant
said she did not post many status updates because she saw them as ‚Äúpolluting‚Äù her
Friends‚Äô pages with irrelevant information, which might have a negative impact on
people with whom she regularly interacted offline. A male graduate student
described Friend Lists so he could post updates about his teaching experiences
but make them non-viewable to specific groups, such as his former students or
current professors, saying, ‚ÄúWhenever I do post, people are kind of separated into
the limited profile, like the student group, and that kind of filters out what I would
say to those people anyway.‚Äù
   An older female participant‚Äôs comments most closely reflect users‚Äô attempts to
maximize rewards while minimizing risks of disclosure. She said she uses
Facebook because of its convenience in keeping in touch with her children,
extended family, and geographically dispersed friends, but she refrains from
going into depth in the content she posts to the site:
     It‚Äôs very public and I‚Äôm a private sort of person. So while some people would say [by] just
     being on Facebook, I‚Äôm sharing more about myself than they would consider reasonable or
     safe or whatever, I have limits to what I would post and, you know, things I won‚Äôt, so it just
     depends. There is a balance that you can be involved in a social networking site and share
     personal information, but without going overboard. . . I have my own level of privacy
     concerns and I don‚Äôt put a lot of things out there that other people seem to feel the need to
     share with the world.

   A final theme to emerge from our interviews that relates to our variables of
interest reflects the notion of ‚ÄúFacebook literacy‚Äù among older users, such that users
who may not be familiar with the various privacy settings available to control
content distribution may experience more negative outcomes of their use or may
3 Negotiating Privacy Concerns and Social Capital Needs in a Social Media Environment   29


use the site in ways that do not promote social capital benefits. For instance, a
number of the adults we spoke to commented that they were unsure about their
privacy settings or did not know how to limit content to specific Friends or groups
of Friends. We speculate that users with low Facebook literacy might be reluctant
to engage in certain kinds of interaction on the site because they are unsure how
to limit their audience, which, in turn, could lower the social capital benefits they
gain from those interactions. Alternatively, if this lack of understanding leads to
assumptions of privacy in a public or semi-public space, there could be negative
consequences for the discloser. For example, a male participant said he had become
more careful in posting content to Friends‚Äô pages after he got in trouble at work
because a Friend of a Friend saw a wall post he wrote that included negative
comments about a coworker. Based on these preliminary data from our interviews
with adult Facebook users, we suspect that efforts to increase user awareness about
our three privacy-related behaviors (especially those surrounding privacy settings)
are important for enabling those with low Facebook literacy to reap social capital
benefits from these tools.



3.6    Conclusion

In this chapter, we have argued for a conception of privacy that acknowledges that
users have many options for controlling access ‚Äì privacy settings are just one. Users
may also choose to limit their actual disclosures by reducing the number of
disclosures or limiting the content of their disclosures to mundane topics. Friending
criteria also play a role. For instance, very selective Friending is one strategy by
which users may control audiences. These three areas ‚Äì Friending, disclosures, and
privacy settings ‚Äì can be seen as operating in conjunction with one another. We
were not able to fully flesh out the relationships among these behaviors given our
current data, but hope that future investigations will utilize more granular measures
of social, technical, and communication-based activities to describe privacy
strategies. Research should also explore the interactions of these behaviors
among various populations. For example, two chapters in this book consider how
adolescents (see Peter and Valkenburg, this volume, Chap. 16) and seniors (see
Maa√ü, this volume, Chap. 17) navigate privacy and disclosures in an online space.
These populations, often neglected in academic studies, are migrating to SNSs at a
rapid rate, and their concerns and behaviors should be considered as well when
developing models of privacy online.
   In addition to focusing on user actions, considering the structural aspects of these
technologies themselves in relation to privacy is also important. For example, it can
be difficult for users to determine who can see which posting (e.g., to know who is
included in the ‚ÄúFriend of a Friend‚Äù group), which Friends are being displayed in
the News Feed, or what a privacy action (e.g., ‚Äúblocking‚Äù another user on
Facebook) will actually do. When on Facebook, for instance, it is fairly easy to
gain access to the photo album of a non-Friend after a mutual Friend comments on a
30                                                                          N.B. Ellison et al.


photo. While access to this kind of information may be positively related to
bridging social capital, which is associated with novel information from weak
ties, it may also result in negative personal or professional outcomes associated
with the unanticipated disclosure of information about the self to unintended
audiences. Helping users to understand how they can control their information by
using tools in the system, and aiding in understanding the implications of those
tools, allows users to choose how much they share and with whom. This kind of
knowledge, and the self-efficacy that accompanies it, will help enable users to
maximize the potential social capital benefits from these sites while minimizing the
harms that can accompany sharing some kinds of disclosures with some audiences.
As noted in our qualitative findings, the role self-efficacy plays in encouraging
social capital accrual through disclosures on SNSs must be considered. boyd and
Hargittai (2010) found that those with low overall Internet skills are less likely to
change their Facebook privacy settings and are less confident in doing so. If these
populations experience negative outcomes from their SNS use (due to less optimal
use of privacy settings) and fewer positive outcomes (because they are not
empowered to share disclosures that may be necessary to read these benefits),
they may be less likely to continue using these sites than those with higher levels
of Internet skills.
   In conclusion, we believe privacy behaviors on SNSs are not limited to privacy
settings; Friending behaviors and disclosures are also strategies by which users may
control their audience. The degree to which users employ these strategies may be
instrumental for gaining social capital and avoiding privacy risks because they give
users the opportunity to calibrate their disclosures to various subsets within their
overall Facebook network. The intersection of privacy and social capital is an
important topic, and we are hopeful that research continues to explore this topic
to help enable more equable access to ‚Äúthe benefits of Facebook Friends‚Äù (Ellison
et al. 2007).




References

Acquisti A, Gross R (2006) Imagined communities: awareness, information sharing, and privacy
   on the facebook. In: Privacy enhancing technologies: 6th international workshop, PET 2006,
   Springer, Cambridge, pp 36‚Äì58
Adler P, Kwon S (2002) Social capital: prospects for a new concept. Acad Manag Rev 27(1):17‚Äì40
Barnes S (2006) A privacy paradox: social networking in United States. First Monday 11(9): n.p
Berger CR, Calabrese RJ (1975) Some explorations in initial interaction and beyond: toward a
   developmental theory of interpersonal communication. Hum Commun Res 1:99‚Äì112
Bourdieu P (2001) The forms of capital. In: Granovetter M, Swedberg R (eds) The sociology of
   economic life, 2nd edn. Westview Press, Boulder, pp 96‚Äì111
boyd d (2008a) Facebook‚Äôs privacy trainwreck: exposure, invasion, and social convergence.
   Convergence 14:13‚Äì20
boyd d (2008b) Taken out of context: American teen sociality in networked publics. PhD
   Dissertation, University of California, Berkeley
3 Negotiating Privacy Concerns and Social Capital Needs in a Social Media Environment        31

boyd dm, Ellison NB (2008) Social network sites: definition, history, and scholarship. J Comput
    Mediat Commun 13:210‚Äì230
boyd d, Hargittai E (2010) Facebook privacy settings: who cares? First Monday 15(8)
Burke M, Marlow C, Lento T (2010) Social network activity and social well-being. In:
    Proceedings of ACM CHI 2010: conference on human factors in computing systems, ACM,
    New York, pp 1909‚Äì1912
Clark HH, Brennan SE (eds) (1991) Grounding in communication. APA Press, Washington, DC
Coleman JS (1988) Social capital and the creation of human capital. Am J Sociol 94(Supplement):
    S95‚ÄìS120
Debatin B, Lovejoy JP, Horn A, Hughes BN (2009) Facebook and online privacy: attitudes,
    behaviors, and unintended consequences. J Comput Mediat Commun 15:83‚Äì108
DiMicco JM, Millen DR (2007) Identity management: multiple presentations of self in facebook.
    In: Proceedings of the 2007 ACM conference on supporting group work, ACM Press, Sanibel
    Island, pp 383‚Äì386
DiMicco JM, Geyer W, Dugan C, Brownholtz B, Millen DR (2009) People sensemaking and
    relationship building on an enterprise social networking site. In: Proceedings of the 42nd
    Hawaii international conference on system sciences. (CD-ROM), Computer Society Press,
    Hawaii
Donath JS, boyd d (2004) Public displays of connection. BT Technol J 22(4):71‚Äì82
Dwyer C, Hiltz SR, and Passerini K (2007) Trust and privacy concern within social networking
    sites: a comparison of facebook and MySpace. In: Proceedings of the Americas conference on
    information systems 2007, AIS, Keystone
Ellison N, Steinfield C, Lampe C (2007) The benefits of Facebook ‚Äúfriends‚Äù: exploring
    the relationship between college students‚Äô use of online social networks and social capital.
    J Comput Mediat Commun 12:1143‚Äì1168
Ellison N, Lampe C, Steinfield C, Vitak J (2010) With a little help from my friends: how social
    network sites affect social capital processes. In: Papacharissi Z (ed) The networked self:
    identity, community, and culture on social network sites. Routledge, New York, pp 124‚Äì145
Ellison NB, Steinfield C, Lampe C (2011) Connection strategies: social capital implications of
    facebook-enabled communication practices. New Media Soc
Granovetter MS (1973) The strength of weak ties. Am J Sociol 78(1360):1480
Gross R, Acquisti A (2005) Information revelation and privacy in online social networks.
    In: Proceedings of the workshop on privacy in the electronic society, ACM, Alexandria,
    pp 71‚Äì80
Hampton K, Wellman B (2003) Neighboring in Netville: how the Internet supports community
    and social capital in a wired suburb. City Commun 2(4):277‚Äì311
Hogan B (2010) The presentation of self in the age of social media: distinguishing performances
    and exhibitions online. B Sci Technol Soc 30:377‚Äì386
Kim J, Lee JE (2011) The facebook paths to happiness: effects of the number of facebook friends
    and self-presentation on subjective well-being. CyberPsychol Behav Soc Netw 14:359‚Äì364
Ko H, Kuo F (2009) Can blogging enhance subjective well-being through self-disclosure?
    CyberPsychol Behav 12:75‚Äì79
Krasnova H, Spiekermann S, Koroleva K, Hildebrand T (2010) Online social networks: why we
    disclose. J Inf Technol 25:109‚Äì125
Kraut R, Patterson M, Lundmark V, Kiesler S, Mukopadhyay T, Scherlis W (1998) Internet
    paradox. a social technology that reduces social involvement and psychological well-being?
    Am Psychol 53:1017‚Äì1031
Lampe C, Ellison N,Steinfield C (2007) A familiar face(book): Profile elements as signals in an
    online social network. In: Proceedings of the SIGCHI conference on human factors in
    computing systems, ACM, New York, pp 435‚Äì444
Lenhart A, Madden M (2007) Social networking websites and teens: an overview. Pew Internet &
    American Life Project, Washington, DC
32                                                                                N.B. Ellison et al.


Lin N (2001) Building a network theory of social capital. In: Lin N, Cook K, Burt R (eds) Social
    capital theory and research. Transaction Publishers, New Brunswick, pp 3‚Äì30
Marwick AE, boyd d (2011) I tweet honestly, I tweet passionately: twitter users, context collapse,
    and the imagined audience. New Media Soc 13:113‚Äì114
Mazer JP, Murphy RE, Simonds CJ (2009) The effects of teacher self-disclosure via facebook on
    teacher credibility. Learn Media Technol 34:175‚Äì183
Nie NH (2001) Sociability, interpersonal relations, and the internet. Am Behav Sci 45:420‚Äì435
Olson GM, Olson JS (2000) Distance matters. Hum Comput Interact 15:139‚Äì178
Putam R (2000) Bowling Alone: The collapse and revival of American community. New York:
    Simon & Schoster
Portes A (1998) Social capital: its origins and applications in modern sociology. Annu Rev Sociol
    22:1‚Äì24
Quan-Haase A, Wellman B (2004) How does the internet affect social capital? In: Huysman M,
    Wulf V (eds) Social capital and information technology. MIT Press, Cambridge, MA,
    pp 113‚Äì135
Resnick P (2001) Beyond bowling together: socio-technical capital. In: Carroll J (ed) HCI in the
    new millennium. Addison-Wesley, New York, pp 647‚Äì672
Rheingold H (1993) The virtual community: homesteading on the electronic frontier. MIT Press,
    Cambridge, MA
Steinfield C, Ellison NB, Lampe C (2008) Social capital, self-esteem, and use of online social
    network sites: a longitudinal analysis. J Appl Dev Psychol 29:434‚Äì445
Steinfield C, DiMicco JM, Ellison NB, Lampe C (2009) Bowling online: social networking and
    social capital within the organization. In: Proceedings of the fourth international conference on
    communities and technologies, ACM, New York, pp 245‚Äì254
Strater K, Richter H (2007) Examining privacy and disclosure in a social networking community.
    In: Proceedings of the 3rd symposium on usable privacy and security 2007, ACM, New York,
    pp 157‚Äì158
Stutzman F, Kramer-Duffield J (2010) Friends only: examining a privacy-enhancing behavior in
    facebook. In: Proceedings of the 28th international conference on human factors in computing
    systems, ACM, New York, pp 1553‚Äì1562
Stutzman F, Capra R, Thompson J (2011) Factors mediating disclosure in social network sites.
    Comput Hum Behav 27:590‚Äì598
Tufekci Z (2008) Can you see me now? Audience and disclosure regulation in online social
    network sites. Bull Sci Technol Stud 11:544‚Äì564
Uslaner EM (2000) Social capital and the Net. Commun ACM 43(12):60‚Äì64
Valenzuela S, Park N, Kee K (2009) Is there social capital in a social network site?: facebook use
    and college students‚Äô life satisfaction, trust, and participation. J Comput Mediat Commun
    14:875‚Äì901
Wellman B, Gulia M (1999) Net surfers don‚Äôt ride alone: virtual communities as communities.
    In: Kollock P, Smith M (eds) Communities and cyberspace. Routledge, New York, pp 167‚Äì194
Westin AF (1967) Privacy and freedom. Atheneum, New York
Williams D (2006) On and off the Net: Scales for social capital in an online era. J Comput Mediat
    Commun 11:593‚Äì628
Chapter 4
Digital Crowding: Privacy,
Self-Disclosure, and Technology

Adam N. Joinson, David J. Houghton, Asimina Vasalou,
and Ben L. Marder




4.1    Introduction

In this chapter, we introduce and develop the concept of ‚Äúdigital crowding.‚Äù
Traditionally, crowding has been conceptualized as excessive social contact or
insufficient personal space (Altman 1975). Under these circumstances, not only
do people show signs of stress, but they also engage in a number of techniques to
escape excessive social contact (Baum and Valins 1977). For instance, studies of
students in shared, crowded spaces find that they spend more time in their bedrooms
than in social spaces, are more likely to seek friendships outside of the crowded
area, and even sit further away from strangers in waiting rooms (Baum and Valins
1977). We argue that while much of the discussion of privacy and technology has
focused on information flow and leakage, it has ignored the interactive, interper-
sonal impact of new technology. In this chapter, we begin by examining the key
issues raised by technology for privacy. We then discuss earlier, non-technology
focused theories that cover interpersonal aspects of privacy. Finally, we examine
some ways in which technology might impact on interpersonal privacy, with a
specific focus on social network sites.




4.2    Privacy, Technology, and Digital Crowding

Concerns about the privacy impact of new technologies are nothing new. Back in
1996, Schatz Byford argued that, ‚Äúat no time have privacy issues taken on greater
significance than in recent years, as technological developments have led to the


A.N. Joinson (*) ‚Ä¢ D.J. Houghton ‚Ä¢ A. Vasalou ‚Ä¢ B.L. Marder
University of Bath, Bath, UK
e-mail: A.Joinson@bath.ac.uk

S. Trepte and L. Reinecke (eds.), Privacy Online,                               33
DOI 10.1007/978-3-642-21521-6_4, # Springer-Verlag Berlin Heidelberg 2011
34                                                                   A.N. Joinson et al.


emergence of an ‚Äòinformation society‚Äô capable of gathering, storing and
disseminating increasing amounts of data about individuals‚Äù (Schatz Byford
1996, p. 1). In the UK, 11 million children‚Äôs details have become accessible to
the scrutiny of 390,000 trained professions (BBC News 2009a); workplace surveil-
lance is an established practice (BBC News 2003; Joinson and Whitty 2008); and
social network sites (SNSs) are thriving on users‚Äô willingness to disclose and
consume personal information (Joinson 2008) while at the same time they provide
users with mixed mechanisms for privacy protection (Bonneau and Preibusch
2009). Recent developments to increase the personalization of website experiences
also pose a problem, with customers who value informational transparency being
the least likely to accept personalization and profiling (Awad and Krishnan 2006).
   We are increasingly building Internet services that elicit ever more detailed
disclosure from individuals. One driver of this is the move towards more socialized
use of technology. For instance, most SNSs cease to function as intended if people
do not disclose information about themselves in the form of profiles, photographs,
status updates, or tweets and, increasingly, their location (e.g., Burke et al. 2009).
The most popular SNS, Facebook, has a strict ‚Äúreal name‚Äù policy, meaning that this
disclosure is usually connected to a non-anonymous individual who relies only on
the privacy settings of the site (and the trustworthiness of the organization behind
the site) to protect their privacy. This move towards increased sharing ‚Äì termed
‚Äúradical transparency‚Äù ‚Äì led Facebook founder Mark Zuckerberg to claim in 2010
that privacy is no longer a ‚Äúsocial norm‚Äù (BCS 2010). This ideological position is
based on two key assumptions ‚Äì firstly, that openness and transparency is a positive
force in society, and secondly, that openness is generally beneficial in interpersonal
relations. Facebook has ten ‚Äúprinciples‚Äù that outline this ideology ‚Äì the first being
‚Äúpeople should have the freedom to share whatever information they want, in any
medium and any format‚Äù (Facebook 2011). Other principles expound the impor-
tance of ‚Äúthe freedom to access all of the information made available to them by
others,‚Äù and, ‚Äúthe freedom to build trust and reputation through their identity and
connections.‚Äù However, this identity must be ‚Äúreal‚Äù ‚Äì the terms and conditions of
Facebook (Oct 2010 version) stipulate that users ‚Äúwill not provide any false
personal information on Facebook‚Äù (Facebook 2010). Indeed, Facebook already
prevents users from creating usernames with ‚ÄúFake‚Äù in the name, and employs
algorithms to attempt to distinguish ‚Äúreal‚Äù from ‚Äúfake‚Äù users (Breyer and
Zuckerberg 2005). This creeping transparency is not limited within Facebook ‚Äì
the use of Facebook Connect as an identity management system that allows users to
log onto other sites using their Facebook credentials further increases the spread of
personal, identifiable information across the Internet.
   The privacy issues raised by SNS use are well documented (e.g., Bonneau and
Preibusch 2009; Christofides et al. 2009). Users post personal, identifiable informa-
tion on their own and other‚Äôs profiles (Christofides et al. 2009; Young and Quan-
Haase 2009). They post, share, and tag photographs of themselves and others
(Binder et al. 2009; Gross and Acquisti 2005; Nov and Wattal 2009), update their
status with inappropriate information (BBC News 2009b), boast about illegal
activity (BBC News 2010), and openly discuss their personal relationships on
4 Digital Crowding: Privacy, Self-Disclosure, and Technology                          35


‚Äúwalls‚Äù (a semi-public forum) (Houghton and Joinson 2010). Such information
revelation can be detrimental to the user or can implicate others (Acquisti and Gross
2006, 2009; Christofides et al. 2009), and is often based on optional self-disclosure
and encouraged by site settings (Acquisti and Gross 2006; Bonneau and Preibusch
2009; Burke et al. 2009; Nov and Wattal 2009).
   It is not just self-disclosed information that puts users under threat but the visible
communications linked to them by ‚Äúfriends.‚Äù This co-creation of users‚Äô profiles is
carried out through actions such as wall posts, comments, and the tagging of photos
or location. Arguably, these activities may be thought to pose a greater risk than
disclosure by users themselves, for the reason that concerns over privacy and
possible harms may not be fully internalized by other users within the decision to
disseminate information (e.g., Houghton and Joinson 2010). Protection from this
can be offered through site privacy settings, which allow users control over who and
what can contribute to their online image, although these are often too simple or too
complex (Bonneau and Preibusch 2009).
   However, threats originate not only from users‚Äô and their friends‚Äô posting of
information but from outside access. While a user can be careful and deliberate in
what information they post, outside access can also result in privacy violations and
personal harm. The use of unsecured login connections by SNSs may allow third
parties easy access to account information (Gross and Acquisti 2005). The default
settings of SNSs allow profile pictures, demographic data, and network groupings to
be visible to anybody with an Internet connection. The seemingly benign informa-
tional aspects that users share about their lives, such as contact information (including
mobile phone numbers and e-mail addresses), hometown, sexual and political
preferences, date of birth, and partner‚Äôs name, can be mined, stored, and abused
(Acquisti and Gross 2006, 2009; Acquisti and Grossklags 2004; Christofides et al.
2009; Govani and Pashley 2005; Gross and Acquisti 2005; Nov and Wattal 2009;
Tufekci 2008; Young and Quan-Haase 2009). This can result in phishing, information
leakage, social security fraud, identity fraud, and both online and offline stalking
(Acquisti and Gross 2009; Gross and Acquisti 2005; Hasib 2009; Westlake 2008).
   Not all privacy threats on SNSs come from loss of information privacy or control
over personal information ‚Äì they may also come from excessive social contact, or
digital crowding. We argue that the evolution of SNSs has led to a situation akin to
offline crowding where inability to control interaction, in particular the boundaries
between self, small intimate groups, and the public audience, leads to deleterious
consequences both for the individual concerned and for the quality of social
relations between people. Our argument is based on an analysis of the nature and
role of self-disclosure and privacy maintenance in social interaction, and the ways
in which SNSs disrupt established practices. Specifically, we argue that SNSs may
create digital crowding in three main ways:
1. By disrupting the dynamic nature of boundary regulation as social interaction
   progresses, through the use of discrete privacy settings and preferences.
2. By providing multiple audiences, with limited or overly complicated methods to
   control sharing within set boundaries.
36                                                                    A.N. Joinson et al.


3. By encouraging unfettered sharing of personal information that intrudes upon
   other users.
   In the following section, we discuss the nature of self-disclosure, its role in
relationships, and its links to privacy theory.




4.3    Self-Disclosure, Relationships, and Privacy Theory

Self-disclosure has been defined as ‚Äúthe process of making the self known to
other persons‚Äù (Jourard and Lasakow 1958, p. 91). This results in the sharing of
knowledge between pairs of individuals, individuals within groups, or between an
individual and an organization (Joinson and Paine 2007; Petronio 2002). The notion
of simply ‚Äúdisclosing more‚Äù must appreciate the duality of self-disclosure that can
be measured along two dimensions, breadth and depth (Spiekermann et al. 2001).
Breadth is related to the quantity of information, and depth to the quality
(Spiekermann et al. 2001). Depth can range from biographic information to deeper
aspects such as revelations of trust violations or one‚Äôs sexual fantasies (Joinson and
Paine 2007). Altman and Taylor (1973) suggest a penetrative, ‚Äúlayered‚Äù model
of disclosure, akin to an onion. The core layer contains fewer, but deeper, aspects of
personality. Towards the peripheral layers of the model are an increasing number
of personality aspects, although somewhat shallower. For example, being empa-
thetic would be a core personality construct, whereas types of clothing and basic
interaction with others are towards the peripheral layers (Altman and Taylor 1973).
Breadth varies along two planes, frequency and category. Category refers to the
number of elements within each layer and frequency refers to their occurrence
(Altman and Taylor 1973).
   Self-disclosure is critical to the development and maintenance of relationships.
Uncertainty reduction theory (URT) (Berger 1979; Berger and Calabrese 1975)
posits that greater knowledge of others is associated with greater liking, and
uncertainty has been linked to relationship problems (Knobloch 2007). In a meta-
analysis of liking and self-disclosure, Collins and Miller (1994) report three distinct
self-disclosure effects: (1) people who disclose are liked more, (2) people disclose
more to those they like, and (3) people like those to whom they have previously
disclosed. Open disclosure has consistently been related to marital satisfaction and
feelings of love (e.g., Hendrick 1981; Rubin et al. 1980), and levels of disclosure
from one partner to another in dating couples predicts liking (Sprecher 1987).
   Variations in the breadth and depth of self-disclosure are a form of regulation
(Derlega and Chaikin 1977) that serves on the one hand to maintain privacy and on
the other hand to determine the type of relationship kept with others; by controlling
disclosure, individuals manage the degree of intimacy in a relationship. To give an
example, in a public space we cannot help but reveal some peripheral information,
such as our clothes, gender, and approximate age. We keep other members of the
public in a non-intimate relationship with ourselves by concealing deeper aspects of
4 Digital Crowding: Privacy, Self-Disclosure, and Technology                          37


our lives. During the process of regulation, people (or individuals) allow themselves
to be open and accessible to varying degrees. In order to manage this openness, they
engage in a process of boundary regulation. Altman (1975) likens boundaries of
interpersonal relationships to a selectively permeable cell membrane where the flow
of inputs and outputs can be adjusted to reach a desired level of privacy. An
important aspect of this theory is that privacy is non-monotonic and is determined
as a dialectic process involving a desire for and against various interaction types.
The dialectic process suggests that the achievement of privacy requires a balance of
opposing forces. For example, the desire to reveal information opposes the desire to
conceal information. Depending on the circumstances at a particular moment, one
may choose a position on such a continuum that aids the achievement of the desired
level of privacy (Altman 1975). In the context of relationships, desired levels of
privacy are partly driven by an individual‚Äôs need to maintain certainty about
another individual or group. Certainty allows them to develop informed judgments
about others‚Äô personality orientation in order to predict their attitudes or behaviors
in a variety of situations (Berger and Bradac 1982; Berger and Calabrese 1975).
To achieve certainty requires reciprocal information disclosure between those
involved while managing the boundaries of communication (Berger 1993; Berger
and Bradac 1982).
    The dialectic management of disclosure and privacy is subject to norms as
individuals interact in line with the social situation they are in (Berger and Bradac
1982). At a cocktail party, it is the social norm to interact with unknown others and
begin the conversation with reciprocal peripheral information sharing, slowly
moving conversation towards more central constructs (Altman and Taylor 1973;
Berger and Bradac 1982). However, an individual that shares too much information
in such an environment would be labeled a social deviant (Altman and Taylor 1973)
and suspicions would be raised as to their objectives (Berger and Bradac 1982). For
example, taking off one‚Äôs clothes in a public environment is not only a social faux
pas, but also illegal. However, change the environment to a doctor‚Äôs surgery and
this is in line with expected social norms (Berger and Bradac 1982). It is not just the
environment that dictates social norms and expectancies of self-disclosure, but also
the nature of the relationship between the interaction partners. In the above example
of the doctor‚Äôs surgery, the doctor-patient relationship alongside the environment
of the doctor‚Äôs surgery dictates that we can take off our clothes, and it is acceptable.
If one were to get naked in the doctor‚Äôs surgery but in front of the receptionist,
it would again become a social taboo (Berger and Bradac 1982).
    From a relational perspective, the decision to disclose information to others is
subject to a series of explicit and implicit rule negotiations. Groups or individuals
with whom people share become co-owners and may feel entitled to disclose the
shared information further (Petronio 2002). When discussing the state of a romantic
relationship with a close friend, it can be explicitly stated, ‚Äúdon‚Äôt tell anyone,‚Äù or it
can be expected that the friend knows this implicitly (Petronio 2002). Therefore,
alongside privacy norms that are shaped by individual characteristics (e.g., gender,
culture), norms are communicated when individuals enter pre-existing boundaries
38                                                                  A.N. Joinson et al.


(e.g., the family) or are negotiated when new boundaries are formed (Petronio
2002).



4.4     Boundaries, SNSs, and ‚ÄúDigital Crowding‚Äù

We contend that a privacy threat of SNSs that has been underrepresented in the
extant academic literature comes from excessive self-disclosure, socialization, and
social contact ‚Äì what we term ‚Äúdigital crowding.‚Äù As discussed above, the regulation
of boundaries and management of disclosure are central to maintaining interpersonal
distance between people, and thus establishing different types of relationships.
Just as excessive physical contact can lead to a sense of crowding, we hypothesize
that excessive digital social contact via SNSs may lead to digital crowding.
    We focus on two ways in which digital crowding ‚Äì through excessive contact or
sharing ‚Äì can be detrimental to privacy and the quality of relationships. The first
is the dangers inherent in radical transparency or unregulated openness. The second
is through overlapping social spheres and users‚Äô inability to maintain dynamic
boundaries.



4.4.1    Digital Crowding and Radical Transparency

As discussed above, much social media involves disclosure in some form ‚Äì whether
location, identity, pictures, contact information, or more intimate aspects of one‚Äôs
life. Indeed, many of the services currently popular simply do not work without
disclosure ‚Äì or the design of the site is such to encourage sharing and openness.
    While there is ample evidence that self-disclosure is generally positive in
relationships, this is not universally true. Non-disclosure, secrecy, and deceit are
also key components of successful relationships (Afifi et al. 2007; Burgoon and
Hale 1988; Petronio 1991), and over-disclosure can be as detrimental to relation-
ship development as unwillingness to disclose (Altman and Taylor 1973; Berger
and Bradac 1982). While studies of the mere exposure effect (Zajonc 1968)
consistently show that familiarity and repeated exposure to objects is associated
with increased liking, there is also evidence that over-exposure leads to reduced
liking (Erdelyi 1940; Smith and Dorfman 1975). Norton et al. (2007) found that
although people expected that increased knowledge of possible romantic partners
would be associated with increased liking, this was rarely the case, and more often
than not it was associated with reduced liking. In a similar vein, Stafford and Reske
(1990) found that students in geographically distant relationships reported being
more in love than those who lived in the same town. Before the radical transparency
that SNSs imposed, Walther (1996) argued that it is the ability while online to
manage the flow of information, and to self-present selectively, that leads to
‚Äúhyperpersonal interaction.‚Äù Similarly, Petronio (1991) notes that, ‚ÄúThere are
4 Digital Crowding: Privacy, Self-Disclosure, and Technology                        39


good reasons to balance openness with secrecy in a relationship,‚Äù and Afifi et al.
(2007) argue that, ‚Äúwithholding information is sometimes benign or even useful‚Äù
(Afifi et al. 2007, p. 78).
    However, in the era of radical transparency there is little scope for secrecy. With
its emphasis on sharing, lack of sharing not only leads to a reduced user experience
on many web 2.0 sites, but could also be seen as anti-normative (or at least, contrary
to the principles and terms and conditions of Facebook). As noted by other privacy
researchers (e.g., Acquisti and Gross 2009; DeCew 1997), sharing does not need to
be intimate to impinge on privacy ‚Äì indeed, with the opportunity to collect
information about others across time and locations, and to aggregate and process
that data, the multitude of banalities usually seen on social media services may be
more telling than the single intimate outpouring. With the advent of social media
and particularly SNSs, alongside ‚Äúradical transparency,‚Äù it is inevitable that we will
end up knowing more about people, and also more likely that we end up disliking
them because of it.



4.4.2    Digital Crowding and Overlapping Social Spheres

Self-disclosure is used in different ways in different types of relationships (e.g.,
between same-sex friends, romantic partners, colleagues). Typically when the most
popular SNSs were launched their content was targeted at specific markets.
Myspace was aimed at teenagers and music lovers, LinkedIN at professionals in
high-tech industries, and Facebook at university students. However, the growth in
the popularity of these sites, alongside a loosening of entry rules, has brought a
widening of user demographics. To give an example, Facebook began by confining
entry to people with a Harvard e-mail address, followed by a slow roll out across US
campuses using the same ‚Äú.edu‚Äù criteria. When opened up globally, it again began
with a focus on university campuses, to be followed in 2006 by being open to all
potential users. In recent years Facebook has become popular not only with older
generations, but also with social groups very different to those associated with the
site in the early days (Gonzalez 2010). Furthermore, it should be noted that not just
users, but also uses themselves may change over time as usage of any complex
software is expected, to some extent, to be socially shaped (Dutton et al. 2004;
MacKenzie and Wacjman 1985; Selwyn et al. 2005). Widening demographics,
especially age, has a crucial role within the nature of shared information across
boundaries, as users start to befriend parents, grandparents, employers, religious
elders, and teachers. As a consequence, a user‚Äôs profile may be scrutinized by a
number of critical members from different social spheres simultaneously.
    Skeels and Grudin (2009) define this group co-presence as ‚Äúa situation in which
many groups important to an individual are simultaneously present in one context
and their presence is salient for the individual.‚Äù People generally make decisions on
what information they share based on which distinct persons or groups are the
intended audience (Davis et al. 2005; Jones and O‚ÄôNeill 2010; Lederer et al. 2004).
40                                                                    A.N. Joinson et al.


Privacy issues occur when content meant for one social sphere becomes visible to
another. This simultaneity of surveillance can present a challenge for users who
endeavor to control information flows (Hewitt and Forte 2006). The chance that
harm may arise out of negative broadcasts increases, particularly when we consider
that information online is ‚Äúpersistent‚Äù and subject to record permanence (Binder
et al. 2009; Sparck Jones 2003).
    While Facebook provides mechanisms for controlling access to information
from different spheres, in the form of ‚Äúfriends lists,‚Äù lack of use or over complexity
make it likely that they are not effective in separating groups. Binder et al. (2009)
refer to this as the ‚Äúproblem of conflicting social spheres,‚Äù which they argue leads to
an increase in ‚Äútension‚Äù either between the maintainer of the network and one of
their connections, or directly between connections (boyd and Ellison 2008).
    Binder et al. (2009) argue that this increased diversity leads to tension, particu-
larly when the ties involve kinship. They propose that such tension could arise out
of disparities between the norms of different social spheres (Binder et al. 2009).
Similarly, DiMicco and Millen (2007), in a study of IBM employees, found that
managing profiles with regards to visibility to work-related friends could cause
problems. What is fundamental to both these pieces of research is that different
social spheres hold different norms, values, and expectations. The issues of
conflicting social spheres require rule negotiation and boundary maintenance,
otherwise the boundaries become turbulent (Petronio 2002). Failure to manage
boundaries successfully may encourage individuals to become enclosed in their
own ‚Äúself‚Äù boundary, severely restricting information throughput (Altman 1975),
and thus the content disclosed to SNSs.
    From the perspective of privacy and communication, these overlapping social
spheres cause a number of problems. Firstly, we argue that it becomes difficult for a
person to manage their boundaries ‚Äì either through negotiation or acceptance of
norms of behavior. Because we may be sharing with multiple audiences, each with
its own understanding of what is and is not appropriate, the time and effort to
negotiate sharing becomes prohibitive. Secondly, we argue that the role of trust is
subverted since while we may have trusted ‚Äúfriends‚Äù with whom we have implicit
or explicit rules about disclosure, we may also have ‚Äúfriends‚Äù who are considerably
less close, and with whom there are either no set expectations and rules, or the rules
are loosely defined and based on social norms of behavior. The offline equivalent is
Altman‚Äôs (1975) notion of crowding, where a failure of two privacy mechanisms ‚Äì
control over territory and personal space ‚Äì leads to too much social contact. In
instances of overbearing social contact, the individual (or group) will try to close
the boundary around the self to prevent information disclosure, or others gaining
access to them, to regain control. Consequently, this individual becomes isolated,
creating dissonance between their desired level of privacy and their experienced
level of privacy.
    Overcrowding offline has been studied in terms of personal space, considered to
be less than 50 cm between two or more individuals. This distance, like privacy, is
non-monotonic. It can differ depending on environment, gender, age group, role,
activity, social class, region, desire to be intimate or personal with another, and
4 Digital Crowding: Privacy, Self-Disclosure, and Technology                         41


culture (Aiello and Jones 1971; Beaulieu 2004; Evans and Howard 1973; Freedman
1975). For many SNS users, the online equivalent to personal space is equidistant
across audiences and environments. As well as loss of control and the aforemen-
tioned issues of information flow on SNSs, individual differences of appropriate
disclosure and intimacy demonstrate an array of possible reasons that personal
space can be violated, resulting in ‚Äúdigital crowding.‚Äù
    In online instances we suggest that any unwanted information disclosure or
‚Äúcross-talk‚Äù between multiple audiences that results in the release of information
from core layers of the self-construct, is akin to others physically encroaching on
one‚Äôs intimate or personal space. Both core constructs and personal space relate to
intimacy, and a deep level of information, requiring trust for its disclosure or
contact. A variation of individual and cultural preferences affects both concepts,
and both result in the individual using behavior as a mechanism to regain control. In
physical crowding of personal space one might step back from the intruder. In the
release of core information, one might close the self-boundary and become isolated.
Therefore, difficulties can emerge online when social spheres overlap, when infor-
mation is leaked to those considered ‚Äúperipheral‚Äù when it is intended for close
friends. There may be an emotional reaction, a feeling of privacy violation, and a
behavioral mechanism to overcome it.
    Digital crowding can also occur from the bombardment of peripheral informa-
tion disclosure by another: the increased intensity of revelation of the shallower
aspects of daily life by other users. An offline example would be a friend that
telephones you several times a day with mundane or trivial personal concerns that
could be solved easily without consultation, or a child consistently pestering its
parents for sweets on a shopping trip. To give an online example, this translates to
the continual posting of mundane, useless information via status updates that can
result in frustration and annoyance of its readers, ultimately ending in the de-
friending of individuals.
    We hypothesize that the failure of online privacy mechanisms and site designs
that allow crowding to occur, such as those on SNSs, will effectively result in the
same outcome ‚Äì stress and eventual withdrawal. Paradoxically for SNSs,
the success of a site makes it more likely that crowding will occur, meaning that
the seeds of failure are sown only in success.
    For users, there are a number of possible ways that digital crowding can be
reduced. One option is to rely on existing privacy mechanisms to reduce crowding ‚Äì
that is, to engage with the myriad of privacy settings in order to differentiate social
spheres, and to re-establish manageable boundaries. This approach will require
perseverance to change the settings in parallel with changes to the dynamic social
communication boundaries. An alternative approach is one increasingly seen on
sites like Twitter ‚Äì establishing multiple accounts (e.g., one for work, one for family
and friends). Multiple sites could also fulfill this option ‚Äì for instance, LinkedIn for
work, Facebook for social interaction. Users might also establish their desired state
of privacy behaviorally ‚Äì for instance, by limiting the depth of the information
about the self that is communicated to others. This solution suggests that the
information communicated by users will become increasingly banal as they gain
42                                                                             A.N. Joinson et al.


more contacts in different social spheres, assuming that they do not manage their
privacy via the site settings.
    Failure to adopt multiple accounts, multiple sites, or the privacy settings offered
on social media may result in a withdrawal or inhibited posting of content. SNSs
encourage continual content provision by their users ‚Äì otherwise the site lacks any
real motivation for visiting (Burke et al. 2009). The danger for sites is that digital
crowding encourages withdrawal ‚Äì and hence less engagement with the site. An
alternative method of controlling digital crowding is to severely limit who is added
to the ‚Äúfriends‚Äù list on a user‚Äôs account. For example, users conscious of these data
control issues may only have a small social network of strong ties, but even in these
cases, the privacy settings of these strong ties may render the network penetrable.




4.5     Conclusion

People are able to maintain their interpersonal boundaries by managing the amount
and depth of information they disclose to others. New technology, in particular
social media, makes this more difficult ‚Äì the sites often rely on disclosure for
functionality, personal information can be aggregated across time, and the com-
plexity of privacy settings often makes it difficult for users to differentiate multiple
audiences. Together, these effects might equate to a form of digital crowding,
where excessive social contact prompts users to search for coping mechanisms or
to withdraw. The danger, otherwise, is a reduction in liking between contacts and
increased tension between an individual and members of different social spheres.
The writing of this chapter was supported by funding by the EPSRC (‚ÄúPrivacy
Value Networks‚Äù, Grant reference: EP/G002606/1).




References

Acquisti A, Gross R (2006) Imagined communities: awareness, information sharing, and privacy
    on Facebook. Paper presented at the Privacy Enhancing Technology workshop, Cambridge
Acquisti A, Gross R (2009) Social insecurity: the unintended consequences of identity fraud
    prevention policies. Paper presented at the workshop on the economics of information security,
    University College London
Acquisti A, Grossklags J (2004) Privacy attitudes and privacy behavior: losses, gains, and
    hyperbolic discounting. In: Camp J, Lewis R (eds) The economics of information security,
    vol 12. Kluwer Academic Publishers, NY, pp 165‚Äì178
Afifi TD, Caughlin J, Afifi WA (2007) Exploring the dark side (and light side) of avoidance and
    secrets. In: Spitzberg B, Cupach B (eds) The dark side of interpersonal relationships, 2nd edn.
    Erlbaum, Mahwah, pp 61‚Äì92
Aiello JR, Jones SE (1971) Field study of the proxemic behavior of young children in three
    subcultural groups. J Pers Soc Psychol 19(3):351‚Äì356
Altman I (1975) The environment and social behavior. Wadsworth, Belmont
4 Digital Crowding: Privacy, Self-Disclosure, and Technology                                   43

Altman I, Taylor DA (1973) Social penetration: the development of interpersonal relationships.
   Holt, Rinehart and Winston, New York
Awad NF, Krishnan MS (2006) The personalization privacy paradox: an empirical evaluation of
   information transparency and the willingness to be profiled online for personalization. MIS Q
   30(1):13‚Äì28
Baum A, Valins S (1977) Architecture and social behavior: psychological studies of social density.
   Erlbaum, Hillsdale/New York
BBC News (2003) Bugged by the boss. BBC news. http://www.bbc.co.uk/wales/weekinweekout/
   stories/buggedbytheboss.shtml. Accessed 14 Feb 2011
BBC News (2009a) MP‚Äôs fears at child risk register. BBC news. http://news.bbc.co.uk/1/hi/
   england/somerset/8127265.stm. Accessed 14 Feb 2011
BBC News (2009b) Facebook remark teenager is fired. BBC news. http://news.bbc.co.uk/1/hi/
   england/essex/7914415.stm. Accessed 14 Feb 2011
BBC News (2010) A burglar who taunted police on Facebook is jailed. BBC news. http://news.
   bbc.co.uk/1/hi/england/manchester/8492500.stm. Accessed 14 Feb 2011
BCS (2010) Zuckerberg: privacy no longer a social-norm. British Computer Society. http://www.
   bcs.org/content/conWebDoc/34018. Accessed 14 Feb 2011
Beaulieu CMJ (2004) Intercultural study of personal space: a case study. J Appl Soc Psychol
   34(4):794‚Äì805
Berger CR (1979) Beyond initial interaction: uncertainty, understanding, and the development of
   interpersonal relationships. In: Giles H, St. Clair R (eds) Language and social psychology.
   Blackwell, Oxford, pp 122‚Äì144
Berger CR (1993) Uncertainty and social interaction. In: Deetz SA (ed) Communication yearbook
   16. SAGE, London, pp 491‚Äì502
Berger CR, Bradac JJ (1982) Language and social knowledge. Uncertainty in interpersonal
   relations. Edward Arnold, London
Berger CR, Calabrese RJ (1975) Some explorations in initial interaction and beyond: toward a
   developmental theory of interpersonal communication. Human Commun Res 1:99‚Äì112
Binder J, Howes A, Sutcliffe A (2009) The problem of conflicting social spheres: effects of
   network structure on experienced tension in social network sites. Paper presented at the CHI
   2009, Boston
Bonneau J, Preibusch S (2009) The privacy jungle: on the market for data protection in social
   networks. Paper presented at the workshop on the economics of information security, Univer-
   sity College London
boyd dm, Ellison NB (2008) Social network sites: definition, history, and scholarship. J Comput
   Mediat Commun 13(1):210‚Äì230
Breyer J, Zuckerberg M (2005) Mark Zuckerberg discusses Facebook. (Video recording, 26 Oct),
   http://ecorner.stanford.edu/authorMaterialInfo.html?mid¬º1567. Accessed 2 Jan 2010
Burgoon JK, Hale JL (1988) Nonverbal expectancy violations: model elaboration and application
   to immediacy behaviors. Commun Monogr 55(1):58‚Äì79
Burke M, Marlow C, Lento T (2009) Feed me: motivating newcomer contribution in social
   network sites. Paper presented at the CHI 2009 conference, Boston
Christofides E, Muise A, Desmarais S (2009) Information disclosure and control on Facebook: are
   they two sides of the same coin or two different processes? Cyberpsychol Behav 12:341‚Äì345
Collins NL, Miller LC (1994) Self-disclosure and liking: a meta-analytic review. Psychol Bull
   116(3):457‚Äì475
Davis M, Canny J, House N, Good N, King S, Nair R, Reid N (2005) MMM2: mobile media
   metadata for media sharing. Paper presented at the 13th annual ACM international conference
   on Multimedia, Hilton, Singapore, 6‚Äì11 Nov 2005
DeCew JW (1997) In pursuit of privacy: law, ethics, and the rise of technology. Cornell University
   Press, Ithaca
Derlega VJ, Chaikin AL (1977) Privacy and self-disclosure in social relationships. J Soc Issues
   33(3):102‚Äì115
44                                                                           A.N. Joinson et al.


DiMicco JM, Millen DR (2007) Identity management: multiple presentations of self in Facebook.
   Proceedings of the 2007 international association for computing machinery conference on
   Supporting group work, ACM Press, Sanibel Island, pp 383‚Äì386, 4‚Äì7 Nov 2007
Dutton WH, Cheong PH, Park N (2004) The social shaping of a virtual learning environment: the
   case of a university-wide course management system. Electron J e-Learn 2(1):69‚Äì80
Erdelyi M (1940) The relation between ‚Äúradio plugs‚Äù and sheet sales of popular music. J Appl
   Psychol 24(6):696‚Äì702
Evans GW, Howard RB (1973) Personal space. Psychol Bull 80(4):334‚Äì344
Facebook (2010) Statement of rights and responsibilities. Facebook. http://www.facebook.com/
   terms.php?ref¬ºpf. Accessed Oct 2010
Facebook (2011) Facebook principles. Facebook. http://www.facebook.com/principles.php.
   Accessed 17 Feb 2011
Freedman JL (1975) Crowding and behavior. W.H. Freeman, San Francisco
Gonzalez N (2010) About CheckFacebook.com. http://www.checkfacebook.com/. Accessed 15
   Feb 2010
Govani T, Pashley H (2005) Student awareness of the privacy implications when using Facebook.
   http://lorrie.cranor.org/courses/fa05/tubzhlp.pdf. Accessed 6 Oct 2009
Gross R, Acquisti A (2005) Information revelation and privacy in online social networks. Paper
   presented at the 2005 ACM workshop on privacy in the electronic society, Alexandria
Hasib AA (2009) Threats of online social networks. Int J Comput Sci Netw Secur 9(11):288‚Äì293
Hendrick SS (1981) Self-disclosure and marital satisfaction. J Pers Soc Psychol 40(6):1150‚Äì1159
Hewitt A, Forte A (2006) Crossing boundaries: ‚ÄòIdentity management and student/faculty
   relationships on the Facebook‚Äô. Paper presented at the Computer Supported Cooperative
   Work 2006, Banff, Alberta, Canada
Houghton DJ, Joinson AN (2010) Privacy, social network sites, and social relations. J Technol
   Human Serv 28(1):74‚Äì94
Joinson AN (2008) ‚ÄòLooking at‚Äô, ‚Äòlooking up‚Äô or ‚Äòkeeping up with‚Äô people? Motives and uses of
   Facebook. Paper presented at the CHI 2008 ‚Äì Online Social Networks, Florence
Joinson AN, Paine CB (2007) Self-disclosure, privacy and the Internet. In: Joinson AN, McKenna
   KYA, Postmes T, Reips U (eds) The Oxford handbook of Internet psychology. Oxford
   University Press, Oxford, pp 237‚Äì252
Joinson AN, Whitty M (2008) Watched in the workplace. Infosecurity 5(1):38‚Äì40
Jones S, O‚ÄôNeill E (2010) Feasibility of structural network clustering for group-based privacy
   control in social networks. Paper presented at the proceedings of the sixth symposium on
   usable privacy and security (SOUPS) 10, Microsoft, Redmond WA, USA
Jourard SM, Lasakow P (1958) Some factors in self-disclosure. J Abnorm Psychol 56(1):91‚Äì98
Knobloch LK (2007) Perceptions of turmoil within courtship: associations with intimacy, rela-
   tional uncertainty, and interference from partners. J Soc Pers Relat 24(3):363‚Äì384
Lederer S, Hong J, Dey A, Landay J (2004) Personal privacy through understanding and action:
   five pitfalls for designers. Pers Ubiquit Comput 8(6):440‚Äì454
MacKenzie D, Wacjman J (1985) The social shaping of technology. Open University Press,
   Buckingham
Norton MI, Frost JH, Ariely D (2007) Less is more: the lure of ambiguity, or why familiarity
   breeds contempt. J Pers Soc Psychol 92(1):97‚Äì105
Nov O, Wattal S (2009) Social computing privacy concerns: antecedents and effects. Paper
   presented at the CHI 2009, Boston
Petronio S (1991) Communication boundary management: a theoretical model of managing
   disclosure of private information between marital couples. Commun Theory 1(4):311‚Äì335
Petronio S (2002) Boundaries of privacy. State University of New York, Albany
Rubin Z, Hill CT, Peplau LA, Dunkel-Schetter C (1980) Self-disclosure in dating couples: sex
   roles and the ethic of openness. J Marriage Fam 42(2):305‚Äì317
Schatz Byford K (1996) Privacy in cyberspace: constructing a model of privacy for the electronic
   communications environment. Rutgers Comput Technol Law J 24:1‚Äì74
4 Digital Crowding: Privacy, Self-Disclosure, and Technology                                 45

Selwyn N, Gorard S, Furlong J (2005) Adult learning in the digital age. Routledge, London
Skeels MM, Grudin J (2009) When social networks cross boundaries: a case study of workplace
   use of facebook and linkedin. Paper presented at the proceedings of the ACM 2009 interna-
   tional conference on supporting group work, Sanibel Island, FL, USA
Smith GF, Dorfman DD (1975) The effect of stimulus uncertainty on the relationship between
   frequency of exposure and liking. J Pers Soc Psychol 31(1):150‚Äì155
Sparck Jones K (2003) Privacy: what‚Äôs different now? Interdiscip Sci Rev 28(4):287‚Äì292
Spiekermann S, Grossklags J, Berendt B (2001) E-privacy in 2nd generation E-Commerce:
   privacy preferences versus actual behavior. Paper presented at the ACM conference on
   Electronic Commerce, Tampa, 14‚Äì17 Oct 2001
Sprecher S (1987) The effects of self-disclosure given and received on affection for an intimate
   partner and stability of the relationship. J Soc Pers Relat 4(2):115‚Äì127
Stafford L, Reske JR (1990) Idealization and communication in long-distance premarital
   relationships. Fam Relat 39(3):274‚Äì279
Tufekci Z (2008) Can you see me now? Audience and disclosure regulation in online social
   network sites. Bull Sci Technol Soc 28(1):20‚Äì36
Walther JB (1996) Computer-mediated communication: impersonal, interpersonal, and
   hyperpersonal interaction. Commun Res 23(1):3‚Äì43
Westlake EJ (2008) Friend me if you Facebook: generation Y and performative surveillance.
   Drama Rev 52(4):21
Young AL, Quan-Haase A (2009) Information revelation and Internet privacy concerns on social
   network sites: a case study of Facebook. Paper presented at the C&T ‚Äò09, Pennsylvania
Zajonc RB (1968) Attitudinal effects of mere exposure. J Pers Soc Psychol 9(2):1‚Äì27
Chapter 5
Ethics, Privacy, and Self-Restraint
in Social Networking

Bernhard Debatin




5.1    Approaches to Privacy

Privacy is a basic human need. It is anthropologically and psychologically rooted in
the sense of shame and the need for bodily integrity, personal space, and intimacy in
interpersonal relationships. Especially in modern Western cultures, it is understood
as a necessary condition for individual autonomy, identity, and integrity (Altman
1975; Westin 1967; see also Margulis, this volume, Chap. 2). The desire for privacy
is historically variable and has increased noticeably throughout the process of
modernization. As J‚Ç¨ urgen Habermas (1962) has shown in his seminal study The
Transformation of the Public Sphere, this process led to the emergence of
the private sphere as a corollary to the public sphere: the private sphere offers the
protection and freedom necessary for the undisturbed growth and self-fulfillment of
the modern subject, who then, as a citizen, can participate in exchanging opinions
and forming public discourse in the communicative space of the public sphere.
   Privacy is to be distinguished from secrecy. While privacy can be understood in
a broad way as the ‚Äúright to be let alone‚Äù (Warren and Brandeis 1890) and the right
not to reveal information about oneself, secrecy refers to blocking or hiding any
type of information. A person‚Äôs privacy is characterized by ‚Äúa series of concentric
circles of intimacy in which the degree of intimacy diminishes from the innermost
circle outward‚Äù (Hodges 2009, p. 277f.). The more intimate something feels to a
person, the more it is considered a private issue that will only be shared with
someone who is close to them. While specific personal information, such as
embarrassing facts, will sometimes be kept secret by an individual, secrecy has
usually more to do with keeping certain places, persons, or information hidden from
any unauthorized eye (e.g., arcane places, secret agents, state or business secrets).



B. Debatin (*)
Ohio University, Athens, OH, USA
e-mail: debatin@ohio.edu

S. Trepte and L. Reinecke (eds.), Privacy Online,                                 47
DOI 10.1007/978-3-642-21521-6_5, # Springer-Verlag Berlin Heidelberg 2011
48                                                                          B. Debatin


   There is no single definition of privacy because it is a complex and ambiguous
notion, serving as an umbrella term for a variety of loosely related issues and
problems (Solove 2008). However, it can be conceptualized in both positive and
negative terms. Privacy is positively conceptualized as an individual‚Äôs control over
his or her circles of intimacy in four dimensions: personal space in the physical
dimension, personal integrity in the psychological dimension, interaction with
others in the social dimension, and personal data in the informational dimension
(Leino-Kilpia et al. 2001). It can be defined negatively as the absence of invasion of
privacy by the government, businesses, or other actors. The focus here is on
different types of privacy violations and their disruptive or destructive effects on
the integrity of certain human activities; consequently, much attention is given to
attempts to protect privacy from intrusions. In his taxonomy of privacy, Solove
(2008, pp. 101‚Äì170) identifies four types of privacy problems, most of which are
related to informational privacy: firstly, information collection, encompassing
surveillance and interrogation; secondly, information processing, with the sub-
types of aggregation, identification, insecurity, secondary use, and exclusion;
thirdly, information dissemination, including breach of confidentiality, disclosure,
exposure, increased accessibility, blackmail, appropriation, and distortion; fourthly,
invasion of one‚Äôs private sphere, in the forms of intrusion or interference with
personal decisions. Similarly, Nissenbaum (2010, pp. 21‚Äì64) identifies three types
of technology-based privacy problems: tracking and monitoring, aggregation and
analysis, and dissemination and publication. However, in order to avoid ‚Äúconcep-
tual sprawl,‚Äù her notion of privacy focuses on ‚Äúpublic/private‚Äù as a guiding norma-
tive distinction in the three dimensions of actors, realm/space, and information.
Here, the right to privacy is not understood as mere access control but as the ‚Äúright
to appropriate flow of personal information‚Äù while maintaining the ‚Äúcontextual
integrity‚Äù of the information (Nissenbaum 2010, p. 127).



5.2   Privacy Protection

Because of the rapid advances of information technology and its enormous
processing and storing capacity, privacy protection has become particularly impor-
tant in the informational dimension. Moreover, the ubiquity of information and
communication technology also increasingly permeates the other three dimensions
of privacy (For a systematic and detailed discussion of information technology-
based invasions of privacy, see Nissenbaum 2010, pp. 21‚Äì64). For instance, per-
sonal space and territorial privacy are subject to invasive technologies, such as the
increasing use of surveillance cameras at workplaces and in public or semi-public
places (e.g., in shopping malls and airports) or the use of RFID tracking devices
(Van den Hoven Aspen and Vermaas 2007). Personal communication can easily be
intercepted and retained with wiretapping technology and the surveillance of e-mail
and other Internet-based communication media, as warranted under the USA
PATRIOT Act (Solove et al. 2006, pp. 107ff). Bodily privacy is infringed upon
5 Ethics, Privacy, and Self-Restraint in Social Networking                         49


by large-scale biometric checks at stadiums and other gathering places and also by
the much debated body scanners in airports (Lombard 2010).
   Privacy can be protected through three main mechanisms: legal regulation,
ethical self-regulation, and privacy-enhancing technology. These three mechanisms
will be discussed briefly in the following.
   In modern societies, privacy enjoys specific legal protection, although the extent
and range of the protection varies considerably. While most countries explicitly
recognize basic privacy rights in their constitutions and have adopted comprehen-
sive and general data protection laws, the United States Constitution does not
mention a right to privacy. Yet, the protection of personal beliefs in the first
Amendment, the search and seizure limits of the third and fourth Amendments,
and the self-incrimination limit of the 5th Amendment protect at least certain
aspects of personal privacy. In addition, a good dozen Supreme Court decisions
have used the liberty clause of the 14th Amendment to establish a somewhat
broader right of privacy. However, case law decisions and sectoral legislation,
such as the Health Information Privacy Protection Act (HIPPA), the Family Edu-
cational Rights and Privacy Act (FERPA), and the Children‚Äôs Online Privacy
Protection Act (COPPA), only lead to ‚Äúpatchwork coverage‚Äù and fail to guarantee
privacy as a basic right (Bennett and Raab 2006, p. 132).
   Privacy as a basic human right is guaranteed in the UN Declaration of Human
Rights (United Nations 1948, Art. 12), the European Convention on Human Rights
(ECHR 1950, Art. 8), and many other international agreements and national
statutory laws. Initially, legal regulations focused on preventing intrusion into
personal privacy, home, family, and correspondence, but the rapid development
of information technologies soon necessitated specific data protection laws. For
instance, the OECD ‚ÄúGuidelines governing the protection of privacy and transbor-
der flows of personal data‚Äù define basic fair information practices and principles
(FIPP) regarding individual rights and accountability in the collection, use, purpose,
and security of data (OECD 1980, Part 2). The Data Protection Directive of the
European Union (European Parliament 1995) even defines information privacy
explicitly as a basic human right. This stands in stark contrast to the situation in
the US, where ‚Äúthe government is constitutionally prohibited under the First
Amendment from interfering with the flow of information, except in the most
compelling circumstances‚Äù (Cate 1999, pp. 179f.). Differences in national and
international law, the lack of comprehensive privacy laws in some countries, and
the rapid evolution of technology make legal regulation a cumbersome, inconsis-
tent, and often outdated instrument of privacy regulation.
   A different approach is voluntary ethical self-regulation of privacy. Although
ethical regulation lacks the power of external sanctions (such as a legal penal
system), it can be quite effective, particularly if based on the binding power of
socially entrenched norms. Informal privacy norms are akin to rules of etiquette and
personal morality. They govern reasonable expectations of privacy in interpersonal
relationships, groups, and subcultures. More formal norms of privacy are embedded
in professional norms, ethics codes, and express policies of organizations and
institutions that typically deal with any kind of personal information. Such formal
50                                                                             B. Debatin


policies often mix different types of privacy regulation, such as privacy
commitments, privacy codes, privacy standards, and privacy seals (Bennett and
Raab 2006, pp. 151‚Äì175). Professional discretion and confidentiality thus belong to
the privacy standards that clients may reasonably expect in their interactions with
agencies such as health care providers or educational institutions.
    As Nissenbaum (2010, pp. 129ff.) has shown, all of these norms are entrenched in
specific contexts, within which they regulate the flow of personal information,
which is why they are referred to as informational norms. From this perspective,
the right to privacy can be understood as a right to context-appropriate flow of
personal information. In other words, privacy does not mean the indiscriminate
control of personal information, but a highly differentiated practice of sharing and
withholding information depending on its meaning and sensitivity in different
contexts. Consequently, violations of privacy are seen as violations of contextual
integrity or ‚Äúbreaches of context-relative informational norms‚Äù (Nissenbaum 2010,
p. 140). This contextual approach to privacy not only allows a detailed descriptive
analysis of privacy, it also provides a strong normative basis for an ethical critique of
privacy invasion as an unjustified transgression of contextual integrity. The trans-
gression would be deemed unjustified whenever (a) expectations of the established
context-appropriate flow of information are breached, and (b) the novel flow is not
morally superior to the existing contextual norms and practices (Nissenbaum 2010,
p. 164). The task of the ethical evaluation is, then, ‚Äúto compare entrenched and novel
flows in terms of values, ends, and purposes of respective contexts‚Äù (Nissenbaum
2010, p. 227). The concept of contextual integrity thus provides both a rational
explanation of the moral outrage individuals feel when their privacy is invaded and
an ethical framework for assessing the legitimacy of their claims.
    The technicization of privacy invasion, particularly in the realm of information
technology, has led to an increased demand for the third approach to privacy
protection, i.e., privacy-enhancing technology. This approach is broader than just
protecting privacy with the help of specific information technology. For centuries,
simple mechanical solutions have been used to protect people‚Äôs privacy: screens,
curtains, doors, fences, and sound insulation protect against the unwanted gaze and
eavesdropping; sensitive paper documents are locked in filing cabinets and often
shredded after their intended purpose expires. In digital information environments,
technological privacy protection can be achieved through access control and
privacy-sensitive data management. Access can be controlled with a variety of
hard- and software tools, such as authentication tools, firewalls, spyware detectors,
filters, secure connections, and privacy settings. In addition to this, privacy-
sensitive digital data management employs techniques such as data encryption,
anonymization tools, blocking of data aggregation, automatic data expiration, and
secure data deletion tools (Bennett and Raab 2006, pp. 177‚Äì202).
    Unfortunately, much as fences can be climbed and locks picked, digital access
control and data management tools can be circumvented or hacked into. The
reliability and trustworthiness of privacy technologies are thus rather questionable.
They are a necessary but not sufficient condition for informational privacy. Sole
reliance on such technologies often creates a false sense of security and may
5 Ethics, Privacy, and Self-Restraint in Social Networking                           51


actually lead to careless and imprudent behavior. As will be shown in Sect. 5.4,
citizens must not only insist on their privacy rights but also acquire privacy literacy,
which encompasses an informed concern for their privacy and effective strategies
to protect it. First, though, ethical arguments that analyze the normative status of
privacy and develop moral principles to justify its protection must be considered.



5.3    Ethical Justification of Privacy Protection

Similar to the conceptualization of privacy, the ethical justification of privacy and
its protection can be founded on positive and negative arguments. The positive
argument claims that the social-psychological need for privacy and the legal right to
privacy imply that privacy possesses a specific moral value for individuals,
relationships, and society, and therefore deserves special protection. Privacy is
regarded both as an inherent value and as interrelated with a number of other
essential human values, among them moral autonomy and freedom, equality and
justice, dignity and self-fulfillment, and trust and variety in relationships. Privacy
also draws moral value and legitimacy from its crucial role for the functioning of
key social institutions and the well-being and freedom of citizens (Nissenbaum
2010, pp. 67‚Äì83; Solove 2008, pp. 77‚Äì100). The demand for privacy protection thus
rests upon value-based moral claims and can be ethically justified by the moral
value of privacy and its links to related basic values.
    A central value and guiding principle of the positive ethical justification of
privacy and its protection is the individual‚Äôs right to self-determination, i.e., the
right to freely determine what is necessary and desirable for a fulfilling and
meaningful life and to freely pursue one‚Äôs social, cultural, political, and economic
development. Self-determination is thus part of an individual‚Äôs autonomy and
freedom. Self-determination and autonomy are, as Kant has shown, intrinsically
connected: ‚ÄúAutonomy of the will is the property the will has of being a law unto
itself‚Äù (Kant 1785/1964, p. 108). In short, self-determination of the free will is the
basis for moral action and at the same time an inalienable natural right. Applied to
privacy, self-determination is the underlying moral principle and right that enables
individuals to control access to their private sphere and to regulate the flow and
context of their information. Self-determination can thus be regarded as a basic
positive moral and legal principle of privacy protection (Baker 2008, p. 10).
    Given the ubiquity and influence of information technology in our society,
informational self-determination has become a central positive concept in
the privacy debate and also in privacy policy. As Hornung and Schnabel (2009,
p. 85) have pointed out, privacy and informational self-determination guard the
borders among different societal contexts, ‚Äúas they prevent sensitive information
from one context (e.g., the working world, medical treatment, family life, etc.) from
proliferating into other ones.‚Äù They also stress the fundamental role of informa-
tional self-determination for the development of autonomous individuals and for
their unhampered participation in the political process. It is noteworthy that, in a
52                                                                           B. Debatin


groundbreaking decision, the German Federal Constitutional Court in 1993
established the right to informational self-determination and data protection,
linking them explicitly to ‚Äúthe fundamental values those rights are assumed to
protect and which were identified by the German Constitutional Court as human
dignity and self-development‚Äù (Rouvroy and Poullet 2009, p. 46). The right to
informational self-determination is also expressed in the 1995 data protection
directive of the European Union (European Parliament 1995). Even though some
countries do not recognize the right to informational self-determination, the signifi-
cance of this concept cannot be overemphasized.
    The negative ethical argument for protecting privacy is based on the harm
principle (Mill 1851/1991), which postulates the duty to avoid harming others for
one‚Äôs own benefit. As an ethical principle, harm avoidance is not just built upon a
selfish interpretation of the Golden Rule, which simply advises us not to harm
others so that we will not be harmed. Rather, it is based on a universal appreciation
of a shared capacity for suffering, human connectedness, and compassion
(Linklater 2006). It also does not exclude the causation of any harm (otherwise,
for example, many medical procedures would be impossible). Instead, it specifi-
cally refers to harm that both violates a person‚Äôs right and at the same time can
actually be avoided without creating greater harm elsewhere. This necessitates
applying a cost-benefit analysis that weighs the interest in invading a person‚Äôs
privacy against the individual‚Äôs right to and need for privacy.
    In media ethics, for instance, the cost-benefit analysis is typically based on two
interdependent criteria: firstly, a privacy invasion is only acceptable if no other
means are available for obtaining the needed information; secondly, any invasion of
privacy requires the existence of an overriding public interest (Hodges 2009,
p. 281). This approach, however, has been criticized insofar as it leaves open
what exactly constitutes an overriding public interest, so that definitional power is
inevitably vested in the privacy invaders, as they can always claim a higher interest
in the name of the public. In the media, intrinsic journalistic news values and the
frequently invoked audience‚Äôs ‚Äúright to know‚Äù quickly cancel out the individual‚Äôs
privacy claims (Christians 2010, p. 209). However, protection of privacy is a matter
of general ethics and must not be subordinated to the imperatives of professional
ethics or, worse, pragmatic purposes (Christians 2010).
    There are two approaches to remedy this problem: Firstly, a balance test, as
proposed by Whitehouse (2010), demands that the benefit to the public must be
considerably higher than the potential damage to the journalistic profession and the
victim of privacy invasion. Here, too, the cost-benefit ratio remains somewhat
speculative and arbitrary because it lacks clear and fair criteria for determining
what constitutes ‚Äúconsiderably higher‚Äù benefits. Secondly, the ‚Äúinformed consent‚Äù
criterion is based on the maxim of informational self-determination and thus
requires the unforced and well-informed consent of the individual whose privacy
is at risk (Van den Hoven Aspen and Vermaas 2007, p. 285). For private citizens
(as opposed to public figures), an overriding public interest could only be claimed
if public safety is at stake and if no alternative, less invasive courses of action are
available to reach the same goal.
5 Ethics, Privacy, and Self-Restraint in Social Networking                          53


   While it makes sense that the public interest might override the individual‚Äôs right
to privacy in certain instances, the issue becomes much more complicated when
special interests, such as businesses, are the driving force of privacy invasion.
Nissenbaum (2010, p. 111) argues that such particular interests are often disguised
as legitimate superior values, with the result that costs and benefits are unevenly
distributed at the expense of the individual. An ethically justifiable approach,
however, would require a fair distribution of costs and benefits. This could be
achieved with the above described framework of contextual integrity, which would
weigh the context-relative norms of the individual‚Äôs flow of information against the
new flow intended by the special interest actor. The invasion of privacy would only
be justified if the new flow was demonstrably at least as beneficial to the individual
as to the special interest.
   However, the contextual integrity framework has two minor conceptual flaws:
one is its preference for existing norms in present contexts, which may lead, as the
author concedes, to conservatism and the ‚Äútyranny of the normal‚Äù‚Äì just because a
social practice is well established does not mean it is a morally good practice. The
suggested remedy, the principle of moral superiority, is somewhat weak because it
relies on the optimistic assumption of a commonly accepted morality and is based
on a circular assessment of ‚Äúhow effective each (competing practice) is in
supporting, achieving, or promoting relevant contextual values‚Äù (Nissenbaum
2010, p. 166). Here, a normative ethical concept that provides a standard of
moral quality would be needed, such as the question of whether a new technology
or a new flow of information fosters autonomy, self-determination, and self-fulfillment
for both individuals and society as a whole; in other words, a standard that
foregrounds an emancipatory potential.
   The second flaw is that the contextual integrity framework provides little room
for the individual as an autonomous decision maker. The comparison of the
context-relative norms of the existing flow of information to those of the new
flow seems to operate like a court with the assumption of a generally accepted morality
as the judge. However, based on the principle of individual self-determination and
autonomy, one could argue that the informed consent criterion should govern the
comparison, and not some external moral force. This would also imply that
the default setting for privacy decisions must be positive consent: the proactive
opt-in choice, rather than the retroactive opt-out (Bowie and Jamal 2006, p. 330).
   Though preferred by online businesses, opt-out solutions are always problematic
from an ethical point of view because they shift the burden to the individual: the
opt-in approach disallows any privacy invasion unless the individual explicitly
agrees to share his or her information. Contrary to this, the opt-out approach
implicitly allows the invasion of privacy unless the user actually opts out. In
addition, individuals often do not know about the opt-out possibility, and opt-out
solutions often entail confusing piecemeal procedures or are hidden at the end of
lengthy and complicated user agreements (Bowie and Jamal 2006, p. 330). Indeed,
true self-determination and actual consumer choice can only be achieved through
opt-in as the default standard (Gandy 1993; Bowie and Jamal 2006). The more
consumer-friendly privacy laws in the European Union often include an opt-in
54                                                                           B. Debatin


requirement while US law, favoring business interests, does not even require
general opt-out procedures (Bowie and Jamal 2006, p. 331).
   In conclusion, the above discussion on the ethical justification of privacy
protection has shown that privacy and its protection are not negligible or secondary
values. Rather, they belong to the inner core of basic human rights and needs. The
discussion of privacy must be centered on the idea of contextual integrity and the
individual‚Äôs right to self-determination. This, then, provides the basis for an ethical
approach to privacy that prioritizes the individual‚Äôs privacy rights over others‚Äô
interest in privacy invasion. It leads to three moral principles:
1. The positive right to self-determination and the negative duty to minimize harm
   require a fair distribution of costs and benefits, determined by the comparison of
   the existing and the intended flow of information.
2. Individuals must have access to informed and positive consent (opt-in) when
   their context-appropriate flow of personal information is in danger of being
   breached.
3. An overriding interest in privacy invasion is justified only under special
   circumstances, such as a threat to public security or the individual, and only
   when no other, less invasive procedures would reach the same goal.




5.4    Privacy Protection in Online Social Networks

Privacy protection in online social media seems to be an oxymoron. After all, the
main purpose of participating in social networks is the exchange of information,
most of it highly personal, and the maintenance and expansion of one‚Äôs social
relationships. The informal character of online social networking and the possibility
to communicate casually with few words through wall posts and status updates
enables users to manage a large number of rather superficial contacts with relatively
little effort ‚Äì a phenomenon discussed in network sociology as ‚Äúweak ties in the
flow of information‚Äù (Gross and Acquisti 2005, pp. 2f.). The pervasiveness and
user-friendliness of social networking sites provide additional motivation for users
to post frequently. Thus, they voluntarily disclose large amounts of personal
information and contribute continually to the creation and maintenance of extensive
dynamic user profiles.
    However, social networking sites pose many privacy risks for their users,
ranging from unauthorized use of their information by government agencies and
businesses to attacks by hackers, phishers, and data miners (Lynch 2010; Clark and
Roberts 2010; WebSense 2010). Risks can also result from harmful activities by
other users, such as cyberstalking, harassment, and reputation damage (boyd and
Ellison 2008; Hoy and Milne 2010; Mishna et al. 2009). The potential risks can
actually be plotted on two dimensions: a horizontal axis, which is visible to the user,
and an invisible vertical one. The horizontal axis represents social interactions
among the users, where people present themselves though their profiles and engage
5 Ethics, Privacy, and Self-Restraint in Social Networking                           55


in communicative exchanges. The vertical axis is the systematic collection, aggre-
gation, and use of data by the networking company. The horizontal interactions
occur in the visible tip of the iceberg, while the data generated by the users trickle
down into the submerged part of the iceberg. For the average user, the vertical
invasion of privacy and its potential commercial or criminal exploitation by third
parties therefore tend to remain invisible (Debatin et al. 2009, p. 88; Nissenbaum
2010, pp. 221 ff.).
   The situation is aggravated by insufficient, sloppy, and misleading privacy
practices in online social networks, which have been criticized early on (Jones
and Soltren 2005; Privacy International 2007). The world‚Äôs largest online social
network Facebook, which had over half a billion users at the end of 2010, is known
for its cumbersome and confusing privacy features and its invasive and deceptive
practices (EPIC 2010). The default setting for its privacy features is usually at the
lowest, most open level and opt-out procedures are burdensome and convoluted,
which means that users have to be very proactive if they want to protect their
privacy effectively. All in all, social online networks perform poorly with respect to
privacy protection and data security. A 2010 study by the German consumer
organization ‚ÄúStiftung Warentest‚Äù found data protection in online social networks
to be rather weak. In the overall evaluation, only two of the ten networks tested
showed ‚Äúminor flaws,‚Äù while four displayed ‚Äúclear flaws‚Äù and four ‚Äúsevere
flaws‚Äù‚Äìamong the latter were the mega-networks Facebook, LinkedIn, and
MySpace (Test 2010).
   Studies on online privacy behavior have shown that social network users tend to
be rather careless with their personal data. Most users have a general awareness of
possible risks but do not act accordingly: they often know little about privacy
policies and use privacy settings inconsistently or not at all (Debatin et al. 2009).
The most common privacy risk management strategy is building fences, i.e.,
managing spatial boundaries by using the ‚Äúfriends only‚Äù setting to restrict the
visibility of one‚Äôs information, while users are ‚Äúless aware of, concerned about,
or willing to act on possible ‚Äútemporal‚Äù boundary intrusions posed by future
audiences because of the persistence of data‚Äù (Tufekci 2008, p. 33). And even the
‚Äúfriends-only‚Äù strategy is only used by a third to a half of the users (Ellison et al.
2007; Debatin et al. 2009). Moreover, the term ‚Äúfriend‚Äù is ambiguous in the online
world, designating soulmates, acquaintances, and strangers alike. Most Facebook
users have hundreds of friends, and statistically, about one third of users will accept
complete strangers as friends (Jones and Soltren 2005; Jump 2005).
   Even if a user profile is restricted to ‚Äúfriends only,‚Äù the restriction can easily be
bypassed through tagging, so that at least the friends of the friend who tagged
something can view this information. Worse yet, the ‚Äúfriends only‚Äù restriction
obviously affects only the horizontal dimension of interactions among users, but
has no impact on the vertical dimension of data harvesting by the networking
company and its partners. Therefore, it is highly questionable if one can call the
‚Äúfriends only‚Äù strategy a real ‚Äúprivacy-enhancing behavior,‚Äù as Stutzman and
Kramer-Duffield (2010) suggest. Might this particular strategy ‚Äì like privacy
technologies in general‚Äìsimply create a false sense of security among its users?
56                                                                            B. Debatin


This would be consistent with the finding that users tend to be satisfied with the
mere idea of privacy control without much real control: while they may use privacy
restrictions, ‚Äúthey do not quite understand that their level of privacy protection is
relative to the number of friends, their criteria for accepting friends, and the amount
and quality of personal data provided in their profiles, which they tend to divulge
quite generously‚Äù (Debatin et al. 2009, p. 102).
    Though ignorance and a false sense of security play an important role, it remains
perplexing why social networking users tolerate deep invasions of their privacy. An
important explanation lies in the expected benefits of social networking. The most
important gratification is arguably the social capital from creating and maintaining
contacts and friendships (see Ellison et al., this volume, chap. 3). In addition, social
media are now deeply rooted in everyday habits and routines. Routinized social
networking allows users to maintain relationships while keeping people at a
ritualized distance, thus enabling large scale weak ties management (Debatin
et al. 2009, p. 101). However, whether social network users follow a rational choice
model in weighing the benefits and risks, such as Petronio‚Äôs communication privacy
management model (Xu et al. 2008), is still questionable. Similarly unconvincing is
the hypothesis that they are just willing to take more risks than other people (Fogel
and Nehmad 2009; Ibrahim 2008). More likely, disclosure of private information in
online social networks happens through a kind of bargaining process in which the
perceived concrete benefits of networking outweigh the abstract interest in guarding
one‚Äôs privacy. The potential impact of the disclosure is a hypothetical event in
the future, while the benefits of social networking are tangible and immediate.
Moreover, in analogy to a third-person effect, possible risks are typically projected
into the environment and thus seen as happening to others, not to oneself (Debatin
et al. 2009).
    It is noteworthy, though, that users react with outrage to concrete and visible
violations of their privacy. When Facebook launched the ‚ÄúNews Feed‚Äù in September
2006, a feature that tracks users‚Äô activities and displays them on the pages of
their friends, users protested massively against this intrusive feature. They
formed anti-News Feed groups on Facebook, including the 700,000 member
group ‚ÄúStudents Against Facebook News Feed.‚Äù Facebook reacted to this by
introducing specific privacy controls for the News Feed (boyd 2008). Similarly,
the Facebook advertising platform Beacon, which broadcasted online shopping
activities to the users‚Äô friends, met great resistance when it was introduced in
November 2007. Facebook responded by first offering various opt-out features
and then, after continuing protests, changing to an opt-in policy for Beacon
(Nissenbaum 2010, p. 223).
    These privacy invasions visibly breached users‚Äô reasonable expectations of the
context-appropriate flow of their personal information. Applying the three moral
principles introduced earlier, the following conclusions can be drawn: Firstly, the
comparison of the existing and novel flow shows in both cases that costs and
benefits were unfairly distributed, thus violating principle 1. Secondly, massive
protest led to a repair of the disrupted flow of information (appropriate privacy
control tools in one case, and opt-in in the other). This reinstated principle 1 and
5 Ethics, Privacy, and Self-Restraint in Social Networking                             57


followed the requirements of principle 2. Thirdly, there was obviously no
overriding interest and no lack of alternative options that might have justified the
continuation of the invasive practices, as stated in principle 3. Finally, these
examples also show that moral outrage, public discourse, and political pressure
are necessary to effect change in privacy policies and practices. Only then can
businesses and governmental agencies be held accountable and compelled to adhere
to fair privacy standards.



5.5    Conclusion: Toward an Ethics of Self-Restraint

In order to have a vital public discourse about privacy invasions, they must be
brought to light and no longer be carried out under cover of invisibility or obscured
by ‚Äútechnological constraints.‚Äù Unfortunately, the widespread focus on technologi-
cal solutions to privacy problems not only results in a false sense of security, it also
encourages unthinking self-subordination to ostensible technological constraints.
This is part of the broader problem that technology creates a universe of immanence
with its own putatively inherent necessities and constraints, leading people to
believe that there are no alternatives to technological solutions and that they have
no agency and responsibility (Jonas 1984a).
   The first step toward regaining agency and responsibility is the development of
an enlightened understanding of technology and its unintended consequences. In
the case of privacy in social media, it means that users develop privacy literacy that
enables them to see through the technological veil and to make educated choices. In
other words, users of social media need to develop an informed concern about their
privacy, avoiding both moral panic and ignorant or naive indifference toward
information technology. This implies that users must inform themselves proactively
about the potential negative impact of social media on their privacy and that they
must acquire the skills necessary to mitigate or entirely prevent negative
consequences.
   A privacy-literate user would thus not simply make use of technical privacy
settings, because they are merely spatial access barriers that can always be
bypassed somehow. Additionally, this user would employ temporal privacy protec-
tion, i.e., limit the availability of free floating private information from the outset so
that it cannot be abused in the future. As long as there are no effective mechanisms
for user-driven data annulment, any personal information that is put out on the
Internet must be considered as if it were public, because information in digital
networks is persistent and can arbitrarily be copied, distributed, and repurposed
without the original owner‚Äôs knowledge and consent. Reducing the flow of infor-
mation is therefore a reasonable and effective strategy for maintaining the integrity
of personal information. Admittedly, this would require users to readjust their
expectations and behavior in social networking environments. It would require a
user-centered ethics of self-restraint as the guiding principle of operation
(Jonas 1984b). In a Kantian test of universalization, users who follow the principle
58                                                                                 B. Debatin


of self-restraint should always ask themselves, when posting information, if they
can at the same time will that this information become known not only to their
friends but to the whole world.
    This should not be misread as carte blanche for social network owners and others
to harvest user data. Rather, the user‚Äôs informed concern and the subsequent ethics
of self-constraint are corollaries to the three principles set forth above. Thus, the
onus is on all parties involved:
‚Ä¢ Network owners and third parties are expected to follow principles of fair
  information practices, i.e., to respect the user‚Äôs right to self-determination, to
  foster a fair distribution of costs and benefits, and to employ positive consent
  (opt-in) as a default. The ethics of self-restraint can be applied to them too, as
  they should put themselves in the shoes of their users and ask if they, in the
  position of the user, could at the same time will that their information become
  known not only to their friends but to the whole world.
‚Ä¢ Users have a responsibility to be sufficiently educated about their choices and
  actions in social media. After all, truly informed consent presupposes the user‚Äôs
  informed concern for his or her privacy.
‚Ä¢ And finally, ethicists, educators, system developers, and service providers are
  also responsible for creating an environment that fosters privacy literacy among
  the users of social media and in society as a whole.
   A turn toward respectful, fair, and open information practices, based on
informed consent and the ethics of self-restraint, may sometimes mean short-term
losses with regard to the data harvesting business. However, long-term benefits will
not only be enjoyed by users who interact in a safer and more trustworthy environ-
ment, they will also extend to social network owners and third parties because they
can be trusted and will thus gain and sustain a positive reputation among their
customers.




References

Altman I (1975) The environment and social behavior: privacy, personal space, territory,
   crowding. Cole Publishing Company, Monterey, CA
Baker DJ (2008) Constitutionalizing the harm principle. Criminal Justice Ethics 27:3‚Äì28, Sum-
   mer/Fall 2008
Bennett CJ, Raab CD (2006) The Governance of Privacy: Policy Instruments in Global Perspec-
   tive, Cambridge, MA, London: MIT Press, 2 ed.
Bowie NE, Jamal K (2006) Privacy rights on the internet: self-regulation or government regula-
   tion? Bus Ethics Q 16(3):323‚Äì342
boyd d (2008) Facebook‚Äôs privacy trainwreck: exposure, invasion, and social convergence.
   Convergence: The International Journal of Research into Media Technologies 14(1):13‚Äì20
boyd d, Ellison NB (2008) Social network sites: definition, history, and scholarship. JComput-
   Mediat Commun 13:210‚Äì230.http://jcmc.indiana.edu/vol13/issue1/boyd.ellison.html. Accessed
   12 Jan 2011
5 Ethics, Privacy, and Self-Restraint in Social Networking                                      59

Cate FH (1999) The changing face of privacy protection in the European Union and the United
    States. Ind L Rev 33:173‚Äì233
Christians CG (2010) The ethics of privacy. In: Meyers C (ed) Journalism ethics: a philosophical
    approach. Oxford University Press, Oxford, pp 203‚Äì214
Clark LA, Roberts SJ (2010) Employer‚Äôs use of social networking sites: a socially irresponsible
    practice. J Bus Ethics 95:507‚Äì525
Debatin B, Lovejoy J, Hughes B, Horn A (2009) Facebook and online privacy: attitudes, behaviors,
    and unintended consequences. J Comput-Mediat Commun 15(1):83‚Äì108. http://onlinelibrary.
    wiley.com/doi/10.1111/j.1083-6101.2009.01494.x/pdf Accessed 11 Jan 2011
ECHR (1950) European convention on human rights. Registry of the European Court of Human Rights
    2010., http://www.echr.coe.int/NR/rdonlyres/D5CC24A7-DC13-4318-B457-5C9014916D7A/0/
    ENG_CONV.pdf Accessed 15 Dec 2010
Ellison N, Steinfield C, Lampe C (2007) The benefits of Facebook ‚Äúfriends‚Äù: exploring the
    relationship between college students‚Äô use of online social networks and social capital.
    JComput-Mediat Commun 12, 4. http://jcmc.indiana.edu/vol12/issue4/ellison.html Accessed
    5 Dec 2010
EPIC (2010). Social Networking Privacy. Epic.Org Electronic Privacy Information Center. http://
    epic.org/privacy/socialnet/ Accessed 10 Dec 2010
European Parliament (1995). Directive 95/46/EC of the European Parliament and of the Council of
    24 October 1995 on the protection of individuals with regard to the processing of personal data
    and on the free movement of such data. Official Journal L 281, 23/11/1995 P. 0031 ‚Äì 0050.
    http://eur-lex.europa.eu/LexUriServ/LexUriServ.do?uri¬ºCELEX:31995L0046:en:HTML
    Accessed 15 Dec 2010
Fogel J, Nehmad E (2009) Internet social network communities: risk taking, trust, and privacy
    concerns. Comput Hum Behav 25:153‚Äì160
Gandy OH Jr (1993) The panoptic sort: a political economy of personal information. Westview
    Press, Boulder, CO
Gross R, Acquisti A (2005) Information revelation and privacy in online social networks.
    Workshop on Privacy in the Electronic Society (WPES). http://privacy.cs.cmu.edu/
    dataprivacy/projects/facebook/facebook1.pdf Accessed 22 Dec 2010
Habermas J (1962) Strukturwandel der OÃàffentlichkeit. Untersuchungen zu einer Kategorie der
    b‚Ç¨urgerlichen Gesellschaft. Darmstadt: Luchterhand Verlag. English Edition: Habermas J
    (1989) The Structural Transformation of the Public Sphere: An Inquiry into a category of
    Bourgeois Society (trans. Burger, T.). Cambridge, MA: MIT Press.
Hodges L (2009) Privacy and the press. In: Wilkins L, Christians CG (eds) The handbook of media
    ethics. Routledge, New York, pp 276‚Äì287
Hornung G, Schnabel C (2009) Data protection in Germany I: the population census decision and
    the right to informational self-determination. Comput Law Security Review 25:84‚Äì88
Hoy MG, Milne G (2010) Gender differences in privacy-related measures for young adult
    facebook users. J Interactive Advertising 10(2):28‚Äì45
Ibrahim Y (2008) The new risk communities: social networking sites and risk. Int J Media Cult
    Polit 4(2):245‚Äì253
Jonas H (1984a) The imperative of responsibility: in search of ethics for the technological age.
    University of Chicago Press, Chicago
Jonas H (1984b) Warum wir heute eine Ethik der Selbstbeschr‚Ç¨ankung brauchen. In: Str‚Ç¨   oker E (ed)
    Ethik der Wissenschaften? Philosophische Fragen. Wilhelm Fink Verlag, M‚Ç¨               unchen,
    Paderborn, Wien, Z‚Ç¨ urich, pp 75‚Äì86
Jones H, Soltren JH (2005) Facebook: threats to privacy (white paper, December 14, 2005). http://
    www-swiss.ai.mit.edu/6805/student-papers/fall05-papers/facebook.pdf. Accessed 12 Jan 2011
Jump K (2005) A new kind of fame: MU student garners a record 75,000 Facebook friends.
    Columbia Missourian, 1.9.2005. http://www.columbiamissourian.com/stories/2005/09/01/a-
    new-kind-of-fame/. Accessed 5 Jan 2011
Kant I (1964) Groundwork of the metaphysic of morals (trans. H.J. Paton). Harper & Row, New
    York (Original work published 1785 in German)
60                                                                                    B. Debatin


Leino-Kilpia H, V‚Ç¨alim‚Ç¨aki M, Dassen T, Gasull M, Lemonidou C, Scott A, Arndt M (2001)
   Privacy: a review of the literature. Int J Nurs Stud 38:663‚Äì671
Linklater A (2006) The harm principle and global ethics. Global Soc J Interdisciplinary Int Relat
   20(3):329‚Äì343
Lombard E (2010) Bombing out: using full-body imaging to conduct airport searches in the United
   States and Europe amidst privacy concerns. Tul J Int Comp Law 19(1):337‚Äì367
Lynch J (2010). New FOIA documents reveal DHS social media monitoring during Obama
   inauguration. Electronic Frontier Foundation, 13.10.2010. http://www.eff.org/deeplinks/
   2010/10/new-foia-documents-reveal-dhs-social-mediaAccessed 11 Jan 2011
Mill JS (1991) On liberty and other writings. Oxford University Press, Oxford (Original work
   published 1841)
Mishna F, McLuckie A, Saini M (2009) Real-world dangers in an online reality: a qualitative study
   examining online relationships and cyber abuse. Soc Work Res 33(2):107‚Äì118
Nissenbaum H (2010) Privacy in context. technology, policy, and the integrity of social life.
   Stanford University Press, Stanford
OECD (1980). Guidelines on the protection of privacy and transborder flows of personal data.
   Organisation for Economic Co-operation and Development, Washington, DC. http://www.
   oecd.org/document/18/0,3343,en_2649_34255_1815186_1_1_1_1,00.html. Accessed 15 Dec
   2010
Privacy International (2007) A race to the bottom: privacy ranking of internet service
   companies‚ÄìA consultation report. Privacy International, June 9, 2007.http://www.privacyin-
   ternational.org/article.shtml?cmd[347]¬ºx-347-553961. Accessed 22 Dec 2010
Rouvroy A, Poullet Y (2009) The right to informational self-determination and the value of self-
   development: reassessing the importance of privacy for democracy. In: Gutwirth S, Poullet Y,
   De Hert P, de Terwangne D, Nouwt S (eds) Reinventing data protection? Springer, New York,
   pp 45‚Äì76
Solove DJ (2008) Understanding privacy. Harvard University Press, Cambridge, MA
Solove DJ, Rothenberg M, Schwartz PM (2006) Privacy, information, and technology. Aspen,
   New York
Stutzman F, Kramer-Duffield J (2010). Friends only: examining a privacy-enhancing behavior in
   facebook. In: CHI ‚Äò10 Proceedings of the 28th international conference on Human factors in
   computing systems, ACM Digital Library. http://portal.acm.org/citation.cfm?id¬º1753559.
   Accessed 26 Dec 2010
Test (2010) Soziale Netzwerke: Datenschutz oft mangelhaft. Stiftung Warentest ‚Äì test.de, 25. 03.
   2010. http://www.test.de/themen/computer-telefon/test/Soziale-Netzwerke-Datenschutz-oft-
   mangelhaft-1854798-1855785/. Accessed 3 Dec 2010
Tufekci Z (2008) Can you see me now? Audience and disclosure regulation in online social
   network sites. B Sci Technol Soc 28(1):20‚Äì36
United Nations (1948) The universal declaration of rights. United Nations. http://www.un.org/en/
   documents/udhr/index.shtml. Accessed 15 Dec 2010
Van den Hoven Aspen J, Vermaas PE (2007) Nano-technology and privacy: on continuous
   surveillance outside the panopticon. J Med Philos 32:283‚Äì297
Warren S, Brandeis L (1890) The right to privacy. Harv Law Rev IV(5):193‚Äì220
WebSense (2010) Facebook used for phishing attacks and open redirects. In: WebSense Security
   Labs Blog, 29. 11. 2010. http://community.websense.com/blogs/securitylabs/archive/2010/11/
   29/facebook-used-for-phishing-attacks-and-open-redirects.aspx. Accessed 12 Jan 2011
Westin AF (1967) Privacy and freedom. Atheneum, New York
Whitehouse G (2010) Newsgathering and privacy: expanding ethics codes to reflect change in the
   digital media age. J Mass Media Ethics 25(4):310‚Äì327
Xu H, Dinev T, Smith HJ, Hart P (2008) Examining the formation of individual‚Äôs privacy
   concerns: toward an integrative view. In: International conference on information systems
   (ICIS) ICIS 2008 proceedings, Paris. http://faculty.ist.psu.edu/xu/papers/conference/icis08a.
   pdf. Accessed 22 Dec 2010
Chapter 6
The Social Web as a Shelter for Privacy
and Authentic Living

Sabine Trepte and Leonard Reinecke




6.1    Introduction

Social network sites are known for intruding their users‚Äô privacy per default. The
networks use and sell demographic information for targeted advertising (Acquisti
et al. 2007). Data are replicated by users and transferred to unknown third parties; the
user‚Äôs utterances (e.g., on fan pages) are searched, analyzed, and scaled in market
research (Nissenbaum 2009). Although users seem to be aware of this situation,
the majority of users do not complain or change their self-disclosure online (boyd
and Hargittai 2010, p. 320; Christofides et al. 2009). We find a very loose and laissez-
faire behavior in terms of how users deal with the threats to and their own concerns
about informational privacy online. Scholars have termed this contradiction
the ‚Äúprivacy paradox,‚Äù indicating that people seem to know about privacy threats
on the one hand, but do not enact their privacy needs on the other (Barnes 2006).
    To this notion of paradoxical privacy behavior, we would like to add a notion that
we think has been neglected in previous debates and research. From our perspective,
users are concerned in terms of informational privacy, but think they have great
control in terms of social privacy and even feel that they benefit in terms of their
perceived psychological privacy (Burgoon 1982). Informational privacy addresses
whether people are able to control which and how much information about them is
shared by others. Social privacy refers to the dialectic process of managing proxim-
ity and distance towards others. It is given if people feel in control of the amount and
kind of interactions they have with others (Burgoon 1982). Psychological privacy is
related to the control over emotional and cognitive inputs and outputs. A high level
of psychological privacy exists if free speech and thought is possible, and if people
may decide with whom to share their feelings and thoughts. That said, we would like
to posit here that the majority of users feel threatened in terms of informational


S. Trepte (*) ‚Ä¢ L. Reinecke
University of Hamburg, Hamburg, Germany
e-mail: sabine.trepte@uni-hamburg.de

S. Trepte and L. Reinecke (eds.), Privacy Online,                                    61
DOI 10.1007/978-3-642-21521-6_6, # Springer-Verlag Berlin Heidelberg 2011
62                                                             S. Trepte and L. Reinecke


privacy online, but that the main benefits that users find in the Social Web are rooted
in perceived social and psychological privacy.
    In terms of social privacy, the Social Web offers the possibility (or illusion) of
controlling with whom to interact and to share information by means of
mechanisms such as friends lists on social network sites. In terms of psychological
privacy, the Social Web offers tremendous possibilities for publishing one‚Äôs
thoughts and feelings without being censored. In sum, users feel able to control
their privacy via privacy settings and friends lists. Thereby, the subjective experi-
ence of privacy may be even richer in the Social Web than offline. People create
online spaces of social and psychological privacy that may be an illusion; however,
these spaces seem to be experienced as private and the technical and social
architecture of the Social Web supports this notion. Within these online spaces of
privacy, people experience the chance to be authentic. We would also go so far as to
claim that these online spaces of psychological privacy predominantly seem to exist
because they allow authenticity. As privacy and authenticity are basic human needs
and important for psychological functioning and well-being (Kernis and Goldman
2006), users might accept trading off their informational privacy. The benefits of
finding online spaces of psychological privacy that allow for authentic living seem
to outweigh the loss of informational privacy.
    In this chapter we will elaborate on this line of thought by firstly arguing that
although the Social Web poses a threat to informational privacy, it offers online
spaces of social and psychological privacy (see Sect. 6.2). We will then review the
psychological groundwork on the concept of authenticity (see Sect. 6.3) and argue
that privacy allows for authentic functioning (see Sect. 6.4). In the following, we
will argue that the Social Web may be perceived as a shelter for authentic living
because it offers online spaces of privacy. This experience is rooted in users‚Äô
perception of successfully controlling audiences, interaction partners, and the
content they are publishing (see Sect. 6.5). In our discussion (Sect. 6.6), we will
suggest that users accommodate online spaces to their full advantage by (more or
less consciously) trading off their informational privacy. We will discuss how this
trade-off might affect future online behavior and online services. Also, we will
argue that upcoming research on online privacy should be fine-grained in terms of
its assumptions about what kinds and types of privacy users do or do not experience
online.




6.2    Informational and Psychological Online Privacy: Trading
       Off One for the Other?

Contemporary conceptions of privacy and privacy management have been strongly
influenced by the work of Alan Westin and Irwin Altman. Both Westin and Altman
refer to privacy as a dynamic process of boundary management (Altman 1975;
Westin 1967). While Westin (2003) defines privacy as ‚Äúthe claim of an individual to
6 The Social Web as a Shelter for Privacy and Authentic Living                       63


determine what information about himself or herself should be known to others‚Äù
(p. 431), Altman (1975) defines privacy as ‚Äúselective control of access to the self or
to one‚Äôs group‚Äù (p. 18). With their emphasis on controlling or regulating access to
the self, both theories can be categorized as examples for ‚Äúlimited-access‚Äù
approaches to privacy (Margulis 2003, p. 423). Furthermore, both Altman (1975)
and Westin (1967) describe privacy as a non-monotonic function: the optimal level
of privacy is not reached by a maximum of solitude or isolation. Rather, privacy
needs to fluctuate dynamically according to specific situations and individuals may
experience too little, just enough, or too much privacy (Margulis 2003). This idea is
expressed well in Altman‚Äôs (1975) distinction between desired privacy and
achieved privacy. While desired privacy represents an individual‚Äôs desire for a
certain level of interaction in a given situation, achieved privacy represents the
actual level of contact resulting from interaction in the respective situation. Thus,
Altman (1975) describes privacy as an optimizing process that aims at matching the
levels of desired and achieved privacy. The importance of successful privacy
regulation is emphasized by Westin (1967), who argues that privacy is a crucial
psychological resource and fosters important processes, such as personal autonomy,
emotional release, self-evaluation, and protected communication.
    The high complexity of privacy has led to further theoretical developments.
Building on the seminal work of Altman, Westin, and other privacy theorists,
Burgoon (1982) has developed a multi-dimensional definition of privacy that
encompasses the distinction of four interdependent dimensions of privacy:
1. Informational privacy is defined as ‚Äúthe ability to control who gathers and
   disseminates information about one‚Äôs self or group and under what
   circumstances‚Äù (Burgoon, et al. 1989, p. 134). Informational privacy is strongly
   affected by the way modern societies collect, store, and process personal infor-
   mation, such as medical or financial records, application data, customer data,
   and ‚Äì since the advent of the Internet ‚Äì a plethora of personal information
   available through online databases, search engines, or social network sites.
2. Social privacy, later referred to as interactional privacy by Burgoon et al.
   (1989), describes an individual‚Äôs ‚Äúability to withdraw from social intercourse‚Äù
   (Burgoon 1982, p. 216) and ‚Äúany efforts to control one‚Äôs degree of social
   contacts‚Äù (p. 217). Social privacy is crucial in establishing closeness among
   some interactional partners while establishing a distance from others.
3. Psychological privacy refers to ‚Äúone‚Äôs ability to control affective and cognitive
   inputs and outputs‚Äù (Burgoon 1982, p. 224). It thus includes both the freedom to
   decide what, when, and to whom to disclose personal feelings and thoughts
   (output), as well as protection from cognitive or affective interference from
   others, such as persuasive pressures (input). Psychological privacy is an impor-
   tant resource that fosters the development of self-identity, autonomy, and per-
   sonal growth (Burgoon 1982).
4. Physical privacy refers to ‚Äúthe freedom from surveillance and unwanted intrusions
   upon one‚Äôs space by the physical presence, touch, sights, sounds, or odors of
   others‚Äù (Burgoon et al. 1989, p. 132) and puts an emphasis on the control over
   the degree of the physical accessibility or inaccessibility to others (Burgoon 1982).
64                                                              S. Trepte and L. Reinecke


   In the following paragraphs we will focus on informational, social, and psycholog-
ical privacy and discuss how users perceive all of these dimensions of privacy while
surfing the Social Web. As the dimension of physical privacy has limited relevance for
online communication and online self-disclosure, it does not seem as important for our
considerations about online privacy and will not be discussed further here.
   Previous research has predominantly addressed informational privacy and users‚Äô
concerns related to their personal information. A number of studies have looked at
general online privacy concerns, for example, with regard to credit card fraud,
identity theft, viruses and spyware, unwanted dissemination of personal data or the
abuse of personal information (e.g., for marketing purposes), and at online privacy
protection behaviors (Buchanan et al. 2007; Cho et al. 2009; Paine et al. 2007; Yao
et al. 2007). With regard to the Social Web, studies have been concerned with the
breadth and depth of information provided in blogs or social network sites and with
the use of privacy settings to protect personal information from unintended
audiences (Acquisti and Gross 2006; Debatin et al. 2009; Lampe et al. 2007;
Lewis et al. 2008; Tufekci 2008).
   Social privacy in online contexts has not been addressed directly in prior
research. However, there are many studies indicating how people enact and per-
ceive their social privacy online (Debatin et al. 2009; Lewis et al. 2008). Users have
been shown to be very well-informed about how to deal with their privacy settings
or friends lists (boyd and Hargittai 2010). Also, they feel to be able to withdraw
from interactions (by turning the computer off or leaving the room) at anytime. In
particular, the technical features of the Social Web and social network sites enable
users to engage in a number of audience management strategies such as joining
groups and inviting or ignoring ‚Äúfriends.‚Äù
   Psychological Privacy online has also not been addressed directly in previous
research, but there are a number of empirical findings indicating that users may have a
strong sense of control with regard to their cognitive and emotional inputs and outputs
in online environments. Firstly, self-disclosure as a correlate of privacy and as a form
of emotional and/or cognitive output (and hence directly related to psychological
privacy) has been shown to be higher in computer-mediated communication as well as
in the Social Web when compared with face-to-face interaction (Joinson 2001;
Tidwell and Walther 2002). Secondly, subjective feelings of anonymity or intimacy
have often been used to explain why self-disclosure is more likely in computer-
mediated communication or online than face-to-face (Walther 1996). In two
experiments, Joinson (2001) demonstrated that computer-mediated communication
was associated with higher levels of spontaneous self-disclosure than face-to-face
interaction, and that this effect could be explained through visual anonymity in
computer-mediated communication. Furthermore, Tidwell and Walther (2002)
demonstrated that in computer-mediated communication, interaction partners engage
in more intimate questions and self-disclosure utterances than individuals in face-to-
face interaction. These results support the assumption that the computer-mediated
setting seems to create a sense of privacy that is experienced in intimate face-to-face
settings.
6 The Social Web as a Shelter for Privacy and Authentic Living                     65


   Similar results can be found for Social Web use. A number of studies demon-
strate a high amount of disclosure and thus indicate that Social Web users exhibit
communication behavior found in contexts experienced as private. The majority of
blog and social network site users disclose detailed personal information such as
personal feelings and thoughts; also, they grant insights into spheres considered
private, such as details about their life with family and friends (Christofides et al.
2009; Debatin et al. 2009; Hinduja and Patchin 2008; Lampe et al. 2007; Nardi et al.
2004; Tufekci 2008; Viegas 2005).
   We believe that the ‚Äúprivacy paradox‚Äù (Barnes 2006) may not be so paradox if
we take a closer look at the users‚Äô behavior and experiences. While users may be
concerned about the uncontrolled dissemination and potential abuse of their per-
sonal data and may thus experience threats to their informational privacy, the Social
Web seems to foster feelings of psychological and social privacy. While the
following section will present an introduction to basic theoretical conceptions of
authenticity, the interaction between privacy and authenticity and the effects of the
Social Web on authentic behavior will be discussed further in Sects. 6.4 and 6.5.


6.3    Authenticity

Authenticity is usually termed as having two suppositions: firstly that people know
their thoughts and emotions, and secondly that they act in accordance with both
(Harter 2002). In previous research on authenticity, it has been found that people
can be most authentic if they feel minimally determined by role expectations
(Sheldon et al. 1997). As such, authenticity has also been termed as being equiva-
lent to self-determination and as an expression of the so called ‚Äútrue-self‚Äù or ‚Äúcore-
self‚Äù (Kernis and Goldman 2005, 2006). Kernis and Goldman (2006) define
authenticity as ‚Äúthe unobstructed operation of one‚Äôs true- or core-self in one‚Äôs
daily enterprise‚Äù (p. 294).
    There are different theoretical approaches and a number of suggestions for
operationalizing authenticity (Harter 2002; Sheldon et al. 1997; Wood et al.
2008). Kernis and Goldman‚Äôs (2006) multicomponent conceptualization of authen-
ticity seems to be particularly helpful in defining and further understanding authen-
ticity and authentic functioning. They suggest breaking down authenticity into four
separate, but interrelated components:
1. Awareness refers to possessing and being motivated to increase knowledge of
   and trust in one‚Äôs emotions and cognitions.
2. Unbiased processing implies that people are able to objectively process positive
   as well as negative self-aspects such as emotions, private knowledge, and
   internal experiences.
3. Behavior as a third component of authenticity refers to behavior that reflects
   one‚Äôs values, preferences, and needs as opposed to acting falsely to please others,
   to obtain or to retain rewards. Inauthentic behavior involves being either unaware
   of or otherwise oversimplifying self-aspects relevant to a behavioral context.
66                                                                       S. Trepte and L. Reinecke


4. Relational orientation involves openness, sincerity, and truthfulness in close
   relationships.
   As the four dimensions are separable, a person might be aware of her or his
values but might not be able to express these values in a certain setting. The
question arising here is whether somebody being aware of his thoughts but lacking
expression thereof may still be considered an authentic personality. It also remains
questionable whether we can be fully authentic at all, as we are bound to cultural
norms and certain role expectations in many settings such as family, work, or
leisure time settings. This question has raised quite a degree of consideration in
previous research (Sheldon et al. 1997). The authenticity theories rooted in self-
determination theory, such as the multicomponent conceptualization of authenticity
(Kernis and Goldman 2006), suggest that people can be authentic as long as they are
self-determined. This does not necessarily mean unveiling one‚Äôs true self in a
setting that does not seem appropriate to do so. Different approaches for staying
authentic may be considered. Firstly, authentic people may actively self-select
environments that have the fewest environmental boundaries and thus allow for
authentic living:
     By having greater self-understanding, individuals high in authenticity seemingly are capable
     of self-selecting appropriate niches in their interpersonal milieu that sustain and promote
     their interpersonal and psychological adjustment. (Kernis and Goldman 2006, p. 320)

    Secondly, the expectations set in a certain situation or environment may not be
fulfilled. For authentic persons it might be more satisfying if they do not live up to
the standard set in a certain environment rather than sacrifice their authenticity for
the sake of social norms and expectations. Furthermore, authentic people might see
it as an opportunity for individual growth and challenge to face an environment
where they have to fight to express their true self while staying in touch with
themselves and others.
    A positive relationship between authenticity and healthy psychological and
interpersonal functioning has been demonstrated in previous research. Well-being
(Hodgins and Knee 2002; Kernis and Goldman 2006; Wood et al. 2008), self-
esteem, life satisfaction (Kernis and Goldman 2005), and other measures of psy-
chological health or distress such as anxiety, depression, stress, and symptomatol-
ogy are significantly related to authenticity (Sheldon et al. 1997). The importance of
authenticity for healthy psychological functioning may explain why people actively
seek spaces where they can be authentic. In the following section we will show that
private spaces are particularly suited to enacting authentic behavior.



6.4      How Privacy Fosters Authenticity

How do privacy and authenticity interact? And why do we need privacy to be
authentic? Firstly and most importantly for the development of an authentic per-
sonality, privacy represents a psychological resource that fosters self-determined
6 The Social Web as a Shelter for Privacy and Authentic Living                       67


behavior (Westin 1967). This psychological impact of privacy is very well
represented in Westin‚Äôs (1967) notion of privacy functions. According to Westin
(1967), privacy encompasses four processes that are crucial to psychological func-
tioning: self-evaluation, autonomy, emotional relief and protected communication.
    Privacy facilitates self-evaluation by creating protected rooms and situations that
allow an individual to reflect upon his feelings and identity without the threat of
social punishment. With regard to authenticity, self-evaluation appears to be a
crucial predisposition for the authenticity component of awareness (Kernis and
Goldman 2006). Without private situations that allow for self-reflection and evalu-
ation, a person‚Äôs ability to gather knowledge about his emotions and cognitions is
severely limited.
    Autonomy, the second function of privacy identified by Westin (1967), refers to
the absence of manipulation or dominance by others and has strong implications
for the authenticity dimension of unbiased processing (Kernis and Goldman 2006).
The absence of external manipulation is a crucial precondition for the objective
processing of positive and negative self-aspects necessary for authentic behavior
and communication.
    Furthermore, privacy facilitates emotional relief (Westin 1967) by allowing
individuals to ‚Äúlay their mask aside‚Äù (p. 35) and to deviate from social norms.
This privacy function is very clearly related to the behavior component of authen-
ticity (Kernis and Goldman 2006). By creating a break from social norms and
expectations, privacy increases the likelihood of behavior that reflects one‚Äôs values,
preferences, and needs.
    Finally, limited and protected communication, the last of Westin‚Äôs (1967)
privacy functions, refers to the ability to create intimacy and confidentiality as
well as boundaries and distance among partners of social interaction. This privacy
function is very likely to foster the authenticity dimension of relational orientation
(Kernis and Goldman 2006). By creating intimate social interactions and enhancing
confidentiality and trust among interaction partners, privacy is very likely to
increase the willingness for openness, sincerity, and truthfulness in close
relationships.
    As demonstrated above, the four privacy functions proposed by Westin (1967)
can be mapped to the four authenticity components of Kernis and Goldman (2006).
In sum, privacy as a psychological resource gives individuals the freedom neces-
sary for an undistorted reflection on the true self and for authentic behavior and self-
presentation. The interrelation of privacy and authenticity helps us to understand
authentic functioning in the Social Web.
    The preceding analysis of privacy functions and authenticity support our initial
notion that the Social Web is a particularly well-suited environment for stimulating
authentic behavior (cf. Sect. 6.2). The specific features of the Social Web increase
the subjective feelings of psychological and social privacy. Online spaces of
psychological as well as social privacy, in turn, empower Social Web users to
create protected virtual spaces for authentic behavior. A deeper analysis of the
mechanisms that foster authentic behavior on the Social Web will be presented in
the next section.
68                                                               S. Trepte and L. Reinecke


6.5     How Online Spaces of Privacy Allow for Authentic Living

Since very early work on the Internet, it has been described as enhancing self-
expression (Turkle 1996). Bargh et al. (2002) hold that ‚Äú[. . .] we would expect a
person to use it first and foremost to express those aspects of self that he or she has
the strongest need to express ‚Äì namely, the true-self [. . .]‚Äù (p. 34).
    In the preceding paragraphs we suggested that private spaces may encourage
authentic living, that people seem to find these spaces particularly online, and that
authentic living leverages well-being and psychological functioning. We showed
that authenticity is rooted in self-determination theory (Deci and Ryan 2000), thus it
can also be termed as a basic need that people strive to fulfill in their day-to-day life.
However, not all people are able to fulfill this need. Their environments may exert
strong role constraints and role pressure and thus prohibit authentic living (Kernis
and Goldman 2006). Online environments may be forums that enable individuals to
be authentic. We posit here that ‚Äì although the Social Web has often been accused
of violating people‚Äôs informational privacy ‚Äì it offers ‚Äúonline spaces of privacy‚Äù to
its users and consequently endorses self-determination and authenticity.
    However, what are the underlying psychological mechanisms that make people
think they can be authentic in these psychological spaces of online privacy? We
suggest that the subjective experience of privacy control grants the experience of
authenticity. Thus we assume that people feel they can be authentic online because
they create online spaces of privacy by controlling audiences, interaction partners,
and content (Ben-Ze‚Äôev 2003). In the following we will differentiate between
controlling audiences and interaction partners on the one hand, and controlling
content on the other. Whereas the control of audiences refers to social privacy,
the control of content refers to psychological privacy.


6.5.1    Control over Audiences and Interaction Partners

Social networks offer a variety of technical settings and mechanisms for privacy
management such as friends lists and privacy settings. These allow users to control
the groups that they communicate with. This particular architecture is designed to
control access and it is very simple to handle. All audience management strategies
only need a couple of mouse clicks to be executed. This is not much compared to
offline strategies for privacy management and access control, such as architectural
features of built environments (Vinsel et al. 1980), the enforcement of social norms,
the use of verbal and non-verbal cues, or other distancing behavior (Altman 1975;
Burgoon 1982). Users gain the impression of being able to control their social
privacy online easily and successfully.
   In Sect. 6.3 we showed that two approaches are suited for making authentic
behavior likely: one being that people actively select environments where they feel
free to show their true self, and the other that people face environmental constraints
and see conflicts as an opportunity for individual growth. The first approach seems
6 The Social Web as a Shelter for Privacy and Authentic Living                     69


to be used particularly on social network sites and in other Social Web services
(Ben-Ze‚Äôev 2003). Users control access of audiences and interaction partners and
thus create niches in which they feel they can be authentic.
    We assume that the controlled selection of interaction partners is the presuppo-
sition for a sense of self-determination and authenticity. Social privacy, the ability
to control the degree of social interaction and the partners of social interaction
(Burgoon 1982), relates to the ability to engage in limited and protected communi-
cation (Westin 1967) and to create niches that allow for authentic behavior.
    Whereas controlling the boundaries to other persons and groups is one mecha-
nism for creating spaces of privacy, the control over content is another. The latter
will be described in the following paragraph.



6.5.2    Control over Content

Controlling what emotions and thoughts we share with others is a ‚Äúsine qua non‚Äù for
the experience of psychological privacy. Psychological privacy refers to the ability
to control emotional as well as cognitive input and output (cf. Sect. 6.2). This seems
to be particularly related to the authenticity component ‚Äúunbiased processing‚Äù (cf.
Sect. 6.3), which implies that authentic people are able to objectively process
positive as well as negative self-aspects such as emotions, private knowledge, and
internal experiences. Both terms ‚Äúpsychological privacy‚Äù and ‚Äúunbiased
processing‚Äù refer to the content of thoughts and emotions. As psychological privacy
refers to the ability to control cognitive and emotional inputs and outputs (Burgoon
1982), it shows a direct connection to the absence of external influences hindering
authenticity. In other words, the more psychological privacy a person possesses, the
higher the person‚Äôs autonomy (Westin 1967), enabling the person to elude social
norms and expectations and to behave authentically.
   Results from Schouten et al. (2007) emphasize the importance of perceived
controllability of message construction and responses to other messages in
computer-mediated communication compared to face-to-face interaction. Based
on data from a survey among 1,340 Dutch adolescents, the authors demonstrated
that the subjective relevance of heightened controllability in computer-mediated
communication was a positive predictor of self-disclosure in instant messaging.
   The perceived ability to control affective and cognitive inputs and outputs is the
main assumption of psychological privacy (Burgoon 1982). It seems to be amplified
through different features of the Social Web. Here we will address two means of
controlling affective inputs and outputs.
   One means of controlling cognitive output is to reread and edit utterances before
posting them online. We believe that the Social Web and particularly social
network sites are experienced as a very instantaneous medium. In their status
notes, blog posts, or instant messages, users publish information that is comparable
to ‚Äúsmall talk‚Äù information. However, in contrast to face-to-face small talk, status
notes may be reread and edited before being published. This editing process may
70                                                             S. Trepte and L. Reinecke


only last a couple of seconds, but increases the experience of control as opposed to
face-to-face interaction.
   A second means of controlling emotional output is reached by communicating
verbally only. Facial cues are not available during interaction in the Social Web and
thus emotional output stays under the user‚Äôs control. Fear, anger, happiness, and
concerns are emotions that can be read in someone‚Äôs face (Berry 1991; Ekman et al.
2001; Hassin and Trope 2000). However, facial cues as an information source about
the interaction partner‚Äôs emotions are not accessible online. Posting a status note on
a social network site gives users‚Äô the chance to verbally focus on the thoughts they
want to express and to filter out unwanted emotional expression. Social Web users
might feel psychologically private online, because there is no risk of conflicting
messages resulting from verbal and facial expressions. From the perspective of the
user, the heightened control over verbal and non-verbal cues in computer-mediated
communication, in combination with the heightened control over interaction
partners in the Social Web (cf. Sect. 6.5.1), grants access to limited and protected
communication, one of Westin‚Äôs (1967) privacy functions, which refers to the
ability to create intimacy and confidentiality as well as boundaries and distance
among partners of social interaction. We suggested in Sect. 6.4 that this privacy
function may support relational orientation as one component of authenticity
(Kernis and Goldman 2006). By creating intimate social interactions and interper-
sonal trust among interaction partners, privacy is very likely to increase openness,
sincerity, and truthfulness.
   In sum, we suggest here that users may experience psychological spaces of
privacy while surfing the Social Web, because they can edit and reread their verbal
expressions online. Also, users may feel psychologically private, because their
interaction partners do not see their facial expressions and other non-verbal cues.
Both kinds of content control allow for protected communication, increase subjec-
tive feelings of psychological privacy, and thus foster authentic behavior in the
Social Web.



6.6    Discussion and Future Perspectives

In this chapter we argued that social network site users may feel threatened or even
exploited in terms of informational privacy online, but that they benefit in terms of
social and psychological privacy. Social privacy is easily found online by
controlling and managing audiences and interaction partners. Psychological privacy
is created by managing the quantity and quality of personal information that is
shared with others. The Social Web offers several mechanisms for regulating access
to the self, such as friends lists, privacy settings, and group creation and participa-
tion. This fine-grained ‚Äúprivacy-tuning‚Äù gives users (the illusion of) private spaces.
Within these online spaces of privacy, they experience fewer role constraints and
expectations than they face in many offline environments. We thus suggested that
these online spaces are perceived as spaces for authentic living. The more control a
6 The Social Web as a Shelter for Privacy and Authentic Living                        71


person has regarding the partners involved in social interaction and the content of
interaction, the easier it is to create social rooms and situations that foster authentic
living.
    We validated single steps in our line of thought with prior research stemming
from privacy and authenticity theories and from research on the Social Web.
However, we made two very new assumptions that require empirical investigation.
Firstly, we suggested that people perceive a loss of informational privacy but
perceive a considerable amount of social and psychological privacy in online
contexts. Secondly, we suggested that in these online spaces of privacy, authenticity
is more likely to be shown. We also went so far as to say that these online spaces of
privacy are controlled and created because they grant spaces for authentic living.
Whereas other claims we made in this chapter have been investigated before, these
two assumptions need further investigation and empirical validation.
    In terms of theory, the critical question about our line of thought seems to be
whether the proclaimed ‚Äúonline spaces of social and psychological privacy‚Äù are an
illusion or whether they carry a certain truth. In terms of data management, psycho-
logical and social privacy online are only possible if the data are handled privately.
As long as online data (e.g., on social network sites) are scaled, mined, and sold, these
data are not private and all utterances that may be perceived as psychologically or
socially private are not private from an informational point of view. However, the
users‚Äô subjective experiences have to be taken into account as well as the objective
facts. Therefore, from a psychological point of view, the truth or existence of online
privacy may be assessed by simply asking the users what they experience. Further-
more, by putting users in conditions of more or less psychological and social privacy, it
would be possible to investigate how these conditions are experienced and how they
may affect authenticity. As long as we find authenticity as an effect of ‚Äúexperienced‚Äù
privacy, we would posit from a psychological perspective that online privacy is surely
not an illusion. Privacy would then ‚Äúexist‚Äù as a user experience. Thus, we ‚Äì as Social
Web or privacy scholars and as a society ‚Äì will have to face the phenomenon that an
experience of social and psychological privacy might be a truth for a majority of users
and consequently endorses privacy-related behavior such as authenticity online.
    The assumption of people experiencing their online lives as private and thus
behaving authentically may lead to different scenarios in the future. One would be
that the ongoing scaling, mining, and selling of data that currently generates the
revenues of social network sites might continue and might be advanced and
extended in the future. In this scenario, the networks will make their users feel
increasingly private, with the aim of triggering private self-disclosure, because
personal data in particular generates revenues from advertisers and targeting
companies. In contrast, a second scenario would be that users might become
more aware of the illusiveness of their private niches online and might want to
create spaces that not only feel private in a psychological and social sense, but
also grant informational privacy. They might also become aware that privacy is
the ‚Äúcurrency‚Äù of the Social Web and that they trade their informational privacy
for server space (e.g., webmail, drop-box services), access to online infrastructure
(e.g., social network sites), or online content. In this scenario they may start to claim
72                                                                    S. Trepte and L. Reinecke


back their informational privacy online. Online services might then generate
revenues by offering services that guarantee informational privacy and bill their
users monetarily. Such online spaces that provide psychological and social privacy
while protecting the users‚Äô informational privacy might eventually be the perfect
niche for authentic online behavior. And it could be a chance for social network
sites: they would not only demand authentic and open communication from their
users, but for a change, could be authentic themselves by communicating what they
sell and what they bill their customers.



References

Acquisti A, Gross R (2006) Awareness, information sharing, and privacy on the Facebook. Paper
   presented at the 6th Workshop on Privacy Enhancing Technologies, Cambridge, 28‚Äì30 June 2006
Acquisti A, Gritzalis S, Lambrinoudakis C, De Capitani di Vimercati S (eds) (2007) Digital
   privacy: theory, technologies, and practices. Auerbach, Boca Raton
Altman I (1975) The environment and social behavior: privacy, personal space, territory,
   crowding. Brooks/Cole, Monterey
Bargh JA, McKenna KYA, Fitzsimons GM (2002) Can you see the real me? Activation and
   expression of the ‚Äútrue self‚Äù on the internet. J Soc Issues 58(1):33‚Äì48
Barnes SB (2006) A privacy paradox: Social networking in the Unites States. First Monday 11(9).
   http://firstmonday.org/htbin/cgiwrap/bin/ojs/index.php/fm/article/view/1394/1312. Accessed
   25 May 2009
Ben-Ze‚Äôev A (2003) Privacy, emotional closeness, and openness in cyberspace. Comput Hum
   Behav 19(4):451‚Äì567
Berry DS (1991) Accuracy in social perception: contributions of facial and vocal information.
   J Pers Soc Psychol 61(2):298‚Äì307
boyd d, Hargittai E (2010) Facebook privacy settings: Who cares? First Monday 15(8)
Buchanan T, Paine C, Joinson AN, Reips U-D (2007) Development of measures of online privacy
   concern and protection for use on the internet. J Am Soc Inform Sci Tech 58(2):157‚Äì165
Burgoon JK (1982) Privacy and communication. In: Burgoon M (ed) Communication yearbook 6.
   Sage, Beverly Hills, pp 206‚Äì249
Burgoon JK, Parrott R, Le Poire BA, Kelley DL, Walther JB, Perry D (1989) Maintaining and
   restoring privacy through communication in different types of relationships. J Soc Pers Relat
   6:131‚Äì158
Cho H, Rivera-Sanchez M, Lim SS (2009) A multinational study on online privacy: global
   concerns and local responses. New Media Soc 11(3):395‚Äì416
Christofides E, Muise A, Desmarais S (2009) Information disclosure and control on facebook:
   are they two sides of the same coin or two different processes? Cyberpsychol Behav
   12(3):341‚Äì345
Debatin B, Lovejoy JP, Horn A-K, Hughes BN (2009) Facebook and online privacy: attitudes,
   behaviors, and unintended consequences. J Comput Mediat Commun 15:83‚Äì108
Deci EL, Ryan RM (2000) The ‚Äúwhat‚Äù and ‚Äúwhy‚Äù of goal pursuits: human needs and the self-
   determination of behavior. Psychol Inq 11(4):227‚Äì268
Ekman P, Friesen WV, Ancoli S (2001) Facial signs of emotional experience. In: Parrott W (ed)
   Emotions in social psychology: essential readings. Psychology Press, New York, pp 255‚Äì264
Harter S (2002) Authenticity. In: Snyder CR, Lopez SJ (eds) Handbook of positive psychology.
   Oxford University Press, Oxford, pp 382‚Äì394
Hassin R, Trope Y (2000) Facing faces: studies on the cognitive aspect of physiognomy. J Pers Soc
   Psychol 78(5):837‚Äì852
6 The Social Web as a Shelter for Privacy and Authentic Living                                 73

Hinduja S, Patchin JW (2008) Personal information of adolescents on the internet: a quantitative
    content analysis of myspace. J Adolesc 31:125‚Äì146
Hodgins HS, Knee CR (2002) The integrating self and conscious experience. In: Deci EL,
    Ryan RM (eds) Handbook of self-determination research. University of Rochester Press,
    Rochester, pp 87‚Äì100
Joinson AN (2001) Self-disclosure in computer-mediated communication: the role of self-awareness
    and visual anonymity. Eur J Soc Psychol 31:177‚Äì192
Kernis MH, Goldman BM (2005) Authenticity, social motivation, and psychological adjustment.
    In: Forgas JP, Williams KD, Laham SM (eds) Social motivation: conscious and unconscious
    processes. Cambridge University Press, New York, pp 210‚Äì227
Kernis MH, Goldman BM (2006) A multicomponent conceputalization of authenticity: theory and
    research. In: Zana MP (ed) Advances in experimental social psychology, vol 38. Elsevier
    Academic Press, San Diego, pp 283‚Äì357
Lampe C, Ellison NB, Steinfield C (2007) A familiar Face(book): profile elements as signals in an
    online social network. In: Proceedings of the SIGCHI conference on human factors in
    computing systems, Association for Computing Machinery, New York, pp 435‚Äì444
Lewis K, Kaufman J, Christakis N (2008) The taste for privacy: an analysis of college student
    privacy settings in an online social network. J Comput Mediat Commun 14:79‚Äì100
Margulis ST (2003) On the status and contribution of Westin‚Äôs and Altman‚Äôs theories of privacy.
    J Soc Issues 59(2):411‚Äì429
Nardi BA, Schiano DJ, Gumbrecht M (2004). Blogging as social activity, or, would you let 900
    million people read your diary? In: Proceedings of computer supported cooperative work 2004,
    Chicago. http://home.comcast.net/~diane.schiano/CSCW04.Blog.pdf. Accessed 10 May 2007
Nissenbaum H (2009) Privacy in context: technology, policy, and the integrity of social life.
    Stanford Law Books, Palo Alto
Paine C, Reips U-D, Stieger S, Joinson AN, Buchanan T (2007) Internet users‚Äô perceptions of
    ‚Äòprivacy concerns‚Äô and ‚Äòprivacy actions‚Äô. Int J Hum Comput St 65:526‚Äì536
Schouten AP, Valkenburg PM, Peter J (2007) Precursors and underlying processes of adolescents‚Äô
    online self-disclosure: developing and testing an ‚Äúinternet-attribute-perception‚Äù model.
    J Media Psychol 10:292‚Äì315
Sheldon KM, Ryan RM, Rawsthorne LJ, Ilardi B (1997) Trait self and true self: cross-role
    variation in the big-five personality traits and its relations with psychological authenticity
    and subjective well-being. J Pers Soc Psychol 73(6):1380‚Äì1393
Tidwell LS, Walther JB (2002) Computer-mediated communication effects on disclosure,
    impressions, and interpersonal evaluations. Getting to know one another a bit at a time. Hum
    Commun Res 28(3):317‚Äì348
Tufekci Z (2008) Can you see me now? Audience and disclosure regulation in online social
    network sites. B Sci Technol Soc 28(1):20‚Äì36
Turkle S (1996) Life on the screen: identity in the age of the internet. Weidenfeld & Nicolson,
    London
Viegas FB (2005). Bloggers‚Äô expectations of privacy and accountability: an initial survey.
    J Comput-Mediat Commun 10(3), Article 12
Vinsel A, Brown BB, Altman I, Foss C (1980) Privacy regulation, territorial displays, and
    effectiveness of individual functioning. J Pers Soc Psychol 39(6):1104‚Äì1115
Walther JB (1996) Computer-mediated communication: impersonal, interpersonal, and
    hyperpersonal interaction. Commun Res 23:1‚Äì43
Westin AF (1967) Privacy and freedom. Atheneum, New York
Westin AF (2003) Social and political dimensions of privacy. J Soc Issues 59(2):431‚Äì453
Wood AM, Linley PA, Maltby J, Baliousis M, Joseph S (2008) The authentic personality: a
    theoretical and empirical conceptualization and the development of the authenticity scale.
    J Couns Psychol 55(3):385‚Äì399
Yao MZ, Rice RE, Wallis K (2007) Predicting user concerns about online privacy. J Am Soc
    Inform Sci Tech 58(5):710‚Äì722
Chapter 7
Fifteen Minutes of Privacy: Privacy, Sociality,
and Publicity on Social Network Sites

Zizi Papacharissi and Paige L. Gibson




7.1    Introduction

In celebration of a burgeoning celebrity pop culture, Andy Warhol famously
proclaimed that in the future, everyone would be famous for 15 minutes. Almost
half a century later, being public online has become so easy that one wonders how,
in the future, one may be truly private for 15 minutes. Both statements reflect the
distance that separates the self from privacy, publicity, and that which lies in
between: sociality.
   In contemporary democracies, privacy is recognized as a basic human right ‚Äì the
‚Äúright to be let alone,‚Äù as defined by the landmark Warren and Brandeis (1890, p.
195) Harvard Law Review article. Allegedly, Warren was inspired to write this
article following the intrusive news coverage of his wife‚Äôs society parties and
reached a breaking point after the invasive press coverage of his daughter‚Äôs private
wedding party. Given the prevalence of media platforms that could so easily render
a private event public, Warren and Brandeis (1890) saw it necessary to assert the
right to privacy, or, in their words, ‚Äúthe right to an inviolate personality‚Äù (p. 211). In
modern societies, this distance between public and private continues to dwindle, as
contemporary media further blur the lines separating private from public. Social
media in particular enable individuals to connect with multiple audiences on online
social planes that are neither conventionally public nor entirely private. In the
publicly private and privately public era of social media, friends or their
acquaintances, not the press, would have tagged photographs of Ms. Warren‚Äôs
guests, making them publicly accessible to outside networks and third parties.
   The question of privacy in a digital era, and in particular, in the Social Web
realm, resurfaces as the structural affordances of networked spaces remediate the
texture of publicity, sociality, and privacy. People digitally record and archive their


Z. Papacharissi (*) ‚Ä¢ P.L. Gibson
University of Illionois at Chicago, Chicago, IL, USA
e-mail: zizi@uic.edu

S. Trepte and L. Reinecke (eds.), Privacy Online,                                      75
DOI 10.1007/978-3-642-21521-6_7, # Springer-Verlag Berlin Heidelberg 2011
76                                                       Z. Papacharissi and P.L. Gibson


performances of self, enacted via social media. The self (and others) can further
edit, duplicate, and remix these performances, which, accessible via a variety of
search protocols, reach a variety of networked audiences and publics. boyd (2010a)
theorizes these properties as the four affordances of networked publics: persistence,
replicability, scalability, and searchability. The self traverses from privacy to
publicity and back by cultivating a variety of social behaviors or performances.
These affordances complicate the circumstances under which the self may do so,
and are augmented in architectures that emphasize sharing information by default
(Papacharissi 2010; Raynes-Goldie 2010). The challenge for individuals is to
manage the persistence, replicability, scalability, and searchability of their
performances fluently in environments that prompt (and in some instances reward)
sharing.
    Shareability, then, presents a fifth affordance of networked digital spaces, as it
constitutes an architectural feature of networked structures that encourages sharing
over withholding information. What renders networks lively is the flow of informa-
tion between individual network nodes. Without information flowing between
individuals, the network becomes a static, asocial environment (Papacharissi
2009). Stutzman (2006) has referred to this attribute as the inherent sociality of
social network communities and has explained that it accounts for the high level of
disclosure of personal information online. In order to stay social, but also manage
private and public information fluently, individuals must make critical decisions
about how to share information in networked environments that thrive on sharing.
This chapter examines the conditions that complicate private performances of the
self in the context of the Social Web. We use the term private performance because
it becomes necessary for the self to adopt behaviors that will semantically (mean-
ing) and syntactically (code) communicate and guarantee privacy. We suggest that
an advanced form of digital literacy can enable individuals to redact performances
of the self online so as to navigate public and private boundaries fluently.



7.2   Privacy on Social Network Sites

Social network sites (SNSs) are abundant in number, diverse in aim and culture, and
far-reaching in scope, penetrating the depths and traversing the global expanse of
the Internet (boyd and Ellison 2007). SNSs not only account for a great portion of
our online activities (Albrechtslund 2008), but the technologies that enable them
converge online and offline aspects of our identity (Schneider and Zimmer 2006).
In an attempt to distinguish social network sites from other forms of computer-
mediated communication (CMC), boyd and Ellison (2007) argued that despite the
interchangeable use of the terms social networking site and social network sites, the
two terms place emphasis on different activities. Networking highlights the forging
of new relationships, an idea that is neither accurate for most SNSs nor a
differentiating characteristic from other CMC (boyd and Ellison 2007). The argu-
ment for this distinction has not been without debate. Understanding social
7 Fifteen Minutes of Privacy: Privacy, Sociality, and Publicity on Social Network Sites   77


networking sites to be a subset of social network sites, some scholars, like Beer
(2008), question the usefulness of drawing such a fine line and criticize the
terminological movement toward breadth rather than pointed classification.
    Still, the distinction is useful for understanding how individuals perceive their
own privacy with regard to the networked platforms they inhabit and the publics
they wish to network with, and this analysis pertains to social network sites. As
boyd (2006) notes, the norm for early adopters of Friendster did not comply with the
expectation that they would simply link to their offline friends. Not until this
practice became challenging for privacy did users handle their information and
friend selection more cautiously. Whether maintaining offline relationships or
initiating new ones, the wide range of web services that fall under the heading of
‚Äúsocial network site‚Äù at their core present the opportunity for individuals to (1)
construct a public or semi-public profile within a bounded system, (2) articulate a
list of other users with whom they share a connection, and (3) view and traverse
their list of connections and those made by others within the system. (boyd and
Ellison 2007 p. 211). This chapter will observe this definition in discussing privacy
and the self in the context of social network sites.
    SNSs have integrated aspects of these features into their architecture in a variety
of ways since the launch of the first SNS, Six Degrees.com, in 1997. Friendster
presented these features in a way that propelled its popularity in 2002, and some of
the most successful features of it were expanded and folded into the design of
MySpace, followed by the subsequent launch of Facebook. Each reiteration of these
SNSs presented a series of different features, but their defining attribute remains the
visible profile displaying social connections embedded in a system centered around
people rather than interests (boyd and Ellison 2007). From the frame of their
architecture to the daily practices, SNSs are centered on sharing with a penchant
for more rather than less (Raynes-Goldie 2010). The SNS profiling structure
capitalizes on identifying information (e.g., hometown, date of birth), access
information (e.g., location), and expressive information (e.g., status updates and
comments) (see DeCew 1997).
    The volume, range, and method of sharing personal information across a variety
of publics and audiences on SNSs pose an issue of growing concern for users. The
persistence, replicability, scalability, and searchability of personal data deposited as
individuals forge social connections present privacy challenges. Individuals gradu-
ally realize that the physical barriers that enable privacy offline are not inherent
aspects of online-networked architectures. The impact of maintaining privacy
without the aid of physical barriers is further augmented as SNSs cultivate practices
that prompt users to be more public with their information by default. While it is
possible for users to edit these settings, the code that belies the structure of the
network makes it easier to share than to hide information. For Facebook, progres-
sive updates of profiles are accompanied with revised privacy settings that users
must monitor, adjust, and master. As a result, Privacy International has placed
Facebook in the second lowest category, that of presenting ‚Äúsubstantial and com-
prehensive privacy threats.‚Äù Only Google, also infamous for its privacy violations,
ranks lower (Debatin et al. 2009). With 500 million plus active users on Facebook
78                                                       Z. Papacharissi and P.L. Gibson


alone, half of whom log in many times on any given day (Facebook 2011), the
impact of SNSs on privacy, sociality, and publicity is irrefutable, spilling over into
offline privacy, too. As Schneider and Zimmer (2006) posit, ‚ÄúOnline and off, the
digitization of identity mediates our sense of self, social interactions, movements
through space, and access to goods and services‚Äù (p. 1). The sharing of private
information online frequently carries consequences for privacy offline, in a manner
that negates the online/offline dichotomy.
    The lack of a coherent regulatory framework for privacy protection in the US
permits digital traces of consumer behavior that remain on partner and third party
sites that users visit, like, or share, to be further exploited. The global nature of
communication in networked environments would also challenge the application of
nationally oriented regulation. Facebook CEO Mark Zuckerberg has argued that
these changes make it easier for users to share information across the social web
(Sutter 2010). By contrast, activist groups such as the Electronic Privacy Informa-
tion Center (EPIC) claim that Facebook frequently pulls a ‚Äúprivacy bait and
switch,‚Äù getting users to provide personal information under one set of privacy
terms, then modifying their privacy policies (Chittal 2010, p. 6). The pattern that
emerges is that following protests mounted by users and activists, Facebook will
take steps to amend privacy settings and make them more accessible and manage-
able for their members, only to spark further uproar with subsequent site updates.
Despite compromises on both sides, this cycle progressively weighs against the
consumer, creating a protocol that positions sharing as default and privacy as
afterthought.
    The (d)evolution of privacy guidelines maps a digital path to sociality taken at
the expense of privacy. This is not new: sociality has always required some
(voluntary) abandonment of privacy. In order to become social, we must give up
some of our private time and space so as to share it with others. The balance
between privacy and sociality has always existed; and when attained, it permits
individuals to pursue rewarding social lives. Many users find the tug-and-pull
between privacy and sociality upsetting now that it takes place on a social plane
that digitally records, archives, and tracks social behaviors by default.
    The privacy question, in its present form, is an urban problem of modernity.
Individuals living in rural communities were preoccupied with privacy, but in ways
and for reasons different from ours. In a world where communal practices were
emphasized, the desire to be private was frequently associated with the need to hide,
and gossip was perceived as a means of expressing solidarity (Norris 2001).
Modern and urban life charged individuals with the responsibility of managing
their sociality, and their privacy, in unknown and urban territory. Urban
environments present a certain measure of distance (Simmel 1971), which might
suggest autonomy in defining private boundaries, but with autonomy comes respon-
sibility to delineate and protect private boundaries. Yet, individuals maintain social
relationships in both urban and agrarian settings, and in doing so, they gradually
confide private information to attain personal closeness with valued others. An
optimal balance between disclosure and privacy can be beneficial for the
individual‚Äôs personal approach to sociality. Problems arise when an individual‚Äôs
7 Fifteen Minutes of Privacy: Privacy, Sociality, and Publicity on Social Network Sites   79


right to make decisions about their own path to privacy, sociality, and publicity is
compromised.
    Here, we must emphasize that privacy, defined as the right be let alone, must not
be confused with a desire to be left alone. Private individuals are not socially
reclusive individuals. We define privacy as control over information about oneself
(Taraszow et al. 2010). Thus, we follow Westin‚Äôs (1967) definition that views
privacy as control over the circumstances under which information is shared:
‚ÄúPrivacy is the claim of individuals, groups, or institutions to determine for
themselves when, how, and to what extent information about them is
communicated to others‚Äù (Westin 1967, p. 7). This definition is aligned with others
who have similarly defined privacy as personal information that an individual does
not desire to share with a general public (Hodge 2006; Etzioni 1997; Kaplin and Lee
1997; Richards 2007; Timm and Duven 2008). Privacy thus guarantees decision
making autonomy for the self, in environments both digital and non-digital. The
following passages will focus on three key aspects of this autonomy: privacy and
the self, privacy and the formation of social relationships, and privacy and democ-
racy. We view these three aspects as representative of activity on SNSs and
reflective of the underlying utility of privacy.



7.3    Privacy and the Self: Autonomy in Performances of Identity

We rarely fight for privacy simply for its own sake; we fight for its underlying
values. Autonomy is central to most understandings of privacy (Hildebrandt 2006).
Warren and Brandeis‚Äô (1890, p. 195) classic call for the ‚Äúright to be let alone,‚Äù the
catalyst of privacy law in the United States, is built on the notion of autonomy, or
our ability to pursue our own path without impediment or external influence.
Privacy is often conceptually reduced to control over our information, and thus
placed into a narrative that associates technological progress with the loss of control
over personal information (Austin 2010). Therefore, the ability to share more
information is perceived as evolutionary and contradictory to the practice of
controlling personal information. And yet, what is problematic is not the practice
of sharing, nor is control over what is shared synonymous with a lack of sharing.
   Facebook‚Äôs ‚ÄúNews Feed‚Äù controversy in 2006 perfectly illustrates this paradox
(as described in boyd and Hargittai 2010; Debatin et al. 2009; Thompson 2008).
The feature broadcasts Friends‚Äô actions from profile changes to application-specific
activities. Although such information had always been present and accessible, the
News Feed highlighted even the most trivial updates, making them immediately
visible, unfiltered, and like all information placed on SNSs, persistent, searchable,
and replicable (Albrechtslund 2008). Such a change is consistent with the differ-
ence between issuing someone a visitor‚Äôs pass and sending out an invitation for
viewing one‚Äôs information. Perceived as a violation of information control, the
News Feed produced significant backlash. Ten thousand people joined a protest
group by noon of the launch day; the next day that number rose to 284,000, and it
80                                                        Z. Papacharissi and P.L. Gibson


would eventually gain as many as 700,000 members (boyd and Hargittai 2010;
Thompson 2008). While the predictions for Facebook‚Äôs future were grim, as
Thompson (2008) reports, ‚ÄúUsers‚Äô worries about privacy seemed to vanish within
days, boiled away by their excitement at being so much more connected to their
friends‚Äù (p. 8). Sociality prevailed at the expense of privacy, and in fact, Facebook
subsequently experienced a massive growth spurt.
    The norm that develops dictates that Facebook actively stretch our comfort
zones until our social norms catch up with technological progress (Thompson
2008). As a result, technological architectures cultivate a newer paradigm for
sociality, one that equates disclosure with being social (Zhang et al. 2010). While
individuals have always formed social relationships through disclosure, they typi-
cally develop hierarchies of social relations on the basis of what is shared, how, and
with whom. In fact, learning how to share is a central process of being socialized
into society, as it enables relationships and presentations of the self.
    Privacy and control are central issues in performances of the self in various
online contexts, which can also be understood as a form of portraiture (Donath et al.
2010). Some have argued that SNSs provide a window to our most private and
deeply felt aspects of self, often trivializing the information as they ‚Äúbroadly [cast]
the private onto scattered planes of the public‚Äù (van Manen 2010, p. 1024). While
both academic and public discourses commonly conflate secrecy and privacy, the
distinction is important because the violation is not that the information is shared but
rather with whom. Secrecy (a concern for what is known) refers to the intentional
concealment of information. While secrecy often entails something private, privacy
does not refer to an unwillingness to share information but rather the need to control
who may know the most intimate aspects of self (Ben-Ze‚Äôev 2003; Bok 1989).
Furthermore, it concerns who partakes in our construction of identity.
    Identity is something unique to the individual, yet constructing an identity does
not take place in isolation nor is it a solitary activity. Privacy allows us the freedom
to ‚Äú[develop] our interests and personalities in a way that is not always compatible
with social norms‚Äù (Ben-Ze‚Äôev 2003, p. 462; Austin 2010; Poullet 2009). Although
claimed as exclusively and uniquely ours, identity is fundamentally social, and the
sense of self is developed through the collaborative, collective experiences of our
social interactions (Mead 1934). The construction and performance of digital
identity is similarly intertwined within a web of complex offline and online social
connections (Austin 2010; Baym 2010; Buckingham 2008; Mallan and Giardina
2009; Marwick and boyd 2010). Mallan and Giardina (2009) use the term
‚ÄúWikidentity‚Äù to capture the highly collaborative nature of forming these digital
identities. Utopian rhetoric frequently presumes SNSs to be digital places ‚Äúwhere
one can ‚Äòtype oneself into being‚Äô‚Äù (boyd and Ellison 2007, p. 211). However, SNSs
are connected, intertwined, and embedded in our offline social spaces, and as a
result, the digital self is often met with similar constraints to the offline self
(Albrechtslund 2008; boyd 2006).
    The networked structure of SNSs affords numerous opportunities for social
connection and expression, but with this freedom comes the responsibility of
producing a performance of the self that makes sense to multiple audiences and
7 Fifteen Minutes of Privacy: Privacy, Sociality, and Publicity on Social Network Sites   81


publics without compromising our sense of who we truly are (Papacharissi 2010).
Whereas conventions of interaction in the offline world permit us to produce and
customize performances to specific social situations and groups, the architecture of
SNSs does not reproduce these distinctions, resulting in what Marwick and boyd
(2010) have termed ‚Äúcontext collapse‚Äù (p. 9). Individuals develop several strategies
in order to retain the autonomy of their identity online. Some take to self-censorship,
imagining their audience to be the most sensitive members, and thus editing their
performances. Following the logic of network television, individuals find them-
selves performing for the lowest common denominator so as to produce a perfor-
mance that will comply with the expectations of the broadest possible audience.
Others become well versed in producing polysemic performances, presentations of
the self that contain layers of meaning, signifying different impressions to various
audiences. Livingstone (2008) has described how teenagers gauge opportunity
against risk as they navigate publicly private and privately public boundaries in
search of intimacy, privacy, and self-expression. boyd (2010b) has written about
social steganography; the process of hiding in plain sight, by creating a message
that signifies different meanings for different audiences. Tufekci (2008) has
explained that college students employ various strategies of disclosure and with-
drawal to engage in virtual identity hide-and-seek online. Lewis et al. (2008)
suggested that personal strategies for privacy are characterized by a unique set of
cultural preferences, thus presenting a matter of a ‚Äútaste for privacy‚Äù (p. 79).
   Online platforms such as Facebook periodically develop technological
workarounds that enable Friends to be divided into separate lists (presumably by
social circle) and allow individuals to control who views individual status updates.
However, other aspects of the architecture remain open to indiscriminate informa-
tion sharing, forcing individuals to militate toward a forced self-surveillance
(Albrechtslund 2008). More importantly, they require the development and learning
of strategies for socializing online. This skill, not yet conveyed through our formal
and informal channels of socialization, is for the most part self-taught and remains
the primary way for attempting to maintain the autonomy of the self on social
network sites.



7.4    Privacy and Social Relationships: Autonomy in Defining
       Sociality

Privacy is fundamentally relational, as it is concerned with the self (formed through
autonomy) and its relationship to the social environment of other selves
(Hildebrandt 2006). Just as we write our self into being on SNS, we ‚Äúwrite [our]
community into being‚Äù (boyd 2006, p. 69). Privacy enables the existence of
relationship and community. If we share all of ourselves with everyone, that sharing
loses all meaning and value. Selectivity permits sharing to become singular and
meaningful. Privacy enables the development of significant social bonds with
others, and the maintenance of ties weak and strong.
82                                                       Z. Papacharissi and P.L. Gibson


    A marker of personal relationships is intimacy. The most pronounced difference
between digital intimacy and proximal intimacy is that of distance and its mediated
form. It seems that digital intimacy can be equated to the apparent oxymoron of
distant intimacy, a phenomenon made possible because of technology‚Äôs ability in
turn to shrink that distance and fill it with the intimacy of the written word (van
Manen 2010). Ambient awareness also plays a role in establishing digital intimacy.
The constant contact SNSs provide works in a similar fashion to physical proximity
in that individuals are able to detect moods through the incessant feed of updates.
These updates give one a sense of constant presence, and despite the mundane
nature of the individual posts, they work toward building an intricate image of the
individual (Thompson 2008; van Manen 2010). In 1998, anthropologist Robin
Dunbar posited that there is a threshold to the number of social bonds any one
human can have (roughly 150); however, technology has amplified that threshold
leading Dunbar to nearly double his original estimates. While the circle of intimates
experiences little increase (though technology enables them to become richer),
SNSs enable a dramatic expansion of one‚Äôs sociality with weak ties (Thompson
2008). The level of self-disclosure and self-reflection that comes with SNS
activities work not only toward digital intimacy with others, but as ‚Äúa kind of
reflexive sphere of intimacy,‚Äù as we gain a better sense of our self (van Manen
2010, p. 1028; Thompson 2008).
    This reflexive sphere is articulated around the nexus of relationships on one‚Äôs
online profile, termed Friends. With SNSs, the notion of friendship, both on and
offline, and the term friend take on new meaning (Beer 2008; boyd 2006; boyd and
Ellison 2007; Debatin et al. 2009). Friendship, as a cultural construct, can differ
accordingly; however, the general understanding of friendship usually points to the
voluntary nature of the relationship, the existence of mutual liking or affection, and
the emotional and practical support that this relationship usually entails. Friendship
transcends the restrictive boundaries of professional relationships and may not
share the intense mutual responsibility of family. The boundaries of Friends and
the motivations for Friending vary widely and remain inconsistent. Friendship has
always held some performative element but SNSs have amplified its reach as any
number of relational types may now rest under this heading (boyd 2006). The same
context collapse that complicates our autonomy can also set hurdles as we manage
our relationships. For example, the collapse of personal and professional contexts
can lead to one‚Äôs boss accessing and being offended by an inside joke known only
among your close friends. Privacy controls become a way of managing one‚Äôs
audience (boyd and Hargittai 2010). However, when those controls are unsatisfac-
tory the task can be complex and difficult to achieve. Despite the clamor for easier
and more manageable privacy controls, SNS technology retains loopholes (Raynes-
Goldie 2010). For instance, a friend may comment on one‚Äôs photo album, thereby
granting her friends access to the entire album because the comment is considered
noteworthy news on her unfiltered newsfeed. One‚Äôs right to be let alone is thus
dependent upon the definition others hold of privacy and the settings they
deem acceptable. People of course encounter similar issues with conflicting
conceptions of privacy in offline architectures as well; however, the discrepancies
7 Fifteen Minutes of Privacy: Privacy, Sociality, and Publicity on Social Network Sites   83


are amplified in SNSs given one‚Äôs performance is then crowdsourced to one‚Äôs
network of friends.
    Interestingly enough, digital intimacy achieved on SNSs frequently becomes a
question of autonomy surrendered, but also, autonomy feigned, through controls
that suggest greater autonomy than we actually possess. Because the architecture
fails to define the context of the space for us, the scope of this public and the
expected social boundaries are defined by the breadth and depth of one‚Äôs Friend list
(boyd 2006). As Marwick and boyd (2010) argue, ‚ÄúWe may understand that the
Twitter or Facebook audience is potentially limitless, but we often act as if it were
bounded‚Äù either through an imagined audience or ideal reader (writing for oneself)
that we use to deal with context collapse (p. 3). Public and private boundaries are
blurred as are our perceptions of them. Privacy settings, even when monitored and
customized, still serve the purpose of negotiated privacy within the terms that the
social network site has defined. The individual, in this case, is only able to attain a
compromised or prescribed autonomy defined by the site‚Äôs architecture. Through
privacy settings that have been predetermined, the individual is confined to a few
options that s/he has played little or no part in shaping.
    We must return to the affordance of shareability, an attribute of social network
sites that encourages a culture of sharing, to appreciate the individual preparation
that privacy requires on social network sites. Younger users of Facebook acknowl-
edge the privacy risk associated with Facebook use but confess to an inability to
react either because they do not possess the necessary technology knowhow to
manage privacy settings, or because they worry about the social cost of a reduced
presence online (Papacharissi and Mendelson 2010). Within an environment that
equates sociality with sharing and differential sharing is typically an afterthought,
privacy is bound to be a concern.
    Modifications that permit differential sharing across groups of friends are typi-
cally introduced with the goal of enhancing sharing but not of guaranteeing privacy.
Many become disillusioned with the possibility of an SNS-based identity, viewing
it as ‚Äúa false choice, a sociotechnical scenario devoid of agency‚Äù rather than a well-
reasoned decision (Bigge 2006, p. 42). However, teens who make avid use of SNSs
express acute concern over privacy issues and develop strategies for privacy that are
congruent with their skill level, gender, age, and mobility narratives (boyd and
Hargittai 2010; Li and Chen 2010; Patchin and Hinduja 2010). Nevertheless,
discrepancies persist between how users understand privacy, how they think they
are protecting themselves, and how they are actually able to establish privacy online
(Acquisti and Gross 2006). Especially for younger adults, attaining balance
between privacy and sociality presents a central part of identity play and formation.
In environments that encourage sharing over privacy by default, dissonance
between learned social behaviors for sociality and privacy can develop easily.
Individuals will frequently simply transfer behaviors to the SNS context, neglecting
to make adjustments that we typically make when moving from one social context
(a bar) to another (a classroom). Indeed, it is this translation of sociocultural norms
across contexts that results in a subsequent loss or change in meaning (LaseÃÅn and
GoÃÅmez-Cruz 2009; Winseck 2002). SNSs encourage such forgetfulness by inviting
84                                                        Z. Papacharissi and P.L. Gibson


users to share upon entering, much like a movie theater invites viewers to be quiet,
and a loud bar requires guests to speak more loudly to friends. In this way,
networked social environments make it challenging for individuals to be private
in spaces that were designed for sharing, not privacy.




7.5    Private Information Commodified, Privacy a Luxury
       Commodity

The balance between privacy, sociality, and publicity takes on new meaning as
Internet-based platforms, like social network sites, afford sociality and publicity at
the expense of personal autonomy in determining privacy. All web-accessible
platforms offer services, mostly of a social nature, in exchange for personal
information. In turn, these services transform personal information of a private
nature into currency. However, regarding information as an economic good
contains unique properties that complicate its treatment as a commodity. Firstly,
unlike other commodities, information remains with its owner, even when traded or
sold. Secondly, the value of information is frequently established subjectively;
information of value to some may be irrelevant to others. Thirdly, information
can never be fully consumed in the manner other goods and services are used up or
depleted. These attributes complicate the trading of information in economic
markets, causing problems that range from minor hiccups to major problems in
the trading system. They also render privacy, viewed as control over information
shared about oneself, a complex problem to manage (Huey 2010).
    Information traded in bits via online networked platforms possesses these
attributes. In addition, it is characterized by the affordances of persistence, replica-
bility, scalability, and searchability, all of which further augment and complicate
the unique properties of information as an economic good. Personal information of
a private nature adds further complications to the process, because not all personal
information is potentially private. Personal information attains a private nature
depending on how individuals subjectively define their unique approach to privacy,
sociality, and publicity. Online networked platforms that accept personal informa-
tion in exchange for access to social services engage in an information trade that
frequently does not specify how the individual retains the autonomy to determine
privacy, sociality and publicity. Thus, it is not just the personal information that is
traded, but also the right to privacy in return for a formula of sociality and publicity
presented by the social network site.
    Byte by byte, our personal information is exchanged as currency to gain digital
access to our own friends. In this manner, personal information is commercialized
into the public realm, with little input from the individual in the process. We have
explained that individuals develop strategies for managing this relationship, and
that social network sites frequently adjust privacy/sociality/publicity settings in
response to user reactions. Hence this is not a problem that is irresolvable.
7 Fifteen Minutes of Privacy: Privacy, Sociality, and Publicity on Social Network Sites   85


What we try to establish, however, is that it emanates from a premise that
commodifies personal information. As personal information is traded in, privacy
gradually attains the characteristics of a luxury commodity, in that (a) it becomes a
good inaccessible to most, (b) it is disproportionately costly to the average
individual‚Äôs ability to acquire and retain it, and (c) it becomes inversely associated
with social benefits, in that the social cost of not forsaking parts of one‚Äôs privacy in
exchange for information goods and services (e.g., free e-mail account, online social
networking) places one at a social disadvantage. Luxury goods not only possess a
price point beyond the average person‚Äôs reach, they also connote social status and
advantage.
   But what renders privacy a luxury commodity is that obtaining it implies a level
of computer literacy that is inaccessible to most, and typically associated with
higher income and education levels, and certain ethnic groups, in ways that mirror
dominant socio-demographic inequalities (Hargittai 2008). As a luxury commodity,
the right to privacy, afforded to those fortunate enough to be Internet-literate,
becomes a social stratifier; it divides users into classes of haves and have-nots,
thus creating a privacy divide. This privacy divide is further enlarged by the high
income elasticity of demand that luxury goods possess: as people become wealthier,
they are able to buy more of a luxury good or higher classes of luxury goods and
services. Privacy as a luxury commodity possesses similar elasticity; as people
become more and more literate, they will be able to afford greater access to privacy.
The goal for regulation is to effectively turn privacy into a normal good ‚Äì a good
that everyone may afford, or even better, a public good. A regulatory solution to the
privacy divide must address market factors that render privacy a luxury commodity.
   The current state of privacy law in the US mirrors that of the general US
regulatory mentality, which is biased toward letting the market self-regulate.
Unlike most European countries, there are few laws concerning privacy, and they
pertain to the government‚Äôs use of personal information. The most recent and
notable of these are the Financial Modernization Act (Gramm-Leach-Bliley Act
of 1999), and the Children‚Äôs Online Privacy Protection Act (1998) (COPPA). The
former specifies that financial institutions must inform customers about their
privacy practices, but provides limited control to consumers regarding the use
and distribution of personal data. Recently, President Obama and several leading
economists criticized the act as prompting subsequent deregulation and leading to
the 2007 subprime mortgage financial crisis. Under the Act, individuals are granted
some privacy protection but must still proactively make certain that their personal
information is not made available to third parties. Children understandably receive
greater protection under COPPA, which lays out specific regulations for companies
targeting individuals under the age of 13 online. Aside from COPPA, regulatory
policy in the US is founded upon the assumption that web operators disclose, but do
not adjust or restrict information gathering and distribution practices. Privacy
statements are descriptive and explanatory of privacy practices but are not
inherently protective of privacy. Such privacy practice disclosures tend to be
employed more as legal safeguards for companies and less as guarantees of the
safety of personal data (Fernback and Papacharissi 2007).
86                                                         Z. Papacharissi and P.L. Gibson


   A regulatory framework must define, protect, and educate about ‚Äúthe right to an
inviolate personality‚Äù online (Warren and Brandeis 1890, p. 211). Doing so can
ensure that individuals retain the right to determine for themselves what this
balance between publicity, sociality, and privacy should be. Each individual
seeks and is satisfied with a different balance. Regulation can help individuals
retain decision-making autonomy in online environments. At the same time, a
regulatory framework would require global cooperation in defining privacy in the
digital era, which would necessitate the reconciliation of different sociocultural
norms and political-economic hierarchies to guarantee individual autonomy over
personal information (Flint 2009). Some suggest moving from ego-centered to
decentralized, link-driven networks as a workaround, but that would only render
a partial solution to the problem (Cutillo et al. 2009). Building safety considerations
into the design of social network sites is also an important aspect of managing their
potential for inviting and rewarding disclosure of personal information
(Livingstone and Brake 2010).
   Ultimately, because online environments work glocally, educating the public about
the ‚Äúright to be let alone‚Äù (Warren and Brandeis 1890, p. 195) online is an important
part of crafting a regulatory solution that ensures privacy becomes a public good for
global users. Education, in the form of technological literacy, can then help individuals
practice this autonomy fluently in digital environments. As individuals use platforms
that blur private and public, it is essential that they retain the right to specify
boundaries when necessary. Networked environments that thrive on shareability
present both opportunities for the self and challenges for performative autonomy
online. Individuals are required to become more conscious editors of their own
behavior online. Editorial skills, and the ability to redact, previously associated with
specific professions only, become the property of individual citizens and part of a
survival toolkit online (Hartley 2000). The idea is not entirely new for socially
motivated beings. We frequently edit our social behavior and the information we
share with others as we interact with a variety of audiences: friends, work colleagues,
acquaintances, and strangers. We even have phrases, norms and acronyms that signal
to others when too much information has been shared in an inappropriate context.
   The process of self-presentation on social network sites involves both the
production of performances and simultaneous or subsequent editing of these
performances. Redaction enables the bringing together and editing of identity traces
to form and frame a coherent performance. Self-editing has always been a part of
how we present the self to others, but online platforms frequently prompt self-
sharing by default without permitting self-editing. The kind of literacy that supports
performative fluency online rests upon one‚Äôs own acumen for redaction. Structured
around the tendency to delete, or otherwise edit aspects of one‚Äôs identity, redac-
tional acumen enables individuals to present a coherent and polysemic performance
of the self that makes sense to multiple publics without compromising one‚Äôs
authentic sense of self. It is this sort of editorial acumen that individuals must
find a way to apply to online environments. And it is this editorial acumen that will
help individuals to not just attain 15 minutes of privacy online, but also perform
their identities autonomously in the digital era.
7 Fifteen Minutes of Privacy: Privacy, Sociality, and Publicity on Social Network Sites         87


References

Acquisti A, Gross R (2006) Imagined communities: awareness, information sharing, and privacy
    on the facebook. Presented to privacy enhancing technologies workshop (PET), Cambridge
Albrechtslund A (2008) Online social networking as participatory surveillance. First Monday 13
    (3). http://firstmonday.org/htbin/cgiwrap/bin/ojs/index.php/fm/article/viewArticle/2142/1949.
Austin LM (2010) Control yourself, or at least your core self. B Sci Technol Soc 30(1):26‚Äì29
Baym NK (2010) Personal connections in the digital age. Polity Press, Cambridge
Beer D (2008) Social network(ing) sites. . .revisiting the story so far: a response to danah boyd &
    Nicole Ellison. J Comput Mediat Commun 13:516‚Äì529
Ben-Ze‚Äôev A (2003) Privacy, emotional closeness, and openness in cyberspace. Comput Hum
    Behav 19:451‚Äì467
Bigge R (2006) The cost of (anti-)social networks: identity, agency and neo-luddites. First Monday
    12(4). http://firstmonday.org/htbin/cgiwrap/bin/ojs/index.php/fm/article/view/1421/1339.
Bok S (1989) Approaches to secrecy. In: Secrets: on the ethics of concealment and revelation,
    Vintage Books, New York, pp 3‚Äì14
boyd d (2006) Friends, friendsters, and top 8: writing community into being on social network
    sites. First Monday 11(12). http://firstmonday.org/htbin/cgiwrap/bin/ojs/index.php/fm/article/
    view/1418/1336
boyd d (2010a) Social network sites as networked publics: affordances, dynamics, and
    implications. In: Papacharissi Z (ed) Networked self: identity, community, and culture on
    social network sites. Routledge, New York, pp 39‚Äì58
boyd d (2010b) Social Steganography: learning to Hide in Plain Sight. Digital Media and Learning
    blog. http://dmlcentral.net/blog/danah-boyd/social-steganography-learning-hide-plain-sight
    Accessed Mar 2011
boyd d, Ellison NB (2007) Social network sites: definition, history, and scholarship. J Comput
    Mediat Commun 13:210‚Äì230
boyd d, Hargittai E (2010) Facebook privacy settings: who cares? FirstMonday 15(8). http://
    firstmonday.org/htbin/cgiwrap/bin/ojs/index.php/fm/article/view/3086/2589
Buckingham D (2008) Introducing identity. In: Buckingham D (ed) Youth, identity, and digital
    media. The MIT Press, Cambridge, pp 1‚Äì24
Chittal N (2010) The case for staying with facebook. American prospect, web-only edition (14
    May). http://www.prospect.org/cs/articles?article¬ºthe_case_for_staying_with_facebook
Children‚Äôs Online Privacy Protection Act of 1998 (1998) } 105‚Äì277, 15 U.S.C. } 6501‚Äì6508
Cutillo LA, Molva R, Strufe T (2009) Safebook: a privacy-preserving online social network
    leveraging on real-life trust. IEEE Commun Mag 50:94
Debatin B, Lovejoy JP, Horn A, Hughes BN (2009) Facebook and online privacy: attitudes,
    behaviors, and unintended consequences. J Comput Mediat Commun 15:83‚Äì108
DeCew JW (1997) In pursuit of privacy: law, ethics, and the rise of technology. Cornell University
    Press, Ithaca
Donath J, Dragulescu A, Zinman A, VieÃÅgas F, Xiong R (2010) Data portraits. Leonardo
    43(4):375‚Äì383
Etzioni A (1997) The new golden rule: community and morality in a democratic society. Basic
    Books, New York
Facebook (2011) Press room: statistics. http://www.facebook.com/press/info.php?statistics.
    Accessed 14 Jan 2011
Fernback J, Papacharissi Z (2007) Online privacy as legal safeguard: the relationship among
    consumer, online portal, and privacy policies. New Media Soc 9(5):715‚Äì734
Flint D (2009) Law shaping technology: technology shaping the law. Int Review Law Comput
    Technol 23(1‚Äì2):5‚Äì11
Fraser N (1992) Rethinking the public sphere: a contribution to the critique of actually existing
    democracy. In: Calhoun C (ed) Habermas and the public sphere. MIT Press, Cambridge,
    pp 109‚Äì142
88                                                                Z. Papacharissi and P.L. Gibson


Gramm-Leach-Bliley Act of 1999 (1999) } 106‚Äì102, 15 U.S.C. } 1338
Habermas J (1968) Knowledge and human interests. (trans: Shapiro JJ) (1972). Beacon Press,
   Boston
Habermas J (1992) Further reflections on the public sphere. In: Calhoun C (ed) Habermas and the
   public sphere. MIT Press, Cambridge, pp 421‚Äì461
Hargittai E (2008) The digital reproduction of inequality. In: Grusky D (ed) Social stratification.
   Westview Press, Boulder, pp 936‚Äì944
Hartley J (2000) Communicative democracy in a redactional society: the future of journalism
   studies. Journalism 1:39‚Äì48
Hildebrandt M (2006) Privacy and identity. In: Claes E, Duff A, Gutwirth S (eds) Privacy and the
   criminal law. Intersentia, Oxford, pp 43‚Äì57
Hodge MJ (2006) Comment: the fourth amendment and privacy issues on the ‚Äònew‚Äô internet.
   Facebook.com and myspace.com. Southern Illinois Univ Law School J 31:95‚Äì122
Huey L (2010) A social movement for privacy/against surveillance? Some difficulties in engen-
   dering mass resistance in a land of twitter and tweets. Case West R J Int L 42:699‚Äì710
Kaplin WA, Lee BA (1997) A legal guide for student affairs professionals. Jossey-Bass, San
   Francisco
LaseÃÅn A, GoÃÅmez-Cruz E (2009) Digital photography and picture sharing: redefining the public/
   private divide. Knowledge Technol Policy 22(3):205‚Äì215
Lewis K, Kaufman J, Christakis N (2008) The taste for privacy: an analysis of college student
   privacy settings in an online social network. J Comput Mediat Commun 14:79‚Äì100
Li N, Chen G (2010) Sharing location in online social networks. IEEE Netw 24:20‚Äì25
Livingstone S (2008) Taking risky opportunities in youthful content creation: teenagers‚Äô use of
   social networking sites for intimacy, privacy and self-expression. New Media Soc
   10(3):393‚Äì411
Livingstone S, Brake DR (2010) On the rapid rise of social networking sites: new findings and
   policy implications. Child Soc 24(1):75‚Äì83
Marwick AE, boyd d (2010) I tweet honestly, I tweet passionately: twitter users, context collapse,
   and the imagined audience. New Media Soc 20(10):1‚Äì20
Mallan K, Giardina N (2009) Wikidentities: young people collaborating on virtual identities in
   social network sites. First Monday 14(6). http://firstmonday.org/htbin/cgiwrap/bin/ojs/index.
   php/fm/article/view/2445/2213
Mead GH (1934) Mind, self and society. University of Chicago Press, Chicago
Mendelson A, Papacharissi Z (2010) Look at us: collective narcissism in college student facebook
   photo galleries. In: Papacharissi Z (ed) The networked self: identity, community and culture on
   social network sites. Routledge, New York, pp 151‚Äì173
Norris K (2001) Dakota: a spiritual geography. Houghton Mifflin, Boston
Papacharissi Z (2009) The virtual geographies of social networks: a comparative analysis of
   facebook, linkedin and asmallworld. New Media Soc 11(1‚Äì2):199‚Äì220
Papacharissi Z (2010) A private sphere: democracy in a digital age. Polity Press, Cambridge
Patchin JW, Hinduja S (2010) Trends in online social networking: adolescent use of myspace over
   time. New Media Soc 12(2):197‚Äì216
Papacharissi Z, Mendelson A (2011) Toward a new(er) sociability: Uses, gratifications and social
   capital on facebook. Media Perspectives for the 21st Century, Stelios Papathanassopoulos
   (Ed.), pp. 212‚Äì231. Routledge
Poullet Y (2009) Data protection legislation: what is at stake for our society and democracy.
   Comput Law Security Review 25:211‚Äì226
Schneider T, Zimmer M (2006) Identity and identification in a networked world. First Monday 11
   (12). http://firstmonday.org/htbin/cgiwrap/bin/ojs/index.php/fm/article/view/1417/1335
Simmel G (1971) The metropolis and mental life. In: Levine D (ed) On individuality and social
   forms: selected writings. University of Chicago Press, Chicago, pp 324‚Äì340
Sutter J (2010) Facebook makes it easier for users to share interests across web. CNN.com. http://
   www.cnn.com/2010/TECH/04/21/facebook.changes.f8/index.html.
7 Fifteen Minutes of Privacy: Privacy, Sociality, and Publicity on Social Network Sites      89

Stutzman F (2006) An evaluation of identity-sharing behavior in social network communities. Int
   Digit Media Arts J 3(1):1‚Äì7
Raynes-Goldie K (2010) Aliases, creeping and wall cleaning: understanding privacy in the age
   of facebook. First Monday 15(1). http://firstmonday.org/htbin/cgiwrap/bin/ojs/index.php/fm/
   article/view/2775/2432
Richards DV (2007) Posting personal information on the internet: a case for changing the legal
   regime created by S 230 of the Communications Decency Act. Texas Law Review
   85:1321‚Äì1322
Taraszow T, Aristodemou E, Shitta G, Laouris Y, Arsoy A (2010) Disclosure of personal and
   contact information by young people in social networking sites: an analysis using facebook
   profiles as an example. Int J Media Cult Polit 6(1):81‚Äì102
Thompson C (2008) Brave new world of digital intimacy. The New York Times. http://www.
   nytimes.com/2008/09/07/magazine/07awareness-t.html
Timm DM, Duven CJ (2008) Privacy and social networking sites. New Direc Student Serv
   124:89‚Äì102
Tufekci Z (2008) Can you see me now? Audience and disclosure management in online social
   network sites. B Sci Technol Stud 11(4):544‚Äì564
Warren SD, Brandeis LD (1890) The right to privacy. Harvard Law Review 4(5):193‚Äì220
Westin A (1967) Privacy and freedom. Atheneum, New York
Winseck D (2002) Illusions of perfect information and fantasies of control in the information
   society. New Media Soc 4:93‚Äì122
Van Manen M (2010) The pedagogy of Momus technologies: facebook, privacy, and online
   intimacy. Qual Heal Res 20(8):1023‚Äì1032
Zhang C, Sun J, Zhu X, Fang Y (2010) Privacy and security for online social networks: challenges
   and opportunities. IEEE Netw 24:13‚Äì18
Chapter 8
The Co-evolution of Social Network Ties
and Online Privacy Behavior

Kevin Lewis




8.1    Introduction

What is the nature of personal privacy in an increasingly digital world? To what
extent should we foster greater information exchange among the public at large,
versus protect the ability to limit disclosure to the people of one‚Äôs choosing? And to
what extent do people say they care about either? Previous research on online
privacy has predominantly been concerned with questions such as these. Noticeably
absent, however, has been research examining actual online privacy behavior and
its causes. In other words, regardless of whether people say they care about online
privacy ‚Äì and regardless of whether they should care about online privacy ‚Äì given
the option to disclose more information or less, what factors are predictive of the
actual privacy decision that people make?
    In this chapter, I use a new longitudinal dataset combined with recent
developments in network modeling to examine the co-evolution of college
students‚Äô friendships and privacy behavior on Facebook. In contrast to past research
approaching the subject from theoretical, ethical, or attitudinal perspectives, I take a
behavioral approach to the study of online privacy ‚Äì one grounded in insights from
social network analysis. Researchers have long been interested in understanding
how friendships evolve among college students (e.g., Newcomb 1961), and increas-
ingly this work has been extended to the online sphere (e.g., Kossinets and Watts
2009; Mayer and Puller 2008; Wimmer and Lewis 2010). At the same time, given
the unprecedented global popularity of Facebook on one hand, and media attention
regarding its privacy measures on the other, the topic of Facebook and privacy has
recently attracted the attention of academic research as well (Debatin et al. 2009).
To date, however, no one has examined the interconnectedness of these two topics:
How does social network evolution among college students depend on their privacy


K. Lewis (*)
Harvard University, Cambridge, MA, USA
e-mail: kmlewis@fas.harvard.edu

S. Trepte and L. Reinecke (eds.), Privacy Online,                                    91
DOI 10.1007/978-3-642-21521-6_8, # Springer-Verlag Berlin Heidelberg 2011
92                                                                           K. Lewis


behavior; and how does privacy behavior among college students depend on their
social networks?
   In the following sections, I first briefly review current findings regarding online
privacy behavior and its causes. Next, I sketch seven theoretical mechanisms that
we can expect to influence changes in students‚Äô network ties on one hand, and
privacy settings on the other. I then describe the dataset and methodological tool
used in this study; present results from statistical models of the co-evolution of
network ties and privacy settings; and conclude with an interpretation of findings, a
summary of the limitations of these analyses, and suggestions for future research.



8.2   Previous Research

In the voluminous literature on online privacy, there have been remarkably few
published studies on the topic of online privacy behavior; and what research has
been published is almost exclusively based on self-report rather than natural
observation. Debatin et al. (2009), for instance, found a general disconnect between
users‚Äô understanding of privacy issues and their willingness to upload large
amounts of personal information. However, respondents also claimed to be more
likely to change their privacy settings if they had personally experienced a privacy
invasion. Tufekci (2008) similarly found little to no relationship between college
students‚Äô online privacy concerns and information disclosure, while Youn and Hall
(2008) examined the relationship between gender and privacy protection behaviors
‚Äì both using survey data. Finally, Livingstone (2008) used interviews to explore
teenagers‚Äô use of social networking sites for intimacy, privacy, and self-expression.
   In a previous analysis of the dataset used in this study, Lewis et al. (2008)
examined the predictors of college students having a private versus a public profile
on Facebook. They found that women were more likely to have a private profile
than men; that having a private profile is associated with a greater degree of online
activity; and that students who have a private profile are characterized by distinct
cultural tastes. They also found that students were more likely to have a private
profile if their Facebook friends and, especially, their roommates also had private
profiles ‚Äì but due to the cross-sectional nature of the analyses, conclusions about
causality were tentative.
   Researchers have identified the importance of approaching privacy from a
behavioral perspective ‚Äì particularly as it follows (or fails to follow) from users‚Äô
privacy-related beliefs or prior experiences. However, most studies are based on
self-report rather than actually observed behavior. Further, what little work exists
has been largely concerned with assessing the relationship between privacy behav-
ior and one additional variable, rather than modeling this behavior as the outcome
of several possible processes; and the one paper exploring actual privacy behavior
in a multivariate framework has been unable to make strict causal inferences about
privacy behavior as cause or consequence. This is the gap in the literature that the
current chapter aims to address.
8 The Co-evolution of Social Network Ties and Online Privacy Behavior              93


8.3     Mechanisms of Network and Behavioral Change

In recent years ‚Äì and corresponding with the development of new longitudinal
datasets as well as analytical tools for modeling longitudinal network data ‚Äì
tremendous advances have been made in our understanding of how social networks
evolve over time. In an important review article, Rivera et al. (2010) document
three types of mechanisms, or causal factors, that can account for the development
and persistence of a network tie between two people. Less work has been published
on the dependence of behavioral change on one‚Äôs network environment, but this is
quickly changing. Below, I organize this research into a framework of seven types
of mechanisms that can be used to understand the joint evolution of social network
ties on one hand, and online privacy behavior on the other.



8.3.1     Network Dynamics

8.3.1.1   Relational Mechanisms

The first type of mechanism that Rivera et al. (2010) describe has to do with the
impact of current relationships on the formation of new ties. These effects have
nothing to do with characteristics of the particular individuals involved ‚Äì but rather
their location in a broader landscape of relations. One of the most widely-
documented regularities in social networks is the tendency for friends-of-friends
to become friends, or for individuals to ‚Äúclose triangles‚Äù in networks. This is
because I am much more likely to meet the friends-of-friends than I am to meet
other strangers (because, for instance, our shared acquaintance may invite us both to
a party), and also because I am much more likely to feel positively towards these
people for reasons of structural balance (Davis 1963; Kossinets and Watts 2009).
Another regularity is the tendency for people with large networks to accumulate
friendships at a faster rate than do people with smaller networks ‚Äì both because a
larger baseline network may be reflective of a more sociable personality, and
because ‚Äúpopularity‚Äù is attractive to other individuals (cf. Snijders et al. 2010).
Finally, every social network is characterized by a particular ‚Äúdensity,‚Äù or baseline
tendency for a tie to be present versus absent. Networks of acquaintances, for
instance, will naturally have many more ties than networks of close confidants ‚Äì
and unless one controls for this tendency, it will be impossible to pinpoint the
contribution of other causal factors.


8.3.1.2   Assortative Mechanisms

A second fundamental determinant of network evolution is the principle of ‚Äúlike
attracts like‚Äù or ‚Äúbirds of a feather flock together‚Äù ‚Äì often called homophily
94                                                                                        K. Lewis


(McPherson et al. 2001). Homophily has been studied with respect to a wide variety
of attributes, though racial background is typically held to be the most divisive
feature of American social networks (but see Wimmer and Lewis 2010). Social
networks are also often segregated according to socioeconomic status (Marsden
1988) and gender (Marsden 1987).1


8.3.1.3    Proximity Mechanisms

The third set of mechanisms involves the focused organization of social interaction,
and amounts to the simple fact that people will be more likely to meet and become
friends with others who live, work, or otherwise spend time in the same place (Feld
1981). Among college students in particular, propinquity in living arrangements ‚Äì
e.g., sharing the same residence ‚Äì has been shown to be one of the most powerful
determinants of who befriends whom. Sharing an academic major can at times be
equally consequential, given that students are more likely to take classes and study
with those in their major (Marmaros and Sacerdote 2006; Mayer and Puller 2008;
Wimmer and Lewis 2010).


8.3.1.4    Privacy Mechanisms

Finally, independent of the above three mechanisms, network evolution can also
depend on students‚Äô privacy behavior in two basic ways. On one hand, students with
a private profile may have a greater or lesser tendency to form ties overall, leading
to a larger or smaller overall network size than the average student. Comparable to
what others (Goodreau et al. 2009; Wimmer and Lewis 2010) have called a
‚Äúsociality‚Äù effect, students with private profiles ‚Äì whose personal information is
hence blocked by default to all non-friends ‚Äì may tend to extend or receive a larger
number of friend requests precisely because this is the only way others may view
their information. An opposite effect could also occur, whereby the activation of
privacy settings precedes a general conservatism about extending and accepting
friend requests and hence leads to students with private profiles forming fewer ties
overall. In both cases, I refer to this as a ‚Äúmain effect‚Äù of privacy behavior on tie
formation (cf. Snijders et al. 2010).
    On the other hand, much like the assortative mechanisms above, students may
self-segregate not on the basis of demographic characteristics but on the basis of
privacy behavior itself. In other words, alongside the tendency to befriend students
of the same racial or socioeconomic background, students may display an affinity
with others who share their perspective on information disclosure ‚Äì students with


1
 It is also possible that individuals self-segregate based on structural position ‚Äì people with many
ties befriending other people with many ties, and people with few ties befriending other people
with few ties (Newman 2002). Such ‚Äúdegree-based‚Äù assortative mixing is not considered here.
8 The Co-evolution of Social Network Ties and Online Privacy Behavior                             95


public profiles seeking out others with public profiles, and students with private
profiles seeking out others with private profiles. In both cases, I refer to this as a
‚Äúsimilarity effect‚Äù of privacy behavior on tie formation (cf. Steglich et al. 2010).



8.3.2     Behavior Dynamics

8.3.2.1    Exogenous Mechanisms

Of all possible explanations for a shift in a given student‚Äôs privacy behavior,
perhaps the most plausible has nothing to do with the student at all. In other
words, before considering mechanisms that involve the unique situation of particu-
lar students, it is important to account for exposure to ‚Äúexternal‚Äù events or
conditions that affect all students equally and may spur a general change in privacy
behavior across the population. Such conditions are not hard to imagine: an incident
occurs in the college community that increases general awareness about privacy, or
perhaps a newspaper article is published to the same effect. Such a change may also
be an effect of the website itself ‚Äì e.g., Facebook alters the ease with which a private
profile may be activated ‚Äì or an ‚Äúexternal‚Äù effect of time ‚Äì e.g., students
approaching graduation may be more likely to switch to a private profile to avoid
the scrutiny of potential employers. In any case, unless one has specific data on such
externalities ‚Äì or particular reason to believe that some students would be more or
less susceptible to their effects than others ‚Äì such effects may be subsumed under a
general ‚Äúbaseline tendency‚Äù mechanism representing the baseline likelihood of a
student adopting a private profile, all else being equal.2


8.3.2.2    Associational Mechanisms

Researchers have long documented the effects of ‚Äúpeer influence‚Äù with respect to a
wide variety of characteristics and behaviors. Much work, both popular and aca-
demic, has addressed the diffusion of ideas, innovations, and trends throughout the
population or even the globe (Gladwell 2002; Kaufman and Patterson 2005; Rogers
2003). Other research has focused specifically on interpersonal influence with
respect to drug use (Kandel 1978), smoking (Mercken et al. 2010), music tastes
(Steglich et al. 2006), and a variety of other (often health-related) outcomes (Smith
and Christakis 2008). Each of these findings stems from a fundamental insight of
social science: that our behavior depends intimately on the behavior of those with
whom we associate. The implication for a longitudinal study of privacy behavior is


2
 In stochastic actor-based modeling, one typically also controls for potential curvilinearity in this
tendency by including a quadratic term. This is unnecessary here because the behavioral variable is
dichotomous.
96                                                                            K. Lewis


clear. Above and beyond any tendency to adopt a private profile as a result of
external factors, students who are friends with other students who have a private
profile may become additionally sensitive to privacy concerns themselves; mean-
while, students who are friends with other students who have a public profile may
be less likely to be the sole person to deviate from this norm.


8.3.2.3   Structural Mechanisms

Finally, there are reasons to expect that one‚Äôs structural position in a social network
‚Äì irrespective of the specific people to whom one is connected ‚Äì will have an
independent effect on the likelihood of adopting a private versus public profile. One
measure commonly emphasized in the networks literature is degree centrality.
Sometimes called ‚Äúneighborhood size,‚Äù degree centrality refers to one‚Äôs total
quantity of direct network connections (Freeman 1978). While peer influence has
to do with the specific people one associates with, then, ‚Äúdegree‚Äù effects stem only
from having a larger versus smaller friend network. In the context of privacy
behavior, students with larger social networks may be particularly informed about
public concern regarding online safety. Students with a large social network may
also feel as though the costs of a private profile in terms of information sharing are
relatively low (because relatively more people will still be able to see their
information anyway); although students with a small overall network may be
closer, in turn, with each of these friends, and therefore more content to share
information only with them and no one else.


8.3.3     Summary

What are the determinants of online privacy behavior among college students? In
particular, what is the relationship between college students‚Äô online privacy behav-
ior and college students‚Äô social network ties? Above, I outlined four general
categories of factors that may influence students‚Äô friendship choices ‚Äì relational
mechanisms, assortative mechanisms, proximity mechanisms, and privacy
mechanisms ‚Äì and three categories of factors that may simultaneously influence
students‚Äô privacy behavior ‚Äì exogenous mechanisms, associational mechanisms,
and structural mechanisms. In order to pinpoint the contribution of each of these
categories of factors to observed network and behavioral change, it is analytically
necessary to control for all of them: Firstly, because two very different mechanisms
may produce effects that are otherwise indistinguishable; and secondly, because
otherwise it is impossible to disentangle the direction of causality. In particular, if
students with private profiles are found to have larger or smaller networks, is this a
‚Äúmain effect‚Äù of privacy behavior on network activity, or a ‚Äúdegree effect‚Äù of
network position on privacy behavior? And if students are found to cluster together
according to privacy setting, is this because these students seek each other
out (‚Äúsimilarity effect‚Äù) or because privacy behavior ‚Äúspreads‚Äù among peers
8 The Co-evolution of Social Network Ties and Online Privacy Behavior                            97


(‚Äúpeer influence‚Äù)? It is therefore only with appropriately sophisticated modeling
tools ‚Äì combined with fine-grained, longitudinal data on students‚Äô networks and
privacy behavior ‚Äì that such questions can be answered.



8.4     Data and Methods

8.4.1     The ‚ÄúTastes, Ties, and Time‚Äù Dataset

Data for these analyses are drawn from the ‚ÄúTastes, ties, and time‚Äù social network
dataset (Lewis et al. 2008). Together with colleagues ‚Äì and with permission from
both Facebook and the college in question ‚Äì I downloaded longitudinal profile and
friendship data for the class of 2009 at an American private college (N ¬º 1640 at
wave 1). Students were located on Facebook using an official class roster with all
students‚Äô names and e-mail addresses, though the data were immediately stripped of
all identifiers. Data draws took place once a year for 4 years, in March of 2006,
2007, 2008, and 2009 ‚Äì such that we could view the evolution of students‚Äô social
networks and profile data over the 4 years of college. For the purpose of the
following analyses, I restrict attention to those 876 students who (1) were members
of the study cohort for all 4 years (i.e., they did not transfer in or out) and (2) had
publicly available data on Facebook friendships for all 4 years (i.e., they did not set
their Facebook friend data to ‚Äúprivate‚Äù).3 While procedures for dealing with
missing data in longitudinal network studies are available (Huisman and Steglich
2008), given that the central question of this study relates to the interrelatedness
between network ties and privacy behavior ‚Äì and further, that stable model estima-
tion generally relies on having no more than about 20% missing data (Snijders et al.
2008), but 27% of students have missing network data in wave 4 alone ‚Äì I chose
instead to only include students with public Facebook friendship data for all 4 years.
While practically motivated, this decision has the unfortunate consequence that
those students who arguably disclosed the least ‚Äì i.e., who hid both profile and
network data from non-friends ‚Äì are not considered.4


3
 Because it is not possible to distinguish between a student who is not on Facebook and a student
who is on Facebook but has hidden herself from searches, I first restricted attention to only those
students who could be located on Facebook for all 4 years. Of the 1,421 students who remained in
the study cohort for all 4 years, 1,272 (89.5%) met this criterion. The remaining 396 students who
were dropped from my analyses were active on Facebook for all 4 years, but did not have available
network data for at least one year. Comparing these 396 students with the final population of 876,
dropped students were significantly more likely to have a private profile in every wave ‚Äì creating
some risk of selection bias ‚Äì and significantly more likely to be Asian. Otherwise, however, the two
samples were statistically indistinguishable with respect to gender, race, and socioeconomic status.
4
 An alternative approach would have been to simply maximize the available data for each
transition period separately (see below). However, this would have the undesirable consequence
98                                                                                       K. Lewis


    In the analyses that follow, the central network variable is the presence or
absence of a Facebook friendship between two students, and the central behavioral
variable is whether each student maintained a public or a private profile at the time
of the data draw.5 Data on housing assignments and academic majors were provided
by the college. Gender was coded based on self-report; racial background was
coded based on online photos and any listed affiliations with Facebook groups or
college organizations signaling race/ethnicity; and socioeconomic status was coded
using the median household income of each student‚Äôs ‚Äúhometown‚Äù ZIP Code
Tabulation Area based on the 2000 Census (coded as missing data in the event of
a private profile for all 4 years).



8.4.2     Stochastic Actor-Based Modeling

Stochastic actor-based models were designed to overcome prior limitations in the
joint analysis of networks and behavior, and in particular disentangling social
selection versus peer influence. In short, these models respect the network depen-
dence of actors; account for alternative possible mechanisms of network and
behavioral change; and model the co-evolution of social networks and individual
behaviors in continuous time (Steglich et al. 2010).
   An accessible introduction to stochastic actor-based models is available in
Snijders et al. (2010). Here, it is sufficient to note that the heart of these models
consists of two ‚Äúobjective functions‚Äù ‚Äì one for changes in dyadic network ties, and
one for changes in individual behavior ‚Äì that represent the short term ‚Äúobjectives‚Äù
that each actor will probabilistically pursue. The function fiX √∞b; x; z√û ¬º
P X X
   bk ski √∞x; z√û represents the network component of this function for actor i given
 k
x state of the network, where effects sXki(x,z) correspond to the various mechanisms
for network dynamics described above and weights bXk are effect strengths. Simi-
                                   P
larly, the function fiZ √∞b; x; z√û ¬º bZk sZki √∞x; z√û represents the behavioral component
                                       k
of this function, where effects sZki(x,z) represent the different mechanisms for
behavioral dynamics described above and bZk are effect strengths. In short, the


that results could no longer be compared over time, because each model would be estimated over a
slightly different subset of students.
5
 It is important to note that this dataset was not compiled with the intention of studying privacy
behavior, and hence some distortion in the central behavioral dependent variable was introduced
insofar as research assistants were recruited from the college of study. Consequently, an unknown
minority of students in the study population may have falsely appeared to have ‚Äúpublic‚Äù profiles if
they happened to be Facebook friends with the specific research assistant assigned to download
their profiles. However, because research assistant assignments were random, this scenario would
only be more likely to have occurred the more Facebook friends the given student had; and
therefore the ‚Äúdegree effect‚Äù of Facebook friendships on privacy behavior can be expected to
capture (and control for) much of this variation.
8 The Co-evolution of Social Network Ties and Online Privacy Behavior                    99


strength of these models is that they are able to pinpoint the precise contribution of a
number of distinct mechanisms to both network and behavioral change, each while
controlling for all of the others.
   Previous applications of stochastic actor-based models have primarily focused
on adolescent substance use (Mercken et al. 2010; Steglich et al. 2010) as well as
visible versus non-visible attributes (de Klepper et al. 2010). These models have
yet to be applied to the topic of online social network ties and online behavior
of any sort.




8.4.3    Model Specification and Interpretation

At the end of their freshman year, each student at the college was randomly
assigned to one of 12 upper-class residences where the student would live
during her sophomore through senior years. These residences fall naturally into
four ‚Äúneighborhoods,‚Äù each containing three residences in relatively close
proximity ‚Äì though the size, individual character, and physical arrangement of
each neighborhood varies (neighborhood 1, for instance, is slightly smaller and
more geographically isolated from the main campus than the other three). In order
to consider possible variation of model parameters across these sub-populations,
all results are presented separately for each neighborhood. In order to capture
variation in the importance of the various mechanisms over time, results are also
presented separately for each ‚Äútransition period‚Äù that was observed: wave 1 to
wave 2 (period 1), wave 2 to wave 3 (period 2), and wave 3 to wave 4 (period 3).6
   Each model contains two distinct components: a set of terms related to network
dynamics, and a set of terms related to behavior dynamics. Table 8.1 presents a
summary of the mechanisms described above as well as the specific model terms
that correspond to each mechanism. Also included are two ‚Äúrate parameters‚Äù that
refer, respectively, to the average number of opportunities each student receives to
change a network tie and change privacy settings in the given transition period.
Interpretation of parameters varies depending on the specific term in question; but
in general, a positive and significant ‚Äúnetwork dynamics‚Äù coefficient means that the
given mechanism plays a significant role in the evolution of network ties while a
positive and significant ‚Äúbehavior dynamics‚Äù coefficient means that the given
mechanism plays a significant role in the evolution of privacy settings. Negative
and significant coefficients indicate that the mechanism is consequential but in the
opposite direction.



6
 The average within-neighborhood density at wave 1 is 0.076, compared to an average across-
neighborhood density of 0.059. At wave 2, these numbers are 0.124 and 0.080 respectively; at
wave 3, 0.150 and 0.091; and at wave 4, 0.166 and 0.100.
100                                                                                        K. Lewis


Table 8.1 Summary of mechanisms and corresponding model terms
Network dynamics
1. Rate parameter            Rate at which students receive the opportunity to change
                                 a network tie
Relational mechanisms
2. Density                   Overall tendency for ties to be present
3. Triadic closure           Tendency for A and B to become friends if A and B are both
                                 friends with C
4. Degree accumulation Tendency for popular students to become more populara
Assortative mechanisms
5. Gender homophily          Tendency for males to befriend males and females to befriend
                                 females
6. Racial homophily          Tendency for students from the same racial background to become
                                 friends
7. Socioeconomic             Tendency for students with similar SES to become friends
        homophily
Proximity mechanisms
8. Shared residence          Tendency for students who live in the same residence to become
                                 friends
9. Shared major              Tendency for students who share the same major to become friends
Privacy mechanisms
10. Privacy main effect      Tendency for students with a private profile to form more
                                 friendships overalla
11. Privacy similarity       Tendency for students with the same privacy setting to become
        effect                   friends
Behavior dynamics
12. Rate parameter           Rate at which students receive the opportunity to change privacy
                                 settings
Exogenous mechanism
13. Baseline tendency        Baseline tendency to adopt a private profile
Associational mechanism
14. Peer influence           Tendency to adopt the privacy behavior of one‚Äôs friends
Structural mechanism
15. Degree                   Tendency for popular students to have a private profile
a
 Because Facebook friendships are undirected, it is impossible to determine whether this is
because popular students/students with a private profile initiate more friendship requests or receive
more friendship requests


8.5     Results

From March 2006 through March 2009, and in each of the four neighborhoods, we
see a pronounced trend towards more students adopting a private profile over time
(Fig. 8.1). At wave 1, a mere 53 students (6.1% of the study population) had a
private profile. This number increases to 133 students (15.2%) at wave 2, 190
students (21.7%) at wave 3, and 353 students (40.3%) at wave 4. While a minority
of students in each transition period shifted their privacy settings from ‚Äúprivate‚Äù
back to ‚Äúpublic,‚Äù the vast majority of change was in the opposite direction
(Table 8.2). These trends were roughly consistent in all four neighborhoods, though
8 The Co-evolution of Social Network Ties and Online Privacy Behavior                                     101


                                       Neigh 1       Neigh 2          Neigh 3           Neigh 4

                              50%

                              40%
            Private profile


                              30%

                              20%

                              10%

                              0%
                              Wave 1              Wave 2             Wave 3               Wave 4

Fig. 8.1 Percentage of students with a private profile in each of four residential neighborhoods

Table 8.2 Descriptive statistics of changes in network structure and privacy behavior
                                                           Network structure
                Wave 1                 Period 1   Wave 2       Period 2        Wave 3     Period 3     Wave 4
Network density
Neigh 1       0.073                               0.132                        0.162                   0.177
Neigh 2       0.070                               0.118                        0.142                   0.161
Neigh 3       0.075                               0.116                        0.137                   0.151
Neigh 4       0.086                               0.130                        0.157                   0.173
Ties created
Neigh 1                                   1088                      552                       324
Neigh 2                                   1280                      644                       516
Neigh 3                                   1159                      560                       414
Neigh 4                                   1155                      689                       451
Ties dissolved
Neigh 1                                     25                        9                           44
Neigh 2                                     31                       37                           21
Neigh 3                                     36                       13                           27
Neigh 4                                     71                       26                           62
                                                           Privacy behavior
                Wave 1                 Period 1   Wave 2       Period 2        Wave 3     Period 3     Wave 4
Proportion private
Neigh 1       0.068                               0.188                        0.230                   0.419
Neigh 2       0.070                               0.127                        0.193                   0.408
Neigh 3       0.064                               0.107                        0.201                   0.359
Neigh 4       0.040                               0.193                        0.247                   0.430
Public to private
Neigh 1                                     25                       21                           39
Neigh 2                                     19                       25                           51
Neigh 3                                     19                       28                           41
Neigh 4                                     36                       27                           45
Private to public
Neigh 1                                      2                       13                           3
Neigh 2                                      6                       10                           2
Neigh 3                                      9                        6                           4
Neigh 4                                      2                       15                           4
102                                                                                    K. Lewis


       Wave 1                                 Wave 2




      Wave 3                                Wave 4




Fig. 8.2 Evolution of Facebook friendships and privacy settings in a single residential neighbor-
hood (Neigh 1, N ¬º 191). Nodes represent students, and lines represent Facebook friendships.
Shaded nodes correspond to students with private profiles; node size is proportionate to degree
centrality (i.e., larger nodes have more friends). Visualizations were generated using SoNIA
(http://sonia.stanford.edu)



students in neighborhoods 1 and 4 (the smallest neighborhoods) displayed a slightly
greater overall tendency to adopt a private profile than did students in
neighborhoods 2 and 3.
   The evolution of privacy behavior can also be visualized a second way, which
provides greater insight into the possible interdependence between students‚Äô friend-
ship decisions and privacy behavior. Figure 8.2 presents ‚Äúsnapshots‚Äù of students‚Äô
social ties and privacy settings in neighborhood 1 at each of the four waves of
observation. In general, we see both a gradual increase in network density (the
quantity of ties present) over time, and also a gradual change in privacy behavior as
more and more students adopt a private profile. There is also some evidence of
clustering according to privacy settings, and a possible tendency ‚Äì particularly
visible at wave 4 ‚Äì for students with many friends to have a private profile. Statistical
models are required to identify the significance of these effects, and also to effec-
tively disentangle the direction of causality between networks and behavior.
8 The Co-evolution of Social Network Ties and Online Privacy Behavior                             103


8.5.1      Network Dynamics

Results for stochastic actor-based models of Facebook friendships and privacy
behavior are presented in Table 8.3. All models were estimated using Siena version
3.18 (Snijders et al. 2008). Given that results are distributed across 12 distinct
models (three transition periods for each of four neighborhoods) ‚Äì each with 15
terms ‚Äì it is helpful to focus on patterns rather than individual coefficients, and to
discuss the network and behavior components of the models separately.7
    Most consistently, I find robust effects of triadic closure and shared residence for
every neighborhood in every transition period. In other words, the two most
dominant forces shaping the evolution of students‚Äô Facebook friendships is the
tendency to become (and remain) friends with one‚Äôs friends‚Äô friends, and to become
(and remain) friends with other students who share the same dorm ‚Äì what could be
thought of as social and physical propinquity respectively (cf. Kossinets and Watts
2009).8 There is also a tendency for Facebook friendships to be relatively sparse
overall (i.e., less than half of possible ties are actually present), and for students to
befriend others who share the same academic major, given that the ‚Äúdensity‚Äù
(negative) and ‚Äúshared major‚Äù (positive) terms are significant in all but two models.
Interestingly, students with relatively many Facebook friendships during period 1
are actually less likely to acquire additional friendships (negative, significant
‚Äúdegree accumulation‚Äù term for all neighborhoods); and the importance of racial
homophily varies according to both neighborhood and period (always significant
for neighborhood 3, significant for nearly all neighborhoods in period 3, two
neighborhoods in period 1, and only one neighborhood in period 2). Gender
homophily does not appear to play a positive role in the evolution of Facebook
friendships, although this term is negative and significant for two neighborhoods in
period two, suggesting that men and women become friends at a particularly high
rate. Finally, socioeconomic homophily is positive and significant only for neigh-
borhood 1 in periods 1 and 2.
    With respect to the focal privacy-related mechanisms of this chapter, I find that ‚Äì
even after controlling for all of the effects described above ‚Äì students‚Äô privacy


7
 All models were estimated using Siena‚Äôs unconditional moment estimation and the ‚Äúinitiative/
confirmation‚Äù model type for undirected networks (Snijders et al. 2008; see also van de Bunt and
Groenewegen 2007). This model type essentially simulates the process whereby Facebook
friendships are actually created and dissolved: a tie is created if and only if one student ‚Äúrequests‚Äù
a friendship and the other student then ‚Äúaccepts,‚Äù while a friendship can be terminated by either
student. All models were run using five phase two subphases and 1,000 phase three iterations.
Model convergence was in all cases excellent: the t-ratios for all parameters were less than 0.1 in
absolute value.
8
 Technically, positive ‚Äúnetwork dynamics‚Äù coefficients refer to both the tendency for new ties to
form and the tendency for old ties to be maintained; while negative coefficients refer to both the
tendency for new ties not to form and the tendency for old ties to be deleted. Because friendship
deletion is very rare in this network, however (Table 8.2), I focus only on the case of new tie
formation for the remainder of my interpretation of results.
                                                                                                                                                         104




Table 8.3 Model results for co-evolution of Facebook friendships and privacy behavior
                                           Period 1                                 Period 2                                 Period 3
                              Neigh 1 Neigh 2 Neigh 3 Neigh 4 Neigh 1 Neigh 2 Neigh 3 Neigh 4 Neigh 1                      Neigh 2 Neigh 3 Neigh 4
Network dynamics
Rate                            8.346     8.268     7.825       9.164     3.741     4.545    3.249     4.508      3.019      3.225    2.798     3.686
Density                       0.633* 0.716* 0.714* 0.754 0.521 0.720* 0.790* 0.396* 0.857* 0.791* 0.948* 0.920*
Triadic closure                 0.354*    0.367*    0.280*      0.244*    0.256*    0.201*   0.225*    0.204*     0.209*     0.194*   0.178*    0.136*
                                      *        *          *          *         *                             *         *
Degree accumulation           0.006 0.005 0.002 0.004 0.003 0.001                      0.001 0.007       0.012 0.001 0.001 0.002
Gender homophily                0.017     0.057     0.008       0.042 0.205*       0.044    0.033 0.229*        0.087 0.110 0.093 0.002
Racial homophily                0.063     0.201*    0.277*      0.003     0.096 0.058       0.314*    0.190      0.193*     0.229*   0.344* 0.068
                                      *                                        *                  *
Socioeconomic homophily         0.911     0.204 0.046          0.351     0.971 0.119 0.654          0.611      0.371      0.102 0.084       0.418
Shared residence                1.054*    0.909*    0.806*      0.648*    0.964*    0.808*   0.894*    0.817*     0.528*     1.004*   0.787*    0.756*
Shared major                    0.575*    0.825*    0.602*      0.448*    0.660*    0.806*   1.019*    0.557*     0.224      0.581*   0.340     0.485*
Privacy main effect           0.099      0.770*    0.026       0.822*    0.001 0.766       0.125 0.146         0.311 0.652* 0.128 0.140
Privacy similarity effect     0.270      0.683*    0.158       0.163     0.113 0.029       0.408 0.038       0.173       0.211    0.075     0.185
Behavior dynamics
Rate                            0.356     0.689     1.174       0.502     0.727     0.646    0.471     0.726      0.376      0.396    0.348     0.466
Baseline tendency               5.222* 1.351 0.021            1.073 0.038 0.438          2.150     0.919 10.740*        0.460 0.095       1.019
Peer influence                  5.245*    2.037     2.875       1.953     2.634     1.617    1.085     4.153* 7.094         0.281    0.968     4.406*
                                      *        *          *                                       *                    a
Degree                        0.078      0.060     0.017       0.003     0.016     0.015 0.072       0.012      0.500      0.043    0.052     0.045
Note: Significant coefficients in bold. N ¬º 191 for Neigh 1, N ¬º 228 for Neigh 2, N ¬º 234 for Neigh 3, and N ¬º 223 for Neigh 4. To test that the rate
parameters are not zero is meaningless, because if the rate parameters were zero we would observe no network or behavioral change between waves
a
 ‚ÄúDegree‚Äù parameter (Neigh 1, Period 3) fixed at this value in order to obtain model convergence
*
  p < .05
                                                                                                                                                         K. Lewis
8 The Co-evolution of Social Network Ties and Online Privacy Behavior                105


behavior plays an independent causal role in the evolution of their social network
ties. Firstly, in period 1 and for two out of the four neighborhoods (neighborhoods
2 and 4), students with private profiles are actually more likely to create and
maintain friendships than are their peers with public profiles (positive and signifi-
cant ‚Äúprivacy main effect‚Äù). Between their freshman and sophomore years, then,
these students either initiate a significantly greater number of new friendships or
receive more friendship requests, on average, than do students with public profiles.
This pattern actually reverses itself for students in neighborhood 2, however, in the
time between their junior and senior years: during this time period, students with
private profiles are less likely to initiate (or receive) new ties (negative and signifi-
cant ‚Äúprivacy main effect‚Äù). Finally, to the extent to which students who are friends
tend to share the same privacy behavior, there is only scant evidence that this results
from a process of social selection whereby students with similar privacy settings
seek one another out to become friends: the effect of ‚Äúprivacy similarity‚Äù on
network dynamics is significant only for neighborhood 2, and only in period 1.



8.5.2    Behavior Dynamics

In the behavioral dynamics section of the model, we see that each of the
mechanisms of behavioral change contributes in some way to the evolution of
privacy behavior in this population ‚Äì though effects again vary depending on
neighborhood and transition period. In neighborhood 4, the sole significant deter-
minant of privacy dynamics is peer influence: students in this neighborhood are
significantly likely to assimilate to the privacy behavior of their peers during the
second and third transition periods. In other words, students tend to adopt and
maintain the average privacy setting (public or private) held among their Facebook
friends ‚Äì but only following their sophomore year.
    Meanwhile, a number of distinct behavioral effects are present for neighborhood
1 (the smallest and most isolated neighborhood, and the neighborhood presented in
the visualization above). Between wave 1 and wave 2 (i.e., period 1), model results
confirm that students do in fact cluster according to privacy settings ‚Äì but that this
results solely from a process of peer influence rather than similarity-based social
selection. Additionally, students in neighborhood 1 in the first transition period
have a strong baseline likelihood of adopting a private profile (positive, significant
‚Äúbaseline tendency‚Äù effect); but students who have relatively large networks of
Facebook friends are less likely to adopt a private profile (negative, significant
‚Äúdegree‚Äù effect). No significant behavioral effects are present for this neighborhood
in period 2. In period 3, however, the model would not converge after repeated runs.
This sometimes happens when a very strong effect is present for a single parameter ‚Äì
i.e., the precise value of the coefficient does not matter, only that the coefficient is
very large or very small, and so the model will have trouble converging on a stable
estimate. In this case, the problematic parameter was the ‚Äúdegree‚Äù effect, which
tended towards very high values in estimation attempts. Therefore, I fixed the
106                                                                             K. Lewis


model parameter at a stable, high value (0.5), and the model had no trouble
converging. Conditional on this fixed parameter ‚Äì indicative of a particularly strong
tendency for students with large networks of Facebook friends to adopt a private
profile (the visual evidence of which is many large, shaded nodes in wave 4 of
Fig. 8.2) ‚Äì students in that neighborhood actually display a significant baseline
tendency away from having a private profile, most likely to counterbalance the
strength of the degree effect.
    Finally, students in neighborhood 2 as well as students in neighborhood 3 (the
two largest neighborhoods) display a positive and significant degree effect in the
first transition period ‚Äì an effect which reverses itself in the second transition period
for neighborhood 3. In other words, between their freshman and sophomore years,
students who have particularly many Facebook friends are particularly likely to
adopt a private profile (perhaps to insulate themselves from additional requests);
but between their sophomore and junior years, it is students with relatively few
Facebook friends who are more likely to adopt a private profile (at least in
neighborhood 3).



8.6    Discussion

These findings present the first available insight into the dynamic unfolding of
online network and privacy behavior. Despite the very different nature of these ties ‚Äì
friendships documented online ‚Äì compared to traditional network measures, results
for the network dynamics section of the models largely uphold what has been found
elsewhere: in particular, the crucial role of both social distance (triadic closure) and
spatial distance (co-residence and shared academic major) in determining the shape
of social networks. Interestingly, the role of ‚Äúassortative mechanisms‚Äù is less
consistent than prior research might lead us to expect: We see no self-segregation
among students according to gender; minimal self-segregation by socioeconomic
status; and significant racial homophily for only about half of all models. There is
also evidence that students with particularly small networks at the end of their
freshman year do some ‚Äúcatching up‚Äù during the following year only.
    Past research on selection and influence has also found that ‚Äì across a wide
variety of attributes that might ‚Äúspread‚Äù through social ties as well as influence their
creation ‚Äì social selection almost always plays a stronger role than does peer
influence. In other words, to the extent to which friends in social networks tend to
resemble one another, this is largely because they seek one another out rather than
become more similar over time (de Klepper et al. 2010). Privacy behavior, there-
fore, appears to constitute a rare exception to this trend: I find little evidence that
privacy behavior impacts the evolution of students‚Äô networks; and to the extent to
which it does, this almost always has to do with variation in students‚Äô ‚Äúsociality‚Äù
according to privacy setting rather than students with similar privacy settings
becoming friends. Meanwhile, peer influence indeed plays a significant role in the
evolution of students‚Äô privacy behavior ‚Äì but one that also varies considerably
8 The Co-evolution of Social Network Ties and Online Privacy Behavior                107


across time and context. On one hand, students in neighborhood 1 display a strong
tendency to assimilate to the privacy settings of their peers early in college (period
1) but not later. On the other hand, students in neighborhood 4 are influenced by
their peers‚Äô privacy behavior late in college (periods 2 and 3) but not earlier.
Finally, privacy behavior is not only influenced by the specific people with whom
one associates, but also by one‚Äôs structural position: I find multiple significant
degree effects on students‚Äô privacy behavior, though primarily positive (i.e.,
students with larger networks are more likely to have a private profile) rather
than negative, and primarily early in college (i.e., period 1) rather than later (though
students with large networks in neighborhood 1 are particularly likely to adopt a
private profile in period 3 ‚Äì an effect so strong it effectively destabilized the model).
    These analyses are limited in a number of ways. Most importantly, they are
restricted to students in a particular college setting ‚Äì a college in which Facebook
use was particularly widespread, even in 2006 ‚Äì which may or may not be
generalizable. Without detailed qualitative descriptions of the four neighborhoods,
which are here omitted in order to preserve the anonymity of the college, I have
only pointed out a few patterns in findings based on the size of the neighborhood(s)
in question. Due to practical limitations regarding missing data as well as
ambiguities regarding how to interpret students who could not be found on
Facebook, I only considered students who could be located on Facebook for all
4 years and who had publicly available friendship data. Finally, while stochastic
actor-based modeling represents the most sophisticated available method for
modeling the joint evolution of social networks and behavior, there are also
nontrivial limitations of applying this method to the study of privacy behavior.
Even in the final wave of observation (when private profiles are most widespread),
only a minority of students had a private profile; and almost all changes in privacy
settings over all three transition periods were due to students moving from ‚Äúpublic‚Äù
to ‚Äúprivate‚Äù rather than the opposite. Consequently, while all models converged to a
satisfactory degree, I was not able to consider additional mechanisms of behavioral
change (such as the impact of demographic background on privacy behavior) due to
insufficient bidirectional variation in the behavioral variable. Future research
should not only replicate these findings in other settings and using other measures
of online privacy behavior, but also consider additional mechanisms of network and
behavioral change that were not examined here.
    This research provides preliminary insight into a topic of clear importance to
academics and policymakers alike ‚Äì yet one that has been strikingly absent from
previous work on online privacy. These findings are also noteworthy for future
research on network and behavioral evolution more generally. In particular, they
demonstrate that mechanisms of change must be sensitive not only to actors‚Äô social
(i.e., relational) environments, but also to the time and setting at which this change
takes place. Some mechanisms are relevant in certain contexts ‚Äì here, college
residential ‚Äúneighborhoods‚Äù ‚Äì but not in others; while other mechanisms vary in
significance depending on the particular time in the life course (or transition
through college) in question. Future research should go beyond simply
demonstrating that such variation exists, and explore in greater detail how such
108                                                                                        K. Lewis


variation may be systematically related to certain key properties of the local
sociohistorical context (cf. Pattison and Robins 2002; van Duijn et al. 2003).
While these possibilities are rarely explored, recent advances in available data
and methods provide the opportunity for much progress; and as online information
disclosure plays an increasingly important role in the conduct of day-to-day life, so
we should be increasingly concerned with understanding who is actually disclosing
what information, and why.




References

Davis JA (1963) Structural balance, mechanical solidarity, and interpersonal relations. Am J
   Sociol 68:444‚Äì462
de Klepper M, Sleebos E, van de Bunt G, Agneessens F (2010) Similarity in friendship networks:
   selection or influence? The effect of constraining contexts and non-visible individual attributes.
   Social Netw 32:82‚Äì90
Debatin B, Lovejoy JP, Horn A-K, Hughes BN (2009) Facebook and online privacy: attitudes,
   behaviors, and unintended consequences. J Comput Mediat Commun 15:83‚Äì108
Feld SL (1981) The focused organization of social ties. Am J Sociol 86:1015‚Äì1035
Freeman LC (1978) Centrality in social networks: conceptual clarification. Social Netw 1:215‚Äì239
Gladwell M (2002) The tipping point: how little things can make a big difference. Little, Brown
   and Company, New York
Goodreau SM, Kitts JA, Morris M (2009) Birds of a feather, or friend of a friend? Using
   exponential random graph models to investigate adolescent social networks. Demography
   46:103‚Äì125
Huisman M, Steglich C (2008) Treatment of non-response in longitudinal network studies. Social
   Netw 30:297‚Äì308
Kandel DB (1978) Homophily, selection, and socialization in adolescent friendships. Am J Sociol
   84:427‚Äì436
Kaufman J, Patterson O (2005) Cross-national cultural diffusion: the global spread of cricket. Am
   Sociol Rev 70:82‚Äì110
Kossinets G, Watts DJ (2009) Origins of homophily in an evolving social network. Am J Sociol
   115:405‚Äì450
Lewis K, Kaufman J, Christakis N (2008a) The taste for privacy: an analysis of college student
   privacy settings in an online social network. J Comput Mediat Commun 14:79‚Äì100
Lewis K, Kaufman J, Gonzalez M, Wimmer A, Christakis N (2008b) Tastes, ties, and time: a new
   social network dataset using Facebook.com. Social Netw 30:330‚Äì342
Livingstone S (2008) Taking risky opportunities in youthful content creation: teenagers‚Äô use of
   social networking sites for intimacy, privacy and self-expression. New Media Soc 10:393‚Äì411
Marmaros D, Sacerdote B (2006) How do friendships form? Q J Econ 121:79‚Äì119
Marsden PV (1987) Core discussion networks of Americans. Am Sociol Rev 52:122‚Äì131
Marsden PV (1988) Homogeneity in confiding relations. Social Netw 10:57‚Äì76
Mayer A, Puller SL (2008) The old boy (and girl) network: social network formation on university
   campuses. J Public Econ 92:329‚Äì347
McPherson M, Smith-Lovin L, Cook JM (2001) Birds of a feather: homophily in social networks.
   Annu Rev Sociol 27:415‚Äì444
Mercken L, Snijders TAB, Steglich C, Vartiainen E, de Vries H (2010) Dynamics of adolescent
   friendship networks and smoking behavior. Social Netw 32:72‚Äì81
Newcomb TM (1961) The acquaintance process. Holt, Rinehart and Winston, New York
Newman MEJ (2002) Assortative mixing in networks. Phys Rev Lett 89:208701
8 The Co-evolution of Social Network Ties and Online Privacy Behavior                       109

Pattison P, Robins G (2002) Neighborhood-based models for social networks. Sociol Methodol
   32:301‚Äì337
Rivera MT, Soderstrom SB, Uzzi B (2010) Dynamics of dyads in social networks: assortative,
   relational, and proximity mechanisms. Annu Rev Sociol 36:91‚Äì115
Rogers EM (2003) Diffusion of innovations, 5th edn. Free Press, New York
Smith KP, Christakis NA (2008) Social networks and health. Annu Rev Sociol 34:405‚Äì429
Snijders TAB, Steglich C, Schweinberger M, Huisman M (2008) Manual for SIENA version 3.2.
   University of Groningen, ICS, Groningen
Snijders TAB, van de Bunt G, Steglich C (2010) Introduction to stochastic actor-based models for
   network dynamics. Social Netw 32:44‚Äì60
Steglich C, Snijders TAB, Pearson M (2010) Dynamic networks and behavior: separating selection
   from influence. Sociol Methodol 40:329‚Äì393
Steglich C, Snijders TAB, West P (2006) Applying SIENA: an illustrative analysis of the
   coevolution of adolescents‚Äô friendship networks, taste in music, and alcohol consumption.
   Methodology 2:48‚Äì56
Tufekci Z (2008) Can you see me now? Audience and disclosure regulation in online social
   network sites. B Sci Technol Soc 28:20‚Äì36
van de Bunt GG, Groenewegen P (2007) An actor-oriented dynamic network approach: the case of
   interorganizational network evolution. Organ Res meth 10:463‚Äì482
van Duijn MAJ, Zeggelink EPH, Huisman M, Stokman FN, Wasseur FW (2003) Evolution of
   sociology freshmen into a friendship network. J Math Sociol 27:153‚Äì191
Wimmer A, Lewis K (2010) Beyond and below racial homophily: ERG models of a friendship
   network documented on Facebook. Am J Sociol 116:583‚Äì642
Youn S, Hall K (2008) Gender and online privacy among teens: risk perception, privacy concerns,
   and protection behaviors. CyberPsychol Behav 11:763
Chapter 9
Self-Protection of Online Privacy:
A Behavioral Approach

Mike Z. Yao




9.1    Introduction

Major shifts in information and communication technologies often reshape the
ways in which we produce and share personal information. For example, the
development of writing systems allowed personal information to be recorded and
stored; the invention of printing technology made it easy to reproduce private
information and distribute it to the public; and electronic communications
maximized the efficiency and the speed of information sharing. Each of these
technological advancements forced human society to redefine the boundaries
between the public and private and to re-conceptualize the concept of personal
privacy. Not surprisingly, advances in digital communication technologies and the
rapid proliferation of social media during the last two decades have once again
challenged our views about privacy and privacy protection.
   While scholars from various disciplines have all examined the notion of privacy
and have each added unique angles to its understanding, there is surprisingly little
agreement on its definition and conceptualization. While some saw privacy as the
degrees to which people can actively control their own personal information
(Bennett 1967; Jourard 1966; Westin 1967), others viewed privacy as a matter of
accessibility to one‚Äôs body and mind (Altman 1975; Leino-Kilpi et al. 2001;
Marshall 1974). The concept of privacy has been defined either as a legal preroga-
tive (Warren and Brandeis 1890; Westin 1967), an objective state of being (Jourard
1966; Leino-Kilpi et al. 2001), or a subjective state of mind (Bates 1964). Any
discussion of privacy would involve physical, psychological, social, as well as
informational aspects (Burgoon 1982; Parrot et al. 1989).




M.Z. Yao (*)
City University of Hong Kong, Hong Kong, PR China
e-mail: mike.yao@cityu.edu.hk

S. Trepte and L. Reinecke (eds.), Privacy Online,                               111
DOI 10.1007/978-3-642-21521-6_9, # Springer-Verlag Berlin Heidelberg 2011
112                                                                          M.Z. Yao


    Despite the many different conceptualizations, two general approaches to the
study of privacy can be identified in existing literature. On the one hand, the notion
of privacy has been treated as a normative and legal concept. From this view, for
example, political philosophers and legal scholars have been primarily concerned
with questions such as ‚ÄúWhat is the nature of privacy?‚Äù and ‚ÄúHow much privacy
should a person have?‚Äù On the other hand, privacy has been studied as a social and
behavioral construct. From this perspective, social scientists have focused on how
individuals and/or groups of individuals perceive, protect, and negotiate personal
privacy in various social contexts.
    In this chapter, I maintain that the protection of online privacy would be best
studied from a behavioral perspective. Specifically, I will argue that unlike privacy
issues offline, to which a set of well-established cultural, social, and legal norms
may be applied, the burden of online privacy protection is primarily shouldered by
an individual‚Äôs own conscious effort. Such efforts, characterized by the adoption
of various self-protective strategies to guard personal privacy, might be
conceptualized as deliberate and planned behaviors. To support this view, I will
first provide an overview of the conceptualizations of privacy from the normative
perspective. Then, in the next section, I will discuss the inadequacies of taking
such a normative approach to address online privacy issues. In the third section, I
will propose a theoretical framework based on the theory of planned behavior
(Ajzen 1988) to examine self-protective behaviors. A number of antecedent factors
influencing people‚Äôs attitudes and beliefs with regard to online privacy will also be
discussed in this section.




9.2   Normative Perspectives on Privacy

The philosophical foundation of privacy in Western societies can be traced back to
ancient times. Konvitz (1966) pointed out that the story of Adam and Eve being
expelled from the Garden of Eden could be read as a story about personal privacy.
Indeed, nearly all influential thinkers within the Western philosophical tradition
have made some sort of distinction between ‚Äúpublic‚Äù and ‚Äúprivate‚Äù spaces (Elshtain
1995). The assumption generally has been that there is, or ought to be, a clear
boundary separating the private and the public realms in people‚Äôs lives.
   Aristotle saw life itself as divided into public and private spaces. Private homes
and households were thought of as the private sphere, or ‚Äúoikos,‚Äù and he contrasted
these spaces with the public sphere defined by political activities (DeCew 1997).
Another Greek philosopher, Epictetus, made a distinction between the private and
the public as well (More 1923). He emphasized a distinction between those events
or activities that were under our control and those that were not. Epictetus was
fascinated with the differences between the inner person ‚Äì one‚Äôs own mind and
9 Self-Protection of Online Privacy: A Behavioral Approach                       113


thoughts ‚Äì and the outer person ‚Äì one‚Äôs body and flesh. He argued that it was only
our inner self and inner thoughts that were truly under our own control.
   John Locke also made a clear distinction between the public and the private.
According to Locke (1690), no one person has exclusive rights to nature, which
includes land and what is on it. Locke extended the notion of property to every
thought, intellectual output, writing, or image a human being could produce. If it
belongs to and is acquired by the self through labor and sweat, then it is private
property and is considered distinctly separate from what the public owns or what
remains in nature.
   John Stewart Mill relied on the public/private dichotomy in his thinking as well.
He was concerned with the question of when it would be appropriate for society at
large (the public) to regulate individual (private) conduct. Mill (1976) argued that
there was a realm where people had social responsibilities and where the society
could properly restrain people‚Äôs actions, but that there was also another, more
private sphere of action in which the society would have little interest and should
not interfere.
   Many definitions of privacy have been developed based on these philosophical
views on the private and public realms. The concept of privacy has been defined as
matters that are personal and secretive (Stephen 1967), a universal human right to
be left alone (Cooley 1880; Melvin v. Reid 1931; Warren and Brandeis 1890), the
degree of accessibility to an individual ranging from none (i.e., perfect privacy) to
complete (i.e., no privacy) (Gavison 1980), or one‚Äôs ability to control information
about oneself (Westin 1967).
   While privacy is seen as universally positive in the capitalist Western world,
where the right to privacy is a valuable shield for protecting a realm free of the
scrutiny and intrusion by others (DeCew 1997), alternative views exist. For exam-
ple, privacy was viewed as a state of deprivation from the public good and a lack of
involvement in the community (see e.g., Arendt 1958). Marx saw privacy as
exclusively available to the rich, a protection against the poor (Tucker 1978).
Feminist scholars have also argued that too much attention has been given to the
private over the public, often to the detriment of women (DeCew 1997). The
association between the private sphere and the domestic space that is traditionally
occupied by women presents a domain in which women are deprived of power. The
domestic life and private space free from public interference has become a haven
for men to freely abuse women and oppress their wives and partners while hidden
from the watchful eye of public scrutiny (McKinnon 1989).
   Despite the lack of a clear and consistent conceptualization of privacy, a
review of philosophical and normative views on privacy clearly shows that this
concept, as interpreted in the Western philosophies, is not only closely linked to
tangible things such as physical spaces, information, and properties, but also to
the highly abstract notions of liberty and freedom. Moreover, the conceptual nub
of privacy almost always involves a boundary separating the public and the
private spheres. This boundary can either be concrete and physical or ephemeral
and intellectual.
114                                                                             M.Z. Yao


9.3    Inadequacies of Taking a Normative Approach to Study
       Online Privacy Protection

It would be relatively easy to find legal and technical solutions for protecting
personal privacy when it is defined in terms of physical and observable matters.
However, it is far more difficult to reach a consensus on the illusive right to privacy
even within a relatively homogeneous cultural system. For example, in the United
States, although there is a comprehensive legal system that explicitly identifies a
variety of specific situations in which individual‚Äôs privacy is protected, the US
Constitution does not guarantee the right to privacy explicitly (Prosser 1960;
Turkington and Allen 1999). The protection of privacy as a generalized human
right comes only from the interpretations offered by the Supreme Court Justices
(Cate 1997), which may shift from time to time. This reflects the fact that normative
beliefs about privacy are highly sensitive to cultural norms and sociopolitical
systems. Privacy, as a constant and universally accepted value, would be extremely
difficult, if not impossible, to define from a normative perspective.
    A normative approach to personal privacy is further challenged by the ever-changing
information and communication technologies (ICTs). In the physical world, for exam-
ple, observable objects and symbols usually mark the boundaries between private and
public domains, and the size of personal space can be measured in units of distance.
Although different views might exist as to how big or small a person‚Äôs private space
ought to be, the line between where public space ends and where private space starts can
nevertheless be easily observed and agreed upon in a given community. However, in the
virtual online world, the concept of ‚Äúspace‚Äù is merely a metaphor. There is no unit of
measurement for virtual space; there are no walls or markers to clearly divide the private
and public spheres. To make things more complicated, people from different cultures,
often with drastically different privacy beliefs and norms, co-occupy this abstract and
metaphorical space. In such a virtual environment, the normative rules and expectations
related to personal privacy are irrelevant.
    Although there is an ongoing effort for the legislators from governments around
the world to expand the right of privacy to the Internet (Turkington and Allen
1999), the difficulties of defining a private space in the virtual world and the
explosion of remotely accessible personal information challenge the application
of existing legal protections of privacy to the online environment. Furthermore, the
ease of data sharing and matching through digital computing also allows new
information about a person to be created by merging data from seemingly non-
private sources. Such data mining and cross-referencing technologies have also
posed great challenges to the normative perspectives on privacy.
    It is clear that a normative approach to privacy, relying heavily on social norms
and legal traditions, is ineffective when dealing with online privacy threats faced by
netizens of the digital world. Internet users would not be able to rely on legal
systems to protect their personal privacy, nor could they expect the other users to
observe the social and cultural norms of their own. As such, the burden of
protecting personal privacy shifts to the individuals themselves.
9 Self-Protection of Online Privacy: A Behavioral Approach                        115


9.4     Protection of Online Privacy as Planned Behavior

For each individual, the protection of privacy may either be passive or active.
Passive protection involves reliance on external entities such as the government or
other individuals. This type of protection is generally beyond the direct control of
one individual; collective actions and institutional support are required. It is also
highly sensitive to cultural and sociopolitical norms. As discussed earlier, the
online communicative environment poses a significant challenge to such protection.
   Active protection, on the other hand, relies on individuals themselves actively
adopting various protective strategies. In the physical world, for example, walls can
be reinforced to be soundproof; taller fences can be built to block the views from
outside; a door can be closed; a sign can be posted outside a room to indicate the
desire for privacy; and a lock can be added to a personal diary. In the virtual world,
there are also a number of ways in which people can actively protect their online
privacy. For example, Hoffman et al. (1999) found that more than 90% of Internet
users had either declined to provide personal information or had fabricated infor-
mation due to online privacy concerns. Internet users can install firewalls and virus
protection software, scrutinize the online information transmission, and use encryp-
tion for sensitive data.
   From a behavioral perspective, the protection of privacy can be viewed primarily
as a process of boundary management through various means of controlling private
space (Hall 1966, Sommer 1959) and personal information (Buss 2001; Petronio
2002). Such a protection and management process requires individuals to detect
threats to personal privacy from the external environment, weigh such threats
against their privacy preferences, evaluate the possible outcomes of either losing
or maintaining privacy in a given social situation, and then select and adopt the
boundary management strategies accordingly.




9.4.1    A ‚ÄúTheory of Planned Behavior‚Äù Model of Online Privacy
         Protection

In contrast to the offline environment, individuals cannot easily rely on their
physical senses to detect threats to privacy online. The often visually anonymous
communicative space may also hinder a person‚Äôs ability to rely on the usual social
and cultural cues to evaluate the target of self-disclosure. Additionally, individuals
must have a certain amount of knowledge about the Internet and online communi-
cation in order to assess privacy risks; many online privacy management strategies
also require technical skills beyond that of an average user. Therefore, while
privacy management and protection might be performed unconsciously and effort-
lessly in the offline world, effective self-protection of online privacy must involve
deliberate and effortful thoughts and actions.
116                                                                         M.Z. Yao


    A dominant approach to understanding deliberative actions people undertake in
a variety of domains has been the expectancy-value research tradition. Within this
tradition, the theory of planned behavior (TPB) (Ajzen 1988, 1991; Ajzen and
Fishbein 2005) is a leading model. The TPB lays out the underlying processes
leading to an individual‚Äôs intention to perform and the actual performance of a
target behavior. It maintains that the performance of a particular human action is
predominantly determined by the intention to perform it. The TPB postulates three
conceptually independent determinants of behavioral intention: The first predictor
is the individual‚Äôs attitude toward the target behavior; it refers to the degree to
which a person has a favorable or unfavorable evaluation of the behavior itself and
the outcome of performing such a behavior. The second predictor is the subjective
norm; it refers to the individual‚Äôs perceived social pressure to perform or not
perform the target behavior. The third predictor of behavioral intention is perceived
behavioral control, which refers to the individual‚Äôs perceived ease or difficulty of
performing the target behavior. In combination, attitude, subjective norm, and
perceptive behavioral control would lead to the formation of a behavioral intention.
As a general rule, the more favorable the attitude and subjective norm, and the
greater the perceived control, the stronger the person‚Äôs intention to perform the
behavior in question should be.
    Although the usefulness of a TPB-based approach to human actions is most
amplified in the research of health-related behaviors such as dieting, quitting
smoking, and practicing safe sex (Godin and Kok 1996), this theory is highly
generalizable and can be adapted to almost all planned behaviors under volitional
control (Ajzen 1988, 1991). With regard to online privacy protection, the TPB
should also be a useful theoretical framework because behaviors associated with
adopting various boundary management strategies to reduce threats of online
privacy are similar to many health-related behaviors. Both types of behavior
involve an individual‚Äôs conscious and deliberate decision to adopt a target behavior
in order to prevent a perceived harm.
    According to the TPB, the intention to adopt various behaviors and strategies to
protect personal privacy on the Internet would be affected by the person‚Äôs overall
evaluations of the necessity and effectiveness of a protective behavior of interest,
the perceived social norm regarding privacy protection and behavior, and his/her
ability to perform the behavior. For example, one of the privacy protection
strategies used most frequently by Internet users is providing false personal infor-
mation (Hoffman et al. 1999). When facing a choice of providing either real or
altered personal details to a website or another individual (i.e., behavioral inten-
tion), Internet users would evaluate the need and effectiveness of lying about
personal details (i.e., attitude). While providing a false personal identity might be
an effective way to protect privacy, doing so might reduce the chance of forming
meaningful social relationships. In this specific social context, people would weigh
the risks against the possible benefits and form an overall attitude toward the
behavioral choices of either providing real or false personal information (Petronio
2002). Moreover, the intention to lie about personal identity would also be
influenced by appropriateness and prevalence of providing false personal
9 Self-Protection of Online Privacy: A Behavioral Approach                       117


information in a given context (i.e., subjective norm). For instance, people are much
more likely to stay anonymous or use pseudonyms on a website if other users of the
site do so. Finally, the intention and actual action of posting false personal infor-
mation would be determined by individuals‚Äô subjective evaluation of how easy or
difficult it would be to not only create but also maintain a fake identity (i.e.,
behavioral control); it would be much easier for a user to hide behind a fictional
identity when posting a one-time comment in a public forum than when using a
social networking site regularly.
    The application of the TPB in predicting people‚Äôs online privacy self-protection
has received some empirical support. In a study of college students‚Äô adoption of
four online privacy protection strategies, Yao and Linz (2008) found that the three
main constructs in the TPB explained 17% of the variability in behavioral intention
and 24% of the variability in the actual adoption of online protective behaviors.
This finding is consistent with previous studies that used the TPB to predict other
types of planned behaviors.
    The primary concern of the TPB is to provide a reliable model that accurately
predicts the intention and the actual performance of a wide range of deliberate
human actions (Ajzen 1988, 1991). In order to maintain parsimony, antecedents and
moderators of these behavior-specific attitudes and beliefs are not included in the
TPB‚Äôs formal model. While this approach provides a robust yet simple theoretical
explanation for the underlying process leading to a person‚Äôs decision to perform
a target behavior, the TPB‚Äôs practical use in promoting a specific social behavior
is limited. As such, antecedents of specific attitudes and beliefs must be taken into
account when studying a specific type of behavior such as online privacy protec-
tion. For instance, the TPB model‚Äôs capacity for predicting a person‚Äôs intention to
adopt an online privacy protection strategy is determined by this person‚Äôs attitude
toward the strategy, perceived social pressure of using it, and perceived behavioral
control over its adoption. From a theoretical standpoint, these variables may be
sufficient to explain and predict the target behavior. From a practical standpoint,
however, in order to effectively promote online privacy protection, a researcher
must also examine the factors that influence privacy-related attitudes and beliefs.
    While many situational, contextual, and demographic factors might influence an
individual‚Äôs privacy-related attitudes, beliefs, and behaviors, I will highlight four
frequently studied variables in online privacy research within the general frame-
work of the TPB in the remaining part of this chapter: (1) concerns about
online privacy, (2) need for privacy, (3) self-efficacy, and (4) Internet use experi-
ence. The proposed conceptual framework is illustrated by Fig. 9.1.



9.4.2    Concerns About Online Privacy

By far the most commonly studied online privacy issue is consumer concerns and
worries about various online privacy threats. Within a planned behavior framework,
such concerns would strongly influence one‚Äôs attitude toward online privacy
118                                                                                  M.Z. Yao


                                         Positive Effect
                                         Negative Effect
       Need for
       Privacy


                                   Attitude


        Privacy
       Concerns

                                  Subjective                                        Self-
                                    Norm                    Intention
                                                                                 Protection

      Knowledge/
      Experience

                                  Behavioral
                                   Control
                                                     Theory of Planned Behavior (Ajzen, 1991)
         Self-
       efficacy


Fig. 9.1 A planned behavior model of online privacy self-protection


protection. The more a person is worried about privacy violation, the more likely
he/she would hold a positive attitude regarding the protective strategy.
   There is no doubt that Internet users are concerned about online privacy. An
analysis of more than 16 opinion polls taken between 1998 and 2002 reveals that
nearly two thirds of respondents were either ‚Äúvery‚Äù or ‚Äúsomewhat‚Äù concerned about
privacy when they use the Internet (Metzger and Docter 2003). Hoffman et al.
(1999) found that more than 90% of Internet users have either declined to provide
personal information or have fabricated information due to online privacy concerns.
   Recent development and rapid proliferation of online social media have trig-
gered a new wave of public concerns about online privacy (Barnes 2006; boyd
2008). While social media are great platforms for users to quickly expand
and maintain their personal or professional social networks, such a benefit would
require users to disclose a large amount of personal information, which would lead
to higher risks of privacy violations (boyd and Ellison 2005). Researchers found
that sensitive personal information can be constructed from information often found
in SNS users‚Äô public profiles (Gross and Acquisti 2005). A recent poll of American
Internet users has shown that a majority of SNS users were concerned about
personal privacy and had reported taking certain steps to minimize potential risks
(Lenhard and Madden 2007)
   A number of studies examined the factors that may influence consumer online
privacy concerns. Phelps et al. (2001) conducted a national mail survey and found
that a consumer‚Äôs attitude toward direct marketing and desire for information
control are antecedents to privacy concerns. Specifically, attitude toward direct
marketing is negatively associated with online privacy concerns whereas desire for
9 Self-Protection of Online Privacy: A Behavioral Approach                          119


information control is positively related to concerns of online privacy.
Demographics such as gender, age, race, and social economic status are also
predictors of online privacy concerns (Dommeyer and Gross 2003; Graeff and
Harmon 2002; Milne and Rohm 2000; O‚ÄôNeil 2001; Phelps et al. 2000; Sheehan
1999). For example, Sheehan (1999) surveyed 889 Internet users and found that
women are more concerned than men about their personal privacy in information
gathering situations (see Thelwall, this volume, Chap. 18 for a more detailed
discussion of gender and privacy concerns). Other factors influencing online pri-
vacy concerns include perceived creditability of the website (Flanagin and Metzger
2003), perceived security of transaction (Swaminathan et al. 1999), and trust
(Jarvenpaa et al. 1999). Yao et al. (2007) found that people‚Äôs belief in the general
right to privacy and technical knowledge were both predictors for concerns about
online privacy. Yao and Linz (2008) found that individuals‚Äô fear of becoming a
victim of online privacy violations had led to a positive attitude toward online
privacy protection, but lowered the level of their perceived behavioral control.



9.4.3    A Psychological Need for Privacy

In addition to worries about privacy violation, a dispositional psychological need
for privacy would be another important factor that would influence people‚Äôs
attitudes toward online privacy protection. The more an individual desires privacy,
the more positive this person‚Äôs attitude toward online privacy protection is likely to
be. Additionally, more need for privacy might also lead to greater concern about
online privacy threats and therefore indirectly affect attitude toward online privacy.
    The need for privacy has been addressed in several lines of research. The
evolutionary perspective, for example, postulates that humans have an innate
drive to be gregarious but territorial (Halmos 1953; Klopfer and Rubenstein
1977). Halmos opined that the desire for solitude is natural to both primitive and
post-primitive societies; such a desire functions to regenerate social life for its more
harmonious living. Klopfer and Rubenstein argued that having some level of
privacy is essential to many animals‚Äô survival. Most animals exhibit some patterns
of social withdrawal. Privacy may also aid survival by reducing competition for
food and reproductive resources. Although it is difficult to confirm a hardwired
need for privacy, much research has looked at human displays of territoriality as a
possible display of such an innate tendency. Territoriality refers to the possessive-
ness of a physical place, a certain knowledge area, or social status (Altman 1975).
Sommer (1966) has distinguished two strategies of territorial defenses ‚Äì avoidance
and offensive display ‚Äì that are purposed to protect or attain privacy. Marshall
(1974) found that people with greater need for privacy tended to have a greater
amount of fencing around their homes.
    Individual differences in the need for privacy might also be explained from a
developmental perspective. For example, family environment may directly influence
the development of an individual‚Äôs independence and autonomy (Ittelson et al. 1974).
120                                                                           M.Z. Yao


Lawton and Bader (1970) found that preference for a private room increases with
age from years 10 through 40. Marshall (1974) also found age to be a significant
predictor of privacy preference. Wolfe and Laufer (1974) found that with matura-
tion the concept of privacy preference becomes more cognitively complex. Parke
and Sawin (1979) found that the use of physical privacy markers and privacy rules
at home (e.g., putting signs on the door, knocking before entering the room, etc.)
both increased with age among children. Further discussions of a developmental
perspective can be found in Chap. 16 of this volume by Peter and Valkenburg in
their analyses of adolescents‚Äô online privacy preferences, and in Chap. 17 by Maa√ü,
which deals with privacy concerns among elderly Internet users.
    A number of recent studies provided empirical evidence that the need for privacy
would directly and indirectly influence attitude and beliefs related to online privacy.
For example, need for privacy has been found to influence user concerns about
online privacy among American college students as well as their counterparts in
Asia (Yao et al. 2007; Yao and Zhang 2008). Yao and Linz (2008) also found
individuals‚Äô need for privacy to be a direct and significant predictor of people‚Äôs
attitude toward online privacy protection strategies.




9.4.4    Self-Efficacy

Perceived self-efficacy is a person‚Äôs beliefs in his/her capabilities and cognitive
resources required to cope with given events (Bandura 1997). Bandura posited that
self-efficacy influences how people feel, think, and act. In terms of feeling, a low
sense of self-efficacy is associated with depression, anxiety, and helplessness.
Individuals with low self-efficacy would also have low self-esteem and harbor
pessimistic thoughts about their accomplishments and personal development. In
terms of thinking, a strong sense of self-efficacy is thought to facilitate cognitive
processes and performance in a variety of settings, including quality of decision-
making and academic achievement. In terms of actual behaviors, self-efficacy has a
major impact on motivation. People with high self-efficacy choose to perform more
challenging tasks (Bandura 1997). These persons set high goals and stick to them.
High levels of self-efficacy also allow people to select challenging settings, explore
their environments, or create new environments (Schwarzer 1992).
   According to the theory of planned behavior (TPB), self-efficacy is a closely
related construct to the notion of perceived behavior control. Ajzen (2002) posited
that while perceived behavioral control is an evaluation of external factors that
may influence the performance of a behavior in relation to one‚Äôs ability, perceived
self-efficacy is an assessment of the actor of a behavior in relation to various
external factors. These two concepts are closely related to and predictive of each
other (Ajzen 1988, 1991). Thus, one can expect that individuals with higher self-
efficacy will be more likely to transfer this sense of confidence to the specific
9 Self-Protection of Online Privacy: A Behavioral Approach                        121


context of online privacy protection through an increased level of perceived
behavioral control. A high level of self-efficacy will allow an Internet user to be
more confident at using online privacy protection tools. A highly self-efficacious
individual will also be more willing to try new protection strategies than will
individuals with lower levels of self-efficacy.




9.4.5    Knowledge and Experience of Internet Use

It is intuitively easy to deduce that the more experience people have with various
features and functions of the Internet, the more confident or efficacious they will
feel about using this medium. Previous research on Internet usage and online
privacy has supported this view (LaRose et al. 2001; Yao et al. 2007). As such,
individuals with more knowledge and experience of using the Internet might be
more likely to adopt various tools and strategies to online privacy through an
increase in perceived behavioral control.
    However, findings from other studies indicate that an increase in computer
knowledge and Internet use experience might also lead to a decrease in concerns
about online privacy, and therefore reduce self-protection intention and behavior.
For example, in a multiyear longitudinal investigation, the UCLA Center for
Communication Policy (2000, 2001, 2003, 2004) found that the level of privacy
concern has decreased over time, especially among experienced Internet users.
Phelps et al. (2000) found that consumers who had made a catalog purchase via
the telephone within the past 6 months were less concerned about disclosing their
credit card information than those who had not made a telephone catalog purchase.
This finding indicates that as consumers become more familiar with e-commerce,
they may be less concerned about privacy issues.
    Taken as a whole, these findings are indicative of two contradicting processes at
work. On one hand, as discussed earlier, an increase in computer knowledge and
Internet use experience may enhance a person‚Äôs control over the use of online
privacy protection tools. Such persons are more likely to use these tools and thus
feel less concerned about their privacy. This relationship is consistent with the TPB.
On the other hand, however, it can also be argued that an increase in computer
knowledge and Internet use experience may lead to a false sense of security,
especially if a person does not encounter any negative experience. For example, a
computer user may be careless about protecting personal information but, luckily,
has never experienced any harmful consequences. As a result, this person is less
likely to pay attention to online privacy threats and therefore decide not to utilize
online privacy protection tools as a precaution. In other words, there could be a
situation where higher levels of Internet use experience may lead to a decrease in
perceived threat to online privacy.
122                                                                          M.Z. Yao


9.5   Conclusion

The development of information and communication technologies and the prolifer-
ation of social media in recent years have triggered a new wave of public concerns
about personal privacy. However, the social norms and rules pertaining to personal
privacy in the offline world are usually not applicable in this virtual environment.
Protection of privacy in the virtual space thus would require individuals to con-
stantly monitor and evaluate privacy risks, and deliberately adopt various self-
protective strategies. As such, a behavioral approach to the study of online privacy
protection would be more preferable to the normative perspectives.
    In this chapter, a general theoretical framework based on the theory of planned
behavior was proposed to predict individuals‚Äô self-protection of online privacy.
According to this model, an individual‚Äôs intention to adopt various protective
strategies is a function of an overall attitude toward the target behaviors, a subjec-
tive evaluation of the social norm and pressure related to them, and an overall
perception of behavioral control.
    This chapter also discussed several frequently studied variables in online privacy
research within the proposed framework. Specifically, people‚Äôs online privacy
concerns and their dispositional need for privacy would positively influence their
attitude toward online privacy protection; individuals‚Äô perceived behavioral control
might be positively influenced by their sense of self-efficacy and Internet use
experience. However, Internet user experience, in certain conditions, might reduce
concerns about privacy and indirectly reduce the likelihood of online privacy self-
protection.
    Compared to previous online privacy research that focused primarily on privacy-
related concerns and preferences, a planned behavior approach does not assume the
link between privacy concerns and self-protection. Indeed, findings from previous
research suggest that, although Internet users report high levels of concern about
online privacy, they have little specific knowledge of practices that may violate
their privacy and little general knowledge of online privacy policies as a whole
(Dommeyer and Gross 2003). Moreover, while many consumers claim to be fairly
well informed about privacy protection strategies, they often do not adopt them
when using the Internet. The use of privacy protection strategies, such as carefully
reading privacy statements, managing cookies, and other precautionary measures,
is low even among those who claim to be very concerned about their privacy
(Berendt et al. 2005; Dommeyer and Gross 2003; Tavani 2000). The proposed
theoretical model may shed light on this seemingly paradoxical pattern by taking
into consideration the psychological process linking one‚Äôs attitude and actual
behavior.
    Overall, this chapter should provide a broad and fundamental understanding for
the rest of the book, focusing on specific issues related to online privacy and
personal boundary management.
9 Self-Protection of Online Privacy: A Behavioral Approach                                    123


References

Ajzen I (1988) Attitudes, personality, and behavior. Open University Press, Milton Keynes
Ajzen I (1991) The theory of planned behavior. Organ Behav Hum Decis Process 50:179‚Äì211
Ajzen I, Fishbein M (2005) The influence of attitudes on behavior. In: Albarracin D, Johnson BT,
    Zanna MP (eds) The handbook of attitudes. Erlbaum, Mahwah, pp 173‚Äì221
Altman I (1975) The environment and social behavior. Brooks/Cole, Belmont
Arendt H (1958) The human condition. Chicago, IL: University of Chicago Press
Bandura A (1997) Self-efficacy: the exercise of control. W. H. Freeman, New York
Barnes S (2006) A privacy paradox: social networking in the United States. First Monday, 11.
    http://www.firstmonday.org/issues/issue11_9/barnes/index.htm
Bates A (1964) Privacy: a useful concept? Soc Forces 42:432‚Äì437
Bennett C (1967) What price privacy. Am Psychol 22:371‚Äì376
Berendt B, G‚Ç¨unther O, Spiekermann S (2005) Privacy in e-commerce: stated preferences vs. actual
    behavior. Commun ACM 48(4):101‚Äì106
boyd d (2008) Facebook‚Äôs privacy trainwreck: exposure, invasion, and social convergence.
    Convergence 14:13‚Äì20
boyd d, Ellison N (2005) Social network sites: definition, history, and scholarship. J Comput
    Mediat Commun 13, Article 11. http://jcmc.indiana.edu/vol13/issue1/boyd.ellison.html
Burgoon JK (1982) Privacy and communication. In: Burgoon M (ed) Communication yearbook 6.
    Sage, Beverly Hills, pp 206‚Äì249
Buss A (2001) Psychological dimensions of the self. Sage, Thousand Oaks
Cate FH (1997) Privacy in the information age. Brooking Institute Press, Washington, DC
Cooley T (1880) A treatise on the law of torts or the wrongs which arise independent of contract.
    Callaghan, Chicago
DeCew JW (1997) In pursuit of privacy: law, ethics, and the rise of technology. Cornell University
    Press, Ithaca
Dommeyer CJ, Gross BL (2003) What consumers know and what they do: an investigation of
    consumer knowledge, awareness, and use of privacy protection strategies. J Interact Mark
    17:34‚Äì51
Elshtain JB (1995) Democracy on trial. Basic Book, New York
Flanagin AJ, Metzger MJ (2003) The perceived credibility of web site information as influenced
    by the sex of the source. Comput Hum Behav 16:683‚Äì701
Gavison R (1980) Privacy and the limits of law. Yale Law J 89:420‚Äì435
Godin G, Kok G (1996) The theory of planned behavior: a review of its applications to health-
    related behaviors. Am J Heal Promot 11:87‚Äì98
Graeff TR, Harmon S (2002) Collecting and using personal data: consumers‚Äô awareness and
    concerns. J Consum Mark 19:302‚Äì313
Gross R, Acquisti AA (2005) Information revelation and privacy in online social networks. In:
    Proceedings of WPES‚Äô05, ACM, Alexandria, pp 71‚Äì80
Hall ET (1966) The hidden dimension. Doubleay, New York
Halmos P (1953) Solitude and privacy: a study of social isolation, its causes and therapy.
    Philosophical Library, New York
Hoffman DL, Novak TP, Peralta M (1999) Building consumer trust online. Commun ACM
    42(4):80‚Äì85
Ittelson W, Proshansky H, Rivlin L, Winkel G (1974) An introduction to environmental psychol-
    ogy. Holt, Rinehart and Winston, Oxford
Jarvenpaa SL, Tractinsky N, Vitale M (1999) Consumer trust in an internet store: a cross-cultural
    validation. J Comput Mediat Commun 5. http://www.ascusc.org/jcmc/vol5/issue2
Jourard S (1966) Some psychological aspects of privacy. Law Contemp Probs 31:307‚Äì318
Klopfer P, Rubenstein D (1977) The concept of privacy and its biological basis. J Soc Issues
    33:52‚Äì65
124                                                                                  M.Z. Yao


Konvitz MR (1966) Privacy and law: A philosophical prelude. Law and Contemporary Problems
   31:272‚Äì288
LaRose R, Mastro D, Eastin M (2001) Understanding internet usage: a social-cognitive approach
   to uses and gratifications. Soc Sci Comput Rev 19:395‚Äì413
Lawton MP, Bader J (1970) Wish for privacy by young and old. J Gerontol 25:48‚Äì54
Leino-Kilpi H, Vaelimaeki M, Dassen T, Gasull M, Lemonidou C, Scott A, Arndt M (2001)
   Privacy: a review of the literature. Int J Nurs Stud 38:663‚Äì671
Lenhart A, Madden M (2007) Teens, privacy, & online social networks. Pew internet and American
   life project report. http://www.pewinternet.org/pdfs/PIP_Teens_Privacy_SNS_Report_Final.
   pdf
Locke J (1960) The second treatise on government. Bobbs-Merrill, Indianapolis
Marshall NJ (1974) Dimensions of privacy preferences. Multivar Behav Res 9:255‚Äì271
McKinnon C (1989) Toward a feminist theory of the state. Harvard University Press, Cambridge,
   MA
Melvin v. Reid, 112 Cal.App. 92 (1931)
Metzger M, Docter S (2003) Public opinion and policy initiatives for online privacy protection.
   J Broadcast Electron Med 47:350‚Äì374
Mill JS (1976) On Liberty. New York, NY: Penguin
Milne GR, Rohm AJ (2000) Consumer privacy and name removal across direct marketing
   channels: exploring opt-in and opt-out alternatives. J Public Policy Market 19:238‚Äì249
More PE (1923) Hellenistic philosophies. Princeton University Press, Princeton
O‚ÄôNeil D (2001) Analysis of internet user‚Äô level of online privacy concerns. Soc Sci Comput Rev
   19:17‚Äì31
Parke R, Sawin D (1979) Children‚Äôs privacy in the home: developmental, ecological and child-
   rearing determinants. Environ Behav 11:87‚Äì104
Parrot R, Burgoon JK, Burgoon M, LePoire BA (1989) Privacy between physicians and patients:
   more than a matter of confidentiality. Soc Sci Med 29:1381‚Äì1385
Petronio S (2002) Boundaries of privacy: dialectics of disclosure. State University of New York
   Press, New York
Phelps JE, Nowak GJ, Ferrell E (2000) Privacy concerns and consumer willingness to provide
   personal information. J Public Policy Market 19:27‚Äì41
Phelps JE, D‚ÄôSouza G, Nowak GJ (2001) J Interact Mark 15:2‚Äì17
Prosser WJ (1960) Privacy. Calif Law Rev 383:48
Schwarzer R (ed) (1992) Self-efficacy: thought control of action. Hemisphere, Washington, DC
Sheehan KB (1999) An investigation of gender differences in on-line privacy concerns and
   resultant behaviors. J Interact Mark 13:24‚Äì38
Sommer R (1959) Studies in personal space. Sociemetry 22:247‚Äì260
Sommer R (1969) Personal space. Prentice-Hall, Englewood Cliffs
Stephen JF (1967) Liberty, equality, fraternity. Liberty Fund, Indianapolis
Swaminathan V, Lepkowska-White E, Rao BP (1999) Browsers or buyers in cyberspace? An
   investigation of factors influencing electronic exchange. J Comput Mediat Commun 5(2).
   Retrieved on December 6, 2003, from http://www.ascusc.org/jcmc/vol5/issue2
Tavani H (2000) Privacy-enhancing technologies as a panacea for online privacy concerns.
   J Inform Ethics fall:26‚Äì36
Turkington RC, Allen AL (1999) Privacy law: cases and materials. West Group, St. Paul
UCLA Center for Communication Policy (2000) The UCLA internet report: surveying the digital
   future: year one. http://www.ccp.ucla.edu. Accessed 7 Oct 2001
UCLA Center for Communication Policy (2001) The UCLA internet report 2001: surveying the
   digital future: year two. http://www.ccp.ucla.edu. Accessed 21 Jan 2002
UCLA Center for Communication Policy (2003) The UCLA internet report: surveying the digital
   future: year three. http://www.digitalcenter.org/pdf/InternetReportYearThree.pdf. Accessed 1
   Oct 2005
9 Self-Protection of Online Privacy: A Behavioral Approach                                 125

UCLA Center for Communication Policy (2004) The UCLA internet report: surveying the digital
  future: year four. http://www.digitalcenter.org/downloads/DigitalFutureReport-Year4-2004.
  pdf. Accessed 1 Oct 2005
Warren S, Brandeis L (1890) The right to privacy. Harvard Law Rev 4:193‚Äì220
Westin A (1967) Privacy and freedom. Atheneum, New York (Witte, 1994)
Wolfe M, Laufer R (1974) The concept of privacy in children and adolescence. In: Carson D (ed)
  Man-environment interactions: evaluations and applications: part 2, vol 6, Privacy. Environ-
  mental Design Research Association, Washington, DC, pp 29‚Äì54
Yao MZ, Linz D (2008) Predicting internet users‚Äô self-protection of online privacy violations.
  CyberPsychol Behav 11:615‚Äì617
Yao MZ, Zhang JG (2008) Predicting user concerns about online privacy in Hong Kong.
  CyberPsychol Behav 11:779‚Äì781
Yao MZ, Rice R, Wallis K (2007) Predicting user concerns about online privacy. J Am Soc Inf Sci
  Technol 58:710‚Äì722
Chapter 10
Online Self-Presentation: Balancing Privacy
Concerns and Impression Construction
on Social Networking Sites

Nicole C. Kr‚Ç¨
            amer and Nina Haferkamp




10.1    Social Networking Sites and Privacy

Reaching the milestone figure of 500 million members in July 2010, the growth of
the social networking site Facebook has rapidly accelerated. Currently, its mem-
bership figures would make it the third largest country in the world, suggesting that
participation in online social networks has become more than a cursory phenome-
non. Members of Facebook are required to create an individualized online profile
that provides information about themselves, their physical appearance, individual
tastes, and preferences (see Liu 2007; Liu et al. 2006), and that highlights certain
aspects of their own personality. By means of these features, users inevitably
construct and manage impressions of their self. Research has already shown that
such a personal webpage even allows a more detailed self-presentation than a casual
face-to-face interaction and that people indeed make use of it in order to emphasize
certain aspects of their ‚Äútrue‚Äù self (Bargh et al. 2002; Haferkamp and Kr‚Ç¨amer 2010).
Additionally, empirical findings indicate that social networking sites (SNSs) are not
only a potential means for self-presentation but that people are indeed highly
motivated to use this new arena for presenting themselves (Haferkamp and Kr‚Ç¨amer
2010). In doing this, they even adopt profile elements that have originally been
provided for other purposes (e.g., people become a member of a group in order to
display their attitudes and interests instead of in order to communicate with others,
Haferkamp and Kr‚Ç¨amer 2009). This tendency might be due to the fundamental
motive of every human being to present him/herself in a positive way and, in doing




N.C. Kr‚Ç¨amer (*)
University of Duisburg-Essen, Duisburg, Germany
e-mail: nicole.kraemer@uni-due.de
N. Haferkamp
Technical University of Dresden, Dresden, Germany

S. Trepte and L. Reinecke (eds.), Privacy Online,                                127
DOI 10.1007/978-3-642-21521-6_10, # Springer-Verlag Berlin Heidelberg 2011
128                                                       N.C. Kr‚Ç¨amer and N. Haferkamp


so, gain positive reactions from those forming an impression (Leary 1995; Leary
and Kowalski 1990).
    However, especially when presenting oneself online, the motive to leave a
favorable impression can collide with the motive to maintain privacy. On the one
hand, due to their desire to present themselves, users often choose to display
individual information such as their physical appearance, individual tastes, likes,
hobbies, or even their names or addresses. On the other hand, they are aware of the
potential disadvantages when publishing this information in a more or less self-
defined community (Lenhart and Madden 2007; Lewis et al. 2008; Livingstone
2008; Reinecke and Trepte 2008; Utz and Kr‚Ç¨amer 2009). This contradiction
regarding the disclosure of private information (especially by teenagers) on an
online profile on the one hand and worries about privacy on the other has been
called the privacy paradox (Barnes 2006).
    The main goal of this chapter will be to discuss the conflicting motives and the
corresponding strategies. In doing this, we will first comment on the various forms of
self-presentation in SNSs and specifically, we will primarily address self-presentation
by means of profile elements. Also, we will compare offline and online self-presen-
tation and discuss in what way models from face-to-face self-presentation have to be
extended in order to be able to account for online self-presentation. Here, we focus
on the two-component model of self-presentation created by Leary and Kowalski
(1990) that describes self-presentation as the result of two different processes:
impression motivation and impression construction. In addition, we describe differ-
ent aspects of people‚Äôs privacy concerns (based on Burgoon 1982) and link them to
potential strategies for ensuring privacy when presenting oneself online. In the
conclusion, we discuss how users might deal with these conflicting motives and
interests and which strategies can be used to balance self-presentation and privacy.




10.2    Defining Self-Presentation from a Social Psychological
        Perspective and Forms of Presenting Oneself via SNSs

Whenever people want to be perceived in a particular way, certain self-presentation
strategies are activated in order to comply with other people‚Äôs expectations (Leary
1995; Leary and Kowalski 1990). Goffman (1959) describes self-presentation as an
attempt to control or guide the impression that others might make of a person by
using verbal and nonverbal signals. There have been attempts to distinguish the
terms self-presentation and impression management (Schlenker 1980; Schneider
1981), however, no clear-cut distinction has emerged that scholars can agree on.
Therefore, common practice is to use the terms interchangeably (Leary and
Kowalski 1990). In this chapter, we will use the term self-presentation. More
importantly, self-presentation has to be differentiated from self-disclosure: self-
disclosure is defined as the act of revealing private information to others and is thus
closely related to privacy (Archer 1980). The Internet in particular is a medium that
10   Online Self-Presentation: Balancing Privacy Concerns and Impression Construction   129


has increased willingness for self-disclosure due to reduced social cues and per-
ceived anonymity (Joinson 2001; Reingold 1993; Tidwell and Walther 2002).
Unlike self-presentation and impression management, self-disclosure ‚Äì as
conceptualized, for instance, by Joinson (this volume, Chap. 4) ‚Äì places a higher
priority on the quantity and the kind of personal information provided, while self-
presentation focuses on controlling the impressions that other persons form based on
the given information. Self-presentation/impression management is therefore more
concerned with the quality of information and the (sometimes unconscious yet
ubiquitous) intention to influence observers, while self-disclosure solely considers
the distribution of information, without necessarily intending to achieve a certain
impression effect (although depending on the conceptualization of self-presentation
there may be some doubt regarding whether any behavior/display of information is
possible without an at least implicit motive to present oneself, Leary 1995).
    With regard to online profiles on SNSs, both concepts, self-disclosure and self-
presentation, are highly relevant and strongly interrelated. Self-disclosure refers to
the act of presenting private information, which is a fundamental precondition for
the act of self-presentation online. In contrast, self-presentation refers to the process
of (consciously or unconsciously) trying to influence the impression formed by the
observer (e.g., through profile pictures or information regarding one‚Äôs preferences,
interests, and hobbies). While the present chapter focuses on self-presentation in
the Social Web, Joinson (this volume, Chap. 4) describes the role of self-disclosure
for online privacy.
    When discussing differences between online and offline self-presentation as well
as strategies to self-present and yet maintain privacy ‚Äì as we intend to do in the
following ‚Äì it is important to distinguish between different possibilities for self-
presentation on social networking sites. The ‚Äúclassic‚Äù form of engaging in self-
presentation on social networking sites is certainly to display information on
sociodemographic aspects, job, hobbies, etc. on the profile. Motives for presenting
oneself via this means as well as effects when reading other people‚Äôs profiles have
been analyzed in numerous studies in recent years (boyd and Ellison 2007; Ellison
et al. 2006; Haferkamp and Kr‚Ç¨amer 2010; Mehdizadeh 2010). However,
current developments on social networking sites permit various other forms of
self-presentation. For example, many SNS providers enable users to provide a
short note on their current activities, thoughts, or emotions (e.g., status update on
Facebook, Buschfunk on the German equivalent StudiVZ). Here, self-presentation
is possible in a similar way as compared to the classic profile elements: people can
decide how much and what kind of information they want to present about current
aspects of their lives. People use this feature to self-disclose current activities and
emotions but inevitably also to engage in self-presentation ‚Äì since even a statement
like ‚ÄúBored with learning for the next exam‚Äù can be seen as a means to influence
other people‚Äôs impressions. Similarly, people can leave postings on walls of their
friends and these postings are intended to be seen by the particular addressee as well
as his/her friends. This can also be used for self-presentation although the topic of
the messages will ‚Äì according to the function of this particular channel ‚Äì be more
concerned with the addressee instead of the sender. However, even a statement like
130                                                       N.C. Kr‚Ç¨amer and N. Haferkamp


‚ÄúWow! Summa cum laude! Congrats on your success‚Äù certainly also includes self-
presentational aspects. Furthermore, it is increasingly popular especially on
Facebook to write messages that are sent to a predefined group of friends and that
appear as a ‚ÄúNews Feed‚Äù on their starting page when they open their profile on
Facebook. However, this sort of communication is similar to writing e-mails or text
messages to a specific group of friends, as the sender can very flexibly decide who
should receive this message. This means that ‚Äì as in every conversation ‚Äì self-
presentation can be present in a more or less implicit or explicit way. Here, the
strategies and mechanisms with regard to self-presentation will resemble those
known from offline conversation or e-mail. This is due to the fact that (a) self-
presentation might be a secondary goal but the primary goal is communication
(compared to the [static] self-presentation on the profile where self-presentation is
predominant) and (b) communication is directed to a small and well-known group
that is selected according to the present communication goal. Given these different
possibilities for self-presentation, it is apparent that mechanisms and strategies for
self-presentation for each of the features presented above will differ, as do the
possibilities for establishing privacy. In order not to risk giving an overly simplistic
account of self-presentation and privacy strategies, in the remainder of the chapter
we will only focus on self-presentation via profile elements (profile picture,
sociodemographic information, jobs, hobbies, groups). This is due to the fact that
the profile ‚Äì as described above ‚Äì is most closely connected to self-presentation in a
pure form. Also, this form of self-presentation differs more clearly from other forms
of online and offline self-presentation directed to a clearly defined group of people
(e.g., when sending e-mails or talking to a group of people). Related to this, as will
be discussed below, the profile is still most interesting with regard to balancing
privacy concerns and self-presentation goals. Moreover, the profile (including its
usage and functions) has been analyzed in greater detail than the other features,
allowing us to base our analysis and conclusions on a rich body of empirical
research. However, one question that must be asked is whether design and reception
of profile elements are still relevant aspects of SNS usage given that numerous new
features, such as status updates, are becoming increasingly popular. Here, against
the background of recent results on the importance of, for example, choosing the
profile photo (Haferkamp and Kr‚Ç¨amer 2010), we assume that in spite of the new and
widely-used possibilities for using SNSs, the profile still plays a major role and has
not lost its importance for self-presentation.




10.3    Impression Construction Online ‚Äì Extending
        the Two-Component Model of Self-Presentation

Based on the considerations above, we will now discuss whether self-presentation
via profile elements on SNSs and self-presentation offline as it has been described in
the two-component model of Leary and Kowalski (1990) differ. In the present
10   Online Self-Presentation: Balancing Privacy Concerns and Impression Construction   131


section we will first amend the model to make it applicable to the particularities of
online self-presentation, and in the next section we will discuss the way in which
privacy concerns are influential. Before describing the model by Leary and
Kowalski (1990) in greater detail, we must first discuss general differences between
offline and online self-presentation via the profile elements of social networking
sites. Being able to carefully and consciously choose the presented contents, people
have more control over the selected profile elements (e.g., profile picture, group
names) on SNSs than they do over verbal and nonverbal cues during spontaneous
face-to-face communication. Moreover, in contrast to face-to-face communication,
other users‚Äô reactions and responses to users‚Äô self-presentation online are not
directly observable and are difficult to anticipate. Self-presentation online is ori-
ented toward an imagined, ‚Äúnon-present‚Äù audience and is inevitably linked to
uncertainty.
    Thus, online self-presentation differs substantially from offline self-presentation
and may challenge our traditional theoretical understanding of self-presentation
processes as described, for example, in the model by Leary and Kowalski (1990).
The authors describe self-presentation as the result of two different processes:
impression motivation and impression construction. While the former focuses on
the desire to create a particular impression in other people‚Äôs minds, the latter can be
regarded as the process of creating this specific impression. Thus, the model
considers not only the reasons why people are concerned with others‚Äô impressions,
but also why people choose one specific self-presentation strategy over another.
The motivation to conduct self-presentation (impression motivation) is impacted by
(a) goal relevance of impressions referring to the desire to reach certain goals by
means of one‚Äôs self-presentation, (b) the adequate value of desired goals, and (c) the
discrepancy between the desired and the current image. Secondly, the authors
consider the specific strategy of creating specific images (impression construction).
More precisely, Leary and Kowalski (1990) review five variables that impact a
person‚Äôs impression construction: (a) self-concept, (b) the person‚Äôs desired identity,
(c) role constraints, (d) the current or potential social image, and (e) target values.
    By definition, the self-concept refers to an individual‚Äôs perception of ‚Äúself.‚Äù
Research has demonstrated that offline self-presentation is often consistent with
how people see themselves (e.g., Jones and Pittman 1982). The tendency to portray
a character different from oneself can generally be seen as an exception. We can
thus conclude that the person‚Äôs self-concept is the primary determinant of the
images the person tries to create of him/herself.
    Besides the self-concept, the desirability of a certain identity image accounts for
a specific self-presentation strategy. According to Markus and Nurius (1986), a
desirable identity image refers to what a person would like to be and it is thus not
surprising that self-presentation is biased in the direction of these desired images.
    However, as already discussed in self-completion theory (Gollwitzer 1986),
people follow desired identities while sticking to the boundaries of reality. Here,
specific role constraints, i.e., specific roles within the social system, such as the role
of a father/mother or people‚Äôs occupational status (e.g., lawyer, nurse etc.) constrain
people‚Äôs self-presentation insofar as these roles are related to certain expectancies
132                                                       N.C. Kr‚Ç¨amer and N. Haferkamp


within the social system. Closely related to this, Leary and Kowalski (1990) focus
on a person‚Äôs current or potential social image, i.e., how a person thinks others
currently or prospectively regard him/her, as another determining variable of
impression construction. Also, information that others have can constrain
individual‚Äôs self-presentation because the person has a low probability of creating
an alternative impression (Schlenker 1980). A clear deviation from the current
social image involves the risk of misunderstandings and discrepancies between
other people‚Äôs expectations and the current behavior. Finally, the anticipation
of other people‚Äôs values (target values) ‚Äì i.e., the preferences and values of
significant others (Jones 1964) ‚Äì impacts people‚Äôs self-presentation. That does not
necessarily mean that individuals choose to form inaccurate impressions to meet the
values of others, rather that they choose to select those impressions that are most
likely to meet with desired reactions (Leary and Kowalski 1990).
   As the model was developed to predict offline self-presentation, it must be revised
with regard to self-presentation online. With regard to impression motivation, we
argue that the individual motivation is unaffected by the specific ‚Äúarena‚Äù of self-
presentation. Rather it should be seen as a necessary prerequisite for engaging in self-
presentation in the first place ‚Äì regardless of whether this takes place on SNSs or in
face-to-face communication. Indeed, as alluded to in the introduction, individuals
have a strong motive to self-present since a beneficial self-presentation might lead to
numerous advantages such as favorable actions and behavior by others (Leary 1995).
Therefore, the presentation of one¬¥s positive aspects online to a more or less broad
audience can be seen as a unique possibility to satisfy this important human need.
However, with regard to impression construction, various parallels but also
differences between online and offline strategies can be observed. Concerning the
self-concept, a connection to self-presentation has also been confirmed for online
contexts: prior research has argued that profiles display a person‚Äôs true self rather
than fake information (Bargh et al. 2002; Haferkamp and Kr‚Ç¨amer 2010). Gosling,
Gaddis, and Vazire (2007) indicate that self-generated images of Facebook users are
closely related to how they are seen in everyday life. The authors showed patterns of
convergence between impressions formed of the profile owner by strangers, who
were merely able to base their impression on Facebook profiles, and the
corresponding self-evaluations of the users. We can thus conclude that people want
to ensure that others perceive them accurately on SNSs, while their self-concept
serves as a guide for creating these impressions (Baumeister and Jones 1978).
   Moreover, in line with the assumption on the desirability of a certain identity
image, Toma and Hancock (2009) examined the role of physical attractiveness in
online daters‚Äô self-presentation. The authors revealed that less attractive daters
enhanced the attractiveness of their profile photograph and lied more about height
and age than did attractive daters. This demonstrates that just as in face-to-face
settings, people also prefer attractive self-presentations in online contexts.
   The third variable, role constraints, also impacts self-presentation online, as can
be concluded from results showing that women are more concerned with the
selection of their profile picture than men are (who in turn are more preoccupied
with career issues on profiles; Haferkamp and Kr‚Ç¨amer 2010).
10   Online Self-Presentation: Balancing Privacy Concerns and Impression Construction   133


    Also, the influence of the person‚Äôs current or potential social image (how he/she
thinks others currently or prospectively regard him/her) will be parallel in offline
and online self-presentation. False or inaccurate impressions that are inconsistent
with the current social image of a profile user can be identified by observers. For
instance, users of the German social networking site StudiVZ stated that their desire
to create a realistic impression is based on the apprehension that exaggerated
impressions would be detected and disclosed by friends who look at their profile
pages (Haferkamp and Kr‚Ç¨amer 2010). A brief comment on the wall, for instance,
could lever out each kind of idealized self-presentation. Therefore, information that
others have can constrain individuals‚Äô self-presentation because the person has
a low probability of creating an alternative impression (Schlenker 1980). Hence,
self-presentation online is likely to be guided by the expectations of acquaintances
in order to avoid embarrassing discrepancies between the chosen self-presentation
and the social image of a person.
    While self-presentation online and offline seems to be executed in a similar way
as far as the four factors mentioned above are concerned, with regard to the last
determinant, target values, online and offline self-presentation need to be
contrasted: In their model, Leary and Kowalski (1990) postulate that the target
values of the audience affect people‚Äôs selection of images. This does not necessarily
mean that they endeavor to choose inaccurate impressions, but rather that they
select those impressions that are most likely to meet with desired reactions (i.e.,
‚Äúpackaging‚Äù [Leary 1995]). From a wealth of attributes that constitute a person,
he/she will only choose those aspects that are presumably most appealing to the
specific addressee in the specific situation. This type of packaging, however, is not
possible with regard to self-presentation online, since there is no feature, for
instance, for designing different profiles for different visitors (e.g., friends, parents,
colleagues). Although users can decide which particular category is visible to a
certain group of people (e.g., via Friend lists, see below), they cannot present
adapted information within one profile feature. For instance, users cannot reveal
to their closest friends that Lady Gaga is their favorite musician while presenting to
another group of people that they like Beethoven. They have to decide whether to
disclose or to conceal that Lady Gaga is their favorite singer. Consequently, online
self-presentation is constrained by technological boundaries allowing only limited
flexibility with regard to online self-presentation. People can compose different
forms of online self-presentation based on the quantity of information (more or less
categories are visible) but they cannot change the quality of information within one
category. Thus, ‚Äúpackaging‚Äù in the sense of selecting the information that might be
most appealing to a specific audience (Leary 1995) is rather unlikely. Against the
background of people¬¥s goal to leave a favorable impression, several self-presenta-
tion strategies are possible that can help in dealing with this problem: (a) They
could choose to present all information about themselves (i.e., reveal that they like
Lady Gaga AND Beethoven) and trust that each addressee will thereby be provided
with all necessary information to build a positive attitude. (b) They could choose to
(openly) display only that information that will probably appeal to all sorts of
people who have access to their profile. In doing this, people would ensure that their
134                                                              N.C. Kr‚Ç¨amer and N. Haferkamp


Table 10.1 Comparison between self-presentation offline and self-presentation online on social
networking sites (SNSs) under consideration of the dimension ‚Äúimpression construction‚Äù
Impression        Self-presentation offline            Self-presentation
construction      (Leary and Kowalski 1990)            online on SNSs
Self-concept      Primary determinant of the           Users tend to display true self (Bargh
                  images a person tries to             et al. 2002), self-concept as a guide for
                  create of him/herself                creating online impressions
Desirability      Self-presentation is biased in the   People tend to present themselves in a
of a certain      direction of desired images          positive manner, choosing beautiful
identity image                                         profile pictures (Toma and Hancock
                                                       2009)
Role constraints Inconsistent behavior can be          Online self-presentation is related to
                  identified in communication          specific offline roles in the social system
                                                       (e.g., women preoccupied with physical
                                                       attractiveness, Haferkamp and Kr‚Ç¨amer
                                                       2010)
Current or        Behavior should meet the             Idealized self-presentation online can
potential social expectations of the present           cause negative comments by friends who
image             interaction partners                 know the profile owner from face-to-face
                                                       communication (Walther et al. 2008)
Target values     Self-presentation can be adapted     ‚ÄúPackaging‚Äù is limited, users address a
                  to each interaction partner          broad audience or a group of people; in
                  (‚Äúpackaging‚Äù [Leary 1995])           order to provide relevant information for
                                                       all addresses, numerous attributes would
                                                       have to be presented




self-presentation is suitable for each member who has access. As will be discussed
in the following, the first self-presentation strategy certainly conflicts with privacy
concerns, while the latter is likely to lead to a superficial and uninformative way of
self-presentation and might conflict with the goal of providing a detailed and
accurate view of oneself. Table 10.1 provides an overview of the comparison
between online and offline self-presentation strategies.



10.4     Informational, Psychological, and Social Privacy
         and Corresponding Strategies

As has already been mentioned above, self-presentational goals and strategies are
likely to conflict with privacy concerns. While optimal online self-presentation
might render it necessary to provide a wealth of information about oneself, this
strategy is disadvantageous from a privacy point of view. In order to be able to
discuss how self-presentational goals and privacy concerns might be balanced, in
this section we will present an overview of different aspects of privacy as they have
been described by Burgoon (1982). These will then be transferred to online privacy.
We will then present different strategies for establishing and maintaining privacy
10   Online Self-Presentation: Balancing Privacy Concerns and Impression Construction   135


and finally, we will discuss what strategies can be used to achieve a compromise
between self-presentation goals and privacy concerns.



10.4.1 Types of Privacy

In an influential paper, Burgoon (1982) distinguished different types of privacy in
communication contexts: informational, physical, psychological, and social pri-
vacy. While physical privacy is not easily applicable to online communication, the
other types relate nicely to specific privacy risks that can be encountered in online
settings. For example, by giving factual information such as names and addresses,
users‚Äô ‚Äúinformational privacy,‚Äù which Burgoon (1982) defined in the context of
face-to-face communication as the right to decide to what extent factual data about
oneself is released to others, is inevitably threatened. Besides factual information,
users also disclose information about their emotional states, thoughts, and
preferences when, for instance, they publish status updates or leave comments on
the wall of other users‚Äô profiles. These private cognitive inputs and outputs only
pertain to the individual and are thus part of a person‚Äôs ‚Äúpsychological privacy,‚Äù
defined by Burgoon (1982) as the ability to control affective and cognitive inputs
and outputs. However, by expressing them on online profiles, they become public
information and can be perceived by other users. Finally, each communication
episode between the profile owner and another user displayed on the News Feed is
distributed within the Facebook universe. Even friends of users with whom the
profile owner is connected can follow this private communication on the News Feed
even though they have never met the persons concerned in real life. This public
access to the social interactions of social network users is closely connected to
‚Äúsocial privacy,‚Äù defined by Burgoon (1982) as the ability to withdraw from social
intercourse, for example, to achieve greater intimacy among a selected group of
communication partners.



10.4.2 Privacy Strategies

The most obvious privacy strategy for maintaining informational, psychological,
and social privacy is certainly to choose to not make the profile and all of its
contents publicly available, and to limit the number of people that are granted
access by only accepting well-known people as ‚Äúfriends‚Äù (for an overview of the
strategies, see Table 10.2). However, with regard to self-presentation goals and the
related goal of building up social capital (in this case by leaving a favorable
impression on strangers in order to facilitate future interactions), this strategy can
be seen as disadvantageous. Another, less rigorous way is to employ the so-called
Friend list ‚Äì a technological feature that allows users to organize observers of their
online profile into lists. By means of these lists, profile users can decide which kind
136                                                               N.C. Kr‚Ç¨amer and N. Haferkamp


Table 10.2 Overview on privacy strategies and corresponding consequences for self-presentation
Privacy strategy                                   Consequences for self-presentation goal
Invite only a limited number of (well-known) Hinders making a favorable impression on
people as friends                                  potentially interested and interesting strangers
Use Friend list feature to exclude a number        A number of people do not get any information
of ‚Äúfriends‚Äù from privacy-relevant aspects         on specific aspects
of profile
Permit access by all friends (and even strangers) Information that might be appealing to one
to all profile elements but only post superficial, group but not to the other is not presented and
not privacy-relevant information                   cannot be employed to make a favorable
                                                   impression


of profile feature is revealed to a certain audience group and which information
stays hidden. Although it is not possible to change the content within one feature,
users can decide which piece of information is shown to a specific group of persons.
This, for instance, allows users to selectively open more of their profile (e.g., profile
pictures, personal information, likes, tastes) to the people closest to them while
hiding this private information from rather ‚Äúofficial‚Äù or loose contacts. By
customizing each setting, users can decide which category of information (e.g.,
profile picture, personal information, status updates) can be perceived by the
specific audience. With this feature, a user can differentiate, for example, between
acquaintances/strangers (people the user has never met or has met only once in
everyday life), friends (i.e., persons the user is also friends with in everyday life),
family members (i.e., mother/father/brother/sister), and official contacts (i.e.,
important occupational contacts such as the user‚Äôs employer). By employing the
Friend list feature, the user might ensure informational, psychological, and social
privacy with regard to strangers/acquaintances, psychological, and social privacy
with regard to official contacts, as well as psychological privacy with regard to
family members. For actual friends probably the least restrictions are necessary.
The disadvantage of the Friend list strategy is that a number of people do not get
any information about specific aspects of the user. In terms of self-presentation, this
could be similarly disadvantageous to the strategy to completely exclude people. A
different strategy can be to grant access to information to a large number of people
but to provide only superficial and therefore not privacy-relevant information. This
complies with one of the self-presentational strategies depicted above in that only
information that is compatible with mainstream attributes is displayed. The down-
side of this, however, is that the user is not able to present any specific information
even though it might be appealing to some of the receivers.



10.4.3 Potential Strategies to Solve the Conflict

The previous analyses show that from a self-presentational perspective, it would be
best to provide a wealth of information on the SNS profile, which is accessible to a
10   Online Self-Presentation: Balancing Privacy Concerns and Impression Construction   137


large number of people, while effective privacy strategies all lead to the obstruction
of exactly this. In order to solve the conflict, every user has two options: (a) to
decide whether self-presentational goals or privacy concerns are more important to
him/her personally and to behave accordingly, or (b) to find new strategies that
enable him/her to self-present and maintain privacy at the same time. With regard to
the first aspect, the individual decision about what to do will be related to the
strength of self-presentation motives on the one hand and privacy concerns on the
other. Indeed, it has been shown that users with higher privacy concerns conceal
more of their profile (Utz and Kr‚Ç¨amer 2009). However, even when people have high
privacy concerns, this does not necessarily mean that they have a low self-presentation
motive. In terms of the extended model of self-presentation by Leary and Kowalski
(1990), we would state that it is not the impression motivation that is influenced by
privacy concerns but the impression construction. This means that even people with
high privacy concerns might still want to find a way to convey a specific image even
though they are not willing to display private details and actual facts. This conflict ‚Äì
in our view ‚Äì has already led to the development of at least one new strategy to self-
present in a way that is not as detrimental to privacy as the usage of common
aspects of the profile: in order to present attitudes, attributes, and preferences,
people become members of a group that is then displayed on their profile. Here,
instead of communicating on their profile that they have bought a car or even
choosing a profile picture with them in front of it, people might simply become a
member in the group ‚ÄúI love my car!‚Äù In fact, empirical findings have confirmed
that group membership is first and foremost used as a means of self-presentation
rather than a possibility to communicate with others (Haferkamp and Kr‚Ç¨amer
2009).




10.5     Conclusions Regarding the Balance Between Privacy
         and Impression Construction on SNSs

Based on the comparison of online and offline self-presentation, we concluded that
a strategy like packaging, in terms of selecting specific information for a specific
audience, is not possible when using the profile elements of SNSs. Whereas in real-
life situations self-presentation only includes giving information to a selected,
perceptible audience, self-presentation online via the profile elements of SNSs is
often directed to a broad and sometimes imperceptible audience. Therefore, the
two-component model created by Leary and Kowalski (1990) has to be extended
for online self-presentation: in contrast to face-to-face contexts, the target values,
which are one factor in impression construction, can only be considered when (a)
the information given on the person includes a number of different attributes so that
it might be appealing to different audiences or (b) by including no specific infor-
mation at all in order to not contradict any values of any potential recipient.
138                                                          N.C. Kr‚Ç¨amer and N. Haferkamp


   Given the fact that people have a strong motive to self-present relevant aspects
of themselves in order to leave favorable impressions, a reasonable strategy seems
to be to provide a wealth of information (except maybe extreme attributes). This
strategy, however, conflicts with privacy strategies that are derived from privacy
concerns regarding informational, psychological, or social privacy. Thus, the model
created by Leary and Kowalski (1990) requires further amendments to be able to
account for online contexts: we argue that privacy concerns play a particularly
important role in online self-presentation and should be understood as a potential
inhibitor that may impact users‚Äô impression construction on SNSs and may influ-
ence the form of self-presentation that is eventually chosen (see Fig. 10.1).


             Goal relevance of impressions
             Value of desired goals
             Discrepancy between desired
             and current image



                      Impression
                      motivation



             Impression construction

             Self-concept
             Desired/undesired identity
             image
             Role constraints                      Privacy concerns
             Current or potential social           Informational privacy
             image
                                                   Psychological privacy
             Lack of perceptible target            Social privacy
             values for each interaction
             partner



             Self-presentation strategies             Privacy strategies
             Display a wealth of informa-          Invite only limited number of
             tion to appeal to various reci-       people
             pients                                Use Friend lists
                                                   Post no privacy-relevant
                                                   information


             Individual decision based on strength of self-presentation motive and
             privacy concern
             New strategies that compromise between privacy and self-presentation
             (e.g., indirect self-presentation via groups)

Fig. 10.1 Extended two-component model of self-presentation online (Based on Leary and
Kowalski 1990)
10   Online Self-Presentation: Balancing Privacy Concerns and Impression Construction       139


The individual decision to disclose more or less information is thus dependent on
the relative strength of the impression motivation on the one hand and privacy
concerns on the other. Interestingly, another possibility is to engage in self-presen-
tation that is not based on private facts and details but indirectly alludes to
important aspects of one‚Äôs life: here, the display of group memberships has become
increasingly popular, allowing an implicit presentation of interests and attributes.
This is in line with the creative potential of online users that Walther (1996)
described for earlier aspects of computer-mediated communication.
    However, our theoretical conceptualization of online self-presentation requires
empirical investigations that determine how the factors mentioned combine to
affect self-presentational choices. So far, it remains unclear how the factors have
to be weighted and whether the eventual decision depends on people¬¥s personality
and individual motives or whether there are specific situations in which one variable
is more salient than the other. Moreover, the question of whether our model can also
be applied to Social Web applications other than social networking sites is still
unanswered. As mentioned above, the current conceptualization is only valid for
applications in which a broad audience is addressed and it is not valid, for example,
when presenting oneself in online settings that allow for one-to-one settings as in
instant messaging chats (or message services within SNSs). We conceptualized our
ideas based on the features of SNSs such as Facebook and more specifically on the
profile elements. However, other applications, such as microblogging systems like
Twitter (see Schmidt, this issue), demand other data and are characterized by
different features and communication opportunities. In conclusion, we believe
that our model has provided a first exploratory basis for a theoretical account of
self-presentation online under consideration of users‚Äô privacy concerns. It should
provide a first framework for future research regarding online self-presentation
via SNSs.




References

Archer JL (1980) Self-disclosure. In: Wegner D, Vallacher R (eds) The self in social psychology.
    Oxford University, London, pp 183‚Äì204
Bargh JA, McKenna KYA, Fitzsimons GM (2002) Can you see the real me? Activation and
    expression of the ‚Äútrue self‚Äù on the internet. J Soc Issues 58(1):33‚Äì48
Barnes SB (2006) A privacy paradox: social networking in the United States. First Monday 11.
    www.firstmonday.org/issues/-issue11_9/barnes/index.html. Accessed 8 Aug 2008
Baumeister RF, Jones EE (1978) When self-presentation is constrained by the target‚Äôs prior
    knowledge: consistency and compensation. J Personal Soc Psychol 36:608‚Äì618
boyd dm, Ellison NB (2007) Social network sites: definition, history, and scholarship. J Comput
    Mediat Commun 13(1):210‚Äì230
Burgoon JK (1982) Privacy and communication. In: Burgoon M (ed) Communication yearbook 6.
    Sage, Beverly Hills, pp 206‚Äì249
Ellison N, Heino R, Gibbs J (2006) Managing impressions online: self-presentation processes in
    the online dating environment. J Comput Mediat Commun 11(2):415‚Äì441
Goffman E (1959) The presentation of self in everyday life. Double Day, Garden City
140                                                               N.C. Kr‚Ç¨amer and N. Haferkamp


Gollwitzer PM (1986) Striving for specific identities: the social reality of self-symbolizing. In:
   Baumeister R (ed) Public self and private self. Springer, New York, pp 143‚Äì159
Gosling SD, Gaddis S, Vazire S (2007) Personality impressions based on facebook profiles. Paper
   presented at the international conference on weblogs and social media 2007, Boulder. http://
   www.icwsm.-org/papers/3-Gosling-Gaddis-Vazire.pdf. Accessed 2 Mar 2008
Haferkamp N, Kr‚Ç¨amer NC (2009) When I was your age, Pluto was a planet‚Äù: Impression
   Management and Need to belong as motives for joining groups on social networking sites.
   Paper presented at the annual meeting of ICA 2009 (International Communication Associa-
   tion), Chicago
Haferkamp N, Kr‚Ç¨amer NC (2010) Creating a digital self: impression management and impression
   formation on social networking sites. In: Drotner K, Schr√∏der KC (eds) Digital content
   creation: creativity, competence, critique. Peter Lang, New York, pp 129‚Äì149
Joinson AN (2001) Self-disclosure in computer-mediated communication: the role of self-aware-
   ness and visual anonymity. Eur J Soc Psychol 31:177‚Äì192
Jones EE (1964) Ingratiation. Appleton, New York
Jones EE, Pittman TS (1982) Toward a general theory of strategic self-presentation. In: Suls J (ed)
   Psychological perspectives on the self. Erlbaum, Hillsdale, pp 231‚Äì262
Leary MR (1995) Self presentation: impression management and interpersonal behavior. Brown &
   Benchmark, Madison
Leary MR, Kowalski RM (1990) Impression management: a literature review and two-component
   model. Psychol Bull 107:34‚Äì47
Lenhart A, Madden M (2007) Teens, privacy & online social networks. Pew internet & American
   life project 2007. www.pewinternet.org/pdfs/PIP_Teens_Privacy_SNS_Report_Final.pdf.
   Accessed 8 Aug 2008
Lewis K, Kaufman J, Christakis N (2008) The taste for privacy: an analysis of college student
   privacy settings in an online social network. J Comput Mediat Commun 14:79‚Äì100
Liu H (2007) Social network profiles as taste performances. J Comput-Mediat Commun 13(1).
   http://jcmc.indi-ana.edu/vol13/issue1/liu.html. Accessed 10 May 2007
Liu H, Maes PM, Davenport G (2006) Unraveling the taste fabric of social networks. Int J
   Semantic Web Inf Syst 2(1):42‚Äì71
Livingstone S (2008) Taking risky opportunities in youthful content creation: teenagers‚Äô use of
   social networking sites for intimacy, privacy and self-expression. New Media Soc 10:339‚Äì411
Markus H, Nurius P (1986) Possible selves. Am Psychol 41:954‚Äì969
Mehdizadeh S (2010) Self-presentation 2.0: narcissism and self-esteem on Facebook.
   Cyberpsychol Behav Soc Netw 13(4):357‚Äì364
Reinecke L, Trepte S (2008) Privatsph‚Ç¨are 2.0: Konzepte von Privatheit, Intimsph‚Ç¨are und Werten
   im Umgang mit ‚Äúuser-generated-content‚Äù (Concepts of privacy, intimacy, and values with
   regard to ‚Äúuser-generated-content‚Äù). In: Zerfa√ü A, Welker M, Schmidt J (eds) Kommunikation,
   Partizipation und Wirkungen im Social Web. (Communication, participation, and effects in the
   social web) Band 1: Grundlagen und Methoden: Von der Gesellschaft zum Individuum. Halem
   Verlag, K‚Ç¨oln, pp 205‚Äì228
Reingold H (1993) The virtual community. Addison-Wesley, New York
Schlenker BR (1980) Impression management: the self-concept, social identity, and interpersonal
   relations. Brooks/Cole, Monterey
Schneider DJ (1981) Tactical self-presentations: toward a broader conception. In: Tedeschi JT (ed)
   Impression management theory and social psychological research. Academic, New York,
   pp 23‚Äì40
Tidwell LS, Walther JB (2002) Computer-mediated communication effects on disclosure,
   impressions, and interpersonal evaluations. Getting to know one another a bit at a time. Hum
   Commun Res 28(3):317‚Äì348
Toma CL, Hancock J (2009) Self-presentation in online dating profiles: the role of physical
   attractiveness. Paper presented at the annual meeting of the international communication
   association, Chicago
10   Online Self-Presentation: Balancing Privacy Concerns and Impression Construction   141

Utz S, Kr‚Ç¨amer NC (2009) The privacy paradox on social network sites revisited: the role of
   individual characteristics and group norms. J Psychosoc Res Cyberspace. http://www.
   cyberpsychology.eu/view-.php?cisloclanku¬º2009111001. Accessed 3 Mar 2010
Walther JB (1996) Computer-mediated communication: impersonal, interpersonal, and
   hyperpersonal interaction. Commun Res 23:3‚Äì43
Walther JB, Van der Heide B, Kim SY, Westerman D, Tong ST (2008) The role of friends‚Äô
   appearance and behavior and evaluations of individuals on Facebook: are we known by the
   company we keep? Hum Commun Res 34:28‚Äì49
Chapter 11
The Uses of Privacy Online: Trading a Loss
of Privacy for Social Web Gratifications?

Monika Taddicken and Cornelia Jers




11.1     Introduction

According to Etzioni (1999), the first step in analyzing privacy is to determine
whether or not there is a problem. Given the easy availability of private information
on the Internet and the seemingly great readiness of Social Web users to disclose
personal data, it would appear that the protection of privacy is not a major problem
for users. However, empirical evidence demonstrates that Social Web users are
in fact quite concerned about their privacy (Barnes 2006; Tufekci 2008; Debatin
et al. 2009).
   Nevertheless, the individual need for privacy seems to have only little influence
on online behavior. This discrepancy between privacy concerns and actual privacy
behavior is often referred to as the ‚Äúprivacy paradox‚Äù (Barnes 2006; Awad and
Krishnan 2006; Norberg et al. 2007). Apparently, extensive concern about the
safety of one‚Äôs private data does not necessarily correspond to privacy-related
behaviors such as reducing the accessibility of one‚Äôs Social Web profile, changing
the privacy settings if possible (Acquisti and Gross 2006; Tufekci 2008; boyd and
Hargittai 2010), or limiting self-disclosure (Debatin et al. 2009).
   The reasons for this are manifold. On the one hand, they include a lack of
problem awareness or media competence, such as ignorance of privacy settings and
uncertainty about the audience (Debatin et al. 2009; boyd and Hargittai 2010;
Acquisti and Gross 2006). On the other hand, it can be assumed that Social Web
use offers advantages and gratifications that increase in direct proportion to the
degree of self-disclosure (see also the chapter by Peter and Valkenburg in this



M. Taddicken (*)
University of Hamburg, Hamburg, Germany
e-mail: monika.taddicken@uni-hamburg.de
C. Jers
University of Hohenheim, Stuttgart, Germany

S. Trepte and L. Reinecke (eds.), Privacy Online,                                143
DOI 10.1007/978-3-642-21521-6_11, # Springer-Verlag Berlin Heidelberg 2011
144                                                           M. Taddicken and C. Jers


volume). Lampe et al. (2007) confirm that the quantity of disclosed information in
social networking sites (SNS) is linked to the degree of networking (see also the
chapter by Ellison et al. in this volume). Evidently, the disclosure of private
information is rewarded with social gratifications.
   A suitable approach to study the users‚Äô benefits of social media is the uses and
gratifications approach. It is widely used in communication science and addresses
processes of media use and effects from the users‚Äô perspective as well as the
question of why people use specific media products and with which gratifications
(Rubin 2009). With the help of this approach, it is possible to contrast the costs
and benefits of self-disclosure on the Social Web. This allows a detailed focus on
the gratifications of Social Web use and the relation between self-disclosure, need
for privacy, and these gratifications.



11.2    Social Web Use Versus Privacy: A Users‚Äô Dilemma?

The Social Web can be characterized as an endless online pool of easily available
private information. Apparently, Social Web users are highly willing to disclose
personal data on the Internet and to relinquish control over the amount of contact
with others (Pedersen 1997). From a uses and gratifications perspective, this chapter
tries to shed light on the aforementioned privacy paradox, and to address the
question of why people are concerned about the safety of their personal data but,
at the same time, disclose a high amount of these on the Social Web. To do so, it is
necessary to theoretically explicate the concepts of self-disclosure and privacy
before they can be adapted to the context of the Social Web.
    Self-disclosure is an integral component of every social interaction and can be
described as ‚Äúany message about the self that a person communicates to another‚Äù
(Wheeless and Grotz 1976, p. 338). Self-disclosure, therefore, is a part of the
communication process and has to be considered in relation to specific individuals,
namely the communication partners (Wheeless 1976). In general, self-disclosure
is the basic precondition for every social relationship since it is part of every
communication; the passing on of information about oneself, one‚Äôs thoughts, and
one‚Äôs feelings is necessary to create social proximity (Altman and Taylor 1973;
Laurenceau et al. 1998). This means that self-disclosure and the perception of
privacy are closely related. The regulation of privacy is not to be understood as
a process of retreat, nor is an optimum degree of privacy equal to the highest
possible control over one‚Äôs personal information. Rather, individuals strive
for different degrees of self-disclosure in different situations. Thus, both the
interactional perspective of privacy and its changeable nature must be highlighted
(Newell 1995, p. 100).
    Privacy can be defined in many different ways. Basically, it can be seen as
‚Äúthe right to be let alone‚Äù (Warren and Brandeis 1890). Despite this general
definition, a variety of dimensions and perspectives of privacy have been analyzed
by researchers of different scientific perspectives (for an overview see Newell
11   The Uses of Privacy Online: Trading a Loss of Privacy for Social Web Gratifications? 145


1995). Although various attempts have been made to create a synthesis of the
existing approaches to defining privacy (e.g., Parent 1983; Schoeman 1984;
Burgoon et al. 1989; Newell 1995), a unified single account has yet to emerge
(Paine et al. 2006). Among the most notable of these existing approaches are the
works by Westin (1967) and Altman (1975, 1977). Both researchers focus on
control and regulation of access to private information. According to Westin,
privacy is ‚Äúthe right to prevent the disclosure of personal information to others‚Äù
(1967, p. 7). The desire to keep personal information out of the hands of others is
central to this concept of privacy. Altman, on the other hand, defines privacy as a
‚Äúselective control of access to the self or to one‚Äôs group‚Äù (1975, p. 18). According to
Altman, the regulation of privacy is a dynamic process of optimization that is
influenced by two basic psychological needs: on the one hand, the need to preserve
one‚Äôs privacy and control access to and distribution of personal information, and on
the other hand, the need to interact socially and, therefore, to disclose personal
information. Accordingly, privacy is perceived as being at its optimum when both
needs can be united and the desired and actually achieved levels of privacy
correspond (Altman 1975).
    Following these arguments, privacy does not mean removing oneself from the
presence of others. Rather, different types of privacy have been identified. Different
perspectives can be used for a categorization of these types of privacy. For this
chapter, it is most important to focus on the individual and interactional perspec-
tive. This means that we should highlight the fact that people prefer privacy at some
times and not at others. Westin (1967) proposed different types of privacy ‚Äì
solitude, anonymity, reserve, and intimacy ‚Äì which we will briefly introduce here.
We will then connect them to the use of Social Web applications. A summary of the
results of our analysis concerning the interrelations of privacy types, the use of the
Social Web, and the resulting gratifications and privacy risks are presented in
Table 11.1, which may also serve as an advanced organizer for the rest of this
chapter. In the following section, we will review and transfer the privacy types
proposed by Westin (1967) and discuss their relevance for the Social Web.
    Solitude refers to the condition of being alone. This is what lay persons most
often define as privacy (for a presentation of literary and historical mentions of
privacy, see Hixson 1987). Westin (1967) states that solitude is the most complete
state of privacy that individuals can achieve. In this state, the individual is alone and
unobserved. That means that solitude refers to a situation where other people cannot
see or hear what the individual is doing (Pedersen 1997). Mostly, solitude is
regarded as a condition that is either desirable or neutral (Newell 1995).
    In the Social Web, solitude is rather uncommon since Social Web applications
usually focus on social interactions. However, solitude can be achieved by the
individual‚Äôs use of applications just for themselves. An example of this might be a
blog used as a personal diary for (self-) therapeutic reasons and which is inaccessi-
ble to an external party. However, in most cases, Internet providers have access to
the personal data. Hence, the state of solitude is perceived and not actually given.
    Anonymity is defined as a type of privacy that occurs when it is possible to move
around in public without being recognized or without being the subject of attention
Table 11.1 Interrelations between privacy types (Westin 1967), Social Web use, self-disclosure, and Social Web gratifications
Type of privacy Solitude                                 Anonymity                               Reserve                                 Intimacy
                                                                                                                                                                                 146

Definition      Condition of being alone and           Condition of moving around in public      Controlling of verbal disclosure of    Condition of being alone with family
                   unobserved, other people cannot        without being recognized or               personal information to others          or friends to the exclusion of
                   see or hear what the individual is     without being the subject of                                                      other people to increase
                   doing                                  attention                                                                         interaction
Typical         The use of social web applications     Consuming or participating in the        An active and conscious information Intimacy can be found in the social
    situations     purely for personal purposes.          Social Web without providing             and impression management is             web by using applications that
    and         Examples: Running a blog as a             one‚Äôs real name.                         typical for the social web, e.g.,        limit access to authorized users.
    examples in    personal diary; watching videos on Examples: Taking part in Internet            within a profile of an SNS.          Examples: Running a traveler blog
    the social     YouTube alone                          forums using a pseudonym or a         Reciprocal self-disclosure that may         for family and friends; using in-
    web                                                   nickname, commenting on videos           happen more or less consciously is       group communication in SNSs
                                                          or texts but concealing one‚Äôs real       typical as well, such as discussions     for interacting with others
                                                          name                                     in forums
Potential       Providers usually have access to the   Social Web providers might use           Problem of an unknown and               Problems occur when the level of
    privacy        personal data. Hence, the state of     technical mechanisms to identify         heterogeneous audience that can          intimacy is lower than expected.
    problems       solitude is only perceived             their users.                             easily differ from the desired or        Users expect to be ‚Äúintimate‚Äù
                   subjectively and not actually given The online communication may lead           expected audience.                       with specific other users and to
                                                          to a higher level of perceived        People with different relationships to      interact only with them, but in
                                                          anonymity and to an ‚Äúonline              the audience (e.g., friends,             fact other users are often able to
                                                          disinhibition effect‚Äù                    relatives, colleagues): Possibility      read this information as well
                                                                                                   of ‚Äúre-contextualization effects‚Äù        (e.g., intended audience forwards
                                                                                                   regarding self-disclosed                 information to unintended
                                                                                                   information                              audience)
Gratifications   Affective and cognitive gratifications Affective and cognitive gratifications Personal integrative needs can be        Intimacy in the Social Web can
                    through consuming and                  through consuming and                   satisfied by an active impression        facilitate social integrative
                    participating use forms without        participating without disclosing        management. Additionally,                gratifications.The Social Web
                    disclosing one‚Äôs identity              one‚Äôs identity                          cognitive gratifications can be          may provide even better
                 Examples: Informing people through Personal integrative gratifications            obtained through consciously             conditions for intimacy for some
                    writing wiki articles; seeking         through being able to try things out    presenting thoughts and                  people than the offline
                    guidance by reading blogs; having      in an anonymous environment             experiences to other users               environment, so that these social
                    fun by watching YouTube videos         with an unknown audience.            Examples: Identity building by              integrative needs can be served
                                                        Examples: Running an anonymous             presenting information about             even better
                                                           blog about personal experiences         oneself on an SNS profile; gaining Examples: Keeping in touch with
                                                           and feelings; using nicknames and       status through presenting videos         other people through SNSs;
                                                           avatars while using Social Web          on YouTube                               sharing one‚Äôs feelings with others
                                                           services                                                                         in forums
                                                                                                                                                                                 M. Taddicken and C. Jers
11   The Uses of Privacy Online: Trading a Loss of Privacy for Social Web Gratifications? 147


(Westin 1967). Anonymity can be sought by going unnoticed in a crowd of
strangers (Pedersen 1997). Gavison (1984) points out that an individual loses
privacy when becoming the object of attention and that this is true whether the
attention is conscious and purposeful or inadvertent. Attention is a primary way of
gathering information that leads to a loss of anonymity.
    In the Social Web, anonymity can be found easily. Lurking (i.e., reading others‚Äô
contents, such as profiles or discussions) is a good example of this and a wide-spread
behavior in the Social Web. In addition, in many Social Web applications, it
is possible to become a member of a community without providing one‚Äôs real
name (e.g., by adopting a fake user name). In general, prior research has reported
that Internet users perceive a high level of anonymity online (Joinson 2001; Tidwell
and Walther 2002). The reasons for this are the limited number of communication
channels and, consequently, a reduced perceived presence of other users (Taddicken
2008). The Reduced Social Context Cues Approach (Sproull and Kiesler 1986, 1991)
focuses on the limitation of information about communication partners and the actual
situation framing the communication. Sproull and Kiesler state: ‚ÄúWhen social
context cues are weak, people‚Äôs feelings of anonymity tend to produce relatively
self-centered and unregulated behavior‚Äù (1986, p. 1495). In other words, the com-
munication environment of the Internet has at least two different kinds of impact on
its users: Firstly, online communication may influence the state of self-awareness
meaning the inward focus on thoughts, feelings, and motives (Duval and Wicklund
1972). Matheson and Zanna (1989) found indeed that online communication
increases the state of private self-awareness. Secondly, online communication
influences self-disclosure behavior. Suler (2004) found that people ‚Äúloosen up, feel
less restrained, and express themselves more openly‚Äù (p. 321) when communicating
online and suggested framing this behavior as an ‚Äúonline disinhibition effect‚Äù.
    The possibility of adapting the idea of the Internet as a ‚Äúlimited communication
mode‚Äù (Taddicken 2008) to the Social Web context has yet to be analyzed. The
reduction of social context cues seems to be valid for Social Web applications that
provide no detailed user profiles, such as Internet forums, chats, and video
platforms. However, a lot of context information about SNS users (profiles,
pinboards, photos, etc.) is actually available, even though it is not always directly
integrated into the specific communication processes. Also, Social Web providers
can use technical mechanisms to identify their users if desired (e.g., IP addresses,
log file analyses).
    Reserve is a type of privacy that can be achieved in the interaction process.
It involves the establishment of psychological barriers against intrusion. Pastalan
(1975) defines reserve as the most subtle form of privacy because of its reciprocal
nature and the willing discretion of significant others. Pedersen (1997) refers to it as
the controlling of the verbal disclosure of personal information to others, especially
to strangers.
    In the Social Web, it is possible for users to manage information provided
consciously with regard to their particular target audience, for example, the details
of an SNS profile. There is a wide range of literature on impression management in
148                                                            M. Taddicken and C. Jers


SNSs (e.g., Tidwell and Walther 2002; Ellison et al. 2006; Kr‚Ç¨amer and Winter 2008;
see also Haferkamp and Kr‚Ç¨amer in this volume). In addition to this information,
personal details are disclosed during the communication process, such as discussions
in Internet forums or dialogs in chats. This reciprocal self-disclosure might occur
more or less consciously ‚Äì as is the case in face to face communication as well.
   The process of controlling verbal self-disclosure in the Social Web is affected by
the problem of the unknown and heterogeneous audience. Social Web users might
often be unaware of who actually reads the information revealed as the audience is
temporarily and spatially separated. The desired public of the self-disclosure might
differ from the expected audience, which in turn does not have to be identical to the
audience reached. But even within the intended public, there are typically people to
whom the self-disclosers have different social relations, such as friends, relatives,
acquaintances, and colleagues. Regarding the amount, the tone, and the style of
the self-disclosed information, this might cause serious consequences for users:
information that is suitable to be revealed to close friends, such as party pictures,
might be unsuitable for other people, such as parents or (potential) employers. Self-
disclosed information can also be redistributed and transferred to other contexts by
third parties, for example, by providers that use private information for advertising
purposes or even by friends who forward photographs. Consequently, these
‚Äúre-contextualization effects‚Äù may pose a serious threat to the privacy facet of
reserve in the Social Web.
   Intimacy is another type of privacy that is related to the presence of others. The
need for intimacy is described as the need to be alone with family or friends to
the exclusion of other people. The intent is to reduce interaction with outsiders
while increasing in-group interaction (Pedersen 1997). Westin (1967) defines
intimacy as related to an individual‚Äôs or group‚Äôs desire to promote close personal
relationships. Fried (1984) notes that intimate relationships require the voluntary
relinquishment of parts of one‚Äôs inner self. Pastalan (1975) describes intimacy as a
basic need for human contact. According to Gerstein (1984), intimacy includes a
certain lack of self-observation. Therefore, Gerstein states that the highest level of
invasion is achieved when individuals are observed in an intimate relationship in
which they did not intend to be observed at all, even by themselves (Gerstein 1984).
   In the Social Web, users may achieve intimacy by limiting access to the
information they post online. An example would be a personal blog a traveler
runs for his or her family and friends. Furthermore, the Social Web provides the
opportunity to have intimate interactions with other users known only online.
Separate chat rooms or in-group communication on SNSs are examples of this.
However, a problem occurs when the level of intimacy is lower than expected, for
example, when people reveal personal information because they are unaware that
others have not been excluded. As Gerstein (1984) suggests that intimacy includes a
certain lack of self-observation and that the observation of self-disclosure in
intimate situations has to be evaluated as the highest level of invasion, this
highlights one of the main risks of privacy in the Social Web: users expect to be
‚Äúalone‚Äù with specific other users and to interact only with them, but other users and
the providers are often able to read this information as well (e.g., on pinboards).
11   The Uses of Privacy Online: Trading a Loss of Privacy for Social Web Gratifications? 149


   As demonstrated above, all four privacy types identified by Westin (1967) show
significant relations to the use of the Social Web. However, a comprehensive
understanding of the privacy-related consequence of Social Web use can only be
gained by a careful analysis of the resulting risks and benefits. The wide-ranging
discussions on privacy are often dominated by the general idea that privacy is
beneficial and that people have a legally protected right to privacy (Newell 1994).
At the same time, however, self-disclosure is a behavior that decreases privacy but
also promises benefits and gratifications. This seems to be particularly true for
Social Web use, where people benefit from presenting information about them-
selves by gaining social interactions with other users. The uses and gratifications
approach (Katz et al. 1974) seems to be a helpful framework for drawing a detailed
picture of the benefits of the Social Web. In the following paragraph, we will
analyze the gratifications of Social Web use and their relation to self-disclosure
and privacy on the basis of this approach.



11.3     Uses and Gratifications in the Social Web

The uses and gratifications approach emphasizes the role of the audience in the
process of media use by asking the question ‚ÄúWhat do people do with the media?‚Äù
(Katz and Foulkes 1962, p. 378). Consequently, recipients are seen as variably
active participants in the process of media use and its effects. Aside from the
sociodemographic variables of the audience, psychological and social elements
play an important role for understanding this process (Rubin 2009). More specifi-
cally, the focus is on motives and needs of the audience, or gratifications as fulfilled
needs to explain media choice and behavior. Research grounded in the uses and
gratifications approach puts strong emphasis on the role of the individual recipient
and the differences between users and their decisions. Thus, the uses and
gratifications approach deals with social and psychological origins of needs that
generate expectations of the mass media or other sources. These expectations, in
turn, lead to differential patterns of media exposure, resulting in need gratifications
or other ‚Äì mostly unintended ‚Äì consequences (Katz et al. 1974).
    The framework of uses and gratifications rests on five assumptions (Rubin 2009;
Palmgreen et al. 1985). Firstly, it is assumed that when people use media, it is with
a clear goal, purpose, and motivation. Secondly, people initiate the selection and use
of media according to their needs and desires and are variably active
communicators in this process. Thirdly, social and psychological factors influence,
filter, and mediate communication behavior. Users‚Äô predispositions, social environ-
ment, and interpersonal interactions shape their expectations about the media.
Fourthly, media compete with other (non-medial) sources of need satisfaction
that are considered functional alternatives. Fifthly, people are usually more influ-
ential in the process of media use than the media itself. Although the uses and
gratifications approach quickly became popular and has been applied broadly
within communication research, some scholars have criticized the approach and
150                                                            M. Taddicken and C. Jers


its assumptions. In particular, the lack of clarity of central constructs such as
motives and gratifications, the treatment of the audience as being too active, and
rational and methodological problems have been criticized (Elliott 1974; Swanson
1977; Lin 1996). Most criticism has been addressed in a variety of studies in the
subsequent years. In spite of these advances, parts of the criticism remain.
    The uses and gratifications approach was first established in the context of
traditional mass media, with studies dealing predominantly with the use of radio
and television (e.g., Herzog 1944; Rubin 1979). Nonetheless, the uses and
gratifications approach can be seen as especially suitable for studying new media
environments because of its general idea of an active audience and its user-centric
perspective (Ruggerio 2000; Newhagen and Rafaeli 1996; Rubin 2002).
    Several recent studies have dealt with gratifications of Social Web use. Across
these studies, four dimensions of needs that can be satisfied by the use of the Social
Web became apparent: cognitive needs, affective needs, social integrative needs,
and personal integrative needs (Leung 2009). It has to be highlighted that these four
dimensions are not fully discriminatory but may overlap. The first dimension,
cognitive needs or information needs, is comparable to those needs or gratifications
related to the use of traditional mass media. Researchers found that people use blogs
to seek guidance (Lee 2006), read wikis in order to learn new things (Rafaeli and
Ariel 2008), or watch videos in the Social Web for information seeking (Haridakis
and Hanson 2009). In addition to passive consumption, participating or producing
forms of use can also fulfill cognitive needs such as influencing public opinion or
informing other people through political blogging (Ekdale et al. 2010; Shao 2008).
Furthermore, exercising one‚Äôs knowledge, increasing skills and abilities, and intel-
lectual challenge were identified as chief gratifications of the use of wikis (Nov
2007; Rafaeli and Ariel 2008).
    The second dimension can be referred to as affective needs or entertainment
needs. Users experience fun, pleasure, and entertainment when watching or reading
the Social Web. Also, sharing videos or other content ‚Äì i.e., participating ‚Äì promises
the satisfaction of entertainment needs for Social Web users through co-viewing.
Furthermore, the production of content in wikis or other applications is used as a
means of having fun and passing time (Nov 2007; Rafaeli and Ariel 2008; Haridakis
and Hanson 2009).
    Social integrative needs are the third important dimension in Social Web
gratifications. By using the Social Web in a passive way, social gratifications can
be gained through watching friends‚Äô entries on SNSs or reading blogs of
acquaintances (Ancu and Cozma 2009). Social integrative needs are even more
important for participating and producing in the Social Web. This implies sharing
one‚Äôs views, thoughts, and feelings with others, connecting with like-minded
people, or communicating with friends and family (Leung 2009; Liu et al. 2007;
boyd and Ellison 2008). These needs are particularly well fulfilled by the use of
SNSs, but users who write blogs, contribute to wikis, or share videos with others on
YouTube also emphasize the importance of social integrative needs. More specifi-
cally, the formation of communities, social engagement, and support are named as
11   The Uses of Privacy Online: Trading a Loss of Privacy for Social Web Gratifications? 151


gratifications by these users (Nardi et al. 2004; Rafaeli and Ariel 2008; Haridakis
and Hanson 2009).
   The last dimension encompasses personal integrative needs or recognition
needs. In this dimension, aspects such as identity formation and impression man-
agement are included ‚Äì gratifications that are typically linked with Social Web use
(boyd 2007; Leung 2009). People try to gain respect and support through active
Social Web use, aim to build up their confidence, and like to publicize their
experiences (Leung 2009). Producing content in the Social Web is a way of
expressing one‚Äôs feelings and emotions and articulating ideas through writing.
In the Social Web, it is possible to experiment in a way that is often more difficult
in the offline environment (Nardi et al. 2004; Liu et al. 2007). Additionally, users
document their lives in the Social Web and even try to promote their careers and
increase their social status (Nov 2007). All of these personal integrative needs are
closely related to participating or producing forms of Social Web use. Consuming
forms of use, in contrast to this, do not play an important role here.




11.4     Social Web Gratifications and Their Relation to Privacy
         and Self-Disclosure

As demonstrated above, four relevant need dimensions may be fulfilled by the use
of various Social Web applications at different levels of activity. The Social Web
partly serves the same functions for the users that traditional media do, such as
information and entertainment. On the other hand, the Social Web competes with
interpersonal communication with regard to social and personal integrative needs.
Regarding privacy issues, it is especially interesting to analyze the extent to which
these gratifications are related to self-disclosure.
    Consuming forms of Social Web use that do not require any form of self-
disclosure are possible with regard to cognitive needs. On the one hand,
gratifications in this field partly resemble gratifications of using traditional media
such as seeking guidance or information. Here, users can take a very passive role
that does not reduce any form of privacy. On the other hand, obtaining gratifications
such as influencing public opinion or informing people demands some form of self-
disclosure. This does not necessarily include the disclosure of one‚Äôs real name, but,
relating to reserve, a certain amount of sharing personal information with others
is required. Often, users do not disclose their real names when satisfying informa-
tion needs through blogging or writing wiki articles, but they do disclose their
thoughts and opinions even to strangers to inform these other people and contribute
to public discourse.
    Only some restrictions to privacy can be expected when looking at affective
needs of the Social Web. Again, consuming forms of use such as watching videos or
reading articles do not necessarily involve any form of self-disclosure. The feeling
of anonymity in the Social Web can even improve the opportunity to have fun by
152                                                            M. Taddicken and C. Jers


consuming content. On the other hand, pleasure or fun can be gratifications that
could also be achieved by taking part in the Social Web actively through writing
articles or by communicating with other users. Thus, the use of SNSs for entertain-
ment reasons in particular often includes disclosing various amounts of personal
information.
    Self-disclosure plays an even more important role in fulfilling social integrative
needs. Exchanging opinions with like-minded people in Internet forums, via blogs,
or sharing knowledge with others on wikis affects the privacy type reserve. It is
often possible to protect and maintain anonymity despite satisfying these kinds of
social integrative needs. On the other hand, gratifications such as communicating
with family and friends or keeping in touch with acquaintances inevitably include
the self-disclosure of personal information. It is common to use real names on SNSs
and many bloggers disclose their identity online. Also, personal feelings, thoughts,
and experiences are shared with other users to satisfy social integrative needs.
Beside the aspect of reserve, the privacy type intimacy with friends and family is
important in this context: although the Social Web allows situations where intimacy
with friends or family members can be established through limited access, member
registration, etc., the feeling of being alone with other people in the Social Web is
often illusive. When somebody writes personal messages to friends on pinboards of
SNSs, the friends of both communication partners ‚Äì often 200 or 300 people ‚Äì are
usually able to follow this interaction. Hence, intimacy with family and friends is in
fact decreased with the relocation of a part of the conversation into the Social Web.
To summarize, satisfying social integrative needs normally includes a much higher
degree of self-disclosure than the aforementioned two gratifications: it is not
possible to receive social gratifications without disclosing any form of personal
information such as thoughts, feelings, and experiences.
    Personal integrative needs are predominantly related to the user‚Äôs self and
identity. Reserve plays an important role in the context of personal integrative
needs because it is especially important in controlling the amount and type of
information disclosed in the context of forming one‚Äôs identity. Additionally, ano-
nymity is often a necessary precondition for experiencing and testing one‚Äôs identity
and personality. The impression of being anonymous allows users to feel free to do
and try things that finally lead to identity formation, e.g., by blogging about
personal feelings and experiences using a pseudonym. In contrast, status and
confidence aspects of personal integrative needs require more self-disclosure and
sometimes even the disclosure of one‚Äôs real name if a transfer into the offline world
is desired.
    We can therefore conclude that Social Web gratifications differ in their relation
to privacy issues. Different forms of use correspond to different levels of self-
disclosure: while consuming Social Web applications without active participation
does usually not demand a disclosure of information, active participation does at
least require the disclosure of some experiences and thoughts, sometimes also name
and e-mail address. The highest impact on privacy can be expected for producing
forms of use.
11   The Uses of Privacy Online: Trading a Loss of Privacy for Social Web Gratifications? 153


11.5     Discussion and Outlook

To answer the question of whether or not Social Web gratifications are always
accompanied by a loss of privacy and an obligation to self-disclose, we looked at
different forms of Social Web activities and different types of privacy. The relations
between those phenomena are summarized in Table 11.1. Typical use situations in
the Social Web are named for each privacy type affected, together with potential
privacy problems that might occur. Additionally, gratifications that are obtained
under the specific condition of this privacy type are listed, as well as those that are
obtained for relinquishing this privacy form. The table enables a detailed look at the
opportunities and risks of the Social Web for its users‚Äô privacy. It becomes obvious
that there are various situations that enable the linkage or ‚Äúco-existence‚Äù of privacy
and Social Web gratifications.
    Although a general danger for users‚Äô privacy cannot be affirmed after this
analysis, some important problems concerning privacy issues and user behavior
remain. One crucial question is how aware users are about their self-disclosure in
the Social Web. We can assume that many acts of self-disclosure are not the result
of an elaborate consideration of its advantages and disadvantages. Rather, users
disclose information spontaneously or even unconsciously during communication
processes with others. Additionally, users are often not aware of the possible long-
term consequences of these acts of self-disclosure. Further studies should examine
users‚Äô awareness of these problems and their consciousness of their own behavior in
the Social Web.
    Furthermore, the concepts of privacy, as well as publicity and audience, must be
discussed and potentially modified in the light of the diffusion of the Social Web.
Does people‚Äôs understanding of privacy change due to their changing communica-
tion behavior? The perception of intimacy may alter with an increasing amount of
communicating with closely related people online. For example, the definition of
friends seems to change with the growing use of social networking sites: in SNSs,
another user is either a friend or not ‚Äì varying degrees of friendship, such as from
best friend to distant acquaintance, simply do not exist.
    Taken together, all of these aspects show that further research is needed, as well
as a debate on privacy uses and norms.




References

Acquisti A, Gross R (2006) Awareness, information sharing, and privacy on the Facebook.
   Presentation on the 6th Workshop Privacy Enhancing Technologies, Cambridge, 28‚Äì30 June
   2006
Altman I (1975) The environment and social behavior. Privacy, personal space, territory,
   crowding. Brooks/Cole, Monterey
Altman I (1977) Privacy regulation: culturally universal or culturally specific? J Soc Issues
   33:67‚Äì83
154                                                                   M. Taddicken and C. Jers


Altman I, Taylor DA (1973) Social penetration: the development of interpersonal relationships.
    Holt, Rinehart and Winston, New York
Ancu M, Cozma R (2009) MySpace politics: uses and gratifications of befriending candidates.
    J Broadcast Electron 53:567‚Äì583
Awad NF, Krishnan MS (2006) The personalization privacy paradox: an empirical evaluation of
    information transparency and the willingness to be profiled online for personalization. MIS
    Quart 30(1):13‚Äì28
Barnes S (2006). A privacy paradox: Social networking in the United States. First Monday, 11(9).
    http://firstmonday.org/issues/issue11_9/barnes/-index.html
boyd dm (2007) Why youth (heart) social network sites: the role of networked publics in teenage
    social life. In: Buckingham D (ed) Macarthur foundation series on digital learning ‚Äì youth,
    identity, and digital. MIT Press, Cambridge, pp 119‚Äì142
boyd dm, Ellison NB (2008) Social network sites: definition, history, and scholarship. J Comput
    Mediat Commun 13:210‚Äì230
boyd dm, Hargittai E (2010) Facebook privacy settings: who cares? First Monday 15. http://
    firstmonday.org/htbin/cgiwrap/bin/ojs/index.php/fm/article/-viewArticle/3086/2589
Burgoon JK, Parrott R, LePoire BA, Kelley DL, Walther JB, Perry D (1989) Maintaining and
    restoring privacy through communication in different types of relationship. J Soc Pers Relat
    6:131‚Äì158
Debatin B, Lovejoy JP, Horn A-K, Hughes BN (2009) Facebook and online privacy: attitudes,
    behaviors, and unintended consequences. J Comput Mediat Commun 15:83‚Äì108
Duval S, Wicklund RA (1972) A theory of objective self-awareness. Academic, New York
Ekdale B, Namkoong K, Fung TKF, Perlmutter DD (2010) Why blog? (then and now): exploring
    the motivations for blogging by popular American political bloggers. New Med Soc
    12:217‚Äì234
Elliott P (1974) Uses and gratifications research: a critique and a sociological alternative.
    In: Rosengren KE, Wenner LA, Palmgreen P (eds) Media gratifications research. Current
    perspectives. Sage, Beverly Hills, pp 249‚Äì268
Ellison N, Heino R, Gibbs J (2006) Managing impressions online: self presentation processes in
    the online dating environment. J Comput Mediat Commun 11(2):415‚Äì441 (Article 2)
Etzioni A (1999) The limits of privacy. Basic Books, New York
Fried C (1984) Privacy: a moral analysis. In: Schoeman FD (ed) Philosophical dimensions of
    privacy: an anthology. Cambridge University Press, Cambridge, pp 371‚Äì377 (Reprinted from
    Yale Law Journal, 1968)
Gavison R (1984) Privacy and the limits of law. In: Schoeman FD (ed) Philosophical dimensions
    of privacy: an anthology. Cambridge University Press, Cambridge, pp 421‚Äì471 (Reprinted
    from Yale Law Journal, 1980, 89)
Gerstein RS (1984) Intimacy and privacy. In: Schoeman FD (ed) Philosophical dimensions of
    privacy: an anthology. Cambridge University Press, Cambridge, pp 265‚Äì271
Haridakis P, Hanson P (2009) Social interaction and co-viewing with youtube: blending mass
    communication reception and social connection. J Broadcast Electron 53:317‚Äì335
Herzog H (1944) What do we really know about daytime serial listeners? In: Lazarsfeld PF,
    Stanton FN (eds) Radio research 1942‚Äì1943. Duell, Sloan & Pearce, New York, pp 3‚Äì33
Hixson WL (1987) Privacy in public society: human rights in conflict. Oxford University Press,
    New York
Joinson AN (2001) Self-disclosure in computer-mediated communication: the role of self-awareness
    and visual anonymity. Eur J Soc Psychol 31:177‚Äì192
Katz E, Foulkes D (1962) On the use of the mass media as ‚ÄúEscape‚Äù: clarification of a concept.
    Public Opin Q 26:377‚Äì388
Katz E, Blumler JG, Gurevitch M (1974) Utilization of mass communication by the individual. In:
    Blumler JG, Kath E (eds) The uses of mass communication, Current perspectives on
    gratifications research. Sage, Beverly Hills/London, pp 19‚Äì32
11   The Uses of Privacy Online: Trading a Loss of Privacy for Social Web Gratifications? 155

Kr‚Ç¨amer NC, Winter S (2008) Impression management 2.0: the relationship of self-esteem,
    extraversion, self-efficacy, and self-presentation within social networking sites. J Med Psychol
    20(3):106‚Äì116
Lampe C, Ellison NB, Steinfield C (2007) A familiar face(book): profile elements as signals in an
    online social network. In: Proceedings of the SIGCHI conference on human factors in
    computing systems, New York, pp 435‚Äì444
Laurenceau J-P, Feldman Barrett L, Pietromonaco PR (1998) Intimacy as an interpersonal process:
    the importance of self-disclosure, partner disclosure, and perceived partner responsiveness in
    interpersonal exchanges. J Pers Soc Psychol 74:1238‚Äì1251
Lee JK (2006) Who are blog users? Profiling blog users by media use and political involvement.
    Paper presented at ICA conference in Dresden, Germany. http://www.allacademic.com//meta/
    p_mla_apa_research_citation-/0/9/2/9/6/pages92964/p92964-1.php
Leung L (2009) User-generated content on the internet: an examination of gratifications, civic
    engagement and psychological empowerment. New Med Soc 11:1327‚Äì1347
Lin CA (1996) Looking back: the contribution of Blumler and Katz‚Äôs uses of mass communication
    to communication research. J Broadcast Electronic 40:574‚Äì581
Liu S-H, Liao H-L, Zeng Y-T (2007) Why people blog: an expectancy theory analysis. Issues
    Inform Syst 8:232‚Äì237
Matheson K, Zanna MP (1989) Persuasion as a function of self-awareness in computer-mediated
    communication. Soc Behav 4:99‚Äì111
Nardi B, Schiano D, Gumbrecht M (2004) Blogging as social activity or would you let 900 million
    people read your diary? In: Proceedings of the 2004 ACM conference on computer supported
    cooperative work, Chicago, pp 222‚Äì231. http://portal.acm.org/citation.cfm?id¬º1031643.
    Accessed 8 Dec 2010
Newell PB (1994) A systems model of privacy. J Environ Psychol 14:65‚Äì78
Newell PB (1995) Perspectives on privacy. J Environ Psychol 15:87‚Äì104
Newhagen JE, Rafaeli S (1996) Why communication researchers should study the internet:
    a dialogue. J Commun 46:4‚Äì13
Norberg P, Horne DR, Horne DR (2007) The privacy paradox: personal information disclosure
    intentions versus behaviors. J Consum Aff 41(1):100‚Äì126
Nov O (2007) What motivates Wikipedians, or how to increase user-generated content contribu-
    tion. Commun ACM 50:60‚Äì64
Paine C, Reips U-D, Stieger S, Joinson A, Buchanan T (2006) Internet users‚Äô perceptions of
    ‚Äòprivacy concerns‚Äô and ‚Äòprivacy actions‚Äô. Int J Hum Comput St 65:526‚Äì536
Palmgreen P, Wenner LA, Rosengren KE (1985) Uses and gratifications research: the past ten
    years. In: Rosengren KE, Wenner LA, Palmgreen P (eds) Media gratifications research,
    Current perspectives. Sage, Beverly Hills, pp 11‚Äì37
Parent W (1983) Privacy, morality and the law. Philos Pub Aff 12:269‚Äì288
Pastalan LA (1975) Privacy preferences among relocated institutionalised elderly. In: Carson DH
    (ed) Man-environment interactions, vol 2. Hutchinson & Ross, Stroudsburg, pp 73‚Äì82
Pedersen DM (1997) Psychological functions of privacy. J Environ Psychol 17:147‚Äì156
Rafaeli S, Ariel Y (2008) Online motivational factors: incentives for participation and contribution
    in wikipedia. In: Barak A (ed) Psychological aspects of cyberspace: theory, research,
    applications. Cambridge University Press, Cambridge, pp 243‚Äì267
Rubin AM (1979) Television use by children and adolescents. Hum Commun Res 5:109‚Äì120
Rubin AM (2002) The uses-and-gratifications perspective of media effects. In: Bryant J, Zillmann
    D (eds) Media effects. Advances in theory and research. Erlbaum, Mahwah/London,
    pp 525‚Äì548
Rubin AM (2009) Uses-and-gratifications perspective on media effects. In: Bryant J, Oliver MB
    (eds) Media effects. Advances in theory and research, 3rd edn. Routledge, New York,
    pp 165‚Äì184
Ruggerio TE (2000) Uses and gratifications theory in the 21st century. Mass Commun Soc 3:3‚Äì37
156                                                                    M. Taddicken and C. Jers


Shao G (2008) Understanding the appeal of user-generated media: a uses and gratifications
   perspective. Internet Res 19:7‚Äì25
Schoeman F (1984) Privacy and intimate information. In: Schoeman F (ed) Philosophical
   dimensions of privacy. Cambridge University Press, Cambridge, pp 403‚Äì417
Sproull L, Kiesler S (1986) Reducing social context cues: electronic mail in organizational
   communication. Manage Sci 32:1492‚Äì1512
Sproull L, Kiesler S (1991) Two-level perspective on electronic mail in organizations. J Org Comp
   Elect Com 1:125‚Äì134
Suler JL (2004) The online disinhibition effect. Cyberpsychol Behav 7(3):321‚Äì326
Swanson DL (1977) The uses and misuses of uses and gratifications. Hum Commun Res
   3:214‚Äì221
Taddicken M (2008) Methodeneffekte bei web-befragungen: einschr‚Ç¨ankungen der dateng‚Ç¨           ute
   durch ein ‚Äòreduziertes kommunikationsmedium‚Äô? [Mode effects of Web surveys: Limitation
   of data quality because of a ‚Äöreduced communication mode‚Äô?]. Halem, K‚Ç¨     oln
Tidwell LC, Walther JB (2002) Computer-mediated communication effects on disclosure,
   impressions, and interpersonal evaluations: getting to know one another a bit at a time. Hum
   Commun Res 28:317‚Äì348
Tufekci Z (2008) Can you see me now? audience and disclosure regulation in online social
   network sites. B Sci Technol Soc 28:20‚Äì36
Warren SD, Brandeis LD (1890) The right to privacy. Harv Law Rev 4:193‚Äì220
Westin A (1967) Privacy and freedom. Atheneum, New York
Wheeless LR (1976) Self-disclosure and interpersonal solidarity: measurement, validation, and
   relationships. Hum Commun Res 3:47‚Äì61
Wheeless LR, Grotz J (1976) Conceptualization and measurement of reported self-disclosure.
   Hum Commun Res 2:338‚Äì346
     Part II
Applications
Chapter 12
(Micro)blogs: Practices of Privacy Management

Jan-Hinrik Schmidt




12.1    Introduction

Weblogs (or blogs for short) are a prototypical application of the Social Web. They
lower the barriers for participating in online conversations and the dissemination of
information, blurring the basic dichotomy that is at the heart of traditional mass
communication: the separation of roles between sender and receiver, or between
producers and users of information (Bruns 2008). Intertwined with this develop-
ment, blogs (and their younger sibling the microblogs) are also one of those online
formats that challenge the classic dichotomy of the private and the public, because
they make it feasible to share information of personal relevance with an audience
over time and space.
   A particularly telling case of the possible tensions between privacy and public-
ness has been reported by Johnson (2005): a nanny in New York one day told the
people she worked for about her private blog. The parents followed the blog for a
while and then decided to fire her. The mother, herself a journalist at the ‚ÄúNew York
Times,‚Äù explained in a newspaper article (Olen 2005) her outrage that her and her
baby‚Äôs life had been made public on the Internet. Although she did not mention any
names, the details provided in her article made it possible to track down the nanny‚Äôs
blog. The nanny, in turn, reacted to her case being made public in the New York
Times through blog postings of her own that specifically criticized the sensation-
seeking style of the article: ‚ÄúIf you have come to this little blog today looking for
prurient details of a ‚Äònanny gone wild‚Äô and another ‚Äònanny diary‚Äô detailing the
sordid life of a family she works for, I am very sorry to disappoint you‚Äù (N.N 2005).
She also announced that she was closing her blog and would blog anonymously to
protect her own privacy.



J.-H. Schmidt (*)
Hans-Bredow-Institute for Media Research, Hamburg, Germany
e-mail: j.schmidt@hans-bredow-institut.de

S. Trepte and L. Reinecke (eds.), Privacy Online,                                159
DOI 10.1007/978-3-642-21521-6_12, # Springer-Verlag Berlin Heidelberg 2011
160                                                                       J.-H. Schmidt


   Of course, this story is not representative of bloggers‚Äô experiences and neither
are these events inherent to or inevitably caused by blogs as such. The individual
and social consequences of the appropriation and institutionalization of the format
might differ quite substantially. This is especially true for privacy, which should not
be conceptualized as a fixed state, but rather as a constant and historically variant
process of navigating and managing the boundaries between the private and the
public. This includes, as classic theories of privacy have pointed out, maintaining
and exercising control over the extent of personal information that is communicated
(Westin 1970) or over the access to the self by others (Altman 1975). With regard
to (micro)blogs, then, managing privacy refers to the ways people actively use the
technology to selectively disclose certain personal information to certain audiences
(and also to not disclose certain information to others).
   This paper proceeds in three steps: Firstly, it describes the formal characteristics
of (micro)blogs and presents empirical findings on their prevalence among onliners
as well as on different uses of the technology. It then analyzes (micro)blog-based
practices of privacy management by reconstructing their technological evolution as
well as some of the shared routines and expectations about self-disclosure and
privacy with regard to particular audiences. A summary and outlook to future
research conclude the text.




12.2    Formal Characteristics and Prevalence of (Micro)blogs

In a formal way, blogs can be defined as frequently updated websites that display
content in a reverse chronological order. Single blog entries (‚Äúpostings‚Äù) have
unique URLs and can be linked to individually, rather than to the site as a whole.
They can also usually be commented on by other users. Microblogs usually impose
a limit on the number of characters in a single posting; Twitter, the most prominent
if not generic example of a microblogging service, allows for 140 characters within
one ‚Äútweet.‚Äù Microblogs also rely on articulated social connections for the structur-
ing of conversations and audiences, because users explicitly establish connections
amongst themselves by ‚Äúfollowing‚Äù or ‚Äúbeing followed by‚Äù other users, and by
explicitly referring to other users by replying to or retweeting (i.e., ‚Äúforwarding‚Äù)
their postings.
    Taken together, individual postings or tweets, comments, and articulated
connections through hyperlinks, replies, or retweets between (micro)blogs form
networks of interconnected texts, usually referred to as the ‚Äúblogosphere‚Äù and the
‚Äútwittersphere.‚Äù Not only are these spheres connected (since tweets might refer to
blog postings and vice versa) they are also greatly heterogeneous: which informa-
tion, topics, or events are selected by the (micro)blogger and which are not, how
this content is presented in terms of writing style, illustrations, etc., and how these
‚Äúdistributed conversations‚Äù (Efimova 2009) within and between blogs are
12   (Micro)blogs: Practices of Privacy Management                               161


structured, varies greatly. Thus, there is no such thing as ‚Äúthe‚Äù blog; rather, blogs
and microblogs are prime examples of the contingent and under-determined nature
of new media formats (Lievrouw 2002) that allow for or afford various practices,
including the ways in which privacy management and self-disclosure are
performed.
   According to blog monitoring services, the blogosphere has grown from four
million blogs in 2004 to approximately 150 million blogs at the beginning of 2011
(Sifry 2004; http://www.blogpulse.com). Twitter, the dominant microblogging
service, was estimated to reach 200 million users at the end of 2010 (Murphy
2010). The share of (micro)blog users among the general online population varies
between countries and age groups. In the US, around 11% of adult Internet users
and 28% of the 12‚Äì17-year olds had created a blog in 2009 (Jones and Fox 2009).
Within Europe, 11% of the 9‚Äì16-year old onliners had written a blog or online diary
within the last month (Livingstone et al. 2011, p. 34). 19% of US Internet users were
using Twitter (or similar microblogging services) in October 2009 (Fox et al. 2009),
while in Germany, it is used by only 1% on a weekly basis (Busemann and
Gscheidle 2010, p. 362).
   Parallel to this diffusion of (micro)blogs among Internet users, a growing body
of research has focused on specific practices and contexts, most notably the
relationship and interdependencies with professional journalism (e.g., Lasica
2002; Tremayne 2007; Messner and DiStaso 2008). Other strands of research
have examined the role of (micro)blogs within other fields of professional commu-
nication, specifically political communication (e.g., Keren 2006; Scott 2007; Park
and Thelwall 2008) as well as market communication and organizational commu-
nication, including knowledge management (e.g., B‚Ç¨ohringer and Richter 2009;
Efimova 2009; Puschmann 2010). Somewhat in contrast to this strong research
focus on blogging within professional contexts, various studies, by employing
different methodologies, agree that the majority of blogs deal with personal issues
rather than political, economical, or professional topics as such (e.g., Nardi et al.
2004; Papacharissi 2007; White and Winn 2009).
   For example, in a representative survey among US bloggers (n ¬º 233), Lenhart
and Fox (2006) found that 37% of bloggers consider ‚Äúmy life and personal experi-
ence‚Äù as their main topic, with the next most popular topic, ‚Äúpolitics and govern-
ment,‚Äù reaching only 11%. Accordingly, most bloggers (78%) are inspired to blog
by personal experiences, with female and younger bloggers of age 18‚Äì29 being
even more likely to do so. In a content analysis of n ¬º 457 blogs within a 13 month
period between 2003 and 2004, Herring et al. (2007) found that between 65% and
75% belonged to the ‚ÄúPersonal Journal‚Äù type. A content analysis of n ¬º 207
English tweets found that 41% of all messages were reporting the user‚Äôs personal
experiences (Honeycutt and Herring 2009). And a cluster analysis based on mes-
sage content of n ¬º 350 randomly selected Twitter users revealed that 80% could
be categorized as ‚Äúmeformers,‚Äù since their tweets predominantly focus on their
personal situation, opinions and complaints, or statements and random thoughts
(Naaman et al. 2010).
162                                                                       J.-H. Schmidt


   While the composition of a blog‚Äôs audience, the types of personal information
shared, and the particular communicative strategies for disclosing personal infor-
mation may differ (see below for a more thorough discussion), blogging is never-
theless a fundamentally social activity. It is a hybrid between the modes of
‚Äúpublishing‚Äù and ‚Äúengaging in conversation‚Äù ‚Äì especially in the case of the seem-
ingly paradox online journal, which is both personal and public at the same time.
Rather than being an expression of mere ‚Äúexhibitionistic‚Äù self-disclosure, journal-
style blogs are used to maintain personal relationships: personal information is
disclosed to an audience of readers, which might react to postings by commenting
on them or linking to them on their own blogs.
   Various studies find that the level of self-disclosure within a personal blog has an
impact on the structure and quality of social relations: in a survey of n ¬º 307
female bloggers and a corresponding content analysis of n ¬º 100 blogs (authored
by the respondents), Bane et al. (2010) found that bloggers with a high level of self-
disclosure on their blogs reported a high number of and higher satisfaction with
online friends. Stefanone and Jang (2007), in a survey of n ¬º 154 randomly
selected bloggers, found that bloggers with a higher level of extraversion and
self-disclosure (as personal traits) not only reported larger strong tie networks,
but were also more likely to use blogs to maintain these networks.
   Of particular interest for this paper, however, is not the connection between
personality traits and blogging behavior, but rather the specific communicative
situation in which bloggers engage. It contributes to the emergence of ‚Äúpersonal
public spheres,‚Äù which are one of the defining features of the Social Web
(Schmidt 2009, pp. 105‚Äì128). They are formed when and where users make
available information that is personally relevant to them (instead of the informa-
tion being selected according to journalistic news factors or news values), that is
directed to an intended audience of strong and weak ties (instead of the disperse,
unconnected, and unknown audience of mass-mediated public spheres), and that
is presented mainly to engage in conversation (instead of the one-way mode of
publishing).
   This new type of public sphere, which is not limited to (micro)blogs but is also
visible on social network sites such as Facebook, is blurring the boundaries between
the personal and the public. But rather than simply eroding privacy and fostering
‚Äúdigital exhibitionism,‚Äù as some commentators suspect, personal public spheres
reconfigure the context for identity management and relationship management in a
more complex way. One the one hand, they contribute to the maintenance of
‚Äúconnected presence‚Äù (Licoppe and Smoreda 2005), because they empower users
to share information that is relevant to them within an extended network of strong
and weak ties. On the other hand they demand certain routines and skills. As
Marwick and boyd (2010, p. 11) put it with regard to Twitter: users ‚Äúmust maintain
equilibrium between a contextual social norm of personal authenticity that
encourages information-sharing and phatic communication (the oft-cited ‚Äòwhat I
had for breakfast‚Äô) with the need to keep information private, or at least concealed
from certain audiences.‚Äù The remainder of the paper explores how exactly this
practice of privacy management within (micro)blogs can be described and analyzed
12   (Micro)blogs: Practices of Privacy Management                                 163


‚Äì what do we know about how people use this technology with certain communi-
cative affordances to share personal information with others and to selectively
control access to their selves?



12.3     Practices of Privacy Management in (Micro)blogs

There are various approaches that can be used to account for the diversity of blog
use (Bruns and Jacobs 2006; Schmidt 2006; Walker Rettberg 2008). Most notably,
Herring and colleagues have conducted various studies on blogs as a communica-
tive genre (e.g., Herring et al. 2004, 2005; Herring and Paolillo 2006; see also
Puschmann 2010). Here, I will draw upon an analytical model of blogging practices
that is based in sociological theory and has been developed in more detail in
Schmidt (2007a). In a nutshell, blogging practices consist of and are performed
through individual blogging episodes. How individual bloggers select and present
content online is framed by the technology or code (the underlying software with its
specific technological affordances) but also by rules (shared routines and
expectations) and by relations (hypertextual as well as social connections). Along
these structural dimensions, we can identify groups or communities of blogging
practice, for example, those bloggers who share a specific software such as
Wordpress and its features, or those who belong to a specific subculture and use
blogs in a certain way to express their subcultural identities and norms (e.g.,
Hodkinson 2006 for the Goth subculture; Wei 2004 for knitting blogs).
    Thus, code, rules, and relations frame the situative use of blogs, for example, by
suggesting a certain style of writing, or by providing the technical means to easily
link to other content. However, they are also the result of these individually
performed episodes: expectations or routines might change over time if bloggers
do not follow them, hypertextual and social networks are (re-)produced only by
individual acts of linking or commenting, and even the code might be developed
further in a reaction to direct or indirect user feedback. Thus, (micro)blogging
practices are expressing the recursivity of social action and social structure that has
been explained by Giddens‚Äô theory of structuration (Giddens 1984).
    This analytical model, which accounts both for the social structuredness of
blogging and its dynamic nature, can also serve as a framework to look specifically
at the development of (micro)blog-based practices of privacy management. In a first
step, it allows the reconstruction of the sociotechnical architecture that has evolved
from the rather static personal homepages of online diaries to the distributed
conversation of the blogosphere, and to the constant and near-live streams and
feeds of current (micro)blogging within articulated social networks. In a second
step, it can connect these changes in the communicative architecture to prevalent
communicative routines and shared expectations, including conceptualizations
about the nature and scope of one‚Äôs audience. Figure 12.1 summarizes the main
analytical categories and interdependencies between structure and action, and these
are discussed in more detail in the following chapters.
164                                                                                 J.-H. Schmidt


                                              Code
                                  Microcontent, Permalinks
                                  Automatic notification and
                                  aggregation of new content
                                  Filtering through articulated
                                  social connections
                                                            ... and
                                                       changes in
                                Code affords or
                                                      code might
                                restricts certain
                                                       be inspired
                                actions ...
                                                     by particular
                                                        practices.
                                     Privacy Management in
                                          (micro)blogs

                                 Actively using technology to
                                   selectively disclose certain
                                personal information to certain
                                            audiences.


      Shared routines &         ... and are                            Social relations
  expectations frame        reproduced or                                (as audiences)
  certain actions...        changed through action.                         guide action...


                                                       ...and are formed
                                                            through action.

                Rules                                                         Relations
      Performing authenticity                                          Intended audience
      Identifying self vs.                                             Addressed audience
      anonymity/ pseudonymity                                          Empirical audience
      Identifying others                                               Potential audience

Fig. 12.1 Analytical framework for privacy management in (micro)blogs



12.3.1 Affording Privacy Management: Software Architecture

The term ‚Äúweblog‚Äù was coined by J√∏rn Barger, who stated in 1997 that a weblog is
a ‚ÄúWeb page where a Web logger ‚Äòlogs‚Äô all the other Web pages she finds
interesting‚Äù (cit. by Blood 2004, p. 54). Predecessors of blogs date back to the
early 1990s, when individuals such as Tim Berners-Lee and organizations such as
the ‚ÄúNational Center for Supercomputing Applications‚Äù (NCSA) curated regularly
updated websites where they provided links to other interesting sites. Around the
middle of the 1990s, online journals or online diaries, where people shared and
reflected upon personal impressions and experiences (McNeill 2003) provided an
additional tradition of online-based communication that blogs drew upon.
12   (Micro)blogs: Practices of Privacy Management                                  165


Other predecessors include the personal homepage (with its author-centered way of
presenting content) and online discussion boards (offering options for commenting
on and discussing content).
   While the first blogs were edited using regular HTML editors, around the
Millennium, various blog hosters such as Pitas, LiveJournal, or Blogger.com
launched their services and helped to increase the number of bloggers (Blood
2004). While Pitas provided a field for entering a URL and one for a corresponding
text, thus suggesting a form of blogging that consisted mainly of commented links
(the ‚Äúfilter blog‚Äù), blogger.com offered only one text field. If the user wanted to link
to a different site, he/she had to enter the URL manually using HTML tags ‚Äì this
difference in interface design suggested a blogging style that was more similar to the
style of diaries: chronicling events, reflecting ideas, or disclosing emotions. Blogs
also featured ‚Äúpermalinks‚Äù ‚Äì a unique and stable URL for each single blog posting
that can be linked to individually (rather than having to link to the whole blog if one
wants to refer to a specific idea or text). The ‚Äútrackback‚Äù function introduced an
automatic notification that is added to a blog entry if other blogs link to it.
   Microblogging services such as Twitter provide additional options or restrictions
for the presentation and connection of content. The most obvious feature is the 140-
character limit for each message, which originates from restrictions imposed by
using Twitter via mobile phones and text messages. It also encouraged users and
developers to invent or co-create communicative routines to overcome these
limitations, such as using the ‚Äú@‚Äù symbol to address other users or the abbreviation
‚ÄúRT‚Äù for a ‚Äúretweet‚Äù (forwarding another user‚Äôs message). These social
conventions were in turn incorporated into subsequent versions of Twitter, thus
stabilizing certain emerging routines technologically. Another interface change
made an implicit difference: the textbox for entering a tweet now asks ‚ÄúWhat‚Äôs
happening?‚Äù instead of the former ‚ÄúWhat are you doing‚Äù (Dybwad 2009) ‚Äì
suggesting (but not prescribing) tweets of a somewhat more general relevance
than of journaling one‚Äôs activities.
   The specific features such as permalinks, trackbacks, and the comment feature
on blog postings, but also the referencing signals of Twitter, make it possible for
‚Äúdistributed conversations‚Äù (Efimova 2009) to emerge. Distributed conversations
are asynchronous and non-linear conversations where multiple authors refer to and
discuss a topic on various sites. While such distributed conversation might be
followed or participated in easily within small communities of bloggers, the rapid
growth of the blogosphere has made it rather difficult to follow the constant
updates. Basically, two technological innovations have proven highly important
in assisting readers in keeping up with new content and changing the affordances of
information management.
   Firstly, the development of the RSS feed format and the corresponding feed
readers from 2000 onwards allowed users to subscribe to a variety of blogs. Instead
of having to regularly and ‚Äúmanually‚Äù visit those sites that are of interest, users
aggregate selected sources in their feed reader and this automatically retrieves new
and updated content. Secondly, the articulation of social relations became a relevant
mechanism for filtering content. The ‚Äúblogroll,‚Äù a linklist of favorite blogs on one‚Äôs
166                                                                       J.-H. Schmidt


own blog, provided an early mechanism for expressing social connection and
topical interest. Platforms such as LiveJournal introduced more sophisticated social
networking features to the basic blogging functionalities: by adding other users as
friends, one could not only selectively give access to certain postings to this group
(thus engaging in privacy management; see below), but could also be informed
about updates on one‚Äôs friends‚Äô LiveJournal blogs (boyd and Ellison 2007).
    On microblogging services such as Twitter, the articulation of social connections
has become a basic organizational principle of communication. The act of ‚Äúfollow-
ing‚Äù is akin to subscribing to that account, so that relations on Twitter are not
necessarily reciprocal: they do not signify mutual acquaintance (although this can
be the case), but more often just interest in those users one follows. By explicitly
choosing certain Twitter accounts, a user can customize his/her own repertoire of
sources, thus engaging in active information management. The @ feature and the
Retweet function, which are both used to relate to other users (by addressing them
or forwarding their tweet), also contribute to the organization of conversations on
Twitter, thus structuring social relations and networks (Honeycutt and Herring
2009; boyd et al. 2010).
    To summarize: (micro)blogs have incorporated several technological innovations
that distinguish them from predecessors, such as the easy-to-maintain commented
linklist or hosting platforms for online diaries: the shift to regularly updated
‚Äúmicrocontent‚Äù addressable through permalinks, the automatic notification of new
content, the aggregation of these updates in one single ‚Äúplace‚Äù (the RSS feed reader
or the Twitter interface), and the reliance on articulated social connections to filter
information. The resulting technological architecture has not only significantly
lowered the barriers for making information accessible to others via the Internet,
which includes sharing personal information, but has also contributed to a fundamen-
tal shift in the communicative architecture of web-based publics, which is (maybe
even more prominently) visible in other Social Web applications as well. Instead of
the hypertext networks of separate websites connected by and traversable via
hyperlinks that formed the early web, the Social Web is characterized by ‚Äústreams‚Äù
and ‚Äúfeeds‚Äù: afforded by technological features, personal information is constantly
made accessible, aggregated, and updated within networked publics that are based on
social connections.




12.3.2 Framing Privacy Management: Shared Routines
       and Expectations

The technological architecture of (micro)blogs only partly explains practices of
privacy management. The software use is framed by shared routines and
expectations (i.e., social rules) about how to self-disclose and whom to address in
a blog. It is not the use of the same tool, but rather the shared knowledge about ‚Äì
often informal and latent ‚Äì rules that makes a blogger. In this sense, blogging as a
12   (Micro)blogs: Practices of Privacy Management                                  167


practice has to be learned, and the conventions of the blogging genre have to be
internalized (L‚Ç¨ uders et al. 2010). In doing this, bloggers usually combine existing
knowledge about similar genres ‚Äì such as the paper diary ‚Äì with reflections about
their own experiences and with feedback from people they communicate with
through their blog. Additionally, public discourse about the qualities, benefits, or
drawbacks of (micro)blogs might influence how bloggers see the genre. Press
coverage on Twitter, for example, framed microblogs positively as a tool for
maintaining social contact, but also negatively as increasing information overload
(Arceneaux and Schmitz Weiss 2010).
    The particular rules and expectations about the adequate amount of self-disclosure,
about the topics selected for postings, and about the ‚Äúright style‚Äù for blogging differ
between sub-genres of blogging. In this respect, corporate blogs or blogs by politicians
are different to personal journals, although they use the same software. However,
at the core of the rules, expectations, conventions, and norms framing these different
blogging practices is the idea of personal authenticity and subjectivity: blogs are
considered to be formats where people use their ‚Äúpersonal voice‚Äù and express their
own subjectivity by sharing personal thoughts, observations, or comments about
current events. This leitmotif of blogging not only explains the high share of jour-
nal-type blogs, but has a direct impact on self-disclosure and privacy.
    By following and supporting the norm of authenticity, bloggers predominantly
refer to their ‚Äúreal‚Äù identity. In their study on blogs run by American teenagers,
Huffaker and Calvert (2005) found that a majority of them provided information
about their first name (70%), their age (67%), and even additional contact informa-
tion such as an e-mail address or a phone number (61%). The findings of Herring
et al. (2007) point in the same direction: between 66% and 79% of the blogs in their
three samples contained first or full names of their authors.
    In a survey of 4,220 German-speaking bloggers in 2005, 70% stated that they do
not blog anonymously or pseudonymously (Schmidt 2007b). The particular ways
through which they disclose personal information varies though: approximately
40% state that they include this information in blog postings; a somewhat smaller
share (36%) of bloggers have a separate ‚Äúabout me‚Äù page with personal details.
Around one in ten bloggers (12%) link to a separate personal homepage from their
blog. While anonymity is not the dominating but a prevalent mode of blogging, it is
debated in courts whether there is a right to anonymity for bloggers in the legal
sense (Barendt 2009). This question is fundamentally tied to issues of free speech,
but also touches on the intersection of blogging and journalism, since it raises the
question of whether bloggers should enjoy the same protective rights as profes-
sional journalists (Hendrickson 2007).
    Bloggers also have to build routines on how to deal with the privacy of other
people. A non-representative survey on privacy expectations of bloggers (n ¬º 486)
conducted by Viegas (2005) found that only 3% always ask for permission before
mentioning or citing other people on their blog, while 66% almost never ask; 42%
said that they refrain from mentioning names in their blogs, while 21% stated that
they almost always reveal them. Common strategies to protect others‚Äô identities are
to use initials, nicknames, or particular social roles (such as ‚Äúmy daughter‚Äù or ‚Äúmy
168                                                                       J.-H. Schmidt


husband‚Äù) that might identify them to those in-the-know but not to other readers.
An exception is usually made for those people who blog themselves ‚Äì since they
have chosen to make certain aspects of their personal life public in their blog, they
are considered as having to live with the consequences of being mentioned or linked
to by other bloggers as well. Similar findings were reported by Schmidt (2007b) for
the German-speaking blogosphere; the survey also found that those people who
blog anonymously or pseudonymously were more likely to not disclose information
about others or, if they do disclose information, to use only initials etc. instead of
full names. These findings indicate that reciprocity norms seem to guide the amount
of disclosure about other people.




12.3.3 Performing Privacy Management: Conceptions
       of Audience

The routines and expectations about self-disclosure or disclosure of others are
strongly tied to the conception of the audience. Although (micro)bloggers might
reject this term when talking about their own experiences ‚Äì because they dislike the
corresponding notion of acting prominently on a stage or of broadcasting to a
diverse group of people (Marwick and boyd 2010, p. 6) ‚Äì they nevertheless have
certain assumptions of their readership. Due to the particular technical affordances
of blogging software compared to microblogs, the audience of blogs remains
largely invisible ‚Äì an ‚Äúunseen audience‚Äù (Scheidt 2006) ‚Äì in the absence of
articulated social connections; it is only through comments, through trackbacks
and referrer links, or through one‚Äôs server log files that a blogger can get an
impression of the size and composition of his/her audience (VieÃÅgas 2005).
    Given these limitations and the characteristics of online-based communication in
general, where information is persistent, replicable, scalable, and searchable (boyd
2010), four analytical categories of blogs‚Äô audiences have to be differentiated:
firstly, the intended audience comprises a blogger‚Äôs general idea of the audience
he/she wants to reach or address, for example, friends, colleagues, or those inter-
ested in a specific topic. Secondly, the addressed audience comprises those people
that are addressed in a specific blog posting ‚Äì which might be the same as the
intended audience in general, but might also be a specific subset, for example, when
a posting or tweet is directed to a particular group of readers for feedback. Empirical
studies usually concentrate on the intended audience and find that most bloggers
have a vague idea about its composition. Almost half (49%) of the bloggers
surveyed by Lenhart and Fox (2006) believed that their audience consisted of
people they personally know, about a third (35%) believed that mostly people
they have never met personally read their blogs, and 14% believe that it is a mix
of personally known and unknown readers. Qian and Scott (2007) report a higher
number of bloggers (88%) who identify people they know offline as their main
audience.
12   (Micro)blogs: Practices of Privacy Management                                 169


    Both the intended audience and the addressed audience are conceptualizations
on the blogger‚Äôs side, and are an important point of reference for deciding what
information to disclose online. However, they do not necessarily correspond with
the empirical audience that comprises those people who actually take notice of any
given posting or tweet. In many cases this will be only a subset of the intended
audience, since, for example, not all followers on Twitter will actually read a
particular tweet. Problems with regard to privacy arise especially if the empirical
audience is larger than the intended audience, for example, when tweets get
retweeted or a particular blog posting is found through a search engine. As a result
of network effects, the empirical audience might differ significantly from the
intended audience: in a large scale analysis of Twitter, Kwak et al. (2010) found
that no matter how large the follower base of the original user, a retweeted
(forwarded) tweet reaches on average 1,000 users. Qualitative research suggests
that especially for teenagers it is the ‚Äúknown, but inappropriate others‚Äù (Livingstone
2008, p. 405) who are problematic: parents or teachers reading a blog or discover-
ing a Twitter account that is not intended for them to read.
    Finally, the potential audience has to be considered. This is mainly determined
by the ‚Äútechnological reach‚Äù of a blog within the wider context of networked
communication. Under the conditions of persistence and searchability in particular,
it is hard to assess who might possibly have access to a blog posting or a tweet in the
near or in the more distant future. Features of the software code, for example,
protecting a Twitter account from non-followers, or blocking search engine robots
from a blog, can assist a blogger in restricting his/her potential audience.



12.4     Conclusion

This paper has argued that privacy management in (micro)blogging can be under-
stood and analyzed as a particular practice that is grounded in specific software
affordances, in certain shared rules, and in the addressing of particular audiences.
More specifically, the technological characteristics of (micro)blog code, which
include uniquely addressable microcontent that is regularly updated and aggregated
within feeds or streams of text, which in turn are filtered or channeled with the help
of articulated social connections, provide a particular communicative architecture
for sharing personal information. How exactly these technological features are used
to share personal information with others is framed by shared routines and
expectations. They evolve, stabilize, and change by combining experiences from
(micro)blogging with knowledge about other CMC genres (such as the social
network site) as well as with experiences grounded in other spheres of social life
(such as the workplace or the home), where selective disclosure has to be performed
as well. Important rules of (micro)blogging center around the key norm of authen-
ticity, around the alternative between identifying oneself vs. blogging anonymously
or pseudonymously, and around the ethical question of how to disclose information
about others, where norms of reciprocity play an important role.
170                                                                                 J.-H. Schmidt


   Finally, privacy management in (micro)blogs is inseparably tied to the social
relations that are maintained and established through blogging. Not only do articu-
lated social relations, for example, links in blogrolls, subscribed RSS feeds, or one‚Äôs
followers on Twitter assist in filtering information, social relations also become
relevant for privacy management in the form of particular audiences: bloggers
conceive of an intended audience and might even explicitly write for an addressed
audience. In this respect, privacy management is performed for specific audiences.
Due to the specific technological affordances, however, the intended or addressed
audience might be incongruent with the empirical audience and the potential
audience, which in turn can lead to privacy conflicts or failures to control who
has access to certain personal information.
   Analytically separating and discussing elements of privacy management
practices is only a first step in understanding the impact that (micro)blogging has
on individual users and social life. Ongoing technological innovation and the
convergence with other Social Web applications introduce constant and great
dynamics into the way people communicate via (micro)blogs. Not only do we
lack more detailed knowledge on the various normative guidelines and shared
expectations that frame privacy management under these conditions, especially in
a comparative perspective,, but there is also the need to research the congruence or
disparities between expectations of privacy and actual behavior. This in turn might
lead to a better understanding of appropriate interventions, whether they aim at
better and more sophisticated software-based control, or at improved knowledge
and skills. Both seem to be necessary to guarantee that users can make the best use
of the communication tools while maintaining control over their own personal
information and private sphere.



References

Altman I (1975) The environment and social behavior. Brooks/Cole, Monterey
Arceneaux N, Schmitz Weiss A (2010) Seems stupid until you try it: press coverage of twitter,
   2006‚Äì9. New Media Soc 12(8):1262‚Äì1279. doi:10.1177/1461444809360773
Bane CM, Cornish M, Erspamer N, Kampman L (2010) Self-disclosure through weblogs and
   perceptions of online and ‚Äúreal-life‚Äù friendships among female bloggers. Cyberpsychol Behav
   Social Netw 13(2):131‚Äì139
Barendt E (2009) Bad news for bloggers. J Media Law 1(2):141‚Äì147
Blood R (2004) How blogging software reshapes the online community. Commun ACM
   4(12):53‚Äì55
B‚Ç¨ohringer M, Richter A (2009) Adopting social software to the intranet: a case study on enterprise
   microblogging. In: Wandke H (ed) Proceedings of the 9th mensch & computer conference.
   Oldenbourg, Munich, pp 293‚Äì302
boyd d (2010) Social network sites as networked publics: affordances, dynamics, and implications.
   In: Papacharissi Z (ed) Networked self: identity, community, and culture on social network
   sites. Routledge, New York, pp 39‚Äì58
boyd d, Ellison NB (2007) Social network sites: definition, history, and scholarship. J Comput
   Mediat Commun 13(1), Article 11. http://jcmc.indiana.edu/vol13/issue1/boyd.ellison.html.
   Accessed 10 Feb 2011
12   (Micro)blogs: Practices of Privacy Management                                            171

boyd d, Golder S, Lotan G (2010) Tweet, tweet, retweet: conversational aspects of retweeting on
   twitter. In: Proceedings of the 43rd Hawaii international conference on social systems, Hawaii,
   doi: 10.1109/HICSS.2010.412
Bruns A (2008) Blogs, wikipedia, second life and beyond: from production to produsage. Peter
   Lang, New York
Bruns A, Jacobs J (2006) Uses of blogs. Peter Lang, New York
Busemann K, Gscheidle C (2010) Web 2.0: Nutzung steigt ‚Äì Interesse an aktiver Teilnahme sinkt.
   Media Perspektiven 41(7‚Äì8):359‚Äì368
Dybwad B (2009) Twitter drops ‚Äúwhat are you doing?‚Äù Now asks ‚Äúwhat‚Äôs happening?‚Äù Mashable,
   19 Nov 2009. http://mashable.com/2009/11/19/twitter-whats-happening. Accessed 10 Feb
   2011
Efimova L (2009) Passion at work: blogging practices of knowledge workers. Novay, Enschede
Fox S, Zickuhr K, Smith A (2009) Twitter and status updating, fall 2009. Pew Research Center,
   Washington, DC. http://www.pewinternet.org/Reports/2009/17-Twitter-and-Status-Updating-
   Fall-2009.aspx. Accessed 10 Feb 2011
Giddens A (1984) The constitution of society. Polity Press, Cambridge
Hendrickson L (2007) Press protection in the blogosphere: applying a functional definition of
   ‚ÄúPress‚Äù to news web logs. In: Tremayne M (ed) Blogging, citizenship, and the future of media.
   Routledge, New York, pp 187‚Äì203
Herring SC, Paolillo JC (2006) Gender and genre variation in weblogs. J Socioling 10(4):439‚Äì459
Herring SC, Kouper I, Scheidt LA, Wright E (2004) Women and children last: the discursive
   construction of weblogs. In: Gurak L, Antonijevic S, Johnson L, Ratliff C, Reyman J (eds) Into
   the blogosphere: rhetoric, community, and culture of weblogs. University of Minnesota. http://
   blog.lib.umn.edu/blogosphere/women_and_children.html. Accessed 10 Feb 2011
Herring SC, Scheidt LA, Bonus S, Wright E (2005) Weblogs as a bridging genre. Inf Technol
   People 18(2):142‚Äì171
Herring SC, Scheidt LA, Kouper I, Wright E (2007) Longitudinal content analysis of blogs:
   2003‚Äì2004. In: Tremayne M (ed) Blogging, citizenship, and the future of media. Routledge,
   New York, pp 3‚Äì20
Hodkinson P (2006) Subcultural blogging? Online journals and group involvement among U.K.
   Goths. In: Bruns A, Jacobs J (eds) Uses of blogs. Peter Lang, New York, pp 187‚Äì198
Honeycutt C, Herring S (2009) Beyond microblogging: conversation and collaboration via twitter.
   In: Proceedings of the 42nd Hawaii international conference on social systems, Hawaii, doi:
   10.1109/HICSS.2009.89
Huffaker DA, Calvert SL (2005) Gender, identity, and language use in teenage blogs. J Comput
   Mediat Commun, 10(2) http://jcmc.indiana.edu/vol10/issue2/huffaker.html. Accessed 10 Feb
   2011
Johnson B (2005) The mum, the nanny, her blog and some others. Guardian unlimited technology
   blog, 20 July 2005. http://www.guardian.co.uk/technology/blog/2005/jul/20/themumthenan?
   INTCMP¬ºSRCH. Accessed 10 Feb 2011
Jones S, Fox S (2009) Generations online in 2009. Pew internet project memo. 28 Jan 2009. http://
   www.pewinternet.org/~/media//Files/Reports/2009/PIP_Generations_2009.pdf. Accessed 10
   Feb 2011
Keren M (2006) Blogosphere. The new political arena. Lexington, Lanham
Kwak H, Lee CL, Park H, Moon S (2010) What is twitter, a social network or a news media? In:
   WWW ‚Äò10: Proceedings of the 19th international conference on world wide web, ACM,
   New York, pp 591‚Äì600
Lasica JD (2002) Blogging as a form of journalism. In: Blood R (ed) We‚Äôve got blog. How
   weblogs are changing our culture. Perseus, Cambridge, pp 163‚Äì170
Lenhart A, Fox S (2006) Bloggers. A portrait of the internet‚Äôs new storytellers. Pew Internet &
   American Life Project, Washington, DC. http://www.pewinternet.org/Reports/2006/Bloggers.
   aspx. Accessed 10 Feb 2011
172                                                                                 J.-H. Schmidt


Licoppe C, Smoreda Z (2005) Are social networks technologically embedded? Social Netw
    27(4):317‚Äì335
Lievrouw L (2002) Determination and contingency in new media development: diffusion of
    innovations and social shaping of technology perspectives. In: Lievrouw L, Livingstone S
    (eds) Handbook of new media. Sage, London, pp 183‚Äì199
Livingstone S (2008) Taking risky opportunities in youthful content creation: teenagers‚Äô use of
    social networking sites for intimacy, privacy and self-expression. New Media Soc
    10(3):393‚Äì411
Livingstone S, Haddon L, G‚Ç¨     orzig A, OÃÅlafsson K (2011) Risks and safety on the internet: the
    perspective of European children. full findings. EU Kids Online, London
L‚Ç¨uders M, Pr√∏itz L, Rasmussen T (2010) Emerging personal media genres. New Media Soc
    12(6):947‚Äì963. doi:10.1177/1461444809352203
Marwick A, boyd d (2010) I tweet honestly, I tweet passionately: twitter users, context collapse,
    and the imagined audience. New Media Soc. doi:10.1177/1461444810365313
McNeill L (2003) Teaching an old genre new tricks: the diary on the internet. Biography
    26(1):24‚Äì47
Messner M, DiStaso MW (2008) The source cycle: how traditional media and weblogs use each
    other as sources. J Stud 9(3):447‚Äì463
Murphy D (2010) Twitter ‚Äì on-track for 200 million users by year‚Äôs end. Pcmag.com, 31 Oct 2010.
    http://www.pcmag.com/article2/0,2817,2371826,00.asp. Accessed 10 Feb 2011
N.N. (2005) Sorry to disappoint you. Instructions to the double, 16 July 2005. http://web.archive.
    org/web/20050719001851/http://subvic.blogspot.com/2005/07/sorry-to-disappoint-you.html
    Accessed 10 Feb 2011
Naaman M, Boase J, Lai C-H (2010) Is it really about me? Message content in social awareness
    streams. In: Quinn KI, Gutwin C, Tang JC (eds) Proceedings of the 2010 ACM conference on
    computer supported cooperative work, ACM, New York, pp 189‚Äì192, doi: 10.1145/
    1718918.1718953
Nardi BA, Schiano DJ, Gumbrecht M, Swartz L (2004) Why we blog. Commun ACM
    47(12):41‚Äì46
Olen H (2005) The new nanny diaries are online. New York Times, 17 July 2005. http://www.
    nytimes.com/2005/07/17/fashion/sundaystyles/17LOVE.html. Accessed 10 Feb 2011
Papacharissi Z (2007) Audiences as media producers: content analysis of 260 blogs. In: Tremayne
    M (ed) Blogging, citizenship, and the future of media. Routledge, New York, pp 22‚Äì38
Park HW, Thelwall M (2008) Developing network indicators for ideological landscapes from the
    political blogosphere in South Korea. J Comput Mediat Commun 13:856‚Äì879. doi:10.1111/
    j.1083-6101.2008.00422.x
Puschmann C (2010) The corporate blog as an emerging genre of computer-mediated communi-
    cation: features, constraints, discourse situation. Universit‚Ç¨atsverlag, G‚Ç¨
                                                                              ottingen
Qian H, Scott CR (2007) Anonymity and self-disclosure on weblogs. J Comput Mediat Commun
    12(4), Article 14. http://jcmc.indiana.edu/vol12/issue4/qian.html. Accessed 10 Feb 2011
Scheidt LA (2006) Adolescent diary weblogs and the unseen audience. In: Buckingham D,
    Willett R (eds) Digital generations: children, young people, and new media. Erlbaum, London,
    pp 193‚Äì210
Schmidt J (2006) Weblogs. Eine kommunikationssoziologische Studie. [Weblogs. A study in
    sociology of communication]. UVK, Konstanz
Schmidt J (2007a) Blogging practices: an analytical framework. J Comput Mediat Commun 12(4),
    Article 13. http://jcmc.indiana.edu/vol12/issue4/schmidt.html. Accessed 10 Feb 2011
Schmidt J (2007b) Blogging practices in the German-speaking blogosphere. Empirical findings
    from the ‚Äú‚ÄòWie ich blogge?!‚Äù-survey‚Äô. Working paper of the research centre ‚ÄúNew Communi-
    cation Media‚Äù, No. 07-02. Bamberg. http://nbn-resolving.de/urn:nbn:de:0168-ssoar-9953
Schmidt J (2009) Das neue Netz. Merkmale, Praktiken und Folgen des Web 2.0. [The new net.
    Characteristics, practices and consequences of Web 2.0.]. UVK, Konstanz
12   (Micro)blogs: Practices of Privacy Management                                          173

Scott DT (2007) Pundits in muckrakers‚Äô clothing: political blogs and the 2004 U.S. presidential
    election. In: Tremayne M (ed) Blogging, citizenship, and the future of media. Routledge,
    New York, pp 39‚Äì58
Sifry D (2004) State of the blogosphere, October 2004. http://www.sifry.com/alerts/archives/
    000245.html. Accessed 10 Feb 2011
Stefanone MA, Jang C-Y (2007) Writing for friends and family: the interpersonal nature of blogs.
    J Comput Mediat Commun 13(1), Article 7. http://jcmc.indiana.edu/vol13/issue1/stefanone.
    html. Accessed 10 Feb 2011
Tremayne M (ed) (2007) Blogging, citizenship, and the future of media. Routledge, New York
VieÃÅgas FB (2005) Bloggers‚Äô expectations of privacy and accountability: an initial survey.
    J Comput Mediat Commun 10(3), Article 12. http://jcmc.indiana.edu/vol10/issue3/viegas.
    html. Accessed 10 Feb 2011
Walker Rettberg J (2008) Blogging. Polity, Cambridge
Wei C (2004) Formation of norms in a blog community. In: Gurak L, Antonijevic S, Johnson L,
    Ratliff C, Reyman J (Eds) Into the blogosphere. Rhetoric, community, and culture of weblogs.
    University of Minnesota. http://blog.lib.umn.edu/blogosphere/formation_of_norms.html.
    Accessed 10 Feb 2011
Westin A (1970) Privacy and freedom. Atheneum, New York
White D, Winn P (2009) State of the blogosphere 2008. http://technorati.com/blogging/feature/
    state-of-the-blogosphere-2008. Accessed 10 Feb 2011
Chapter 13
Privacy in Social Network Sites

Marc Ziegele and Oliver Quiring




13.1    Introduction

Are we running out of privacy? Nowadays, for example, we are concerned about
whether the maintenance of a private sphere in online environments has become a
luxury commodity (Papacharissi 2009). Questions of this kind are justified as
online communication plays an increasingly important role in people‚Äôs everyday
life (cf., e.g., Lundby 2009). While it seems exaggerated to stigmatize today‚Äôs
youth as ‚Äúcommunication junkies‚Äù (Patalong 2010), online conversations are
increasingly becoming a functional equivalent to face to face communication
(Beer 2008). However, some significant differences between online and ‚Äúoffline‚Äù
communication remain. Face to face communication may remain largely intimate
in some situations. It does not necessarily require the disclosure of personal data
nor does it leave behind traces (Dwyer et al. 2007; Tufekci 2008). In contrast,
online communication is usually mediated by providers with commercial
interests. These providers do not confine themselves to gathering personal data
and the content of user communications, rather they try to make conversations as
public as possible by default (Gross and Acquisti 2005; Acquisti and Gross 2006).
Additionally, the speed of technological progress often exceeds the time Internet
users need to cultivate awareness for potential risks resulting from the use of these
communication measures (Livingstone 2008). Thus, questions about how users
manage their privacy online are topical for a majority of social services of the
Social Web.
    This is particularly true for social network sites (SNS), which we focus on in this
chapter. SNSs are a global and ‚Äì with respect to their usage ‚Äì still heavily growing




M. Ziegele (*) ‚Ä¢ O. Quiring
University of Mainz, Mainz, Germany
e-mail: ziegele@uni-mainz.de

S. Trepte and L. Reinecke (eds.), Privacy Online,                                  175
DOI 10.1007/978-3-642-21521-6_13, # Springer-Verlag Berlin Heidelberg 2011
176                                                            M. Ziegele and O. Quiring


communication phenomenon. Both user numbers across all age groups (Nguyen
2010; Nielsen 2009; see also the chapters by Peter & Valkenburg and by Maa√ü in
this volume) and the time spent on these platforms are currently increasing faster
than those of any other Internet service (Nguyen 2010; Nielsen 2009). There is no
doubt that SNSs are no longer a niche phenomenon (see boyd and Ellison 2008 for a
summary of the historical development of SNSs) and have become a ‚Äì sometimes
essential ‚Äì part of the daily routine of many Internet users.
   Our chapter approaches privacy issues in SNSs from multiple perspectives.
In the first section, we discuss SNSs from the perspective of communication
services. We provide an explorative taxonomy by systemizing both SNSs‚Äô
service-determined and their usage-determined features. We then discuss specific
privacy theories and conceptualize privacy issues in SNSs as issues of individual
autonomy, control of information disclosure, and restriction of personal and spatial
access to this information.
   The third section expands these considerations of privacy issues in SNSs: we
analyze risks and benefits of different degrees of individual information disclosure
from a provider- and a user-based view. The relevance of the first perspective
results from the fact that ‚Äì although clearly existing within a majority of Social Web
services ‚Äì ‚Äúcapitalism is [. . .] at risk of looming as a black box in understandings of
SNSs‚Äù (Beer 2008, p. 524). For an analysis of the latter perspective, we again shift
our focus to a more psychological view of privacy. In this context, we gather
empirical findings concerning users‚Äô privacy behavior in SNSs before closing our
chapter with a short discussion.



13.2    Social Network Sites: A Taxonomy

Social network sites can be conceptualized as a specific accumulation of different
communication services (Beer 2008) that enable users ‚Äúto construct a public or
semi-public online profile within a bounded system‚Äù (boyd and Ellison 2008, p. 2)
and to interact with specified network connections ‚Äì both human and/or institu-
tional ones. SNSs must be distinguished from online social networks because the
latter are the results of SNS usage. In other words, SNSs facilitate the organization
of online social networks.
    In order to analyze potential privacy issues within SNSs, we need knowledge
about how people interact on these platforms. However, there is no such thing as a
generalizable ‚ÄúSNS usage.‚Äù Rather, SNSs suggest ‚Äúgenres of behavior through their
architectural elements‚Äù (Papacharissi 2009, p. 203) but can be accessed in quite
individual usage patterns. Thus, while the term ‚Äúcommunication service‚Äù focuses
on the communicative potential of SNSs (as the entirety of available communica-
tive tools), the usage of these tools can be conceptualized as communication modes
(Hasebrink 2004, p. 71). Although communication modes cannot be directly
predicted by technological usage potential, it seems important to establish an
integrated service- and usage-oriented systematization of SNSs. A taxonomy
13   Privacy in Social Network Sites                                                     177


should include criteria that allow a distinction, for example, between YouTube and
Facebook and MySpace in general, and in particular for the case of privacy issues
within these SNSs. Such criteria have loosely been mentioned by different authors
(boyd and Ellison 2008; Cachia 2008; Debatin et al. 2009; Beer 2008; Tufekci
2008; Gross and Acquisti 2005). Aggregating them leads to a preliminary classifi-
cation of SNSs as displayed in Fig. 13.1.

     Service-determined SNS features                    Usage-determined SNS features

     Target au-     Privacy                            Network     Activity    Social
     dience                     Access      Locality
                    control                            focus       focus       capital

                                                       user-
       specific     internal   restricted    local                 narrow     generate
                                                       centric




                                            interna-   interest-
        mass        external     open                              broad      maintain
                                            tional     centric


Fig. 13.1 Social network sites taxonomy

   The taxonomy helps to characterize different kinds of SNSs in terms of their
major features. For this purpose, it distinguishes service-determined SNS features
and usage-determined SNS features. The service-determined SNS features are
technological and structural givens that cannot be directly influenced by user
activities. For instance, with regard to access, some SNSs are open for every
Internet user, while others remain exclusive, ‚Äúby invitation only‚Äù SNSs restricted
to a certain audience. In contrast, usage-determined SNS features vary in terms of
the users‚Äô aims, expected gratifications, and experiences. For instance, with regard
to activity focus, some users may just want to use a few of many functions of an
SNS to communicate with friends, while others take advantage of a broad spectrum
of SNS-provided information, communication, and leisure subservices.
   In the following, we will consider Facebook‚Äôs current version as well as other
SNSs to exemplify service-determined and usage-determined features. One further
important aspect when interpreting Fig. 13.1 is that the juxtapositions have to be
seen as a continuum rather than dichotomous characteristics of SNSs: for instance,
a user‚Äôs aim when joining a specific SNS might be somewhere between generating
and maintaining existing social capital.
   Target audience: Concerning the nature of the targeted audience, Facebook
serves as an illustration for both ends of the continuum, specific and mass. Started
as a service solely targeting American college students (e.g., Gross and Acquisti
2005), Facebook soon became open to everyone and now explicitly targets a mass
audience. Other services within the SNS sector continue targeting more specific
audiences (boyd and Ellison 2008): for example, weRead (weread.com) targets
178                                                          M. Ziegele and O. Quiring


book lovers while Buzznet (buzznet.com) aims to bring together people interested
in music and pop culture. However, some of these services are starting to integrate
Facebook connectivity to increase their reach.
    Privacy control: Users‚Äô possibilities to internally control disclosed information
vary with regard to different spaces within and beyond SNSs (Papacharissi 2009):
some services allow their users to specify data visibility (1) beyond the SNS, (2)
within the SNS, and (3) within a user‚Äôs online social network. Other providers make
suggestions for ‚Äúoptimized‚Äù privacy settings while yet others restrict the user in
controlling some of the above mentioned privacy spheres (for example, the German
business SNS Xing prevents members without a premium account from browsing
other members‚Äô profile pages anonymously).
    Access: The criteria for membership vary across different SNSs. While
Facebook and many other services are open to anyone who can access the
Internet, SNSs such as Asmallworld or BeautifulPeople are restricted to a specific
‚Äì call it exclusive ‚Äì audience. Similarly, the degree of accessible source code for
developers to program mash-ups and third-party applications varies from service to
service.
    Locality: Facebook is one example of a global SNS. Other services are primarily
national (Qzone in China), regional (wer-kennt-wen in Germany), or even
hyperlocal (communities of local newspapers).
    Network focus: SNSs can be classified by the centrality of user profiles.
Services such as MySpace and LinkedIn focus strongly on user profiles and offer
a wide range of options for self-expression. Other SNSs build on specific topics
such as music, art, and sports. For example, the travelling network TravBuddy
(travbuddy.com) centers topics and interest areas on a magazine-style landing page
while individual profiles are promoted less prominently. Contrary to boyd and
Ellison‚Äôs (2008, p. 219) view, SNSs forming primarily around interests (not people)
obviously do exist. A combination of both perspectives is possible through func-
tional integration: Facebook‚Äôs public groups are more interest-centric while com-
municative activities beyond those groups are more user-centric. As a result, the
network focus is often primarily usage-determined.
    Activity focus: The spectrum of communication (sub-)services offered varies
from service to service. Facebook offers a wide range of technological features that
might help in obtaining gratifications to satisfy social needs as well as more
individualistic information and entertainment needs. In comparison, Twitter, for
example, is functionally restricted to the microblogging feature while Flickr
concentrates on photo sharing.
    Social capital: The perceived extent to which an individual can draw on
resources from the network of social ties can be conceptualized as ‚Äúsocial capital‚Äù
(Coleman 1988; Putnam 2000; Ellison et al. 2007; Ellison et al. in this volume). For
user activities within SNSs, we suggest differentiating between generating social
capital ‚Äì by establishing contact to previously unknown individuals ‚Äì and
maintaining social capital by connecting (and interacting) with ties from users‚Äô
offline social networks (Ellison et al. 2007). In general, most SNSs allow for both
activities so that the focus depends heavily on users‚Äô communicative behavior.
13   Privacy in Social Network Sites                                              179


However, and as already mentioned, many SNSs explicitly suggest genres of
behavior. Facebook encourages users to ‚Äúconnect and share with the people in
your life‚Äù (Facebook 2010a, authors‚Äô emphasis) to maintain social capital previ-
ously created. In contrast, other SNSs focus on building new connections by
encouraging users to publicly share content or to interact with network members
who share similar interests. YouTube, digg, and last.fm are just three of many
examples where this characteristic form of networking prevails. Although these
connections may predominantly remain what Granovetter (1973) calls weak ties,
the extension of one‚Äôs social network by establishing new contacts online might not
be as rare as boyd and Ellison (2008, p. 221) assume it to be but rather depends on
the specific network under analysis.
   As we will show in the following sections, some of these dimensions are
particularly important for the analyses of both occurring and potential privacy
issues. However, before addressing these issues, we will clarify relevant
dimensions that constitute privacy in SNSs.




13.3     Theoretical Approaches to Privacy Online

13.3.1 Informational Privacy

The concept of privacy as the ‚Äúright to be let alone‚Äù (Joinson and Paine 2007,
p. 242) dates back to the late nineteenth century. The results of an invasion into the
private sphere ‚Äì for instance, by journalists ‚Äì were described as ‚Äúmental pain and
distress, far greater than could be inflicted by mere bodily injury‚Äù (Warren and
Brandeis 1890, p. 196). Continuously recurrent legal cases in which celebrities
claim their right to be let alone show that this perspective is still topical today.
However, this and similar ‚Äúnon-intrusion approaches‚Äù do not entirely describe the
dimensions of privacy in online environments, where questions of self-regulated
access to an individual‚Äôs personal information and information dissemination play a
major role (Joinson and Paine 2007).
   In this context, the rapid and global diffusion of Internet access has raised the
interest of many scholars from various disciplines who have tried to adapt and
extend privacy theories to different forms of online usage (e.g., Cranor 1999;
Gadzheva 2008; Metzger 2004; Tavani 2000; Ben-Ze‚Äôev 2003). Here, the concept
of informational privacy as a distinct category of privacy concerns emerges
(Burgoon et al. 1989; Cohen 2000; Tavani 2000). Informational privacy
originates in privacy theories by Westin (1967) and Altman (1976). Westin
(1967) states that people aim to achieve a situational balance between private
and open behavior. Altman (1976) emphasizes that privacy is inherently a social
and dynamic process of optimization between disclosure and withdrawal (Tufekci
2008). As a result, an individual might modify or rethink applied privacy
mechanisms depending on their success in different social situations (Altman
180                                                         M. Ziegele and O. Quiring


1976, p. 17). Both theories overlap by focusing on the importance of autonomous
control and limited access to an individual‚Äôs self (Margulis 2003, p. 423). Thus,
informational privacy can be understood as ‚Äúan individual‚Äôs right to determine
how, when, and to what extent information about the self will be released [. . .]‚Äù
(Joinson and Paine 2007, p. 244; Westin 1967). Within SNSs, it is particularly
important to complement this definition with both the addressees of information
disclosure ‚Äì these are single persons, dyads, groups, a disperse public, and/or
institutions (see e.g., Schweiger and Quiring 2005) ‚Äì as well as with the nature of
disclosed information. Thus, we expand it to ‚Äúwhat information will be made
available in which way, to whom, when, and to what extent.‚Äù In other words,
informational privacy in SNSs should particularly concern users‚Äô control of the
kind and the content of disclosed information, autonomy in (temporal) decision
making about information release and withdrawal, and spatial and personal
restriction of access to private information (see also Sect. 13.4.2).
   It becomes clear that extended or public access to personal information alone
cannot be considered as a sufficient condition for a loss or a violation of privacy.
Many perceived privacy issues may in fact be the result of deliberate privacy
abandonment. Individuals who seek to maintain social capital may publicly
disclose different information to people who use SNSs primarily to generate
new social capital (cf. e.g., Ellison et al. in this volume). In contrast, from the
user perspective, privacy violations can result from any form of unwanted or
uncontrolled publicness, regardless of whether specific information is publicly
available to one or a thousand persons (see e.g., Joinson and Paine 2007). In other
words, privacy issues here occur when users misinterpret the architecture of
communication services and/or use communication services in an inappropriate
way. This may happen along at least two factors of informational privacy, namely
autonomy and control of information disclosure. Users commenting on their
contacts‚Äô status updates may autonomously decide to disclose the content of
their communicative action. However, they might at the same time be unaware
of the actual reach of their action ‚Äì which is likely to exceed the user addressed.
The gap between perceived and actual reach can be considered as misinterpreta-
tion of the communication service that ultimately results in a loss of control over
who may access the disclosed information. Other urgent issues occur when SNS
providers constrain users in their autonomy so that they may disclose information
of different reach involuntarily. In terms of our taxonomy, this can occur when
providers limit the amount of internally controllable privacy settings. For
instance, some SNSs provide information on the activity level of their members.
Users may be judged by this level although their original intention might have
been to use the SNS solely in a passive reception mode. Further involuntary
communicative action occurs, for example, when users cannot control whether
they may be linked on photos or other multimedia content (see e.g., Debatin
et al. 2009).
   This perspective of informational privacy largely presupposes the disclosure of
authentic information about an individual‚Äôs real self online. In the following
13   Privacy in Social Network Sites                                               181


section, we assess privacy issues that occur when individuals satisfy inherently
social needs with tools of computer-mediated communication (CMC).




13.3.2 Self-Disclosure, Social Network Sites,
       and Computer-Mediated Communication

It can hardly be denied that within many SNSs, ‚Äúnotions of anonymity and pseudo-
nymity [. . .] have been replaced by performative behavior about the real self‚Äù
(Cachia 2008, p. 26). Thus, in order to investigate the nature of online privacy issues
further, it is important to analyze self-disclosure as the ‚Äúprocess of making the self
known to others‚Äù (Jourard and Lasakow 1958, p. 91; Joinson and Paine 2007).
Firstly, and to come back to our taxonomy of SNSs, a high degree of honest self-
disclosure is more likely to occur in user-centric than in primarily interest-centric
SNSs (Tufekci 2008; Walther et al. 2010; see also Sect. 13.2). Secondly, within user-
centric SNSs, real identity disclosure can be seen as a consequence of the
mediatization of everyday life (Hartmann 2009; Beer 2008; Thrift 2005, p. 7).
From a sociological view, those SNSs increasingly become mundane and ‚Äúamal-
gamate with various non-media activities in social life‚Äù (Schulz 2004, p. 98; Beer
2008). As communication services, they provide the architecture for maintaining
and managing real world ties and encourage their users to take advantage of these
features. In the words of our taxonomy, these SNSs encourage maintaining social
capital (Haythornthwaite 2007). The tools to satisfy (offline) social needs online can
instead be found in CMC (see e.g., Etzioni and Etzioni 1999): one key novelty of
many user-centric SNSs is that they enable a comfortable mass management of real
world ties by providing a large spectrum of communication (sub-)services (activity
focus in our taxonomy). Both one-to-one and one-to-many communication can be
executed effortlessly in those SNSs; for example, a user‚Äôs status update may reach a
single recipient or ‚Äì with no additional effort ‚Äì a group of specified addressees or a
disperse public. This management of real world ties with different communication
tools makes many user-centric SNSs hybrid communication phenomena but is also
responsible for informational privacy issues becoming prevalent.
    In a nutshell: there is a need for a differentiated view on users‚Äô privacy behavior
in SNSs that addresses (1) different service-determined and usage-determined SNS
features (some of them are mentioned in our preliminary taxonomy), (2) the
extracted criteria of informational privacy, and (3) SNSs as hybrid communication
phenomena where social needs are satisfied with tools of CMC. In the following
section, we try to provide further connections between criteria of our taxonomy and
the concept of informational privacy by analyzing how SNS providers evaluate
different degrees of intimateness and openness.
182                                                           M. Ziegele and O. Quiring


13.4    Two Perspectives on Privacy in Social Network Sites

13.4.1 A Provider-Based View of Privacy

SNS providers aim to maximize the amount of information users provide and the
public visibility of the information disclosed. This is mainly due to economic and
network reasons. The more status updates and personal information users disclose
via different communication tools, the more traffic providers can sell to advertising
companies. Additionally, information is not only valuable in a quantitative but in a
qualitative sense: the content of information can be seen as a commodity that
companies convert into opportunities for profit (Thrift 2005; see also Barnes
2006). Every information item participants publish might be used to sell targeted
social ads more precisely (Beer 2008; Nielsen 2009). From a provider-based view, a
high user consciousness for privacy issues ‚Äì such as a high awareness of the true
visibility of personal information ‚Äì is seen as a ‚Äúmajor obstacle in generating
revenue‚Äù (Nielsen 2009, p. 5). However, too lax privacy policies result in user
discontent and prevent potential users from joining or using the SNS (Economist
2010; boyd 2008; Debatin et al. 2009, p. 84). Therefore, and particularly in
restricted-access SNSs such as Asmallworld, providers tend to apply strict behav-
ioral rules to determine in which way users can provide what information to whom
(Papacharissi 2009). Other providers obviously see one escape from the described
dilemma in assuring users that they possess disclosed information while at the same
time they reserve some specific exploitation rights (Facebook, 2010b; Nielsen
2009, p. 9). Moreover, one could assume that the more international SNSs are,
the more complicated it seems to users to control possible third-party access to their
information and to demand privacy guidelines that adhere to national specifics in
privacy law.
    The second reason for providers‚Äô endorsement of public information can be
derived from the first one: to make SNSs more dynamic and attractive. SNS
providers can only build the framework of their product while user activity brings
it to life. Providers thus have to rely on ‚Äúprodusers‚Äù (Bruns 2006) who actively
contribute to shaping the SNS as an attractive product. Regularly updated user
profiles, visible activities, and ongoing interpersonal communication suggest to its
members that there is always something going on. This potentially increases both
users‚Äô own activity level and their dwell time, which again can be monetized.
    In sum, privacy issues in the relationship between users and providers of SNSs
mainly concern the unwanted collection, storage, and dissemination of personal
information by providers as well as concerns about potential security leaks from the
platforms that might lead to hacking and identity theft (boyd and Ellison 2008).
Most of these issues usually remain invisible to the average user (Debatin et al.
2009, p. 88). And despite users seeking to reach a (perceived) optimum between
privacy and publicity over time (Lange 2008), new features ‚Äì such as the Facebook
‚ÄúNews Feed‚Äù (see e.g., boyd 2008) ‚Äì tend to circumvent their knowledge about
13   Privacy in Social Network Sites                                              183


privacy management and eventually make them share more content publicly than
they intended to do.



13.4.2 A User-Based View of Privacy

The growing body of empirical research on privacy in SNSs continuously extracts
factors that influence the privacy behavior of their users. In Sect. 13.3.1, we
suggested three categories of informational privacy: (1) individual autonomy,
which may be defined as users‚Äô consciousness for how and when privacy settings
should be revised; (2) access restriction to private information, which may be
assessed in terms of perceived reach and visibility of information disclosure; (3)
control of the kind and the content of disclosed information, which may be specified
as a deliberation between social benefits versus privacy risks.


13.4.2.1    Autonomy: User Engagement with Privacy Settings

One general reason for privacy issues to occur is users‚Äô indifference or lack of
knowledge concerning privacy settings in SNSs. Different studies show that while a
majority of SNS users seem to be aware of the existence of privacy settings (such as
limiting the profile‚Äôs visibility to specified friends), they seldom make use of this
autonomy and change the default settings: for instance, Acquisti and Gross (2006)
report significant discrepancies in students‚Äô awareness of specific privacy issues and
their actual behavior. Similarly, Govani and Pashley (2005) show indifferent user
behavior concerning the adjustment of privacy settings on Facebook ‚Äì even after
surveyed users were informed about possible risks of information disclosure. This
gap between knowledge about privacy issues and actual behavior has been named
the ‚Äúprivacy paradox‚Äù (Barnes 2006; Utz and Kr‚Ç¨amer 2009). While perceiving a
high degree of current control, the privacy paradox emerges as SNS users often
seem too shortsighted concerning prospective issues of the current behavior (Dwyer
et al. 2007; Tufekci 2008).
   In contrast, one factor that leads to an increase in ‚Äúapplied privacy awareness‚Äù
is the establishment of SNSs. This term naturally combines the influence of
technological and social developments such as evolving default privacy settings,
public attention to SNSs, user experiences, and others. Nevertheless, at this
aggregated level, the shift in users‚Äô privacy behavior is notable: boyd and
Hargittai (2010) use longitudinal data to explain that there were significant
increases in the frequency with which users modified Facebook‚Äôs privacy settings
between 2009 and 2010. The same tendency of increased privacy awareness can
already be found in Debatin et al.‚Äôs 2007 sample (Debatin et al. 2009). Both
findings suggest major increases compared to the ‚Äúvanishing small number of
users‚Äù (Gross and Acquisti 2005) who had changed their default Facebook
184                                                           M. Ziegele and O. Quiring


privacy settings in 2005. Another reason for users to revise their privacy settings
is the personal experience of invasions into privacy such as unwanted contacting
or profile hacking. Empirical findings by Debatin et al. (2009) substantiate this
factor as a strong predictor for an individual to revise their own privacy settings.
Furthermore, demographic factors are found to influence users‚Äô applied privacy
behavior, mainly concerning the amount of personal information they disclose
(see e.g., Stutzman and Kramer-Duffield 2010; Lewis et al. 2008; Tufekci 2008).



13.4.2.2   Access Restriction: The Influence of Perceived Audience
           on Privacy Behavior

As already alluded to in the second section of our chapter, the occurrence of privacy
issues in SNSs can also be traced back to individual unconsciousness or mispercep-
tion of the actual visibility of disclosed information: SNS users may not be aware of
the audience that is able to access status updates, comments, or profile entries.
However, Acquisti and Gross (2006) show that the majority of their surveyed SNS
users are aware of the true visibility of their profile ‚Äì nevertheless, a ‚Äúsignificant
minority‚Äù (p. 53) underestimate its possible reach. Tufekci (2008) substantiates
these findings: a comparison between the privacy behavior of Facebook and
MySpace users in 2006 and 2007 reveals that 95% of surveyed Facebook users
disclose their real names, while this only applies for around 60% of MySpace
members. At the time the study was conducted, MySpace profiles were public to
every Internet user by default (while the visibility of Facebook profiles could easily
be restricted), and thus fewer users were willing to disclose their real name to a
possibly unwanted audience. On Facebook, this fear of an unwanted audience was
addressed by restricting the profile‚Äôs visibility ‚Äì however, and in both cases, most
users did not decide to regulate the amount of disclosed information but only to
apply the given privacy settings.
   While suggesting that SNS users are highly aware of privacy issues, the findings
of these and similar studies are often limited: privacy settings in SNSs and commu-
nication subservices evolve rapidly and become more sophisticated (Utz and
Kr‚Ç¨amer 2009). However, recent studies concentrate on user profiles as communi-
cation subservices and survey college students who might have an increased
awareness of privacy issues. For other populations and especially for a wide
range of communication modes (e.g., commenting on a photo or a news item in
SNSs), it is more likely that privacy awareness has not evolved that much. Here,
users might perceive an ‚Äúimagined audience‚Äù that consists either of (a selection of)
their network contacts or of a more disperse group of people. As mentioned in our
theoretical argumentation, publishing content with this imagined audience in mind
might result in a discrepancy between desired and achieved privacy (cf. also
Altman 1976; Cachia 2008, p. 27).
13   Privacy in Social Network Sites                                                185


13.4.2.3    Control: Pondering the Risks and Benefits of the Presentation of Self

One main motivation behind engaging in SNSs is to start, cultivate, and maintain
social relationships (Gangadharbatla 2008; Ellison et al. 2007). When individuals
seek sociability, they naturally try to show themselves in a favorable light (Siibak
2009; Zhao et al. 2008). SNSs are an ideal place for strategically creating ‚Äúhighly
socially desirable identities‚Äù (Utz and Kr‚Ç¨amer 2009) depending on how individuals
would like to be judged by others. Thus, much of what is perceived as privacy issues
can instead be seen as impression management (Goffman 1959; Kr‚Ç¨amer and Winter
2008). From this perspective, the nature and the publicity of disclosed personal
information has to be co-interpreted as a psychological trade-off between ‚Äúthe need
to be seen‚Äù (Tufekci 2008, p. 34) and the awareness of possible privacy issues
(Livingstone 2008). The specific appearances of impression management vary
depending on individual user characteristics and attitudes on the one hand (Kr‚Ç¨amer
and Winter 2008, for a detailed analysis, see Kr‚Ç¨amer and Haferkamp in this
volume) and architectural aspects of communication services on the other hand:
while users may want to manage their self-presentation within the business SNS
LinkedIn via qualifications and awards (for this might attract future business
partners or employers), the prominent exposure of one‚Äôs network size might serve
as a functional equivalent in other SNSs. Concerning the latter, the desire to
maximize one‚Äôs network size can be seen as a reason for SNS users accepting
unknown people as Friends (Debatin et al. 2009; Kr‚Ç¨amer and Winter 2008). In other
words, users with a high need for extensive self-presentation online tend to take
more risks and consequently also apply less strict privacy settings (Livingstone
2008; Utz and Kr‚Ç¨amer 2009). At first glance, this seems contradictory to findings by
Lewis et al. (2008) who state that private profiles are significantly more common
among more active SNS users. For further investigation, future studies should more
distinctively analyze the interplay between privacy settings, user activity, and
desired social outcomes (e.g., generating or maintaining social capital).
   Besides intrinsic factors, (perceived) social norms seem to play an important role
in determining personal and spatial access restriction to user profiles as well as the
amount and the kind of information individuals provide within SNSs. Lewis et al.
(2008) find empirical support for the hypothesis that SNS users are more likely to
restrict the visibility of their profile if their network contacts‚Äô profiles are private
too. Utz and Kr‚Ç¨amer (2009) extend this finding by revealing significant correlations
between users‚Äô perception of the amount of private profiles in their environment
and their actual behavior of restricting the visibility of their own profile.



13.5       Discussion

Despite many SNSs serve as a functional completion to users‚Äô offline management
of social contacts, individual privacy behavior in both environments is far from
being congruent. In this chapter, we have tried to structure the agenda for privacy
186                                                           M. Ziegele and O. Quiring


research in SNSs by (1) systemizing SNSs with both service-determined and usage-
determined criteria, (2) denominating privacy issues within SNSs primarily as
issues of informational privacy, (3) conceptualizing SNSs as online environments
where users satisfy manifold social needs with CMC tools, and (4) advocating the
need for integrated analyses of privacy issues in SNSs.
   Our suggested taxonomy provides criteria that allow distinctions between SNSs
on an inter-service dimension. Furthermore, it can also be applied to analyses of
privacy issues within SNSs: while service-determined features constitute the
generic privacy framework of an SNS, usage-determined SNS features complement
this perspective by allowing analyses of the extent to which privacy issues might
occur in different communication modes. As our selective insights into the user
perspective on privacy reveal, SNS members currently seem to be quite aware of
some general risks of disclosing authentic information. As a result, they increas-
ingly try to restore an informational balance by restricting the public visibility of
their personal data. However, and in terms of informational privacy, a subversive
diminution of autonomy and control still characterizes situations in which people
unveil their real identity and at the same time largely cede the surveillance of their
private sphere to SNS providers. As our assumptions on the provider-based view of
privacy show, this ultimately leads to a discrepancy between users‚Äô desired and
achieved control. Thus, the privacy paradox currently continues to exist at least
below the surface of visible communication processes and within users‚Äô online
social networks, where private information is (imprudently) disclosed to people
who are little more than strangers.
   As argued in our chapter, an integration of characteristics of CMC with socio-
logical phenomena of the ‚Äúoffline life‚Äù might be helpful to analyze this paradox
further. In rural areas with close-knit communities, it has long been (and still is)
quite usual that information of every kind quickly disseminates across entire
villages. Might it be that SNS users act in a similar fashion? The architecture of
those online environments facilitates fulfilling social needs such as keeping up with
what is going on in one‚Äôs own narrower and broader environment. And concerning
the (inter-)active part of sociability, communication via SNSs increases the proba-
bility of receiving immediate and more diverse feedback on own activities (Cachia
2008; Lange 2008).
   Apart from these more intrinsic motives for information disclosure, more atten-
tion should be paid to the effects of (perceived) external social pressure on the
behavior of individual SNS users. As many SNSs are so closely connected to users‚Äô
offline life (sometimes already a requirement for real world sociability), an online
profile without sufficient (authentic) profile information might result in negative
results; this leads back to the beginning of our article where we questioned why
particularly our youth is so communicative that they are being stigmatized as
‚Äújunkies.‚Äù From the described perspective, it is easy to imagine that keeping an
orphaned and non-informative online profile might increasingly lead to (offline)
victimization.
   The increasing interdependence between on- and offline corroborates our
approach of conceptualizing SNSs as designed environments where CMC tools
13   Privacy in Social Network Sites                                                            187


are used ‚Äì and sometimes misused ‚Äì to satisfy inherently social needs. Individual
privacy behavior here remains a multi-faceted phenomenon where voluntary pri-
vacy abandonment has to be distinguished from real privacy issues. Our concept to
assess these differences was to analyze user privacy behavior with regard to both
the concept of informational privacy as well as systematic characteristics of SNSs.
Further research should evaluate the empirical capability of our SNS taxonomy and
our view on informational privacy to provide a better understanding of whether and
why some people really are running out of privacy.



References

Acquisti A, Gross R (2006) Imagined communities: awareness, information sharing, and privacy
    on the Facebook. In: Golle P, Danezis G (eds) Proceedings of 6th workshop on privacy
    enhancing technologies, Robinson College, Cambridge, pp 36‚Äì58
Altman I (1976) Privacy: a conceptual analysis. Environ Behav 8(7):7‚Äì29
Barnes SB (2006) A privacy paradox: social networking in the United States. First Monday 11(4).
    http://firstmonday.org/htbin/cgiwrap/bin/ojs/index.php/fm/article/view/1394/1312#b1
Beer D (2008) Social network(ing) sites. . . revisiting the story so far: a response to danah boyd &
    Nicole Ellison. J Comput Mediat Commun 13:516‚Äì529
Ben-Ze‚Äôev A (2003) Privacy, emotional closeness, and openness in cyberspace. Comput Hum
    Behav 19:451‚Äì467
boyd dm (2008) Facebook‚Äôs privacy trainwreck. Convergence 14(1):13‚Äì20
boyd dm, Ellison NB (2008) Social network sites: definition, history, and scholarship. J Comput
    Mediat Commun 13:210‚Äì230
boyd d, Hargittai E (2010) Facebook privacy settings: who cares? First Monday 15(8).
    http://firstmonday.org/htbin/cgiwrap/bin/ojs/index.php/fm/article/view/3086/2589
Bruns A (2006) Towards produsage: futures for user-led content production. http://eprints.qut.edu.
    au/4863/1/4863_1.pdf
Burgoon JK, Parrott R, LePoire BA, Kelley DL, Walther JB, Perry D (1989) Maintaining and
    restoring privacy through communication in different types of relationship. J Soc Pers Relat
    6:131‚Äì158
Cachia R (2008) Social computing: study on the use and impact of online social networking: IPTS
    exploratory research on the socio-economic impact of social computing. JRC European
    Commission, Spain. http://ftp.jrc.es/EURdoc/JRC48650.pdf
Cohen JE (2000) Examined lives: informational privacy and the subject as object. Stanford Law
    Rev 52(5):1373‚Äì1438
Coleman JS (1988) Social capital in the creation of human capital. Am J Sociol 94
    (Supplement):95‚Äì120
Cranor LF (1999) Internet privacy. Commun ACM 42:29
Debatin B, Lovejoy JP, Horn AK, Hughes BN (2009) Facebook and online privacy. Attitudes,
    behaviors and unintended consequences. J Comput Mediat Commun 15:83‚Äì108
Dwyer C, Hiltz SR, Passerini K (2007) Trust and privacy concern within social networking sites:
    a comparison of Facebook and MySpace. In: Proceedings of the thirteenth Americas confer-
    ence on information systems, Keystone
Economist (2010) Privacy 2.0, pp 18‚Äì19
Ellison NB, Steinfield C, Lampe C (2007) The benefits of Facebook ‚ÄúFriends:‚Äù social capital and
    college students‚Äô use of online social network sites. J Comput Mediat Commun 12:1143‚Äì1168
Etzioni A, Etzioni O (1999) Face-to-face and computer-mediated communities, a comparative
    analysis. Inf Soc 15(4):241‚Äì248
188                                                                     M. Ziegele and O. Quiring


Facebook (2010) Facebook landing page. http://www.facebook.com
Facebook (2010) Statement of rights and responsibilities. http://www.facebook.com/terms.php?
    ref¬ºpf
Gadzheva M (2008) Privacy in the age of transparency: the new vulnerability of the individual.
    Soc Sci Comput Rev 26(1):60‚Äì74
Gangadharbatla H (2008) Facebook Me: collective self-esteem, need to belong, and internet self-
    efficacy as predictors of the iGeneration‚Äôs attitudes toward social networking sites. J Interact
    Advert 8(2):5‚Äì15
Goffman E (1959) The presentation of self in everyday life. Double Day, Garden City
Govani T, Pashley H (2005) Student awareness of the privacy implications when using Facebook.
    Paper presented at the privacy poster fair at Carnegie Mellon university school of library and
    information science, Dec 14, http://lorrie.cranor.org/courses/fa05/tubzhlp.pdf
Granovetter MS (1973) The strength of weak ties. Am J Sociol 78(6):1360‚Äì1380
Gross R, Acquisti A (2005) Information revelation and privacy in online social networks.
    In: Proceedings of the 2005 ACM workshop on privacy in the electronic society, ACM
    Press, New York, pp 71‚Äì80
Hartmann M (2009) Everyday: domestication of mediatization or mediatized domestication. In:
    Lundby K (ed) Mediatization: concept, changes, consequences. Lang, New York, pp 225‚Äì242
Hasebrink U (2004) Konvergenz aus Nutzerperspektive: Das Konzept der Kommunikationsmodi.
    [Convergence from the user perspective]. In: Hasebrink U (ed) Reihe Rezeptionsforschung, vol
    1, Mediennutzung in konvergierenden Medienumgebungen [Vol. 1: media use in converging
    media environments]. Fischer, M‚Ç¨  unchen, pp 67‚Äì85
Haythornthwaite C (2007) Social networks and online community. In: Joinson AN, McKenna
    KYA, Postmes T, Reips U-D (eds) The Oxford handbook of internet psychology. Oxford
    University Press, Oxford, pp 121‚Äì153
Joinson AN, Paine CB (2007) Self-disclosure, privacy and the internet. In: Joinson AN, McKenna
    KYA, Postmes T, Reips U-D (eds) The Oxford handbook of internet psychology. Oxford
    University Press, Oxford, pp 237‚Äì252
Jourard SM, Lasakow P (1958) Some factors in self-disclosure. J Abnorm Soc Psychol
    56(1):91‚Äì98
Kr‚Ç¨amer NC, Winter S (2008) Impression management 2.0. The relationship of self-esteem,
    extraversion, self-efficacy, and self-presentation within social networking sites. J Media
    Psychol 20(3):106‚Äì116
Lange PG (2008) Publicly private and privately public: social networking on YouTube. J Comput
    Mediat Commun 13(1):361‚Äì380
Lewis K, Kaufman J, Christakis N (2008) The taste for privacy: an analysis of college student
    privacy settings in an online social network. J Comput Mediat Commun 14:79‚Äì100
Livingstone S (2008) Taking risky opportunities in youthful content creation: teenagers‚Äô use of
    social networking sites for intimacy, privacy and self-expression. New Media Soc
    10(3):393‚Äì411
Lundby K (ed) (2009) Mediatization: concept, changes, consequences. Lang, New York
Margulis ST (2003) On the status and contribution of Westin‚Äôs and Altman‚Äôs theories of privacy.
    J Social Issues 59(2):411‚Äì429
Metzger MJ (2004) Privacy, trust, and disclosure: exploring barriers to electronic commerce.
    J Comput Mediat Commun 9(4). http://jcmc.indiana.edu/vol9/issue4/metzger.html
Nguyen J (2010) The state of social networks. http://comscore.com/Press_Events/Presentations_Whi-
    tepapers/2010/The_State_of_Social_Networks_in_Asia_Pacific_with_a_Focus_on_Singapore/
    (language)/eng-US
Nielsen Online (2009) Global faces and networked places: a Nielsen report on social networking‚Äôs
    new global footprint. http://blog.nielsen.com/nielsenwire/wp-content/uploads/2009/03/
    nielsen_globalfaces_mar09.pdf
Papacharissi Z (2009) The virtual geographies of social networks: a comparative analysis of
    Facebook, LinkedIn and Asmallworld. New Media Soc 11(1‚Äì2):199‚Äì220
13   Privacy in Social Network Sites                                                          189

Patalong F (2010) Jugendliche sind Kommunikations-Junkies: Kernergebnisse der JIM-Studie
    2010 [Teenagers are communication junkies: core results of the JIM study]. Spiegel Online.
    http://www.spiegel.de/netzwelt/web/0,1518,731352,00.html
Putnam RD (2000) Bowling alone: the collapse and revival of American community. Simon &
    Schuster, New York
Schulz W (2004) Reconstructing mediatization as an analytical concept. Eur J Commun
    19(1):87‚Äì101
Schweiger W, Quiring O (2005) User-generated content on mass media web sites ‚Äì just a variety of
    interactivity or something completely different? Paper presented at the 55th annual conference
    of the international communication association, New York, 26‚Äì30 May 2005
Siibak A (2009) Constructing the self through the photo selection-visual impression management
    on social networking websites. Cyberpsychol J Psychosoc Res Cyberspace 3(1). http://
    cyberpsychology.eu/view.php?cisloclanku¬º2009061501&article¬º1
Stutzman F, Kramer-Duffield J (2010) Friends only: examining a privacy-enhancing behavior in
    Facebook. CHI Atlanta, 10‚Äì15 Apr 2010. http://fredstutzman.com/papers/CHI2010_Stutzman.
    pdf
Tavani HT (2000) Privacy and the internet. http://www.bc.edu/bc_org/avp/law/st_org/iptf/
    commentary/content/2000041901.html
Thrift N (2005) Knowing capitalism. Theory, culture and society. Sage, London
Tufekci Z (2008) Can you see me now? Audience and disclosure regulation in online social
    network sites. B Sci Technol Soc 28(1):20‚Äì36
Utz S, Kr‚Ç¨amer NC (2009) The privacy paradox on social network sites revisited: the role of
    individual characteristics and group norms. Cyberpsychol J Psychosoc Res Cyberspace 3(2),
    Article 2
Walther JB, DeAndrea D, Kim J, Anthony JC (2010) The influence of online comments on
    perceptions of antimarijuana public service announcements on YouTube. Hum Commun Res
    36(4):469‚Äì492
Warren SD, Brandeis LD (1890) The right to privacy. Harvard Law Rev 4(5):193‚Äì220
Westin A (1967) Privacy and freedom. Atheneum, New York
Zhao S, Grasmuck S, Martin J (2008) Identity construction on Facebook: digital empowerment in
    anchored relationships. Comput Hum Behav 24:1816‚Äì1836
Chapter 14
Mobile Privacy: Contexts

Maren Hartmann




14.1     Introduction

   Not sure what Facebook‚Äôs Wed. announcement will be but I can guarantee two things: It
   will have to do with mobile & It will violate privacy (MarketingAtom, tweet, 10/31/2010)

On the following Wednesday, Facebook announced several changes to its mobile
services, including a new product called ‚ÄúDeals‚Äù ‚Äì a platform for local stores and
places to offer deals to nearby Facebook users. The users‚Äô location data would be
used in the process. The privacy implications were not explicitly addressed in the
announcement. This kind of combination of social media and mobility does,
however, lead to many questions concerning privacy.
    Ever since its first inception as a modern concept, privacy has been a contested
terrain ‚Äì both theoretically and empirically. It is currently facing renewed and
increasing challenges. One important set of challenges, as widely discussed in
this book and seen in the example, is based in social media applications and
services. An additional challenge, less focused on thus far, also relates to new
media applications, but at the same time offers a new focus: it is the question of
mobility and mobile media, i.e., it is the question of privacy in mobile contexts. The
challenges that privacy faces at least double in this context: not only are the existing
privacy concerns also relevant here, but the environments in which mobile media
are used are also extremely privacy-sensitive. Additionally, the technology
provides a privacy challenge through its technical affordances. The basic challenge
is the combination of person, location, and activities ‚Äì both those that are observ-
able from the outside and those that are conducted technologically.




M. Hartmann (*)
Berlin University of Arts, Berlin, Germany
e-mail: hartmann@udk-berlin.de

S. Trepte and L. Reinecke (eds.), Privacy Online,                                         191
DOI 10.1007/978-3-642-21521-6_14, # Springer-Verlag Berlin Heidelberg 2011
192                                                                       M. Hartmann


   Three examples may well illustrate privacy issues in mobile contexts: (1) An
organization can track an employee who uses a smartphone using a location-based
service. However, this employee might not want the employer to know where he/
she is at that moment. (2) A car rental company uses the GPS system to track their
cars and can thereby charge users in case of contract infringements (cf. Ardagna
et al. 2008, p. 308). (3) In the near future, people might use their smartphones to
identify a stranger in the street with facial recognition software. All of these
examples are based on the combination of privacy and mobility.
   The following chapter will focus on the combination of person, location, and
activities and the implied challenges therein. In the context of this contribution, the
term used to describe this complex set of relations is called ‚Äúmobile privacy.‚Äù In
many ways, the chapter already offers a conclusion at the very beginning: it begins
with the assumption that the combination of privacy and mobility is potentially
problematic. Calling it problematic and a challenge hints at a starting point that
states that there is still something to protect when we speak of privacy (in contrast
to, for example, the view of Google‚Äôs CEO Schmidt, who does not support
anonymity, thus underlining that Google knows where we are and also what we
think). The view that mobile privacy is a potentially problematic combination will
also not change during the course of the chapter. Instead, the aim is to substantiate
the claim and come up with a more differentiated definition of ‚Äúmobile privacy‚Äù at
the end.
   Two perspectives might be particularly helpful in defining mobile privacy and to
explaining its meaning further. The first perspective deals with technological
research, the second with a philosophical approach to the contexts of privacy.
Whereas the technological studies elaborate on the factual problems of design
and privacy settings, the context perspective elaborates on its meaning for the
users. Technological research has shown, for example, that users tend to look at
the information receiver, the information usage, and the information sensitivity
(Beckwith 2003). However, users generally seem to lag behind technological
developments. Very often they are not able (or not willing to invest the time) to
either find the right information to answer these questions or to accommodate their
privacy settings to satisfy their needs. Therefore, some technological studies sug-
gest technological answers to these problems (based on the idea of protecting the
user). The perspective of context adds to this point of view that privacy needs are
defined alongside with expectations and user interpretations (Nissenbaum 2010).
Privacy is dynamic and constantly balanced by its users. To understand mobile
privacy, both perspectives should be considered: the technological solutions and the
more general explanations.
   This chapter will begin with a very brief introduction to both mobility and
privacy as theoretical and empirical concepts (Sect. 14.2). We then move on to
technological research (Sect. 14.3) and to philosophical research about the contexts
of (mobile) privacy (Sect. 14.4). Finally, we will summarize these findings in a
definition of mobile privacies (Sect. 14.5).
14    Mobile Privacy: Contexts                                                                    193


14.2       Mobility and Privacy

The background to a concept such as mobile privacy includes two obvious
references: mobility and privacy. They both appear to be moving in different
directions though: mobility is supposedly on the rise and there is talk of a ‚Äúmobility
turn‚Äù (see e.g., Urry 2007, p. 6), while privacy is supposedly diminishing (some
even say it is heavily threatened (Privacy International 2007)). Both are complex
constructs, but they also operate on different levels. Privacy is the more philosophi-
cal and therefore debatable, while mobility (depending on the definition) can at
least partly be ‚Äúmeasured.‚Äù




14.2.1 Privacy

We will now briefly turn to basic definitions of privacy, since many of these are also
helpful in defining current challenges. Privacy is not a topic that needs to be
introduced in great detail within the context of this book (see e.g., Margulis‚Äô chapter
in this volume). Warren and Brandeis famously defined privacy as ‚Äúthe right to be
let alone‚Äù (Warren and Brandeis 1890). This is relevant for our purpose, since it
underlines privacy‚Äôs physical nature, i.e., both place and space are important in this
definition. Warren and Brandeis (1890), as legal experts, also managed to extend
this idea beyond the material into more abstract realms:
     These considerations lead to the conclusion that the protection afforded to thoughts,
     sentiments, and emotions, expressed through the medium of writing or of the arts, so far
     as it consists in preventing publication, is merely an instance of the enforcement of the more
     general right of the individual to be let alone. . . . The principle which protects personal
     writings and all other personal productions . . . but against publication in any form, is in
     reality not the principle of private property, but that of an inviolate personality. (p. 5)

   This combination of physical privacy with more abstract, informational forms
can thus already be found in this early approach. It the basis for the combination of
privacy and mobility. The informational form becomes even more prominent in
another often quoted reference by Professor (Emeritus) of Public Law and Govern-
ment Alan Westin, who summarized privacy as the ability for people to determine
for themselves ‚Äúwhen, how, and to what extent, information about them is
communicated to others‚Äù (1968). To extend Westin a little further, the question
of the public should also be considered when thinking about privacy‚Äìor rather, the
questions of ‚Äúwhere‚Äù and ‚Äúin which situation‚Äù should be added to his list. Or, as
then Privacy Commissioner of Canada, George Radwandski, stated, privacy is ‚Äúthe
right to control access to one‚Äôs person and to personal information about oneself‚Äù
(2002). Part of that would potentially imply that consent is an important aspect
within privacy control (see below).
   Partly because of the question of public and private space as well as public and
private behaviors, these basic ideas on privacy have also been transferred into the
194                                                                            M. Hartmann


broader context of the question of public life and the necessity thereof for function-
ing democracies (Wei√ü and Groebel 2002). The quote by the political theorist
Hannah Arendt (1989) is a case in point here:
   The four walls of one‚Äôs private property offer the only reliable hiding place from the
   common public world, not only from the common public world, not only from everything
   that goes on in it but also from its very publicity, from being seen and being heard.
   A life spent entirely in public, in the presence of others, becomes, as we should say,
   shallow. (p. 71)

   Arendt‚Äôs (1989) approach points to the fact that possibilities for a retreat are a
basis for our participation in public life. At the same time, her statement is not
necessarily fitting anymore: today, these retreats are not necessarily the four walls
of one‚Äôs private property. The home remains relevant, but it has become more
mobile, and for some people mobile media are even becoming home. Here, too,
privacy and mobility are closely related.
   These brief privacy references point to two aspects that will be of importance for
the definition of mobile privacies (cf. Sect. 14.5). Firstly, privacy is closely related
to publicness and they both depend on each other; secondly, privacy is related to
both physical and locational questions as well as to informational ones.




14.2.2 Mobility

The other important aspect in ‚Äúmobile privacy‚Äù is obviously the question of
mobility. The last few years have seen a great rise in interest in the concept of
mobility. Its theorizations are blossoming, as are empirical studies with rather
diverse foci (tourism, work, etc.). In communication studies, the interest has mainly
been fuelled by the increasing emergence of mobile media (see e.g., Green and
Haddon 2009). Beginning with mobile phones, which were initially researched as
an additional form of mediated interpersonal communication, further applications
(such as SMS, then the camera, etc.) soon underlined that the use of these media
was not only broadening the media scope, but was also extending the range of
environments that was suffused with (mainly individual) media use. Since laptops
have become more common (and now smartphones, ipads, etc.), both places as well
as environments and hence also forms of use have diversified.
   Why is all this relevant in this context? With more mobility, different privacy
concerns emerge in different environments. As convergent media, the items
referred to above offer such a range of interactions with both people and content
that using them in diverse public places and while ‚Äúon the road‚Äù necessarily creates
challenges. Mobility is also not a closed and stable entity. Instead, mobility itself is
seen to be rather diverse.
   The first rough differentiation between types of mobilities is between physical/
material, symbolic/informational, and social mobility. John Urry (2002)
14   Mobile Privacy: Contexts                                                    195


differentiates even further and suggests five forms of mobility that he emphasizes as
being interdependent:
‚Ä¢ Corporeal travel of people for work, leisure, family life, pleasure, migration and
  escape
‚Ä¢ Physical movement of objects delivered to producers, consumers and retailers
‚Ä¢ Imaginative travel elsewhere through images of places and peoples upon
  TV (. . .)
‚Ä¢ Virtual travel often in real time on the internet so transcending geographical and
  social distance; as Microsoft asks: ‚Äúwhere do you want to go today?‚Äù
‚Ä¢ Communicative travel through person-to-person messages via letters, telephone,
  fax and mobile (p. 1)
   For the purposes of defining ‚Äúmobile privacy,‚Äù we will follow Urry‚Äôs (2002)
lead. We will use the details of Urry‚Äôs differentiation and apply them to the mobile
privacy definition. The easiest connection between these mobilities and privacy is
the right to privacy as a pre-condition for public life, to which the mobile context
simply adds an additional emphasis. Somewhat more complex is the right to
privacy as something that needs to be created and sustained. Thus, in mobile
contexts, different kinds of privacy might be observable and necessary depending
on the movement and the related location. Hence, in the following, we will refer to
‚Äúmobile privacies‚Äù with the aim of underlining that there might be different forms
of privacies in mobile contexts. Taking Urry‚Äôs (2002) interdependent forms of
mobility into account and assuming that they are all relevant to the mobile privacy
question, it becomes clear that privacies are related not only to people, but also
increasingly to objects and applications. The combination of privacy as both
physical and informational is in many ways the basic description of mobility. The
concept of mobilities ‚Äì especially in Urry‚Äôs (2002) terms ‚Äì therefore helps to
underline that a set of mobile privacies exists or might need to be created. Below,
we will begin to fill this set with some examples.
   Let us now turn to two fields of research ‚Äì technological and philosophical ‚Äì in
terms of mobile privacy.



14.3     A Technology Perspective on Mobile Privacy

A first ‚Äì and most general ‚Äì technological answer to mobile privacy can be found in
the request for comments (rfc) concerning Mobile IPv6 (Perkins et al. 2010). IPv6 is
a protocol that allows nodes to remain reachable while moving around in the IPv6
Internet. The authors suggest that the technical architecture of this protocol should
have privacy as the core concern, since users will not usually take care of privacy
themselves: ‚ÄúThe reason is that most users will not change defaults, and the default
be one of privacy, only moving away from it by customer choice‚Äù (Perkins et al.
2010). The important technical issue is to keep the default at the highest privacy
level rather than the other way round (‚Äúopt-in‚Äù instead of ‚Äúopt-out,‚Äù see also
196                                                                         M. Hartmann


Debatin‚Äôs chapter in this volume). This is in stark contrast to existing practices. At
the moment, one can observe quite a huge difference in the nature of engagement
with the privacy topic when comparing a set of diverse mobile providers. The
policies also tend to remain very abstract. Most importantly though, the policies all
expect the user to act. The default settings will not necessarily offer a satisfying
protection for the user. To ask for this to be handled differently generally implies a
change in the overall presumption about individuals‚Äô responsibilities. Perkins et al.
(2010), in line with their user protection strategy, also stress that connecting
movement and location data with other data should be avoided: ‚ÄúArchitectural
changes MUST avoid requiring exposing a mapping between any of a node‚Äôs
identifiers and IP addresses/locators to unknown observers.‚Äù
    The user is also at the heart of Richard Beckwith‚Äôs research (conducted at Intel
Research), which looked at privacy as a design issue (Beckwith 2003). He
underlines that some seemingly obvious design choices in the context of privacy
are not always the best choices. Unobtrusiveness, for example, is not always a good
choice if user awareness is required. This builds on the idea of the relevance of
context but also poses the question of consent. How should users provide informed
consent when they are not aware of something happening or when they do not
understand the full nature of the consequences of their consent? This is one of the
core problems of privacy in the mobile media context. Users are expected to be
experts concerning their own privacy issues (as with the settings) ‚Äì and to also
adjust these depending on the situation.
    According to Beckwith (2003), users tend to look at three aspects in particular:
(a) the information receiver, (b) the information usage, and (c) the information
sensitivity. Overall, this means that the user needs to know (a) who is involved, (b)
what the information will be used for and how this affects the user, and (c) how
sensitive the data is (Beckwith 2003). With this information at hand, users can
better judge their privacy needs in a given situation. The user problem of needing to
actively engage would remain prevalent here though.
    However, some of the research presented here emphasizes that the users are not
alone in their need to differentiate and act, but rather that the technology can at least
partly take over this differentiation (as in Perkins et al. (2010)). This can be
demonstrated with the work of Ardagna et al. (2008). They understand location
privacy as ‚Äúthe right of the users to decide how, when, and for which purposes their
location information could be released to other counterparts‚Äù (p. 313). Ardagna
et al. (2008) differentiate between three different types of location privacy: (a)
identity privacy (‚Äúthe main goal is to protect users‚Äô identities associated with or
inferable from location information‚Äù); (b) position privacy (‚Äúthe main goal is to
perturb users locations as a way to protect their actual positions‚Äù); and (c) path
privacy (‚Äúthe main goal is to protect the privacy of those users that are continuously
monitored during a certain period of time‚Äù). Ardagna et al. then show different
technological solutions for all three types of location privacy. These are of particu-
lar interest for us, since they offer solutions most concretely in the mobile context.
Furthermore, the technological answers are indirectly philosophical as well: they
14   Mobile Privacy: Contexts                                                      197


state that users are key to the privacy problem, but mostly in the sense that they need
the technology to support them (rather than threaten their privacy).
   These technological answers include anonymity but also partial identification.
Overall, the idea of a decrease in the accuracy of personal information bound to
identities is widely seen as useful (e.g., in something such as ‚Äúless-than-optimal
location tracking‚Äù). Depending on which kind of location privacy is meant, differ-
ent versions of this apply. To a non-technical mind, ‚Äúless-than-optimal‚Äù sounds
potentially problematic, but it appears to be a good technological solution. As these
references also indicate though, most solutions thus far are technical
approximations and still require the other players (users, providers, the legal
framework, etc.) to ‚Äúplay along.‚Äù
   One more provocative idea in this context is mentioned by Varun Singh (who
also offers other possibilities for differentiations that are not treated here ‚Äì see
Singh 2008). Singh mentions the need for plausible deniability, which ‚Äúallows users
to customize their context data, in situations when the users wish to hide or fake
their identity‚Äù (Singh 2008, p. 6). Singh thereby adds the possibility for a more
active take on the overall range of possibilities, re-adjusting our focus back to the
active user, but also to the generally accepted rules. It is not a solution for user
inactivity, but instead offers a more social (and playful) version of the ‚Äúless-than-
optimal‚Äù (as an ‚Äúother-than-accurate‚Äù version). As a ‚Äútechnological‚Äù solution, this
seems unusual but useful.
   Overall, in our technological takes on privacy and mobility, we find an emphasis
on the provision of opt-in rather than opt-out mechanisms (now extended to ‚Äúother-
than-accurate‚Äù and ‚Äúplausible deniability‚Äù); we find the who/what/how-questions as
a possible differentiation as well as different location privacies (identity, position,
path). This will be picked up again below. For the moment we ask what they have in
common. First of all, they show the range of differentiations necessary to assess the
privacy needs and privacy solutions in a given situation. This already implies the
question of context, which will be discussed in the next section. They also begin and
end with the user and implicitly (and explicitly) emphasize that the user needs to be
protected (rather than protection for the freedom to aggregate data, for example).
We also see that technological solutions to privacy problems are possible ‚Äì plus
these solutions also tend to imply philosophical answers. In the question of context,
we can find both technological and philosophical answers.



14.4     (Mobile) Privacies in Context

One issue that can be found in more than one research field in relation to privacy is
the question of context (for an example from the technological field, see Singh
2008). Singh (2008), following Dey (2001), defines context as ‚Äú[. . .] a set of
suitable environmental states and settings concerning a user, which are relevant
for a situation sensitive application in the process of adapting the services and
information offered to the user‚Äù (p. 1). As shown above, this underlines the need for
198                                                                                     M. Hartmann


specific solutions in different contexts (place, time, personal needs, etc. all play a
role in this) rather than ‚Äúone-for-all.‚Äù
   A somewhat broader and therefore very helpful recent book was not without
reason entitled ‚ÄúPrivacy in Context‚Äù (Nissenbaum 2010). In this book, Helen
Nissenbaum (a philosopher with an interest in technologies) develops the useful
concept of ‚Äúcontextual integrity.‚Äù She defines the right to privacy as
   a right to live in a world in which our expectations about the flow of personal information
   are, for the most part, met. [. . .] achieved through the harmonious balance of social rules, or
   norms, with both local and general values, ends and purposes. (Nissenbaum 2010, p. 231)

    What her concept captures well is not only the dynamic nature of privacy, but
also a reliance on expectations. The user, as I have called the person desiring
privacy of some sort or another, is key here. This user sometimes balances the
risks and benefits and then decides what to do. Such a rational, thoughtful approach
is, however, not always possible or not always the case. The user tends to show a
gap between perception and action (see also Debatin‚Äôs chapter in this volume). Not
surprisingly then, many of the technological solutions are concerned with easing
users‚Äô possibilities to determine their level of privacy in given contexts and adding
responsibilities to the list of the technology developers and providers (expectations
need to be met). Most of the approaches mentioned so far also emphasize the
context-related nature of these issues. At the same time, they underline the problem
that users should ideally make many informed decisions about their privacy settings
in diverse contexts all the time.
    What does Nissenbaum (2010) contribute to this problematic? She defines
contexts ‚Äì with an obvious parallel to her privacy definition ‚Äì as ‚Äústructured social
settings characterized by canonical activities, roles, relationships, power structures,
norms (or rules), and internal values (goals, ends, purposes)‚Äù (2010, p. 132). She
refers to a range of existing social theories (e.g., Bourdieu 1984) and current
phenomena (e.g., Google Maps‚Äô Street View). This emphasis is clearly useful: (a)
as a reminder that this kind of approach does not necessarily have to start from
scratch, and (b) because it stresses that contexts are only specific in the given
situation. All the aspects mentioned by Nissenbaum (2010), such as roles,
relationships, and norms, may play a role in how users define privacy in given
contexts. When we take Nissenbaum‚Äôs (2010) definition and compare it to some of
those discussed as part of the technological approaches (Beckwith 2003), however,
a problem emerges. While the context of privacy is surely as complex and contin-
gent as Nissenbaum (2010) hints, mobile privacies need to be defined more rigidly ‚Äì
rigid in the sense of ‚Äútranslatable into technological solutions.‚Äù What Nissenbaum
provides is a broad philosophical debate. What the technological debates underline
is that the other approach ‚Äì defining a problematic context and then discussing what
is at stake and how it can be solved ‚Äì is sometimes more productive.
    Nissenbaum (2010) does not particularly address the specificities of the mobile
context either. Only in (briefly) referring to work of one of her former PhD students,
Michael Zimmer, does mobility shine through. Zimmer (2007, in Nissenbaum
2010, pp. 198‚Äì199) created the concept of ‚Äúspheres of mobility.‚Äù While airports
14   Mobile Privacy: Contexts                                                      199


might be one such sphere, a web search might be another (ibid.), i.e., his spheres
cover different versions of Urry‚Äôs (2002) mobilities. A possible combination of
airport and websphere (i.e., of corporeal and virtual mobilities), however, is not
addressed. The dominance of individual autonomy that Zimmer (in Nissenbaum
2010) proclaims for these spheres of mobility is also rather debatable in the context
of mobile privacies and privacy in general.
   Another version of context can be found in danah boyd‚Äôs (2010) work, which
claims that ‚Äúprivacy and publicity‚Äù is not ‚Äúa black-or-white attribute for content,
when really it‚Äôs defined by context and the implications of what we‚Äôve chosen to
share‚Äù (boyd 2010). She additionally raises the question of (private) material in
public places, which might have consciously been put there, but is not necessarily
meant to be aggregated. Data aggregation is potentially one of the most contested
areas at the moment. If the industry‚Äôs aim were indeed ‚Äúto unite information on the
customer‚Äôs age, gender, web-browsing habits, home address and buying patterns
with a record of their daily movements, and subject that to behavioral analysis
techniques‚Äù (Warren 2009), then one important aspect of mobile privacies would be
exactly to prevent the different data sets merging. boyd (2010) also emphasizes that
public availability is still different to widespread publication. Here, context is more
virtual than we had implied thus far ‚Äì and more content-related ‚Äì but this, too, is an
important aspect.
   Overall, Nissenbaum‚Äôs (2010) emphasis on context is useful. Her approach,
however, shows that such an emphasis can also lead to a loss of the specific nature
of privacy definitions necessary for acting on them. boyd‚Äôs (2010) approach seems
too limited for our purpose. Nissenbaum‚Äôs (2010) statement, for example, that
certain privacy intrusions in airport settings tend to be acceptable exactly because
they take place in airports (Nissenbaum 2010, p. 198), on the other hand, seems to
open the doors to a relativity claim. This kind of relativity and generalization is
something that most of the technological approaches mentioned previously (cf.
Sect. 14.3) would potentially want to work against. It also underlines that the
combination of the individual and the social (which the context often is) does not
necessarily simplify matters. Nonetheless we hold on to Nissenbaum‚Äôs (2010) idea
of context and contextual integrity as crucial for privacy and to her hint at what
factors tend to play a role in such contexts.



14.5     Mobile Privacy

We began with the idea that mobility and privacy are interlinked but provide at least
a double challenge: not only do we have to deal with the problems that digital
privacy already faces, but we additionally have locality issues to deal with. How the
concept of mobile privacy should be defined was not yet clear. In many ways, this
double bind problematic was already present in early privacy definitions (see
Warren and Brandeis 1890) ‚Äì but not necessarily as a clear-cut combination or in
any sense thought through as mobility. Our journey through different sets of
200                                                                        M. Hartmann


literature provided technological as well as philosophical ideas and also repeatedly
the hint at differences in user behavior and attitudes. This was the first step into a
rather complex set of differentiations on both very basic, but also somewhat specific
levels. We will now look at three different possible combinations that could be
useful for a more detailed definition of mobile privacy. A further study would have
to apply these empirically and further develop and test them.
    Let us begin with the most basic observation made in Sect. 14.2: one point that
emerged during the research was that in defining privacy, it makes sense to also
include the definitions of publicness (this includes both public space and the public
sphere). This would be the basis for a spectrum of privacies, offering a range
between publicness and privacy. Another axis that could be added would be the
immobility-mobility axis. This would begin to turn the spectrum into a very simple
matrix (a mobility-privacy matrix). In this model, the core would not be to limit the
definition of mobile privacy to a set of only two primary aspects, but to enable both
users and researchers to locate different actions, situations, and contexts on this
spectrum/matrix and thereby enable awareness of different variations and
implications. It does emphasize the importance of context. The limitations of this
spectrum/matrix idea lie clearly in the problematic nature of defining the ends of the
scale or rather in the limitation of these to one dimension. Plus it assumes that both
privacy/publicness and mobility/immobility are already ‚Äúmeasurable.‚Äù But what
about someone sitting in a cafeÃÅ using a laptop to access certain Social Web
applications ‚Äì is this person immobile or semi-mobile? Is the location on the
scale dependent on the physical or informational level? The same applies to
questions of privacy and publicness in this context. This spectrum/matrix then is
rather an additional illustration tool in the context of more refined definitions as for
instance Nissenbaum‚Äôs (2010) idea of contextual integrity.
    The second possibility is structurally similar to the first, but brings different
aspects to the forefront. It combines the who/what/how questions with the location
privacies (identity/position/path or who/where/when), both of which were
addressed in Sect. 14.3. This then leads to another seemingly simple matrix of
who/what/how/where/when of mobile privacy (or rather mobile privacies, cf.
Sect. 14.2.2). With regard to content, these are the questions that users should be
aware of as sensitive data issues, while technologically, the different nodes related
to each question should not be connected with each other identifiably (not even
between two of the five nodes). Temporarily, connections will always be necessary,
but a common technological answer to this problem seems to be (amongst others)
the question of scale. Hence the where and when can be located technologically
without being too specific (less-than-optimal).
    The second matrix matches well with the first spectrum/matrix suggested above.
It provides a set of questions that help to locate nodes on the spectrum/matrix.
Nonetheless, this second matrix by far also does not match the complexity of a
mobile privacies definition.
    We therefore tried a third option. This was in fact a pursuit of the obvious (with
the caveat of being work-in-progress): what about the simple combination of Urry‚Äôs
(2002) mobilities with different privacies? We already saw a parallel in the
14   Mobile Privacy: Contexts                                                     201


differentiation between physical and symbolic/informational mobilities and the
same distinction in terms of privacy. We will therefore use Urry‚Äôs (2002) five
mobilities and map the privacy issues discussed in this contribution onto those:
(a) Corporeal privacy as the privacy of the body in its movement but also its
    attributes (traveling where and for what purposes, meeting with whom, etc.).
    This builds on Ardagna et al.‚Äôs (2008) location privacies as well as
    Nissenbaum‚Äôs (2010) contextual integrity.
(b) Physical privacy as the privacy of objects, but also of consumption habits, etc.,
    i.e., the combination of the corporeal with other objects. Physical privacy builds
    on the same references as the corporeal privacy, but additionally emphasizes
    that not only the person in question, but also related objects, are part of the
    privacy problematic.
(c) Imaginative privacy as a protection of thoughts and imaginations, i.e., of
    playfulness, of media use, of other kinds of ‚Äúescape.‚Äù In terms of the imagina-
    tion, Singh (2008), with his emphasis on plausible deniability, is the obvious
    reference. Additionally, suggestions from the social media field (see other
    contributions in this volume) are relevant for not only the imaginative, but
    also the virtual and communicative privacy.
(d) Virtual privacy as a protection again of online data both in terms of content but
    also connections (with whom, where, when, what). Virtual privacy reflects
    most of the questions that were asked in the technological field (cf.
    Sect. 14.3). Answers can again be found in Ardagna‚Äôs et al. (2008) identity,
    position, and path privacies as well as Singh‚Äôs (2008) playful identities.
(e) Communicative privacy as protection related to interpersonal communication
    on every level and with every medium. Here, too, the basis builds on techno-
    logical questions and answers. The ‚Äúopt-in‚Äù mechanism suggested by Perkins
    et al. (2010) is particularly important here, as is any technological aid in
    obscuring the data (any feasible ‚Äúless-than-optimal‚Äù solution) and Beckwith‚Äôs
    (2003), Singh‚Äôs (2008), and Nissenbaum‚Äôs (2010) context awareness.
   This simple adaptation shows how most of the issues discussed above map
nicely onto this range of mobile privacies. In terms of the questions that need to
be answered and the ranges of privacy/publicness and mobility/immobility, we are
finding parallels to the spectrum and matrices developed above. An empirical
mapping and layering of this range of mobile privacies would be a logical next
step. This would, as we have seen from the technological answers, at the same time
provide useful insights into the applicability of the theoretical concepts implied.
   One question that has not yet been explicitly addressed by the set of mobile
privacies above is the question of context. In my view, however, it is the combina-
tion of the different privacies that makes up the different contexts, i.e., it becomes
interesting when, for example, corporeal and virtual privacy meet (as in the cafeÃÅ
example mentioned earlier). Contexts are therefore different combinations of these
mobile privacies.
   The privacy of the body as well as of objects is thereby covered as much as
different forms of information and communication (as well as imagination ‚Äì quite
202                                                                             M. Hartmann


an important aspect). In all of them, mobility is implied. Rather than differentiating
between the two different sets of privacy and mobility such as in the spectrum/
matrix idea above, here the two aspects are combined into one. It is a set of
attributes that still needs to be refined further. But it does (hopefully) underline
one of the main points raised in all of the above: that mobile privacy is in fact a set
of mobile privacies, and that context is the crucial basis for any informed debate on
privacy ‚Äúon the move.‚Äù Whether the above is enough to speak of a situation-driven
contextual integrity (the ideal) is too early to say.
    One other final point: it is important to stress ‚Äì as actually all of the approaches
mentioned have done ‚Äì that users and their individual, situational needs are the core
of any privacy definition. Nonetheless, asking users about their definitions of
privacy (rather than their actions) and comparing these definitions to our theoretical
ideas could provide an additional insight. Moving even further into the normative,
one could say that users should be involved in the process of developing these
technological solutions. On the policy level, users might need to be trained to be
more aware of the potential consequences of their actions. And companies might
need to be forced to move from ‚Äúopt-out‚Äù to ‚Äúopt-in‚Äù instead. Plus clear-cut
explanations about what happens with location-based data and about who is using
the data should be the norm. Furthermore, the combination of location-based with
other data should be made difficult (apart from the basic technological needs). First
of all though, the model developed above should be refined, and tested in liaison
with technology developers.
    Having thought more about technology than usual, one is sometimes tempted to
retreat to a totally different scenario instead: the establishment of ‚Äúno data zones,‚Äù
i.e., zones that are protected from any kind of electronic signal ‚Äì either going in or
out. That does not mean, however, that privacy is not an issue there either. . .




References

Ardagna CA, Cremonini M, Damiani E, De Capitani di Vimercati S, Samarati P (2008) Privacy
   enhanced location services information. In: Acquisti A et al (eds) Digital privacy: theory,
   technologies, and practices. Auerbach, New York, pp 307‚Äì326
Arendt H (1989/1958) The human condition. The University of Chicago Press, Chicago
Beckwith R (2003) The human experience. Designing for ubiquity: the perception of privacy.
   Pervasive Comput 2(2):40‚Äì46
Bourdieu P (1984) Distinction: a social critique of the judgement of taste. Harvard University
   Press, Harvard
boyd d (2010) Making sense of privacy and publicity. Talk presented at SXSW, Austin, 13 Mar.
   http://www.danah.org/papers/talks/2010/SXSW2010.html. Accessed 6 Dec 2010
Dey AK (2001) Understanding and using context. Pers Ubiquit Comput 5(1):4‚Äì7
Green N, Haddon L (2009) Mobile communications: an introduction to new media. Berg, Oxford/
   New York
Nissenbaum H (2010) Privacy in context: technology, policy, and the integrity of social life.
   Stanford University Press, Palo Alto
14   Mobile Privacy: Contexts                                                               203

Perkins C, Johnson D, Arkko J (2010) Mobility support in IPv6. http://tools.ietf.org/html/draft-
   ietf-mext-rfc3775bis-08. Accessed 10 Jan 2011
Privacy International (2007) The 2007 international privacy ranking. http://www.privacyinter
   national.org/article.shtml?cmd%5B347%5D¬ºx-347-559597. Accessed 6 Dec 2010
Radwanski G (2002) Letter to the honourable Roy Romanow. http://www.priv.gc.ca/media/
   le_ehr_020627_e.cfm. Accessed 10 Jan 2011
Singh V (2008) Context-awareness: control over disclosure and privacy in a social environment.
   TKK technical reports in computer science and engineering, Helsinki University of Technol-
   ogy Department of Computer Science and Engineering. http://www.cse.tkk.fi/en/publications/
   B/1/papers/VSingh_final.pdf. Accessed 10 Jan 2011
Urry J (2002) Mobility and connections. Unpublished paper. Paris, Apr 2002. http://www.ville-en-
   mouvement.com/telechargement/040602/mobility.pdf. Accessed 10 Jan 2011
Urry J (2007) Mobilities. Polity, Cambridge
Warren P (2009) The end of privacy? The guardian, 2 Apr 2009. http://www.guardian.co.uk/
   technology/2009/apr/02/google-privacy-mobile-phone-industry. Accessed 29 Jan 2011
Warren SD, Brandeis LD (1890) The right to privacy. Harv Law Rev 4(5)
Wei√ü R, Groebel J (eds) (2002) Privatheit im ‚Ç¨   offentlichen Raum. Medienhandeln zwischen
   Individualisierung und Entgrenzung? Leske & Budrich, Opladen
Westin A (1968) Privacy and freedom. Atheneum, New York
Chapter 15
Online Privacy as a News Factor in Journalism

Wiebke Loosen




15.1    Privacy and Journalism ‚Äì A Paradox?

The meaning, value, and organization of privacy are associated with the cultural,
normative, and social disposition of a society. Therefore, the distinction and
relationship between private and public is in constant transformation (see e.g.,
Westin 2003, p. 434). It is these circumstances that make investigating privacy so
complex. The Italian democracy theorist Noberto Bobbio (1989, p. 1) has stressed
this, naming the public/private distinction the often cited ‚Äúgreat dichotomy‚Äù (in
political theory). In occidental thinking, it stands for the fundamental differentia-
tion between a public realm (including everything that is significant for a society
as a whole) and a private realm (including everything that is significant for
individuals or groups, e.g., the family) (Seubert 2010, p. 9), as well as for the
interfaces and ambivalences resulting from the oscillatory connections between
both ‚Äúpoles.‚Äù
   These diverse aspects and levels of privacy (e.g., Westin 2003 differentiates
between privacy at the political level, at the sociocultural and organizational level,
as well as at the personal/individual level) show that privacy is, and also needs to
be, a concept in a wide range of disciplines and of inquiry. Privacy has, for instance,
been described as an elastic concept, associated with a variety of meanings, of
multidimensional nature, relevant on micro-, meso- up to macro-theoretical levels,
which altogether lead to widely different and often wholly separate discourses on
privacy (for an overview see e.g., R‚Ç¨ossler 2005, p. 2; Burgoon 1982). Therefore, it
seems to be less productive or even counterproductive to ask for ‚Äúa unified, single




W. Loosen (*)
Hans-Bredow-Institute for Media Research at the University of Hamburg, Hamburg, Germany
e-mail: w.loosen@hans-bredow-institut.de

S. Trepte and L. Reinecke (eds.), Privacy Online,                                    205
DOI 10.1007/978-3-642-21521-6_15, # Springer-Verlag Berlin Heidelberg 2011
206                                                                           W. Loosen


account of privacy‚Äù (Paine et al. 2007, p. 526), as this necessarily needs to be
context-sensitive.
   In this chapter, on the one hand, a broad and not prematurely restricted charac-
terization of privacy and privacy-related topics is needed to provide compatibility
with journalism and journalism research. This requires an investigation of privacy
with respect to public and the public sphere. On the other hand, the attempt to
discuss privacy as a news factor requires a more precise characterization. There-
fore, at least a brief (and selective) revision of some of the several attempts to define
privacy as well as of the attempts to synthesize existing literature (e.g., Burgoon
1992) is helpful.
   Within psychological literature in particular, Westin‚Äôs (1967, 2003) and
Altman‚Äôs (e.g., 1975) theories of privacy are very prominent (for an overview see
Margulis 2003 or Margulis‚Äô chapter in this volume). Both take the idea of control
over (the access to) specific areas of privacy as a starting point. Westin especially
(1967, p. 7) focuses on information privacy by defining privacy as ‚Äúthe claim of
individuals, groups, or institutions to determine themselves when, how, and to what
extent information about them is communicated to others.‚Äù The (active) term
‚Äúclaim‚Äù already demonstrates that this demand can vary individually to a large
extent.
   This definition addresses an individual level of privacy, and therefore is of
limited (or of specific) use where journalism (and its function for society) is
concerned. Furthermore, for example, due to the characteristics of contents in
networked publics (persistence, replicability, scalability, and searchability; for a
detailed discussion see the chapter by Peter and Valkenburg in this volume),
individuals‚Äô control over information is limited per se. Nonetheless, the definitions
of Westin and Altman (and the underlying larger theories of privacy) prove the
fundamental role of privacy as a ‚Äúregulatory process by which a person (or group)
makes himself more or less accessible and open to others‚Äù (Altman 1977, p. 3) and
consequently for self-realization and individual development (Margulis 2003b).
This fundamental role may serve as one explanation for the general attraction to,
and the critical observation of privacy-sensitive contents in media and journalism as
well as for its ambiguity between voyeurism and liberation.
   In seeking to systematize the meaning of ‚Äúprivate‚Äù R‚Ç¨ossler (2005, p. 6) suggests
three basic types (simultaneously including the overlaps between them): (1) Private
modes of action and conduct (in public) (e.g., what clothes I wear on the street); (2)
Private knowledge (e.g., who I live with); (3) Private spaces (e.g., dwellings,
rooms). Furthermore, she distinguishes a spatial, naturalized meaning of ‚Äúprivate‚Äù
(everything that has its place in the sphere of the private household) as well as its
description in terms of dimensions of action and responsibility, and dimensions of
interest and concern. Against that background, she differentiates the dimensions of
privacy on a more abstract level as: (1) Decisional privacy (violations can be
defined as illicit interference in one‚Äôs actions, p. 79); (2) Informational privacy
(violations can be defined as illicit surveillance, p. 111); (3) Local privacy
(violations can be defined as illicit intrusions in rooms or dwellings, p. 142).
15   Online Privacy as a News Factor in Journalism                                207


    All of these definitions, meanings, and dimensions reveal the complex structure
of privacy as a concept. This complexity even increases when we try to relate
privacy to (mass) media and journalism. Nonetheless, the public/private relation-
ship in modern societies and the impacts influencing its transformation cannot be
described, discussed, and evaluated without the consideration of (mass) media, for
it is evident that privacy has changed under the influence of the developments of
electronic media in various ways (Papacharissi 2010; Meyrowitz 2002) that have
created a ‚Äúnew visibility‚Äù (Thompson 2005). One very self-evident phenomenon is
the mobile phone: it has established telephone conversations that are considered as
private matters in the public space (R‚Ç¨  ossler 2005, p. 171).
    Basically, it seems to be one of the ‚Äúconstants of media evolution‚Äù (Schmidt and
Zurstiege 2000, p. 206) that the discursive polarities of public and private have to
become a concern of a societal discourse (e.g., with regard to regulations, norms,
literacy, etc.) with the advent of every new medium and with every new media
technology.
    At first glance, privacy and (mass) media seem to be mutually exclusive: the
media provide public information, which therefore is not private but public by
definition. Nonetheless, private and intimate issues are to a large extent distributed
via media and are significant topics of public communication. Thus, privacy in the
media always has to be characterized as mediated privacy (Pundt 2008, p. 234),
which is produced by the media: ‚Äúthe difference between private and public within
the medium itself can clearly only ever be an apparent one ‚Äì the medium knows
privacy only as something publicized‚Äù (R‚Ç¨    ossler 2005, p. 175).
    This is true for traditional mass media as well as for the Internet and especially
for social media, as they virtually depend on self-disclosure and produce an
increasing availability of private information leading to an increased awareness
of privacy issues. The availability of private information distributed via social
media has introduced a recursive process of individual and mass communication.
The boundaries between public and private spheres are rearranged and redefined,
and this process of change inspires debates on the individual and societal meaning
of public and private spheres.
    This chapter addresses these aspects with a focus on journalism as a social
system and does this in a dual perspective. (1) The first perspective regards privacy
primarily as a heuristic for the challenges journalism has to face in a social media
environment. (2) The second perspective looks at the meaning of privacy and
privacy-related issues for journalism. Against that background, the chapter is
structured as follows: firstly, it looks at the relevance and meaning of the public/
private distinction in journalism (cf. Sect. 15.2). As social media and their reliance
on private information affect journalism in various ways, Sect. 15.3 explores the
(news-) worthiness of social media within journalism. In an initial attempt, privacy
is then considered as a news factor. This perspective requires a brief look into the
referring logic of news value research. (cf. Sect. 15.4). Finally, the question is
raised of the extent to which privacy and social media offer a chance for journalism
to keep (and get) in touch with its declining audiences (cf. Sect. 15.5).
208                                                                                        W. Loosen


15.2     The Distinction Between Public and Private in Journalism

   Whatever we know about our society, or indeed about the world in which we live, we know
   through the mass media. (Luhmann 2000, p. 1)

This is by far one of the most cited quotes from the German sociologist Niklas
Luhmann as far as mass media are concerned. Corresponding adaptations and
reformulations of Luhmann‚Äôs specification of systems theory on the mass media
in journalism research are based on the assumption that processes of self-observation
of society are mainly constituted by the profession of journalism. In a sociological
system theoretical perspective, journalism is defined as a social system that
   [. . .] operates on the basis of a generalized symbolic communication medium which can be
   called ‚Äòactuality‚Äô. This artificial term includes three dimensions: event-related facts (instead
   of fiction), relevant information (concerning all other function systems in society) and
   current issues (to facilitate the synchronization of society). (G‚Ç¨
                                                                    orke and Scholl 2006, p. 651)

   This definition illustrates that the journalistic observation of society, and there-
fore the ‚Äúperformance and provision of themes for public communication‚Äù (R‚Ç¨uhl
2008, p. 32), follows specific mechanisms to identify themes as newsworthy or not
Weischenberg (2007). It is self-evident that all of the above mentioned dimensions
(event-related facts, relevant information, current issues) are not ‚Äúnaturally‚Äù given,
but part of a journalistic construction of reality, and therefore observer-related and
relative.
   What does this abstract definition of journalism as a social system imply for the
distinction between public and private in journalism? With this constitutional
definition it becomes obvious that journalism provides public communication, a
public service for society, and therefore has a strong preference for the ‚Äúpublic side‚Äù
of the public/private distinction. As a consequence, more frequently public, the
public sphere or even the public opinion is the relevant object of inquiry. This does
not mean that privacy is irrelevant, but that it is predominantly defined negatively
with respect to public as not public (Pundt 2008, p. 231). In such a (public sphere
theoretical) perspective, privacy is more or less ‚Äúdesignated ‚Äòthe private home‚Äô or
the ‚Äòrealm of intimacy‚Äô and not further differentiated‚Äù (R‚Ç¨ossler 2005, p. 2).
   Furthermore, theories of the public sphere mostly focus on a critically diagnosed
decline of the public realm (as a discursive space for public discussion) through the
incursion of intimacy and private issues (e.g., Habermas 1992; Sennett 1977). The
opposite (feminist theoretical) perspective emphasizes the positive effects of a
public awareness for privacy-related issues. It is argued that such issues, which
are often simply qualified as trivial, and not worth a medial presentation nor public
discussion (e.g., home stories, intimate details, personal conflicts), not naturally
only need to serve (or be treated as) entertainment and voyeurism (Herrmann and
L‚Ç¨unenborg 2001).
   The predominant perspective in communication/media/journalism research is to
look (often with a media critical attitude) into the way privacy-sensitive issues are
dealt within the media (e.g., with reference to daily talk shows, reality TV) and by
15    Online Privacy as a News Factor in Journalism                                               209


journalism (e.g., sensationalized journalism, privatization, and emotionalization in
political news). This perspective reveals that what we see in the media is always a
medial construction of privacy, a media image of privacy, or rather, of issues
qualified (by a scientific, journalistic, or other observer) as privacy-related or
privacy-sensitive.
    It is exactly this medial construction of privacy that often elicits the question of
which private matters may be (or should be) revealed by journalists in the public
interest. This matter is intertwined with the role of journalism for and within a
society. Normatively, journalism has to balance public interest and the individual‚Äôs
interest. Due to the very fact that both of these interests are located on different
layers, in different realms they often conflict and raise different privacy concerns: in
one case the unit is seen as the individual (or groups, organizations that have an
interest in keeping information private or rather unpublished), and in the other case
it is seen as the society (that may have an interest in revelation). For both of them
the (self-)disclosure of private information has different meanings, relevance,
functions, and consequences. Therefore, the qualification of information as privacy-
sensitive is likely to differ from an individual and from a journalistic perspective.
    Consequently, the appropriate balance between the two interests ‚Äì individual
and public ‚Äì is an important object of inquiry in different contexts, for instance, law,
politics, ethics, media criticism, and journalism itself. As a result, in journalism,
and particularly in its critical (self-)evaluation, the public/private distinction is
(implicitly) used as a conceptual framework for demarcating the boundaries
between private or of public interest. This distinction is slightly different from
the public/private distinction as it implies that private issues can be of public
interest and therefore are an appropriate matter of journalistic revelation. Therefore,
in a journalistic perspective the first question is: newsworthy or not? In contrast, the
decision between private or of public interest is subordinated and only occasionally
relevant when it comes to deciding whether a public interest outweighs the
individual‚Äôs interest or not. Consequently, public or to be published is always
constructed in contradiction to something else. In journalism, this something else
can be private, but this is not necessarily the case for all situations. It is more likely
to be simply not newsworthy.
    Nevertheless, the coverage of private issues always had a strong newsworthiness in
journalism and in several cases the media came under sharp criticism for invading
privacy. This is especially true when celebrities, a certain kind of mainstream media,
and/or ‚Äúmediated scandals‚Äù (Thompson 2000) or criminal cases are concerned (Imhof
and Schulz 1998). In fact, the much cited article ‚Äúthe right to privacy‚Äù by the U.S.
American lawyers Samuel Warren and Louis Brandeis (1890) already had a media-
critical attitude and was written with regard to an increasingly diffused yellow press
and the development of new technologies in photography (Solove 2004, p. 57):
     The press is overstepping in every direction the obvious bounds of propriety and of
     decency. Gossip is no longer the resource of the idle and of the vicious, but has become
     a trade, which is pursued with industry as well as effrontery. To satisfy a prurient taste the
     details of sexual relations are spread broadcast in the columns of the daily papers. To
210                                                                             W. Loosen

   occupy the indolent, column upon column is filled with idle gossip, which can only be
   procured by intrusion upon the domestic circle. (Warren and Brandeis 1890)

   Since then, privacy has become commoditised and part of the deal between the
(boulevard) media and public figures. This is expressed by the phrase ‚ÄúJanus face of
prominence‚Äù (Schneider 2004), since on the one hand celebrities often lament their
loss of privacy, but on the other hand stage their private life within the media to
preserve their public status.




15.3    The (News-) Worthiness of Social Media Within
        Journalism

All of the above-mentioned examples show that from a normative perspective,
journalism has to take care of privacy concerns. Therefore, privacy concerns were
not raised for the first time when the Internet allowed private persons to distribute
photographs and information about themselves (and others), or with the (scientific
and societal) discourse of online privacy. However, private information distributed
via social media does make a difference, because the creators of such information
may regard it as (semi-)private, whereas journalists may regard it as public (Whittle
and Cooper 2009, p. 2). As a consequence, the perception of privacy, as well as the
way it is dealt within the media and in journalism, is undergoing a fundamental
change. Thus, privacy per se is not a new issue for journalism but its growing
salience is as social media and privacy issues change and challenge longstanding
journalistic conventions about what counts as news.
   To illustrate this idea, within the next paragraphs, privacy is primarily used as a
heuristic for the challenges journalism has to face in a social media environment.
All of these challenges can be boiled down to the changing nature of the relation-
ship journalism has to its (former) audience.
   With its expansion and institutionalization, the Internet has become not only an
alternative for news production and consumption (Mitchelstein and Boczkowski
2009), but has changed the conditions of (public) communication to a large extent.
Journalism as a genuine media phenomenon is strongly affected by this change as
the Internet is threatening the traditional basis, role, and funding of journalism.
   One of the most frequently postulated observations with respect to the new forms
of public communication in an online media environment is the changing nature of
the sender/receiver relationship and the loss of journalism‚Äôs gatekeeper monopoly
(Bruns 2005). Traditionally, this monopoly is based on a business model with
‚Äúnews‚Äù as a marketable product. It is endangered by the Internet (and a major
economic crisis), and in particular by behavioral changes of audiences and
advertisers induced by new technologies (Downie and Schudson 2009). Further-
more, in a social media environment, the asymmetry between professional
journalists and audience can no longer be maintained and loses its separation effect.
Additionally, user-generated content (that may also contain privacy-sensitive
15   Online Privacy as a News Factor in Journalism                                 211


issues) has become a relevant source for journalism, and participatory journalism
practices strongly affect newsroom routines (see e.g. Singer 2010; Domingo et al.
2008; Thurman 2008).
   Therefore, journalism has to deal with both the restriction of journalism‚Äôs ability
to include the audience and the increasing demands for inclusion of the audience.
This situation exemplifies the (news-) worthiness social media has for journalism: a
chance to keep in touch with declining audiences as this former audience has to
some extent turned into (inter-)active users and producers.
   Journalists ‚Äúwho once controlled the space containing their work now share that
space with website users‚Äù (Singer 2010, p. 127). Singer‚Äôs study (like others e.g.,
Neuberger et al. 2009) on perceived effects of user-generated content on newsroom
norms, values, and routines shows that journalists are in a dilemma between
doubting their professional autonomy and embracing the change by the more or
less deeper insight that they can no longer assume an attitude of passivity on the part
of their audience, which a lot of journalists see as a supplemental source and traffic
builder (Williams et al. 2010). Traffic to news and media websites increasingly
comes from social media such as ‚ÄúFacebook,‚Äù which reveals the role of social
media as disseminators of news (Purcell et al. 2010).
   Thus, within the Internet and social media, two principles interact insofar as
journalistic revelation and self-disclosure converge (or collide when it comes to
risks and opportunities of privacy) in a recursive process. Thus, contents originally
published for a ‚Äúpersonal public‚Äù (see Schmidt‚Äôs chapter in this volume) may
become journalistic sources and be noticed by a much larger audience via mass
medial publication. Therefore, journalism and mass media can be seen as a trigger
to publishing private information on the Internet via traditional mass media by
relocating the information from a privately public realm to a publicly private one.



15.4     Privacy as a News Factor

Looking at the previous sections, it may have become apparent that it is not trivial
to bring the concept of privacy together with journalism. The same is true for
privacy and news factors. Thus, the following explanations are understood as an
initial attempt or a first compilation of what becomes relevant when considering
privacy as a news factor and when discussing privacy-related news factors. This
requires at least a very brief insight into the logic of news factors in the referring
research field.
    Within news value research, news factors have been used in the attempt to
explain which particular characteristics attributed to an event lead journalists to
perceive and select contemporary events as newsworthy (Galtung and Ruge 1965).
The theory postulates a systematic and stable connection between the
characteristics attributable to an event (news factors) and the news value assigned
to the respective news item by journalists (Scheufele 2006). A lot of analyses in this
research field led to highly differentiated lists of news factors, which vary to a
212                                                                                 W. Loosen


greater or lesser degree (O‚ÄôNeill and Harcup 2009), even though in particular
relevance, damage/aggression/conflict, elite persons/prominence, continuity, prox-
imity, and elite nation could be shown to affect journalistic selection (Eilders 1997,
p. 58). Staab (1990) argues that news factors in news items are not to be regarded as
perceived (or even objective) event characteristics, but can rather be seen as a result
of journalistic attribution.
   Recent studies have increased the explanatory potential of news factors: Eilders
(1997, 2006) conceptualizes news factors as general relevance indicators in human
perception, serving not only as selection criteria in journalism, but also guiding
selection processes by the audience (see also Shoemaker 1996). Eilders also
stresses that the audience not only selects news according to collectively shared
relevance criteria ‚Äì as represented in the professional routines of journalism ‚Äì but
also according to individual interests and preferences (Eilders 2006, p. 10). This
perspective reveals the significant differentiation between individual relevance/
subjective importance and societal/collective relevance (Eilders 1997, p. 92;
Fretwurst 2008, p. 114).
   Selection processes always reduce complexity, and therefore, (journalistic)
‚Äúselection always also generates that other side of the products presented, that is,
the non-selection or the ‚Äòunmarked space‚Äô of the rest of the world.‚Äù (Luhmann
2000, p. 37). This ‚Äúrest of the world‚Äù exclusive from media coverage (which has
always been important when it comes to media criticism and to suspicions/evidence
of/for manipulation and biased information on the media) has become visible and
accessible to a much larger extent on the Internet and within social media, and in
turn relevant for journalism. As a result, this recursive process is about to change
the journalistic perception of newsworthiness and to redefine (at least to some
extent) news factors and values. The definition of news as what ‚Äúis judged to be
newsworthy by journalists [. . .]‚Äù (Harrison 2006, p. 13) seems to be increasingly
less true in a social media environment. As a consequence, it becomes even more
apparent what alternative and media critical approaches argue
   that journalists should be encouraged to counteract the prevailing news factors by, among
   other things: including more background and context in their reports; reporting more on
   long-term issues and less on ‚Äòevents‚Äô; paying more attention to complex and ambiguous
   issues; giving more coverage to non-elite people and nations. (O‚ÄôNeill and Harcup 2009,
   p. 170)

   In fact, this kind of critique on newspaper reporting was the primary concern of
Galtung and Ruge (1965), who proposed some alternative approaches to reporting
conflict. In a wider context, this critique describes the conventional motivating
factors behind the production of alternative media (Atton 2002, 2009). The emer-
gence of the Internet in particular gave rise to renewed hopes that issues, actors, and
arguments marginalized by the mainstream media get a broader public awareness
and the public sphere a shift towards participation (Gerhards and Sch‚Ç¨afer 2010).
   Professional news media and social media observe each other, refer to each
other, and use each other as sources. To a large extent, hyperlinks in the blogosphere
and on social news sites refer to journalistic content and traditional media (Reese
15   Online Privacy as a News Factor in Journalism                                  213


et al. 2007; Messner and Watson DiStaso 2008; R‚Ç¨olver and Alpar 2008;
Xenos 2008). For that reason, a certain kind of newsworthiness and certain news
factors also seem to be relevant within social media (Eilders et al. 2010). However,
one has to keep in mind that news factors are criteria developed for analyses of
news media. Thus, news factors were identified in classical investigations of the
structural characteristics of news media. Therefore, classical news factors are of
limited use when it comes to analyzing social media content. Therefore, the demand
for future research on ‚Äúblog values‚Äù (Xenos 2008, p. 501) is comprehensible.
    Nonetheless, privacy-related issues and self-disclosure have similarities with
traditionally highly important journalistic news factors such as personification,
elite persons/prominence, cultural proximity, social relevance, human touch, and
sex/erotic. All of them are privacy-related to a greater or lesser extent. The closeness
between privacy( related issues) and the news factor personification seems to be
most obvious as personification is a very common journalistic practice of illustrating
a topic by focusing on individuals ‚Äì a practice that is often used in journalistic
formats, for example, interviews, portraits, features, and documentations. This may
be realized in terms of exemplification (Zillmann and Brosius 2000), for instance, to
illustrate the consequences that a revised law has for a family used as a prototype in
such a context (Daschmann 2001). A stronger focus on the individual itself can be
given in cases of personal destinies or dramas. The collective effectiveness of
personification, as well as the other above-mentioned factors, can be interpreted
with general concepts of familiarity, identification, and parasocial interaction,
which, among other things, can explain the ‚Äúsurveillance function of news‚Äù (Shoe-
maker 1996; Eilders 2006, p. 14).
    For a more detailed look, we now come back to the definitions of privacy
(cf. Sect. 15.1). At first glance these definitions are of limited use when it comes
to defining privacy as a news factor: on the individual level they strongly refer to
privacy as ‚Äúa dialectic and optimizing process‚Äù (see Margulis, this volume, Chap. 2)
operationalized often with regard to privacy behavior and concerns of individuals
(see e.g., Paine et al. 2007). This perspective may be useful for comparing privacy
concerns and related attitudes of journalists and recipients as well as their
expectations (and expectation expectations) when privacy is concerned, for exam-
ple, with regard to the treatment of privacy-sensitive issues within the media. On
the societal level, the definitions of privacy discussed above often describe the
normativity, the value of privacy, for example, in terms of dimensions of responsi-
bility and interest.
    In this regard, it is worth noting that nothing is defined as private or as privacy-
sensitive ‚Äúby nature‚Äù. Things become even more complicated if we ask to what
extent privacy-related issues in media coverage are functional or dysfunctional with
respect to the role journalism has to fulfill for society.
    Keeping this in mind, one can relate the news factor logic almost directly to at
least two of the three dimensions R‚Ç¨   ossler (2006) differentiates (see Sect. 15.1):
‚Ä¢ Local privacy: Aspects concerning privacy-related issues in terms of private
  spaces (e.g., home stories, family relationships, intimate relations, daily life)
214                                                                           W. Loosen


‚Ä¢ Decisional privacy: Aspects concerning privacy-related issues in terms of
  actions and conduct (e.g., with whom a person meets in a restaurant, to which
  church one goes)
    The situation is different when it comes to informational privacy, which
represents the central dimension of privacy for a lot of theorists as it refers to the
control over (the access to) personal information (see Sect. 15.1). As a conse-
quence, this dimension is located on a different level of abstraction than the other
two dimensions. Informational privacy is less specific than local and decisional
privacy and may include information referring to the other dimensions as well.
Therefore, whenever (and if logically) possible, there must be a differentiation
between whether the referring issue is done/observed in a public or in a private
realm and whether it is covered by journalists or relies on self-disclosure (e.g., in an
interview). Overall, journalistic coverage can be differentiated with regard to its
varying degrees of exposure of privacy-related issues.
    This is of course not a ‚Äúready to go definition‚Äù for an empirical study as these
basic dimensions are neither mutually exclusive nor exhaustively defined so far.
Above all, it has now become obvious that there are a lot of privacy-related aspects
(e.g., journalistic research methods) that cannot be investigated via content analy-
sis, which is by far the dominant method in news value research.
    While this perspective focuses on content, another perspective that might be
worth looking at refers to the interconnections and recursive processes between
journalism and the social media. In this regard, privacy-related issues (which have
their seeds in audience participation/inclusion) may be important for credibility,
trust, authenticity (for the similarities and differences of privacy and authenticity
cf. Trepte and Reinecke‚Äôs chapter in this volume), and follow-up communication.
Potentially, they are able to moderate what is often described as the gap (as well as
the overlaps) between individual relevance structures and relevance structures of
journalism (see e.g., Deuze 2008).
    Thus, when one starts to think about ‚Äúnews factors 2.0,‚Äù as relevance indicators
for journalists who want to keep in touch with an audience in a social media
environment (and to stay, or rather, to get in touch with the everyday lives of
most people), it may come to
‚Ä¢ Actuality (including three dimensions: event-related facts, relevant information,
  current issues, see p. 2), which provides or rather stimulates
‚Ä¢ ‚ÄúTopics of/for conversation‚Äù in the sense of follow-up communication; and
  enables as well as integrates
‚Ä¢ References to the living environment in terms of individual relevance.
   Altogether this may ‚Äúlead to a journalism,‚Äù to cite the journalism researcher
Mark Deuze (who is paraphrasing the American communications theorist James
W. Carey here), ‚Äúas an amplifier of the conversation society has with itself‚Äù (2008,
p. 848). This understanding of journalism is not so far away from the ‚Äòtraditional‚Äô
definition provided by system theoretical journalism research (G‚Ç¨orke and Scholl
2006, p. 651, in the present paper p. 2) ‚Äì even though the means to facilitate the
15   Online Privacy as a News Factor in Journalism                                215


conversation society has with itself have changed totally. Against that background,
one can assume that audience participation within journalism could lead to an
amalgamation of individual and societal relevance structures and therefore to the
consideration of multiple perspectives on a news item. One consequence may be a
stronger representation of privacy-related issues as well.




15.5     Privacy: A Chance to Keep in Touch with the Audience?

Journalism has lost its gatekeeper monopoly. It used to be based on the asymmetry
between professional journalism and an audience, which is almost restricted to
selective use. This asymmetry can no longer be sustained. The Social Web offers
new forms of public communication ‚Äì and new modes of social participation for the
public with significant remarkable implications from a democratic theoretical per-
spective. There is no doubt that ‚Äúthe people formerly known as the audience‚Äù (Rosen
2006) are increasingly important for journalism ‚Äì and for journalism research. The
ongoing debates show quite high expectations for audience participation. It is
regarded as an advancement for democracy and as journalism‚Äôs chance to reconnect
with declining audiences. As a consequence, the interrelations and mutual
observations between a public sphere constructed by professional journalism and
‚Äúpersonal/private public spheres‚Äù (Schmidt 2009) within social media are increasing.
On the one hand, this may lead to a more commercialized and professionalized use of
social media that will decimate the niches of privacy within social media and
underpin the importance of privacy literacy (see Debatin, this volume, Chap. 5).
On the other hand, this may also lead to a sensible combination of issues of societal
relevance and of personal/private relevance. The outcome of those interfaces of
public and private may be regarded as ambivalent and reflect ‚Äúthe dual nature of
privacy as something that can be conceived as both liberating and alienating,
emancipative and repressive, beneficial and deleterious‚Äù (R‚Ç¨ossler 2005, p. 169).
    This dual nature of privacy is emblematic of journalism, as balancing
ambivalences is part of its identity between market orientation and social responsi-
bility, between advocacy journalism and one of the major invaders of privacy
(to some extent accompanied by journalism‚Äôs self-critical observation of blurring
boundaries between private and public). Hence, journalism ‚Äì what it traditionally has
been and what it can or should be in an online environment ‚Äì is characterized by
several paradoxes and journalists have to balance extremely contradictory purposes
and demands (Loosen et al. 2008). For instance: they should be fast and accurate,
complete and selective, close to the subject as well as keeping a distance, should act
autonomously in a state of dependence, listen to the audience and provide orientation,
balance transparency and secrecy as well as privacy and public interest. In an online
environment, a kind of journalism that supplies privacy even to those who do not
supply it themselves is indispensable ‚Äì to shelter both them and the public sphere.
216                                                                                   W. Loosen


References

Altman I (1975) The environment and social behavior: privacy, personal space, territory,
   crowding. Brooks/Cole, Monterey
Altman I (1977) Privacy regulation: culturally universal or culturally specific. J Soc Issues
   33(3):67‚Äì83
Atton C (2002) Alternative media. Sage, London
Atton C (2009) Alternative and citizen journalism. In: Wahl-Jorgensen K, Hanitzsch T (eds)
   The handbook of journalism studies. Routledge, New York, pp 265‚Äì278
Bergmann J, P‚Ç¨orksen B (2009) Skandal! Die Macht ‚Ç¨    offentlicher Emp‚Ç¨orung. von Halem, K‚Ç¨oln
Bobbio N (1989) Democracy and dictatorship. The nature and limits of state power. Polity, Oxford
Bruns A (2005) Gatewatching. Collaborative online news production. Peter Lang, New York
Daschmann G (2001) Der Einflu√ü von Fallbeispielen auf Leserurteile. UVK, Konstanz
Deuze M (2008) The changing context of news work: liquid journalism and monitorial citizenship.
   Int J Commun 2:848‚Äì865
Domingo D, Quandt T, Heinonen A, Paulussen S, Singer JB, Vujnovic M (2008) Participatory
   journalism practices in the media and beyond: an international comparative study of initiatives
   in online newspapers. J Pract 2(3):326‚Äì342
Donsbach W (1981) Journalisten zwischen Publikum und Kollegen. Forschungsergebnisse zum
   Publikumsbild und zum in-group-Verhalten. Rundfunk und Fernsehen 29(2‚Äì3):168‚Äì184
Downie L, Schudson M (2009) The reconstruction of American journalism. Columbia Journalism
   Review, October 19. http://www.cjr.org/reconstruction/the_reconstruction_of_american.php.
   Accessed 30 Nov 2010
Eberwein T, P‚Ç¨ottker H (2009) Journalistische Recherche im Social Web: Neue Potenziale, neue
   Probleme? Zeitschrift f‚Ç¨ur Kommunikations‚Ç¨   okologie und Medienethik 11(1):23‚Äì32
Eilders C (1997) Nachrichtenfaktoren und Rezeption: Eine empirische Analyse zur Auswahl und
   Verarbeitung politischer Information. Westdeutscher, Opladen
Eilders C (2006) News factors and news decisions. Theoretical and methodological advances in
   Germany. Communications 31(2):5‚Äì24
Eilders C et al (2010) Zivilgesellschaftliche Konstruktionen politischer Realit‚Ç¨at. Eine
   vergleichende Analyse zu Themen und Nachrichtenfaktoren in politischen Weblogs und
   professionellem Journalismus. Medien & Kommunikationswissenschaft 58(1):63‚Äì81
Fretwurst B (2008) Nachrichten im Interesse der Zuschauer. Eine konzeptionelle und empirische
   Neubestimmung der Nachrichtenwerttheorie. UVK, Wiesbaden
Galtung J, Ruge MH (1965) The structure of foreign news. The presentation of the Congo, Cuba
   and Cyprus crises in four Norwegian newspapers. J Peace Res 2(1):64‚Äì91
Gerhards J, Sch‚Ç¨afer MS (2010) Is the internet a better public sphere? Comparing newspapers and
   internet in Germany and the US. New Media Soc 12(1):143‚Äì160
G‚Ç¨
 orke A, Scholl A (2006) Niklas Luhmann‚Äôs theory of social systems and journalism research.
   J Stud 7(4):644‚Äì655
Harrison J (2006) News. Routledge, London
Hermida A, Thurman N (2008) A clash of cultures: the integration of user-generated content
   within professional journalistic frameworks at British newspaper websites. J Pract
   2(3):343‚Äì356
Herrmann F, L‚Ç¨unenborg M (eds) (2001) Tabubruch als Programm. Privates und Intimes in den
   Medien. Leske & Budrich, Opladen
Imhof K, Schulz P (eds) (1998) Die Ver‚Ç¨     offentlichung des Privaten ‚Äì Die Privatisierung des
   OÃàffentlichen. Westdeutscher, Wiesbaden
Loosen W, P‚Ç¨orksen B, Scholl A (2008) Paradoxien des Journalismus. Einf‚Ç¨             uhrung und
   Begriffskl‚Ç¨arung. In: P‚Ç¨
                          orksen B, Loosen W, Scholl A (eds) Paradoxien des Journalismus.
   Theorie ‚Äì Empirie ‚Äì Praxis. VS, Wiesbaden, pp 17‚Äì33
Luhmann N (2000) The reality of the mass media. Stanford University Press, Stanford
15   Online Privacy as a News Factor in Journalism                                            217

Margulis ST (2003a) On the status and contribution of Westin‚Äôs and Altman‚Äôs theories of privacy.
    J Soc Issues 59(2):411‚Äì429
Margulis ST (2003b) Privacy as a social issue and behavioral concept. J Soc Issues 59(2):243‚Äì261
Messner M, Watson DiStaso M (2008) The source cycle. How traditional media and weblogs use
    each other as sources. J Stud 9(3):447‚Äì463
Meyrowitz J (2002) Post-privacy America. In: Wei√ü R, Groebel J (eds) Privatheit im ‚Ç¨  offentlichen
    Raum. Medienhandeln zwischen Individualisierung und Entgrenzung. Leske und Budrich,
    Opladen, pp 153‚Äì204
Mitchelstein E, Boczkowski PJ (2009) Between tradition and change. A review of recent research
    on online news production. Journalism 10(5):562‚Äì586
Muthukumaraswamy K (2010) When the media meet crowds of wisdom. How journalists are
    tapping into audience expertise and manpower for the processes of newsgathering. J Pract
    4(1):48‚Äì65
Neuberger C, Nuernbergk C, Rischke M (eds) (2009) Journalismus im Internet. Profession ‚Äì
    Partizipation ‚Äì Technisierung. VS, Wiesbaden
O‚ÄôNeill D, Harcup T (2009) News values and selectivity. In: Wahl-Jorgensen K, Hanitzsch T (eds)
    The handbook of journalism studies. Routledge, New York, pp 161‚Äì174
Paine C, Reips U-D, Stieger S, Joinson AN, Buchanan T (2007) Internet users‚Äô perceptions of
    ‚Äòprivacy concerns‚Äô and ‚Äòprivacy actions‚Äô. Int J Hum Comput Stud 65:526‚Äì536
Papacharissi Z (2010) A private sphere. Democracy in a digital age. Polity, Cambridge
Paulussen S et al (2008) Citizen participation in online news media. An overview of current
    developments in four European countries and the United States. In: Quandt T, Schweiger W
    (eds) Journalismus online ‚Äì partizipation oder profession? VS, Wiesbaden, pp 263‚Äì283
Pew Research Center for the People and the Press (2005) The media: more voices, less credibility.
    A review of pew research center for the people & the press findings. http://people-press.org/
    commentary/pdf/105.pdf. Accessed 30 Nov 2010
Pundt C (2008) Medien und Diskurs. Zur Skandalisierung von Privatheit in der Geschichte des
    Fernsehens. Transcript, Bielefeld
Purcell K et al (2010) Understanding the participatory news consumer. How internet and cell
    phone users have turned news into a social experience. Pew Research Center‚Äôs Project for
    Excellence in Journalism, March. http://www.journalism.org/sites/journalism.org/files/
    Participatory_News_Consumer.pdf. Accessed 6 Dec 2010
Reese S, Rutigliano L, Hyun K, Jeong J (2007) Mapping the blogosphere. Professional and citizen-
    based media in the global news arena. Journalism 8(3):235‚Äì261
R‚Ç¨olver M, Alpar P (2008) Social News, die neue Form der Nachrichtenverteilung? In: Alpar P,
    Blaschke S (eds) Web 2.0 ‚Äì Eine empirische Bestandsaufnahme. Vieweg + Teubner,
    Wiesbaden, pp 259‚Äì330
Rosen J (2006) The people formerly known as the audience. PressThink. http://journalism.nyu.
    edu/pubzone/weblogs/pressthink/2006/06/27/ppl_frmr.html. Accessed 30 Nov 2010
R‚Ç¨ossler B (2005) The value of privacy. Polity, Cambridge
R‚Ç¨uhl M (2008) Journalism in a globalizing world society: a societal approach to journalism
    research. In: L‚Ç¨offelholz M, Weaver D (eds) Global journalism research. Theories, methods,
    findings, future. Blackwell, Malden, pp 28‚Äì38
Scheufele B (2006) Frames, schemata, and news reporting. Communications 31(1):65‚Äì84
Schmidt J (2009) Das neue Netz. Merkmale, Praktiken und Folgen des Web 2.0. UVK, Konstanz
Schmidt SJ, Zurstiege G (2000) Orientierung Kommunikationswissenschaft. Was sie kann, was sie
    will. Rowohlt, Reinbek bei Hamburg
Schneider UF (2004) Der Januskopf der Prominenz, Zum ambivalenten Verh‚Ç¨altnis von Privatheit
    und OÃàffentlichkeit. VS, Wiesbaden
Seubert S (2010) Privatheit und OÃàffentlichkeit heute. Ein Problemaufriss. In: Seubert S, Niesen P
    (eds) Die Grenzen des Privaten. Nomos, Baden-Baden, pp 9‚Äì22
Shoemaker PJ (1996) Hardwired for news: using biological and cultural evolution to explain the
    surveillance function. J Commun 46(3):32‚Äì47
218                                                                                  W. Loosen


Singer JB (2010) Quality control. Perceived effects of user-generated content on newsroom norms,
    values and routines. J Pract 4(2):127‚Äì142
Solove DJ (2004) The digital person: technology and privacy in the information age. University
    Press, New York
Staab JF (1990) The role of news factors in news selection: a theoretical reconsideration. Eur J
    Commun 5(4):423‚Äì443
Thelwall M, Stuart D (2007) RUOK? Blogging communication technologies during crises.
    J Comput Mediat Commun 12(2). http://jcmc.indiana.edu/vol12/issue2/thelwall.html.
    Accessed 7 Dec 2010
Thompson JB (2000) Political scandal. Power and visibility in the media age. Polity Press,
    Cambridge
Thompson JB (2005) The new visibility. Theory Cult Soc 22(6):31‚Äì51
Thurman N (2008) Forums for citizen journalism? Adoption of user generated content initiatives
    by online news media. New Media Soc 10(1):139‚Äì157
Warren SD, Brandeis LD (1890) The right to privacy. Harvard Law Review, Vol IV. http://groups.
    csail.mit.edu/mac/classes/6.805/articles/privacy/Privacy_brand_warr2.html. Accessed 6 Dec
    2010
Weischenberg S (2007) Genial daneben: Warum Journalismus nicht (Gegen-)Teil von
    Unterhaltung ist. In: Journalismus und Unterhaltung. Thepretische AnsaÃàtze und empirische
    Befunde. VS, Wiesbaden, pp 117‚Äì132
Westin AF (1967) Privacy and freedom. Atheneum, New York
Westin AF (2003) Social and political dimensions of privacy. J Soc Issues 59(2):431‚Äì453
Whittle S, Cooper G (2009). Privacy, probity and public interest. The Reuters Institute for the
    Study of Journalism, Oxford. http://reutersinstitute.politics.ox.ac.uk. Accessed 06 Dec 2010
Williams A, Wardle C, Wahl-Jorgensen K (2010) Have they got news for us? Audience revolution
    or business as usual at the BBC? J Pract 3(2):1‚Äì15
Xenos M (2008) New mediated deliberation: blog and press coverage of the Alito nomination.
    J Comput Mediat Commun 13(2):485‚Äì503
Zillmann D, Brosius H-B (2000) Exemplification in communication. Erlbaum, Mahwah
  Part III
Audiences
Chapter 16
Adolescents‚Äô Online Privacy: Toward
a Developmental Perspective

Jochen Peter and Patti M. Valkenburg




16.1    Introduction

For many Western adolescents, the use of the Internet for social purposes has
become an integral part of their lives. Adolescents are the defining users of the
‚ÄúSocial Web,‚Äù that is, the part of the World Wide Web that is used for socializing
and interacting with others. Teenagers far outnumber adults in the use of Social
Web technologies, such as instant messaging and social network sites (see e.g.,
Lenhart et al. 2007). For example, 53% of US and 91% of Dutch adolescent Internet
users communicate online through instant messaging (Rideout et al. 2010;
Valkenburg and Peter 2009a), and adolescents increasingly use social network
sites (e.g., Facebook), blogs, and photo and video sharing sites (e.g., YouTube).
Across 13 European countries, 66% of all Internet users aged 15 or older visited
social network sites in 2008 (comScore 2009). Finally, data from 2010 show that
74% of all US adolescents aged 13‚Äì18 have created a profile on a social network
site (Rideout et al. 2010).
    Because the Social Web invites the sharing of privacy-sensitive information,
adolescents‚Äô massive use of Social Web technologies has spurred some controversy
about how adolescents deal with online privacy. On the one hand, scholars have
pointed to what they call a privacy paradox, that is, a fundamental contradiction in
how adolescents and adults deal with online privacy (e.g., Barnes 2006). While
adults are concerned about an invasion of their privacy on the Internet, adolescents
seem to present personal and sometimes even intimate information on the Internet,
particularly on social network sites. As a result of this careless distribution of
personal information, adolescents are seen as easy targets for commercial and
identity fraud (Moscardelli and Divine 2007), as well as for emotional and sexual
abuse (for summaries of these concerns, see e.g., Donnerstein 2009; Hinduja and


J. Peter (*) ‚Ä¢ P.M. Valkenburg
University of Amsterdam, Amsterdam, Netherlands
e-mail: j.peter@uva.nl

S. Trepte and L. Reinecke (eds.), Privacy Online,                               221
DOI 10.1007/978-3-642-21521-6_16, # Springer-Verlag Berlin Heidelberg 2011
222                                                        J. Peter and P.M. Valkenburg


Patchin 2008). On the other hand, it has been argued that the Internet may provide
adolescents with just the privacy they need to explore their identity in a relatively
safe space, to experiment with intimate issues beyond the confines of face to face
communication, and to find information and social support regarding developmen-
tally sensitive issues (e.g., Ben-Ze‚Äôev 2003; McKenna and Bargh 2000;
Subrahmanyam and Greenfield 2008).
    Despite the uncertainty about what adolescents‚Äô extensive use of privacy-sensitive
Social Web applications means for issues surrounding their online privacy, research
on the topic is scarce. Although several studies have dealt with the issue from an
empirical perspective (e.g., Hinduja and Patchin 2008; Lenhart and Madden 2007;
Livingstone 2008; Moscardelli and Divine 2007; Patchin and Hinduja 2010; Youn
2009), hardly any study has tried to approach it from a more theoretical angle. This
chapter tries to fill this gap. Our aim is to conceptualize and understand the risks,
but also the opportunities that surround adolescents‚Äô online privacy from a devel-
opmental perspective. In the first section, we outline how the functions of privacy
correspond both with crucial developmental tasks in adolescence and with the skills
that are necessary to achieve these goals. In the second section, we describe
fundamental properties of digital information in the Social Web and explain their
consequences for online privacy. In the third section, we combine the insights from
the two preceding sections and show how and why adolescents use the Social Web
for developmental purposes. In the last section, we evaluate the risks and
opportunities of adolescents‚Äô online privacy within a developmental perspective.



16.2    Privacy and Psychosocial Development in Adolescence

Privacy has been described as an elastic concept (Allen 1988; Burgoon 1982). As a
result, definitions of privacy vary between fields and researchers (for an overview,
see e.g., Margulis 2003b; see Margulis, this volume, Chap. 2). For example, Altman
(1975) has defined privacy as ‚Äúthe selective control of access to the self‚Äù (p. 24).
Others have described privacy as the ability to exert control over self, information,
objects, spaces, and behavior (Wolfe and Laufer 1974), or as the process of creating
interpersonal boundaries with which a person or group regulates interaction with
others (Derlega and Chaikin 1977). In this chapter, we rely on Westin (1967), who
has defined privacy as ‚Äúthe claim of individuals, groups, or institutions to determine
themselves when, how, and to what extent information about them is
communicated to others‚Äù (p. 7). In line with other authors, Westin emphasizes
choice and control as features of privacy. Individuals can choose the point of time,
mode, and amount of personal information disclosure. In addition, they can control
others‚Äô access to that information. We focus on Westin‚Äôs privacy definition for three
reasons. Firstly, Westin‚Äôs definition is widely used and has stood the test of time
(Margulis 2003a). Secondly, it focuses on information privacy, which is central for
questions about adolescents‚Äô handling of personal information in the Social Web.
Thirdly, Westin‚Äôs definition, and the larger theory of privacy in which it is
16   Adolescents‚Äô Online Privacy: Toward a Developmental Perspective              223


embedded, addresses the psychological level of privacy. This is crucial for our aim
of conceptualizing and understanding issues surrounding adolescents‚Äô online pri-
vacy from a developmental perspective.
   In Westin‚Äôs (1967) theory, privacy plays an important role in the development of
individuality. It creates the space for self-exploration and self-assessment, which
are essential components in an individual‚Äôs development, particularly in adoles-
cence. Privacy can be both solitary and social. When privacy is solitary, it allows
individuals to manage, for example, bodily and emotional necessities. When pri-
vacy is social, it provides individuals with information for their self-evaluation and
the development of social competencies. Taken together, privacy enables normal
and healthy bodily and psychosocial development (Margulis 2003b). Thus,
Westin‚Äôs theory of privacy suggests that an individual‚Äôs self-realization is incon-
ceivable without privacy, at least in Western countries (Margulis 2003a).
   In the context of self-realization or, more generally, individual development,
Westin points out four specific interrelated functions of privacy: personal auton-
omy, self-evaluation, limited and protected communication, and emotional release.
Personal autonomy is defined by Westin (1967) as ‚Äúthe desire to avoid being
manipulated or dominated wholly by others‚Äù (p. 33). Privacy protects personal
autonomy by giving individuals the time, space, and opportunity to experiment with
emotions, thoughts, and behaviors before making them public. By self-evaluation,
Westin means that individuals integrate their experiences into meaningful patterns
and exert individuality on events. In this context, privacy helps individuals to
process information and to decide when to test their own evaluations against the
responses of peers.
   Limited and protected communication has two aspects. Firstly, privacy in limited
communication creates boundaries that ensure the psychological distance necessary
for the functioning of interpersonal relationships, regardless of whether they are
formal or intimate. Secondly, privacy for protected communication entails
individuals being able to share intimate information with people they trust. In
protected communications, individuals self-disclose because they know that the
intimate information will not be shared with others. Emotional release, finally, can
be seen as a temporary escape from social obligations. It refers to the liberation
from the pressures of playing social roles, respite from the emotional stimulations
of daily life, coping with adverse experiences, and the management of bodily and
sexual functions.



16.2.1 Linking Privacy Theory and Developmental Theory

Westin‚Äôs privacy theory (1967) has rarely been related to the fundamental develop-
mental tasks of adolescence. At the same time, developmental theories have hardly
ever considered the importance of privacy for the development of individuality.
This is striking because concepts such as self-exploration, self-evaluation, and
psychosocial development are central to both traditions. Notably, Westin‚Äôs four
224                                                          J. Peter and P.M. Valkenburg


specific functions of privacy can easily be linked with four crucial developmental
goals in adolescence.
    Developmental theorists agree that there are at least four important interrelated
developmental goals in adolescence: autonomy, identity, intimacy, and develop-
ment of the sexual self (e.g., Bukatko 2008; Steinberg 2008). Autonomy as a
developmental goal in adolescence refers to young people‚Äôs ability to feel, think,
and act independently. Thus, it entails emotional independence, notably in
relationships with others; cognitive independence, especially in the development
of beliefs, norms, and values; and behavioral independence, particularly in decision-
making (Steinberg 2008). The developmental goal of identity formation implies that
adolescents need to achieve a secure feeling about who they are and who they
become (see e.g., Erikson 1968; Harter 1999). The development of a firm sense of
identity is accompanied by increasingly complex and abstract self-conceptions, that
is, the traits and attributes that adolescents use to describe themselves. Intimacy as a
developmental goal in adolescence means that adolescents have to acquire the
abilities that are necessary to form and maintain close, meaningful relationships
with others (see e.g., Buhrmester and Furman 1987; Buhrmester and Prager 1995;
Furman and Wehner 1994). Finally, the development of the sexual self refers to a
firm understanding of oneself as a sexual person. This implies the awareness and
acceptance of one‚Äôs sexual orientation, the development of sexual self-efficacy, and
the acquisition of sociosexual skills (Breakwell and Millward 1997; Buzwell and
Rosenthal 1996).
    From the definition of the four developmental goals in adolescence, it becomes
clear that they correspond closely to Westin‚Äôs (1967) four specific functions of
privacy. The link between the developmental goal of autonomy and Westin‚Äôs
privacy function of personal autonomy is self-evident. The goal of identity forma-
tion, with its emphasis on the development of a firm sense of one‚Äôs characteristics, is
related to the privacy function of self-evaluation. After all, adolescents can only
develop a firm sense of who they are if they evaluate themselves, notably through
the responses they get from their peers. The developmental goal of intimacy and its
focus on the development of meaningful close relationships corresponds to Westin‚Äôs
privacy function of limited and protected communication. Protected communication
in particular enables adolescents to share intimate information with others to form or
maintain close relationships. Finally, the development of adolescents‚Äô sexual selves
is associated with Westin‚Äôs privacy function of emotional release. Although the
emotional release function encompasses more than just sexual functions, it also
provides adolescents with the opportunity to deal with sexual issues.



16.2.2 Developmental Goals and Pertinent Skills

The close correspondence between established developmental goals in adolescence
and Westin‚Äôs (1967) four specific functions of privacy suggests that privacy is
important or, more precisely, functional for the achievement of these goals (for
16   Adolescents‚Äô Online Privacy: Toward a Developmental Perspective               225


personal goal achievement via social media use in an older age see also Maa√ü, this
volume, Chap. 17). However, the correspondence between the developmental goals
and the privacy functions does not specify how privacy enables adolescents to
achieve the various developmental goals. To understand the link between privacy
functions and the achievement of developmental goals, we need to look at the skills
that are necessary for achieving these goals and that adolescents learn and practice
in privacy. As will become clear later, these skills are also essential for our
understanding of what adolescents do in the Social Web.
   Each of the four developmental tasks outlined above is accompanied by a
specific skill. These skills do not develop automatically but have to be learned
and practiced. For the development of autonomy, adolescents need to practice
individuation. Individuation can be defined as the relinquishing of childish
dependencies on parents in favor of more mature relationships (Steinberg 2008)
that allow for more independency in feeling, thinking, and acting. As Wolfe and
Laufer (1974) have shown, the learning of individuation implies the ability to
function in aloneness. Privacy enables adolescents to choose to be alone and to
control potential intrusion.
   For the development of a firm sense of identity, adolescents have to learn how to
present themselves to others. Moreover, they have to learn how to adjust their self-
presentation according to the responses of others (Harter 2003; Leary 1996).
Privacy provides adolescents with the opportunity to withdraw from social interac-
tion in order to pre-test new self-presentations in solitude. At the same time, privacy
enables young people to engage in social interaction in order to evaluate these new
self-presentations through the responses of their peers (Valkenburg et al. 2006).
   To develop a sense of intimacy and, more specifically, close relationships,
adolescents have to learn to disclose intimate information to others (Franzoi and
Davis 1985). This self-disclosure is important to validate the appropriateness of
cognitions, emotions, and behaviors. In addition, self-disclosure elicits, through the
norm of reciprocity, close relationships (Buhrmester and Prager 1995). Privacy
enables adolescents to confide in trusted others. It also creates the boundaries that
are necessary to reduce the possibility that intimate information shared in self-
disclosure is leaked to non-trusted others.
   For the development of the sexual self, adolescents need to learn to explore their
sexuality (Buzwell and Rosenthal 1996). Adolescents‚Äô sexual self-exploration is
typically accompanied by uncertainty (Breakwell and Millward 1997) and some-
times by moral repercussions. Privacy liberates adolescents from moral pressures
and enables them to explore their bodies, to accept their sexual fantasies, and to
establish sexual relations with others (Wolfe and Laufer 1974).
   In sum, privacy plays an essential role for the attainment of developmental goals
in adolescence because it ensures that adolescents can learn and practice the skills
that are necessary to achieve these goals. Firstly, privacy is functional for
adolescents‚Äô accomplishment of autonomy because it creates, through the choice
and control of aloneness, the independence necessary for individuation. Secondly,
privacy is important for adolescents‚Äô identity formation because it provides
them with an opportunity for self-evaluation by experimenting with their
226                                                        J. Peter and P.M. Valkenburg


self-presentation. Thirdly, privacy is essential for adolescents‚Äô achievement of
intimacy because it creates, through protected communication, the space for self-
disclosure. Fourthly and finally, privacy facilitates adolescents‚Äô sexual self-explo-
ration by liberating them from moral pressures. The functionality of privacy for the
achievement of important developmental tasks through the facilitation of pertinent
skills applies both to adolescents‚Äô offline and online behavior. However, before we
can specify the risks and opportunities of online privacy for adolescents, we need to
understand the characteristics that distinguish communication in the Social Web
from communication in the offline world.




16.3    Communication in the Social Web

Privacy, as defined by Westin (1967), is essentially about limiting the access of
others to personal communication. The risks and opportunities of privacy, then, are
inherently social: they are inextricably linked to how others use, or abuse, this
personal information. In the Social Web, others can be seen as ‚Äúnetworked publics.‚Äù
boyd (2010) defines networked publics as ‚Äúpublics that are restructured by
networked technologies. As such, they are simultaneously (1) the space constructed
through networked technologies and (2) the imagined collective that emerges as a
result of the intersection of people, technology, and practice‚Äù (p. 39). Networked
publics share many functions of offline or non-networked publics, for example,
gathering for social or cultural ends. In contrast to other publics, however,
networked publics depend on networked technologies. These technologies shape
how information flows in networked publics and how people interact both with this
information and with each other (boyd 2010).
   The working of networked technologies and, consequently, the functioning of
networked publics are closely linked to the properties of bits. As a result, the
properties of bits are also important for our understanding of privacy issues in
networked publics. A bit (or binary digit) is the smallest information unit in digital
computing. Comparing the differences between the properties of bits with those of
atoms, Negroponte (1995) has emphasized that bits are superior to atoms because
bits facilitate the compression, alteration, duplication, and an efficient and quick
transmission of information, notably in wired networks. Based on these properties
of bits or, more generally, of digital information, boyd (2010) has pointed out that
the bit-based information in networked publics is easier to store, duplicate, distrib-
ute, and search than atom-based information in non-networked publics. Specifi-
cally, she has identified four affordances of content in networked publics:
persistence, replicability, scalability, and searchability.
   The persistence of content in networked publics refers to the automatic record-
ing and archiving of online expressions. The fact that content in networked publics
is by default persistent presents a radical deviation from the common, and deeply
16   Adolescents‚Äô Online Privacy: Toward a Developmental Perspective                 227


entrenched experience that what we say and do is ephemeral. boyd (2010)
emphasizes that the persistence of online content is particularly pervasive once it
has been distributed in networked publics. When online content has been
disseminated in networked publics, it is impossible to delete it. The persistence of
content in networked publics is advantageous in asynchronous Internet communi-
cation, such as communication through social network sites or e-mail. At the same
time, the persistence of such content fundamentally contradicts the notion of
privacy as limiting others‚Äô access to the content.
    The replicability of content in networked publics refers to the fact that online
expressions can easily be duplicated. boyd (2010) points out two problems that are
related to the replicability of digital information. Firstly, original and duplicate
cannot be differentiated. Secondly, because digital information can easily be
altered, it is difficult to trace back how the original information looked, certainly
when the information has been spread in networked publics. Against the backdrop
of Westin‚Äôs (1967) emphasis on individuals‚Äô control over and choice of when, how,
and to what extent information about them is communicated to others, the replica-
bility of digital information presents a major threat to privacy.
    The scalability of content in networked publics refers to the potential visibility
of online expressions. This affordance captures the opportunity to distribute content
to smaller or larger parts of networked publics. It is important to note that scalability
in networked publics merely provides the possibility of visibility but does not
guarantee it (boyd 2010). The scalability of digital information seems to overlap
with the control over the dissemination of personal information that is essential for
privacy. However, as boyd emphasizes, the public rather than the individual
typically determines what is scaled. What may be intended only for small parts of
networked publics may be distributed widely, and what may be aimed at large parts
of networked publics may hardly be distributed. As a consequence, the scalability
of digital information is more likely to conflict than to harmonize with individuals‚Äô
privacy.
    The searchability of content in networked publics refers to the accessibility of
online expressions through search (engines). Together with the persistence of
digital information, the searchability of such information poses a considerable
threat to individuals‚Äô privacy. This privacy threat not only affects the retrieval of
information about an individual against that individual‚Äôs will, but also pertains to
the possibility that individuals become the target of unwanted contacts.
    In conclusion, the affordances of content in networked publics are at odds with
the notion of privacy as individuals‚Äô control over who has access to information
about them. Due to the bit-based character of information in networked publics, it is
impossible for individuals to choose and control when, how, and to what extent
information about them is communicated to others, as Westin‚Äôs (1967) definition of
privacy would require. However, it is important to note that the affordances of
networked communities do not automatically lead to a violation of individuals‚Äô
privacy. Whether privacy is violated depends largely on how and to what end other
members of networked publics use privacy-sensitive information.
228                                                         J. Peter and P.M. Valkenburg


16.4    Privacy, Psychosocial Development, and Networked
        Publics

The two preceding sections have shown two things: firstly, privacy is essential for
the achievement of important developmental goals; secondly, the affordances of
content in networked publics contradict fundamental properties of privacy. Against
this backdrop, it is surprising that adolescents increasingly use networked publics
for the privacy-dependent learning and rehearsing of the skills that are necessary for
achieving developmental goals. As research has consistently shown, vast numbers
of adolescents engage in individuation, self-presentation, self-disclosure, and sexual
self-exploration in the Social Web. For example, adolescents use the Social Web to
form, maintain, or intensify friendships with peers (e.g., Subrahmanyam and Green-
field 2008), to display personal information on social network sites and personal
homepages (e.g., Livingstone 2008; Schmitt et al. 2008), to discuss intimate issues
online (e.g., Schouten et al. 2007), and to explore their sexual orientation and
various forms of sexuality on the Internet (e.g., Hillier and Harrison 2007; Peter
and Valkenburg 2006a). This raises the important question of why adolescents
engage so massively in potentially privacy-threatening behavior on the Internet.
    One possible explanation is that the same affordances of networked publics that
threaten adolescents‚Äô privacy may also make them functional for achieving devel-
opmental goals, at least from adolescents‚Äô own points of view. More specifically,
adolescents may experience the persistence, replicability, scalability, and
searchability of bit-based information as creating the conditions to learn and
practice the skills that ensure the accomplishment of developmental tasks. Thus,
whereas the affordances of networked publics may actually reduce adolescents‚Äô
privacy, adolescents may perceive them as improving their control over personal
information and, ultimately, their privacy. In adolescents‚Äô view, this privacy even-
tually helps them to engage in individuation, self-presentation, self-disclosure, and
sexual self-exploration.
    As mentioned above, the persistence of bit-based information is the reason why
communication in networked publics can be asynchronous. Asynchronous commu-
nication, in turn, is perceived by adolescents to augment their control over the
information they wish to convey (Peter and Valkenburg 2006b; Schouten et al.
2007). This control, finally, allows adolescents to think about, and edit, information
that is relevant to individuation, self-presentation, self-disclosure, and sexual self-
exploration before sending or posting it. Accordingly, studies have shown that
adolescents‚Äô sense of control determines the quantity and quality of their self-
disclosure (e.g., Schouten et al. 2007), self-presentation (e.g., Schmitt et al.
2008), and sexual self-exploration (e.g., Hillier and Harrison 2007).
    The replicability of bit-based information enables adolescents to choose from
different types of information about themselves. Textual information can as easily
be replicated and posted in networked publics as (audio)visual content. This gives
adolescents enormous opportunities to manage the richness of cues they want to
convey to others. Adolescents can choose whether they present themselves only
16   Adolescents‚Äô Online Privacy: Toward a Developmental Perspective               229


through textual descriptions or whether they add more cues, for example, by
including pictures or video clips in their self-presentation.
    The replicability of bit-based information also implies that the information can
be altered (boyd 2010). Thus, adolescents are not only able to manage the richness
of cues but they can also modify the very nature of these cues (e.g., by means of
specific software). The alteration of information is most obvious when adolescents
modify or update textual information on social network sites, as well as when they
photo-shop images of themselves before posting them. Consequently, research has
suggested that the sense of mastery that accompanies control over the richness and
nature of cues, for example, on social network sites, is an important characteristic of
adolescents‚Äô individuation (Schmitt et al. 2008). Similarly, scholars have pointed
out that cue management increases adolescents‚Äô opportunities to decide indepen-
dently about their self-presentation (Calvert 2002). Finally, adolescents‚Äô control
over the richness and nature of cues seems to facilitate self-disclosure (Valkenburg
and Peter 2009b; Walther 1992, 1996), which in turn enhances the intimacy of
friendships (Valkenburg and Peter 2007b).
    The scalability of content in networked publics is central to adolescents‚Äô
perception of control over the public to which they present personal information.
Adolescents can easily choose whether they convey information in a one-to-one
setting (e.g., in closed dyadic instant messaging) or in a one-to-many setting (e.g.,
on social network sites). As a result, adolescents are able to distribute information
about themselves efficiently and to a variety of people. The controlled contact
with others may help adolescents to evaluate their identities against a vastly
expanded sounding board compared to face to face contacts. Accordingly, studies
have demonstrated that the positive feedback adolescents received on their self-
presentation on social network sites augmented their self-esteem (Valkenburg
et al. 2006). Similarly, when adolescents chose to engage in online self-disclosure
to close friends, as opposed to strangers, their psychosocial well-being improved
(Valkenburg and Peter 2007a). Finally, studies have shown that young people
may use networked publics to form new relationships, which is an important
aspect of individuation (Ellison et al. 2007; Peter et al. 2006).
    The searchability of bit-based information renders any digital information about
persons and things easily accessible for adolescents. Compared to the pre-Internet era,
adolescents thus have a greater control and choice for getting in touch with persons
and information previously difficult to access. For example, they can easily look up
other teenagers whom they may not have met in person for a long time, or with whom
they may not have close relationships. The maintenance of these weak ties in particu-
lar seems to affect young people‚Äôs individuation and identity formation positively
(Steinfield et al. 2008; Valkenburg et al. 2006). Moreover, because sexual information
has become more easily accessible, adolescents can explore sexual issues more
conveniently than before, even if some of the available information may be age-
inappropriate (Lo and Wei 2005; Peter and Valkenburg 2006a). Similarly, adolescents
are provided with the opportunity to look for people who have comparable sexual
problems (Hillier and Harrison 2007; McKenna and Bargh 1998) and engage in self-
disclosure with trusted others (Subrahmanyam et al. 2006; Suzuki and Calzo 2004).
230                                                         J. Peter and P.M. Valkenburg


    Thus, the persistence, replicability, scalability, and searchability of bit-based
information, along with adolescents‚Äô perceptions of choice and control over per-
sonal information, may explain why adolescents use the Internet to learn and
practice the skills that warrant a successful attainment of developmental goals.
However, an analysis of how adolescents deal with online privacy would be
incomplete without considering the possibility to engage in anonymous online
communication (Ben-Ze‚Äôev 2003). Anonymity (which we conceptualize as relative
anonymity given the problems of remaining completely unidentifiable on the web)
is not an affordance of content in networked publics but rather of online communi-
cation. Adolescents increasingly use Internet applications, such as social network
sites, in which anonymity is uncommon. However, they can choose to remain
anonymous, for example, when using social support sites (Subrahmanyam et al.
2006; Suzuki and Calzo 2004) or chat rooms (Valkenburg et al. 2005).
    Anonymous online communication may be seen as the extreme form of
protecting one‚Äôs privacy. Through the possibility to reveal one‚Äôs identity, it entails
maximum choice and control over when, how, and to what extent information is
communicated to others. Research has shown that anonymous online communica-
tion is related to the learning and practicing of all skills that are important for the
achievement of developmental goals. For example, teenagers use anonymous online
communication to get in contact with new people (Peter et al. 2006; Subrahmanyam
and Greenfield 2008). Although this may have adverse consequences (Mitchell et al.
2007; Valkenburg and Peter 2007a), it indicates adolescents‚Äô individuation. Anony-
mous online communication also enables adolescents to engage in identity
experiments. This is an important aspect of their identity formation and may have
positive effects on the development of their social skills (Valkenburg and Peter
2008; Valkenburg et al. 2005). Furthermore, anonymous online communication
facilitates adolescents‚Äô self-disclosure in terms of relationship and sexual health
issues (Suzuki and Calzo 2004). Finally, adolescents engage in anonymous online
communication to discuss moral, emotional, and social issues related to teenage sex
(Subrahmanyam et al. 2004). Anonymous online communication as a relatively safe
way of sexual self-exploration is especially important for gay and lesbian youth.
Because same-sex attraction is still accompanied by repercussions and distress,
many gay and lesbian adolescents use the Internet for discussing problems
surrounding their sexual orientation (Hillier and Harrison 2007).




16.5    Toward a Developmental Perspective of Adolescents‚Äô
        Online Privacy

From a developmental point of view, privacy is an important condition for the
successful achievement of developmental goals in adolescence. Not only do
Westin‚Äôs (1967) functions of privacy overlap with developmental goals in adoles-
cence, privacy also enables adolescents to learn and rehearse the skills upon which
16   Adolescents‚Äô Online Privacy: Toward a Developmental Perspective               231


the successful attainment of these goals depends. In terms of online privacy,
adolescents are faced with a much more fundamental privacy paradox than what
Barnes (2006) described as the contradiction between privacy-concerned adults and
privacy-oblivious teenagers: on the one hand, the affordances of content in
networked publics threaten, and potentially violate, adolescents‚Äô privacy because
they fundamentally reduce adolescents‚Äô choice and control over personal informa-
tion. Adolescents thus have to face the illusion of control. On the other hand,
adolescents seem to experience the affordances of content in networked publics
as enhancing their choice and control over personal information and, eventually,
their online privacy. Overall, communication in networked publics seems to have
more positive than negative consequences for adolescents, as current research
indicates (for a review, see e.g., Subrahmanyam and Greenfield 2008). Adolescents
thus experience the promise and, partly, also the success of control.
   Due to the privacy paradox with which adolescents are confronted, adolescents
have to negotiate constantly between the risks and opportunities of communicating
in networked publics. Their online privacy is permanently threatened and subject to
violation. At the same time, however, adolescents feel and experience that they can
learn and practice developmentally important skills, often even with positive
consequences for their psychosocial development. Of course, the threats to
adolescents‚Äô online privacy cannot be overestimated. Communicating in networked
publics and an adequate handling of privacy issues are crucial tasks that need to be
learned. Specifically, adolescents have to realize that whatever they distribute in
networked publics is risky and can have adverse consequences, for example, for
future relationships, applications, and employment. Moreover, they need to be
taught that the more intimate bit-based information is, the more they should abstain
from sharing it with others, even if they trust these others. Finally, adolescents need
to learn that even the limitation of others‚Äô access to personal information, for
example, on social network sites, does not protect their privacy as their personal
information is still used, for instance, for commercial purposes (see e.g., Debatin
et al. 2009).
   However, our developmental perspective has also shown that we need to under-
stand adolescents‚Äô privacy-sensitive behavior in networked publics in the context of
normal, and usually functional, developmental processes in adolescence. Just
because individuation, self-presentation, self-disclosure, and sexual self-exploration
depend on privacy, they always run the risk of privacy violation, both online and
offline. We also need to realize that adolescents may vary in what they consider a
violation of their privacy. What may be an intrusion into their personal world for
some adolescents may be an acceptable or even desired social interaction for others.
Moreover, we have to take into account that notions of privacy may change over
time. Livingstone (2008), for example, has highlighted that, in contrast to older
generations, many adolescents no longer consider standard information on social
network sites, such as age, relationship status, or sexual orientation, private infor-
mation. Finally, we have to understand that both technologies and the competencies
of their users change. The privacy-oblivious behavior of young people on social
network sites described some years ago (Gross and Acquisti 2005) was as much a
232                                                                J. Peter and P.M. Valkenburg


result of technological limitations as a consequence of users‚Äô deficiencies. Recent
studies have shown that technologies have become more privacy-friendly and
adolescents have become more privacy-sensitive (Hinduja and Patchin 2008;
Patchin and Hinduja 2010).
   In conclusion, a developmental perspective on adolescents‚Äô online privacy
suggests that online privacy presents ‚Äúrisky opportunities‚Äù (Livingstone 2008) to
adolescents. Adolescents run considerable risks of their online privacy being
violated. At the same time, they are provided with the opportunity to achieve
developmental goals. Existing research tends to emphasize either risks (e.g., Barnes
2006; Moscardelli and Divine 2007) or opportunities (e.g., Ben-Ze‚Äôev 2003;
McKenna and Bargh 2000), but devotes little attention to how adolescents learn,
and can be taught, to balance the two. We believe that much of the promise of future
scholarship on the issue lies in embracing both the risks and the opportunities of
adolescents‚Äô online privacy‚Äìand thus its paradoxical nature‚Äìfrom a developmental
perspective.




References

Allen AL (1988) Uneasy access: privacy for women in a free society. Rowman & Littlefield,
   Totowa
Altman I (1975) The environment and social behavior. Wadsworth, Belmont
Barnes SB (2006) A privacy paradox: social networking in the United States. First Monday 11(9),
   no page numbers
Ben-Ze‚Äôev A (2003) Privacy, emotional closeness, and openness in cyberspace. Comput Hum
   Behav 19:451‚Äì467
boyd d (2010) Social network sites as networked publics: affordances, dynamics, and implications.
   In: Papacharissi Z (ed) A networked self: Identity, community, and culture on social network
   sites. Routledge, New York, pp 39‚Äì58
Breakwell GM, Millward LJ (1997) Sexual self-concept and sexual risk-taking. J Adolescence
   20:29‚Äì41
Buhrmester D, Furman W (1987) The development of companionship and intimacy. Child Dev
   58:1101‚Äì1113
Buhrmester D, Prager K (1995) Patterns and functions of self-disclosure during childhood and
   adolescence. In: Rotenberg KJ (ed) Disclosure processes in children and adolescents.
   Cambridge University Press, Cambridge, pp 10‚Äì56
Bukatko D (2008) Child and adolescent development. Houghton Mifflin, Boston
Burgoon JK (1982) Privacy and communication. In: Burgoon M (ed) Communication yearbook,
   vol 6. Sage, Beverly Hills, pp 206‚Äì249
Buzwell S, Rosenthal D (1996) Constructing a sexual self: adolescents‚Äô sexual self-perceptions
   and sexual risk-taking. J Res Adolescence 6:489‚Äì513
Calvert SL (2002) Identity construction on the Internet. In: Calvert SL, Jordan AB, Cocking RR
   (eds) Children in the digital age: influences of electronic media on development. Praeger,
   Westport, pp 57‚Äì70
comScore (2009) Tuenti most popular social networking site in Spain. http://www.comscore.com/
   press/release.asp?press¬º2733. Accessed 4 Mar 2009
Debatin B, Lovejoy JP, Horn A-K, Hughes BN (2009) Facebook and online privacy: attitudes,
   behaviors, and unintended consequences. J Comput Mediat Commun 15:83‚Äì108
16   Adolescents‚Äô Online Privacy: Toward a Developmental Perspective                            233

Derlega VJ, Chaikin AL (1977) Privacy and self-disclosure in social relationships. J Soc Issues
    33:102‚Äì115
Donnerstein E (2009) The internet. In: Strasburger VC, Wilson BJ, Jordan AB (eds) Children,
    adolescents, and the media. Sage, Thousand Oaks, pp 471‚Äì498
Ellison NB, Steinfield C, Lampe C (2007) The benefits of Facebook ‚Äúfriends‚Äù: social capital and
    college students‚Äô use of online social network sites. J Comput Mediat Commun 12:1143‚Äì1168
Erikson E (1968) Identity: youth and crisis. Norton, New York
Franzoi SL, Davis MH (1985) Adolescent self-disclosure and loneliness: private self-consciousness
    and parental influences. J Pers Soc Psychol 48:768‚Äì780
Furman W, Wehner EA (1994) Romantic views: toward a theory of adolescent romantic
    relationships. In: Montemayor R, Adams GR, Gullotta TP (eds) Personal relationships during
    adolescence. Sage, Thousand Oaks, pp 168‚Äì195
Gross R, Acquisti A (2005) Information revelation and privacy in online social networks
    (The Facebook case). Paper presented at the ACM workshop on privacy in the electronic
    society (WPES), Alexandria
Harter S (1999) The construction of the self: a developmental perspective. Guilford, New York
Harter S (2003) The development of self-representation during childhood and adolescence. In:
    Leary MR, Tangney JP (eds) Handbook of self and identity. Guilford, New York, pp 611‚Äì642
Hillier L, Harrison L (2007) Building realities less limited than their own: young people practising
    same-sex attraction on the internet. 10:82‚Äì100
Hinduja S, Patchin JW (2008) Personal information of adolescents on the Internet: a quantitative
    content analysis of MySpace. J Adolesc 31:125‚Äì146
Leary MR (1996) Self-presentation. Impression management and interpersonal behavior.
    Westview Press, Boulder
Lenhart A, Madden M (2007) Social networking websites and teens: an overview. http://www.
    pewinternet.org/pdfs/PIP_SNS_Data_Memo_Jan_2007.pdf. Accessed Retrieved 4 Mar 2009
Lenhart A, Madden M, Smith A, Macgill A (2007) Teens and social media. http://www.
    pewinternet.org/Reports/2007/Teens-and-Social-Media.aspx. Accessed 14 Jan 2010
Livingstone S (2008) Taking risky opportunities in youthful content creation: teenagers‚Äô use of
    social networking sites for intimacy, privacy and self-expression. New Media Soc 10:393‚Äì411
Lo V-h, Wei R (2005) Exposure to internet pornography and Taiwanese adolescents‚Äô sexual
    attitudes and behavior. J Broadcasting Electro Media 49:221‚Äì237
Margulis ST (2003a) On the status and contribution of Westin‚Äôs and Altman‚Äôs theories of privacy.
    J Soc Issues 59:411‚Äì429
Margulis ST (2003b) Privacy as a social issue and behavioral concept. J Soc Issues 59:243‚Äì261
McKenna KYA, Bargh JA (1998) Coming out in the age of the Internet: identity ‚Äúdemargina-
    lization‚Äù through virtual group participation. J Pers Soc Psychol 75:681‚Äì694
McKenna KYA, Bargh JA (2000) Plan 9 from cyberspace: the implications of the internet for
    personality and social psychology. Personality Soc Psychol Rev 4:57‚Äì75
Mitchell KJ, Wolak J, Finkelhor D (2007) Trends in youth reports of sexual solicitations,
    harassment and unwanted exposure to pornography on the Internet. J Adolesc Health
    40:116‚Äì126
Moscardelli D, Divine R (2007) Adolescents‚Äô concern for privacy when using the internet: an
    empirical analysis of predictors and relationships with privacy-protecting behaviors. Family
    Consumer Sci Res J 35:232‚Äì252
Negroponte N (1995) Being digital. Hodder & Stoughton, London
Patchin JW, Hinduja S (2010) Changes in adolescent online social networking behaviors from
    2006 to 2009. Comput Hum Behav 26:1818‚Äì1821
Peter J, Valkenburg PM (2006a) Adolescents‚Äô exposure to sexually explicit material on the
    internet. Commun Res 33:178‚Äì204
Peter J, Valkenburg PM (2006b) Research note: individual differences in perceptions of internet
    communication. Eur J Commun 21:213‚Äì226
234                                                                J. Peter and P.M. Valkenburg


Peter J, Valkenburg PM, Schouten AP (2006) Characteristics and motives of adolescents talking
   with strangers on the internet. Cyberpsychol Behav 9:526‚Äì530
Rideout VJ, Foehr UG, Roberts DF (2010) Generation M2: media in the lives of 8- to 18-year-olds.
   Kaiser Family Foundation, Menlo Park
Schmitt KL, Dayanim S, Matthias S (2008) Personal homepage construction as an expression of
   social development. Dev Psychol 44:496‚Äì506
Schouten AP, Valkenburg PM, Peter J (2007) Precursors and underlying processes of adolescents‚Äô
   online self-disclosure: developing and testing an ‚Äúinternet-attribute-perception‚Äù model. Media
   Psychol 10:292‚Äì314
Steinberg L (2008) Adolescence, 8th edn. McGraw Hill, Boston
Steinfield C, Ellison NB, Lampe C (2008) Social capital, self-esteem, and use of online social
   network sites: a longitudinal analysis. J Appl Developmental Psychol 29:434‚Äì445
Subrahmanyam K, Greenfield P (2008) Online communication and adolescent relationships.
   Future Child 18:119‚Äì146
Subrahmanyam K, Greenfield PM, Tynes B (2004) Constructing sexuality and identity in an online
   teen chat room. J Appl Developmental Psychol 25:651‚Äì666
Subrahmanyam K, Smahel D, Greenfield PM (2006) Connecting developmental constructions to
   the Internet: identity presentation and sexual exploration in online teen chat rooms. Dev
   Psychol 42:395‚Äì406
Suzuki LK, Calzo JP (2004) The search for peer advice in cyberspace: an examination of online
   teen bulletin boards about health and sexuality. J Appl Developmental Psychol 25:685‚Äì698
Valkenburg PM, Peter J (2007a) Internet communication and its relation to well-being: identifying
   some underlying mechanisms. Media Psychol 9:43‚Äì58
Valkenburg PM, Peter J (2007b) Preadolescents‚Äô and adolescents‚Äô online communication and their
   closeness to friends. Dev Psychol 43:267‚Äì277
Valkenburg PM, Peter J (2008) Adolescents‚Äô identity experiments on the internet: consequences
   for social competence and self-concept unity. Commun Res 35:208‚Äì231
Valkenburg PM, Peter J (2009a) The effects of instant messaging on the quality of adolescents‚Äô
   existing friendships: a longitudinal study. J Commun 59:79‚Äì97
Valkenburg PM, Peter J (2009b) Social consequences of the internet for adolescents: a decade of
   research. Curr Dir Psychol Sci 18:1‚Äì5
Valkenburg PM, Schouten AP, Peter J (2005) Adolescents‚Äô identity experiments on the internet.
   New Media Soc 7:383‚Äì402
Valkenburg PM, Peter J, Schouten AP (2006) Friend networking sites and their relationship to
   adolescents‚Äô well-being and social self-esteem. Cyberpsychol Behav 9:584‚Äì590
Walther JB (1992) Interpersonal effects in computer-mediated communication: a relational
   perspective. Commun Res 19:52‚Äì90
Walther JB (1996) Computer-mediated communication. Impersonal, interpersonal, and
   hyperpersonal interaction. Commun Res 23:3‚Äì43
Westin AF (1967) Privacy and freedom. Atheneum, New York
Wolfe M, Laufer R (1974) The concept of privacy in childhood and adolescence. In: Carson DH,
   Margulis ST (eds) Man-environment interactions: evaluations and applications (Part 2), vol 6.
   Environmental Design Research Association, Washington, DC, pp 29‚Äì54
Youn S (2009) Determinants of online privacy concern and its influence on privacy protection
   behaviors among young adolescents. J Consum Aff 43:389‚Äì418
Chapter 17
The Elderly and the Internet: How Senior
Citizens Deal with Online Privacy

Wiebke Maa√ü




17.1    Introduction

The use of the Internet is no longer limited to younger people. Over the past years,
more and more elderly people have started using the Internet and today, older
persons represent a large group of users that has steadily grown since the year 2000
(Pierce 2009). Although e-mail and search engines are still the most important
Internet functions for older people, their use of social media has increased dramati-
cally and nearly doubled from 2009 to 2010 (Madden 2010). While the Internet in
general and the Social Web in particular are becoming more important for senior
citizens, the question arises of how the older generation deals with privacy online.
This chapter analyzes the role of privacy concerns and self-disclosure in seniors‚Äô
Internet use. For a better understanding of elderly people‚Äôs online behavior, a short
overview of their Internet use is presented (Sect. 17.2). This is followed by a review
of the older generation‚Äôs attitudes toward using the Internet and an outline of the
perceived barriers and benefits of using the Internet. Barriers and benefits that users
face while using the Internet may also be described as costs and rewards. In their
social exchange theory, Thibaut and Kelley (1959) suggested referring to
consequences of social interactions as costs and rewards. In this chapter, this
conception of costs and rewards will be transferred to social interactions in the
Social Web. The particular costs and rewards of Internet use (Sect. 17.3) and online
privacy behavior of elderly people (Sect. 17.4) will be elaborated. These theoretical
ideas will be complemented with a short empirical analysis of elderly people‚Äôs self-
disclosing behavior within the Social Web (Sect. 17.5). As research in this area is
quite limited for the group of older Internet users, this explorative study presents a
first insight into the amount of information that elderly people disclose online.
Finally, the results will be discussed and conclusions presented (Sect. 17.6).


W. Maa√ü (*)
Hamburg Media School, Hamburg, Germany
e-mail: w.maass@hamburgmediaschool.com

S. Trepte and L. Reinecke (eds.), Privacy Online,                                  235
DOI 10.1007/978-3-642-21521-6_17, # Springer-Verlag Berlin Heidelberg 2011
236                                                                          W. Maa√ü


17.2    Using the Internet in Later Life

Internet adoption rates are higher among younger generations compared to elderly
individuals, and this trend is observable throughout several countries of the world.
Results of the World Internet Project (Pierce 2010), which analyzed data from ten
countries and regions in America, Asia, and Europe, showed that in general,
‚Äúinternet use increases as age decreases‚Äù (p. 2). Today, the Internet adoption rate
is around 38% among Americans aged 65 years and above, compared to 93%
among people aged 18‚Äì29 years (Rainie 2010). Although Internet adoption rates
are still higher among younger people, Kohut et al. (2006) showed that ‚Äúthe growth
rate for adults over 50 has outpaced that for young adults both in the United States
and throughout Western Europe‚Äù (p. 3). Furthermore, the group of elderly
individuals who use the Internet is large due to the demographic structure of the
industrial nations. As there is a trend toward ‚Äúthe aging of our population‚Äù
(Saunders 2004, p. 573), the total number of older people usually exceeds that of
younger ones in the industrial nations. Therefore, a relatively low percentage of
older Internet users nevertheless represents a high number of elderly people with
regard to the absolute values. Sometimes, the number of Internet-using seniors is
almost analogous to the number of younger people using the Internet. For example,
according to van Eimeren and Frees (2010), 28.2% of German people aged 60 years
and above made use of the Internet in 2010, whereas the usage rate within the age
group of 14‚Äì19 years was nearly 100%. With regard to absolute values, this means
that nearly 5.7 million people over 60 years of age used the Internet compared to
approximately 5.5 million individuals within the younger age group (14‚Äì19 years).
   As there are many Internet-using seniors today, they do not form a homogeneous
group but vary in their use of the Internet. Nevertheless, there are some Internet
functions that seem to be particularly interesting for the majority of the older
generation.
   One of the most important Internet applications for older individuals that is
pointed out by several authors and in different countries is the writing and receiving
of e-mails. For example, according to Fox et al. (2001), 93% of seniors with Internet
access used e-mails, and this application was described as the number one activity
in senior citizens‚Äô Internet use. This view is supported by various authors, such as
Hilt and Lipschultz (2004), who analyzed seniors within the age of 55‚Äì84 years, or
van Eimeren and Frees (2010), who found that 75% of the Internet users aged
50 years and above used e-mails at least once a week, making it the most commonly
used Internet application within this age group.
   Another very important Internet application for elderly people is the use of
search engines. For example, Zickuhr (2010) showed that 87% of Internet-using
seniors aged 65‚Äì73 years made use of this application. Using search engines is also
pointed out by other authors as a very important Internet function (e.g., van Eimeren
and Frees 2010; Fox et al. 2001), and elderly people use popular search engines
such as ‚ÄúGoogle‚Äù or ‚ÄúYahoo‚Äù to find information of personal interest (Hilt and
Lipschultz 2004). With regard to the content of information that elderly people are
17   The Elderly and the Internet: How Senior Citizens Deal with Online Privacy   237


searching for, there are some topics that seem to be of particular interest. For
example, Fox et al. (2001) showed that searching the Internet for health and medical
information was very common among the older generation, a notion that is also
supported by Gatto and Tak (2008). Further important topics are online news in
general (Madden 2010) and reading political news in particular (Fox et al. 2001).
    In addition to e-mails and search engines as two of the most relevant Internet
applications, there are further important functions such as home banking (van
Eimeren and Frees 2010), making travel reservations (Zickuhr 2010), file
downloading (Sum et al. 2009), as well as online shopping and auctions (Hilt and
Lipschultz 2004). Nevertheless, these Internet applications appear to be less impor-
tant for elderly people than using the Internet for aspects of communication and
information.
    Taken together, the Internet applications that are most important for elderly
people do not seem to differ from those of other age groups. Indeed, these Internet
functions are also important for younger Internet users and most of them can be
described as key functions that are ‚Äúuniformly popular across all age groups‚Äù
(Zickuhr 2010, p.2). Nevertheless, there are differences between age groups with
regard to some other Internet applications. With regard to social media, senior
citizens appear to be less interested than younger individuals and this tendency can
be found in several countries. According to the results of the Global Attitudes
Project of the Pew Research Center (2010), there is an age gap with respect to the
use of social networking services in each of the 22 nations that were surveyed all
over the world.
    However, there are two reasons that suggest taking a closer look at elderly
people‚Äôs social media use. Firstly, although younger people dominate the group
of social media users today, social media use among elderly individuals has
dramatically increased (Madden 2010) and is expected to gain further importance
in the future. Secondly, privacy issues are interesting in terms of social media and
social networking sites. As the exchange and disclosure of private information can
be described as key aspects of social media use, it is interesting to look at how
senior citizens deal with online privacy in terms of using such services. Madden
(2010) points out that the adoption of such services almost doubled from 2009 to
2010: whereas 25% of Internet users aged 50‚Äì64 years and 13% of users aged
65 years and above used social networking sites in 2009, in 2010 these percentages
went up to 47% in the age group of 50‚Äì64 years, and 26% in the group of Internet
users aged 65 years and above. Additionally, Madden (2010) describes that the ‚Äúuse
of Twitter and other services to share status updates has also grown among older
adults‚Äù (p. 3), although this increase was considerably smaller compared to the use
of social networking sites such as ‚ÄúFacebook.‚Äù Nevertheless, compared to 2009,
when only 5% of Internet users within the age of 50‚Äì64 years used Twitter or
another status update service, 11% used these tools in 2010. Furthermore, Pierce
(2008) demonstrated that a ‚Äúlarge percentage of Internet users 50 and older who are
members of online communities report extensive involvement in their communities
and benefits from their participation‚Äù (p. 1).
238                                                                          W. Maa√ü


    With regard to the main focus of this chapter, some key findings can be
summarized according to the data presented above. Firstly, elderly people represent
a large and steadily growing group of Internet users. Secondly, seniors make use of
a broad spectrum of Internet applications and dealing with private information
plays an important role for some of these applications. Thirdly, in terms of
communication via e-mails or social networking sites, personal information is
usually disclosed to some extent. Taken together, the patterns of using the Internet
in later life suggest that online privacy is also a relevant issue within the older
generation.




17.3    Elderly People‚Äôs Attitudes Toward the Internet as a Ratio
        of Costs and Rewards

For a better understanding of senior citizens‚Äô Internet use, their attitude toward the
Internet should be considered. There are some key aspects that can be seen as
barriers against using the Internet as well as some motivating factors. On a more
theoretical level, these may be described as costs and rewards, such as in Thibaut
and Kelley‚Äôs (1959) social exchange theory. Thibaut and Kelley‚Äôs (1959) model
was originally used to describe personal relationships; however, here, it will be
suggested to determine how elderly people behave in terms of using the Internet.
One of the main assumptions of Thibaut and Kelley‚Äôs (1959) model of social
exchange is that the consequences of (social) interactions can be termed as costs
and rewards. Thibaut and Kelley (1959) describe costs as various negative
components and ‚Äúany factors that operate to inhibit or deter the performance of a
sequence of behavior‚Äù (p. 12). Accordingly, there are high costs if ‚Äúgreat physical
or mental effort is required, when embarrassment or anxiety accompany the action,
or when there are conflicting forces or competing response tendencies of any sort‚Äù
(Thibaut and Kelley 1959, p. 13). On the other hand, the term rewards describes
various positive components of an interaction, for example, ‚Äúpleasures,
satisfactions, and gratifications the person enjoys‚Äù (Thibaut and Kelley 1959, p. 12).
   The relation of costs and rewards leads to specific outcomes of social
interactions and these outcomes are important for the formation of social
relationships. People try to minimize their costs and maximize their rewards in
order to reach ‚Äúexcellent reward-cost positions‚Äù (Thibaut and Kelley 1959, p. 31).
This set of behaviors can be referred to as minimax strategy (e.g., Hogg and
Vaughan 2005).
   These main assumptions of social exchange theory are very elementary
principles that are easily applicable to other research fields. Therefore, the basic
principles of social exchange have been used in several different studies and within
a variety of contexts, for example, marital relationships (Nakonezny and Denton
2008), the relation between principals and agents (Bottom et al. 2006), and social
conformity (Nord 1969). Furthermore, social exchange theory was used in the field
17   The Elderly and the Internet: How Senior Citizens Deal with Online Privacy   239


of sports (Guillet et al. 2002), knowledge sharing (Liao 2008), and psychotherapy
(Derlega et al. 1992). It has also been described as being ‚Äúamong the most influen-
tial conceptual paradigms for understanding workplace behavior‚Äù (Cropanzano and
Mitchell 2005, p. 847).
    Thibaut and Kelley‚Äôs (1959) model may be used here to describe two sets of
behavior in terms of their costs and rewards: Internet use and privacy behavior.
There are barriers or costs associated with both behaviors on the one hand and
perceived benefits or rewards on the other hand. If there is a good ‚Äúcost-reward
ratio‚Äù (Hogg and Vaughan 2005, p. 511) and the perceived benefits exceed the
barriers, one can assume that there is a higher likelihood of using the Internet.
Furthermore, for elderly people who have already decided to use the Internet, this
cost-reward ratio can help to describe patterns of online behavior. Different kinds of
benefits or motivating factors can be outlined for elderly people‚Äôs use of the
Internet. Gatto and Tak (2008) analyzed participants with a mean age of
71.1 years who pointed out the satisfaction with access to information, positive
learning experiences, and the utility of some Internet activities such as online
financial services and shopping. Another very important motivation mentioned by
the older participants was a sense of connectedness. Similarly, results of Saunders
(2004) suggest that elderly people are interested in the use of e-mails because these
are able to enhance contacts with children and grandchildren. As all of these factors
include aspects of pleasure, satisfaction, or gratification, they can be described as
perceived rewards in terms of using the Internet in later life.
    With regard to the costs of using the Internet, Gatto and Tak (2008) showed
some perceived barriers that were typical for the group of elderly people analyzed.
Some of the older individuals mentioned that they were frustrated that it took so
much time to learn computer skills. Furthermore, perceived barriers against using
the Internet were physical and mental limitations that prevented the older
individuals from using the Internet, frustration with the computer equipment, as
well as the opinion that they do not have enough time to use the Internet.
    Another aspect of perceived barriers was the trustworthiness of information.
Some of the older Internet users were concerned about whether they could trust
information that was retrieved via the Internet. Results of an analysis of focus
groups held with elderly people by Saunders (2004) are in line with the results of
Gatto and Tak (2008). Barriers that were mentioned by elderly people were
problems in learning how to use computers and finding persons that could support
them in acquiring computer skills, as well as physical problems such as reduced
eyesight. Furthermore, Saunders (2004) points out that the costs of computers, fear
of appearing incompetent or causing damage to the computers, and concerns about
junk mail or inappropriate websites were seen as barriers against using the Internet
for older people. Taken together, these results are in line with the assumption of
Charness and Boot (2009), who reviewed ‚Äúevidence indicating that attitudes and
abilities are among the most powerful predictors of technology use‚Äù (p. 253).
    Whereas in this section, the costs and rewards of general Internet use were
specified, the following section will address privacy behavior as a specific aspect
of Social Web use.
240                                                                            W. Maa√ü


17.4    Costs and Rewards of Online Privacy Behavior

While the Internet in general and the Social Web in particular are becoming more
important for elderly people, the question arises of how the older generation deals
with privacy issues. In this context it has to be considered that research in online
privacy among the older generation is quite limited (Chai et al. 2008). Nevertheless,
some studies have addressed online privacy behavior already and can help in
understanding the way elderly people behave online. With regard to Thibaut and
Kelley‚Äôs (1959) theoretical model, it can be assumed that elderly people will only
use Internet services requiring the disclosure of personal data if there is a good ratio
of costs and rewards.




17.4.1 Privacy Concerns as Costs of Using the Internet

A very important barrier to using Internet applications are privacy concerns. For
example, Gatto and Tak (2008) described that privacy concerns ‚Äúcaused many of
the older adults to avoid activities on the Internet that could put their personal
information at risk for identity theft‚Äù (p. 808). Furthermore, there is some evidence
that elderly people are more concerned about privacy issues than younger users.
According to an analysis from Burst Media (2009), there are online privacy
concerns among all age groups, but these concerns increase with age. Within the
age group of 18‚Äì24 years, 67.3% of the participants reported concern about privacy
issues whereas among participants aged 55 years and above, 85.7% were
concerned. Similarly, Zukowski and Brown (2007) found that with increasing age
of Internet users, the level of concern for information privacy grows. Moreover,
elderly Internet users had a strong desire to control the amount of information that is
collected about them. Results from ‚ÄúThe Pew Internet & American Life Project‚Äù
(Fox et al. 2000) were similar: together with women, minorities, and people with
less Internet experience, elderly Internet users were one of the groups that seemed
to be most concerned about privacy issues online. Another aspect was pointed out
by B‚Ç¨ uhlmann (2006), who investigated online shopping behavior and found that
privacy issues are also relevant in this context. The Internet was found to be the
most common source of information on products of interest, followed by specialist
shops and magazines. Nevertheless, there are barriers to buying products online, the
main concern being privacy issues. Fifty-six percent of the participants reported
being concerned about online shopping because of potential data abuse, followed
by 55% of participants who were concerned because of payment methods.
   As mentioned above, privacy concerns can be seen as the costs of using the
Internet. As research suggests, the privacy concerns of elderly people are not
inappropriate: for example, Chakraborty et al. (2009) assume that the increase in
elderly people who use the Internet ‚Äúhas also brought along the problem of privacy
invasions and breaches and the senior citizens are among the most vulnerable
17   The Elderly and the Internet: How Senior Citizens Deal with Online Privacy   241


groups in this context‚Äù (p. 1). This view is also supported by other authors. For
example, Chai et al. (2008) described elderly people as being particularly vulnera-
ble to privacy attacks. Shrewsbury (2002) argued that older people are ‚Äúspecial
targets of scams, and the internet broadens their vulnerability, especially as
government makes information about citizens readily available‚Äù (p. 206).
Chakraborty et al. (2009) stressed that there are two reasons that make elderly
people vulnerable in terms of online privacy issues. The first reason is that elderly
people grew up in a more honest world and therefore tend to trust other people. The
second aspect is that senior citizens normally do not spend as much time on the
Internet as younger users do. Because of this difference, which is sometimes called
the ‚Äúgrey digital divide‚Äù (Millward 2003) or the ‚Äúdigital age divide‚Äù (Clarke and
Concejero 2010), elderly Internet users ‚Äúare not as knowledgeable about internet
frauds‚Äù (Chakraborty et al. 2009, p. 1). As mentioned before, although it seems
reasonable to investigate online privacy issues of elderly people, there is less
research about this topic, as Chai et al. (2008) sum up: ‚ÄúGiven the significance
and vulnerability of this demographic group, research on information privacy and
security of "wired" seniors is paramount, yet, such research is quite limited. Most
research regarding cybersecurity and information privacy is with respect to younger
generations‚Äù (p. 1).
   The privacy issues discussed above are contrasted by a number of positive
outcomes of Internet use. As social interactions are usually guided by a desire to
reach a good ratio of costs and rewards (Thibaut and Kelley 1959), elderly people
should also gain perceived rewards from their online self-disclosure. If this were not
the case, they would not be motivated to disclose personal data online. Therefore, it
appears necessary to take a closer look at the disclosure of personal data and the
perceived rewards of this behavior within the next section.



17.4.2 The Benefits of Self-Disclosure as Rewards of Using
       the Internet

Some evidence about the way elderly people deal with online privacy comes from
Pfeil and Zaphiris (2007). According to the results of their qualitative content
analysis within an online community for older people, 71% of the analyzed
messages included activities of self-disclosure, making self-disclosure the most
frequently observed behavior. Pfeil and Zaphiris (2007) assumed that this ‚Äúshows
how important it is for the members of the online community to tell others about
themselves‚Äù (p. 922). Similar conclusions can be drawn with regard to a study by
Mittil‚Ç¨a and Antikainen (2006), who analyzed what aspects enhanced the attraction
of online communities among adults aged 55 years or above. They found that asking
for advice and giving advice, as well as discussion with other members, were
common factors of attraction. Furthermore, building new relationships and meeting
people online, as well as seeking a dating partner, were important motivations.
242                                                                           W. Maa√ü


All of these behaviors include interpersonal interactions that demand users provide
at least some private information.
    In addition, there are some studies that focused more precisely on the role of
online self-disclosure for interpersonal interactions: Pfeil et al. (2010) analyzed the
behavior of members of an online support community for older people and found
self-disclosure to be an important factor for the functioning of such a community.
The authors analyzed messages that were posted within 6 years in a discussion
group focused on the topic of depression within the online community ‚ÄúSeniorNet.‚Äù
The authors investigated threats of interrelated text messages, so called message
sequences. With regard to self-disclosure, Pfeil et al. (2010) were able to demon-
strate that messages that contained self-disclosure were generally responded to by
messages that also contained self-disclosure (this reciprocity effect is also known
from general privacy literature, e.g., Archer 1979). According to the authors, these
message sequences were used to build ‚Äúa sense of commonality and togetherness‚Äù
(Pfeil et al. 2010, p. 354). The authors come to the conclusion that people ‚Äútalk
about themselves, mutually opening up towards each other, often discovering that
they have a lot in common. This is then used as the basis for further conversation to
happen‚Äù (Pfeil et al. 2010, p. 354). Furthermore, the authors showed that there was a
strong relationship between messages high in self-disclosure and messages that
were related to deep support. Deep support refers to text segments ‚Äúin which people
post support that is customized towards the unique situation of the target that the
message is for. It shows that the poster understands the situation of the other, and
often includes advise or sympathy for this person‚Äù (Pfeil et al. 2010, p. 347). Self-
disclosure was often followed by messages that contained deep support and in a
similar way deep support was often followed by messages that contained self-
disclosure. This suggests that self-disclosing in online support communities is an
activity that is often rewarded by social support.
    Another study that emphasizes the importance of self-disclosure in senior
citizens‚Äô Internet use was conducted by Gradis (2003). She analyzed the communi-
cation behaviors that were central for building friendships via e-mail within a group
of elderly Internet users. The sample of Gradis‚Äô (2003) study comprised 90 elderly
individuals who held either intergenerational friendships or friendships with peers.
Gradis (2003) argues that the content people communicate about is important for
the formation of friendships and most friendships are initiated through behaviors
that include self-disclosure. The author showed that there were reciprocal patterns
of communication behaviors and self-disclosure was one of these behaviors. This
finding is in line with the reciprocity effect reported by Pfeil et al. (2010). More-
over, Gradis (2003) suggests ‚Äúthat self promotion (a self presentation strategy) and
self disclosure were the most important communication behavioral differences
between both senior peer and intergenerational friendship pairs who agreed to
maintain friendships as compared with those who did not agree to stay friends‚Äù
(p. 14). Therefore, self-disclosure was found to be an important factor for the
formation and duration of friendships in the analyzed computer-mediated
communication.
17   The Elderly and the Internet: How Senior Citizens Deal with Online Privacy   243


   With regard to Thibaut and Kelley‚Äôs (1959) theoretical assumptions, it can be
summarized that elderly people do provide personal data within the Internet and
that the exchange of private information is one of the attraction factors of using the
Internet. Therefore, this behavior can be described as a form of reward in terms of
Thibaut and Kelley‚Äôs (1959) ratio of costs and rewards.




17.4.3 The Minimax Strategy in Terms of Online Privacy

As there are perceived costs of online self-disclosure on the one hand and perceived
rewards on the other hand, the relationship of these factors should be considered.
Although online self-disclosure seems to be important for elderly Internet users, the
conditions of self-disclosure have to be considered. Mittil‚Ç¨a and Antikainen (2006)
describe that the ‚Äúpossibility to meet different kinds of people and discuss with
them behind a nickname was one of the seniors‚Äô attraction factors‚Äù (p. 274). In this
regard, they describe anonymity as an important aspect for seniors‚Äô use of online
communities. There seems to be a tendency to self-disclose on the one hand and to
keep up privacy by using synonyms and nicknames on the other hand. This
assumption is consistent with the results of a quantitative content analysis from
Nimrod (2009). She analyzed the contents and characteristics of 14 leading online
communities for elderly people. The five main topics that were discussed within the
online communities were fun, retirement, family and health, as well as work and
studies. Results showed that the topics discussed ranged from very private issues,
for example, fear of death and problems in relationships, to more public topics such
as politics. At the same time, Nimrod (2009) points out that most of the participants
‚Äúuse pseudonyms and do not tend to provide identifying details‚Äù (p. 390). She
further assumes that this ‚Äúanonymity may enable expressing thoughts and emotions
never expressed before and experiencing new roles and relationships‚Äù (Nimrod
2009, p. 390).
   With regard to Thibaut and Kelley‚Äôs (1959) theoretical model, the online privacy
behavior of elderly people seems to follow a minimax strategy: on the one hand,
elderly Internet users self-disclose online to acquire the social rewards of this
behavior, for example, social support within an online community. On the other
hand, there are several privacy concerns that can be seen as perceived costs of using
the Internet, for example, concerns of identity theft. Elderly Internet users try to
maximize the social rewards and minimize the costs or risks of this behavior as they
often self-disclose while using a nickname. Therefore, the tendency to self-disclose
on the one hand and preserve anonymity on the other hand can be seen as using a
minimax strategy.
   As there are many Internet-using seniors today, these cannot be seen as a
homogeneous group. Thus, the theoretical assumption of a cost-reward ratio may
not apply to all seniors using the Internet. Nevertheless, social exchange theory
seems capable of explaining the usage patterns of a noteworthy part of Internet-
244                                                                          W. Maa√ü


using seniors and may be a further step toward understanding the older generations‚Äô
online privacy behavior. To further investigate elderly persons‚Äô online behavior, an
explorative study is presented in the next section.




17.5    Disclosing Private Information in Senior Citizens‚Äô Social
        Networking Sites: An Exploratory Study

It was shown above that older individuals often use nicknames to keep up their
anonymity in the Social Web. Self-disclosure on the one hand and activities to
retain privacy on the other hand may be described as a minimax strategy. However,
previous research is quite limited. To date, only scarce information is available
about the older generations‚Äô online self-disclosure, use of privacy settings, and
privacy-related activities. Thus, this study aims to investigate the amount of
information elderly people self-disclose online and their strategies to retain their
privacy.
    In addition to well-established social networking sites such as ‚ÄúFacebook‚Äù or
‚ÄúLinkedIn,‚Äù which address very different groups of people, a variety of online
communities that particularly address older users has developed (e.g., www.
seniornet.org, www.senior.com). As the studies on online privacy presented
above referred to online communities for senior citizens, for example, ‚ÄúSeniorNet‚Äù
in the study of Pfeil et al. (2010), such a specialized network was also selected for
the present descriptive analysis. The network chosen was the German Internet
platform ‚Äúplanetsenior.de,‚Äù which is targeted at seniors aged 50 years and above.
    Within the present study, the type of information about the community members
that was publicly accessible via the World Wide Web was analyzed. Community
members can usually choose between different privacy settings and decide who will
be able to see the data they provide. For example, their information may be either
limited to certain community members, accessible to all community members, or to
all Internet users, (for a taxonomy of social networking sites see Ziegele and
Quiring, this volume, Chap. 13).
    Measures. To investigate the type and amount of personal information elderly
users publish online, the amount of information that was freely accessible via the
World Wide Web was assessed. Four categories of personal information were
analyzed: first name, surname, marital status, and home state. Furthermore, the
number of users providing information in the category ‚Äúabout me,‚Äù where users can
provide a short statement about themselves, and in the category ‚Äúsearching for,‚Äù
where different choices are available (e.g., conversational partner, travelling, or
dating) was calculated. Additionally, it was analyzed if users had uploaded a photo
on their profile page. The analysis did not consider the type of photo (e.g., portrait
of profile owner vs. other motives) and solely assessed the presence or absence of
any kind of photo on the profile.
17   The Elderly and the Internet: How Senior Citizens Deal with Online Privacy         245


Table 17.1 Categories                                  Number of people
analyzed within the online     Category                who provided data          Percentage
community
                               Home state              128                        83.66
                               Marital status           81                        52.94
                               Searching for            59                        38.56
                               Photo                    37                        24.18
                               About me                 27                        17.65
                               First name                 9                        5.88
                               Surname                    1                        0.65
                               Total number of profiles analyzed: N ¬º 153


    Sample. The 153 most current profiles within the age of 50‚Äì95 years of
planetsenior.de were taken into the sample. Of the 153 profiles analyzed, 44.44%
belonged to female users, 55.56% to male users. Users were unable to restrict
access to their biological sex within their profiles, thus this information was
available for all profiles analyzed.
    Results. Personally identifying information, such as first name and surname,
were only provided by a minority of users (Table 17.1). Only 9 (5.88%) members
reported their first name and only one (0.65%) person provided the surname.
Instead, the majority of social networking site members kept their anonymity by
using nicknames. In terms of social exchange theory, this behavior resembles the
elderly Internet users‚Äô attempt to minimize the costs of their online self-disclosure.
Nevertheless, users provided more personal information in other profile categories.
For example, 52.94% of the users stated their marital status and 83.66% disclosed
the state they were living in. A short statement about themselves within the
category ‚Äúabout me‚Äù was provided by 17.65% of the users, and information within
the category ‚Äúsearching for‚Äù was reported by 38.56%. Additionally, some kind of
photo was provided by 24.18% of the community members.
    In line with the results presented in Sect. 17.4.3, senior citizens used nicknames
within their profiles and only rarely provided their first name or surname. Particu-
larly compared to data collected in samples of college students, these percentages
are low. In Tufekci‚Äôs (2008) analysis of ‚ÄúFacebook‚Äù profiles, 94.9% of users
provided their real name, and 62.7% did so in ‚ÄúMySpace.‚Äù However, this difference
between elderly people and college students is not necessarily a mere age effect. As
the results refer to different social networking sites, there can also be differences in
dealing with online privacy because of different network cultures.
    Furthermore, with regard to the social networking site planetsenior.de, marital
status, home state, and information within the category ‚Äúsearching for‚Äù were
disclosed by more than 38% of the users. Statements about the members and photos
were provided less frequently, perhaps because these were seen as more private
issues. Taken together, a significant amount of information was provided and it has
to be considered that the stated details were accessible not only within the social
network site but were publicly available for every Internet user. On the other hand,
it has to be considered that one cannot prove how much of the data provided is true
and how much information is spurious. In this regard, Fox et al. (2000) showed that
246                                                                           W. Maa√ü


elderly people sometimes provided fake personal information in order to protect
their privacy, although this was more common among younger people: Among
Internet users aged 18‚Äì29 years, 35% provided fake information about personal
aspects, compared to only 17% of Internet users aged 50‚Äì64 years.
   As this explorative analysis only considered information that was publicly
accessible via the World Wide Web, it can be considered a quite conservative
approach. One can assume that in some online communities more personal data and
identifying details are disclosed but limited to social networking site members.
However, the social network site analyzed, planetsenior.de, is just one among
several networks in Germany and therefore does not allow a generalization for all
senior citizens‚Äô social networking sites. Furthermore, the analysis is only an explor-
ative attempt to describe elderly people‚Äôs online behavior and considered only the
information elderly individuals provided through the World Wide Web. There is no
information about the amount of personal data that is visible for community
members only. Nevertheless, as research on the older generation is scarce, the
analysis allows a first insight into the amount of information that elderly people
disclose in a social networking site. It shows that profile information seems to differ
in terms of how private it is perceived to be. Users seem to be open-hearted with
information that would not allow their identification, such as home state. However,
they hold back personally identifying information such as their full name.




17.6    Discussion and Conclusion

With regard to the research reviewed above, some key findings can be identified and
discussed. Firstly, the Internet is no longer a medium that is limited to younger
users. There are notable numbers of ‚Äúwired‚Äù seniors and using the Internet for
communicating with others is one of the most important Internet applications for
elderly people. With regard to privacy aspects, two trends can be observed. On the
one hand, elderly people are particularly concerned about privacy aspects and
report more concerns about privacy issues than younger individuals. On the other
hand, older individuals provide a lot of personal data online. With regard to Thibaut
and Kelley‚Äôs (1959) theoretical assumptions discussed in this chapter, these two
observable trends do not necessarily conflict with each other. It seems plausible to
assume that elderly people use a minimax strategy: they try to maximize the
rewards of using the Social Web and simultaneously minimize the costs ‚Äì in this
case the privacy concerns ‚Äì by using nicknames to retain privacy and anonymity.
The results of the exploratory analysis support this theoretical assumption.
Although the perceived gratifications were not measured, the results regarding the
costs are in line with Thibaut and Kelley‚Äôs (1959) theoretical assumptions. A large
number of the analyzed Internet users of a social networking site for senior citizens
do self-disclose to a comparatively high extent but the majority use a nickname so
that privacy concerns are minimized.
17   The Elderly and the Internet: How Senior Citizens Deal with Online Privacy                247


    According to social exchange theory, users should be willing to engage in self-
disclosure online if there is a good ratio of rewards and costs and the perceived
rewards should be higher than the perceived costs. In fact, there are several positive
aspects of using the Social Web and providing personal data that can be described
as social rewards. For example, as demonstrated by studies of Pfeil et al. (2010) and
Gradis (2003), self-disclosure is an important factor with regard to the formation of
friendships within the Internet and is closely linked to the amount of social support
received in online support communities.
    Despite these positive aspects, the possible risks of providing personal informa-
tion online and the special vulnerability to privacy attacks of elderly people should
not be overlooked. It seems important to further investigate privacy behavior and to
educate a safe online behavior that is concordant with individual privacy needs and
concerns. It can be concluded, that if social media applications are used the right
way, there can be many psychosocial benefits for elderly people without them being
particularly vulnerable to privacy menaces.




References

Archer RL (1979) Role of personality and the social situation. In: Chelune GJ (ed) Self-disclosure.
   Origins, patterns, and implications of openness in interpersonal relationships. Jossey-Bass,
   San Francisco, pp 28‚Äì58
Bottom WP, Holloway J, Miller GJ, Mislin A, Whitford A (2006) Building a pathway to coopera-
   tion: negotiation and social exchange between principal and agent. Adm Sci Q 51:29‚Äì58
Burst Media (2009) Online privacy worries increase with age. http://www.marketingcharts.com/
   direct/online-privacy-worries-increase-with-age-8229. Accessed 27 Nov 2010
B‚Ç¨uhlmann A (2006) Senioren im internet ‚Äì Ein neuer Markt. [Senior citizens within the internet ‚Äì
   a new market]. http://www.seniorweb.ch/files/old/joomla/images/stories/Themen/Konsum/
   senioren_im_internet_als_neuer_markt_umfrage_auf_seniorweb.pdf. Accessed 27 Nov 2010
Chai S, Rao HR, Bagchi-Sen S, Upadhyaya SJ (2008). ‚ÄòWired‚Äô senior citizens and online
   information privacy. Paper presented at the tenth ETHICOMP international conference on
   the social and ethical impacts of information and communication technology, Mantua, 24‚Äì26
   Sept 2008. http://www.ccsr.cse.dmu.ac.uk/conferences/ethicomp/ethicomp2008/abstracts/
   ethicomp2008_chai.php. Accessed 27 Nov 2010
Chakraborty R, Rao HR, Uphadhyahy SJ (2009) BANDES: an adaptive decision support system
   for protecting online privacy for senior citizen centers. Paper presented at the fourth Pre-ICIS
   workshop on information security & privacy (WISP) ‚Äì the official annual workshop of
   association of information systems SIG/SEC. http://www.security-conference.org/sigsec/
   WISP2009papers/4.pdf. Accessed 27 Nov 2010
Charness N, Boot WR (2009) Aging and information technology use potential and barriers. Curr
   Dir Psychol Sci 18(5):253‚Äì258
Clarke A, Concejero P (2010) The digital divide ‚Äì services for the elderly and disabled in 2010 ‚Äì
   the PRISMA project. The PRISMA project. http://citeseerx.ist.psu.edu/viewdoc/download?
   doi¬º10.1.1.138.1285&rep¬ºrep1&type¬ºpdf. Accessed 25 Oct 2010
Cropanzano R, Mitchell MS (2005) Social exchange theory: an interdisciplinary review. J Manag
   31:874‚Äì898. doi:10.1177/0149206305279602
Derlega VJ, Winstead BA, Hendrick SS, Berg JH (1992) Psychotherapy as a personal relationship:
   a social psychological perspective. Psychotherapy 29(3):331‚Äì335
248                                                                                        W. Maa√ü


Fox S, Rainie L, Horrigan J, Lenhart A, Spooner T, Carter C (2000) Trust and privacy online: why
   Americans want to rewrite the rules. Pew Internet & American Life Project. http://www.
   pewinternet.org/~/media//Files/Reports/2000/PIP_Trust_Privacy_Report.pdf.pdf. Accessed
   27 Nov 2010
Fox S, Rainie L, Larsen E, Horrigan J, Lenhart A, Spooner T, Carter C (2001) Wired seniors.
   A fervent few, inspired by family ties. Pew Internet & American Life Project. http://www.
   pewinternet.org/~/media//Files/Reports/2001/PIP_Wired_Seniors_Report.pdf.pdf. Accessed
   25 Oct 2010
Gatto SL, Tak SH (2008) Computer, internet and e-mail use among older adults: benefits and
   barriers. Educ Gerontol 34(9):800‚Äì811. doi:10.1080/03601270802243697
Gradis MT (2003) Seniors and friendship formation online. Paper presented at the annual meeting
   of the international communication association, San Diego, 23‚Äì27 May 2003. http://www.
   allacademic.com/meta/p112117_index.html. Accessed 27 Nov 2010
Guillet E, Sarrazin P, Carpenter PJ, Trouilloud D, Cury F (2002) Predicting persistence or
   withdrawal in female handballers with social exchange theory. Int J Psychol 37(2):92‚Äì104.
   doi:10.1080/00207590143000243
Hilt ML, Lipschultz JH (2004) Elderly Americans and the internet: E-mail, TV news, information
   and entertainment websites. Educ Gerontol 30(1):57‚Äì72. doi:10.1080/03601270490249166
Hogg MA, Vaughan GM (2005) Social psychology. Pearson, Harlow
Kohut A, Wike R, Speulda N (2006) Truly a world wide web. Globe going digital. The pew global
   attitudes project. http://pewglobal.org/files/pdf/251.pdf. Accessed 15 Jan 2010
Liao L (2008) Knowledge-sharing in R&D departments: a social power and social exchange
   theory perspective. Int J Hum Resour Manag 19(10):1881‚Äì1895
Madden M (2010) Older adults and social media. Social networking use among those ages 50
   and older nearly doubled over the past year. Pew Internet & American Life Project. http://
   pewinternet.org/~/media//Files/Reports/2010/Pew%20Internet%20%20Older%20Adults%
   20and%20Social%20Media.pdf. Accessed 25 Oct 2010
Millward P (2003) The ‚Äògrey digital divide‚Äô: perception, exclusion and barrier of access to internet
   for older people. First Monday 8(7). http://firstmonday.org/issues/issue8_7/millward/index.
   html. Accessed 13 Nov 2010
Mittil‚Ç¨a T, Antikainen M (2006) Perceived attraction of online communities among elderly people.
   In: Maula M, Hannula M, Sepp‚Ç¨a M, Tommila J (eds) Frontiers of e-business research 2006.
   Proceedings of ICEB + eBRF 2006, Tampere, pp 267‚Äì276. http://www.ebrc.fi/kuvat/
   Mittila_Antikainen_paper.pdf. Accessed 29 Nov 2010
Nakonezny PA, Denton WH (2008) Marital relationships: a social exchange theory perspective.
   Am J Fam Ther 36(5):402‚Äì412. doi:10.1080/01926180701647264
Nimrod G (2009) Seniors‚Äô online communities: a quantitative content analysis. Gerontologist
   50(3):382‚Äì392. doi:10.1093/geront/gnp141
Nord WR (1969) Social exchange theory: an integrative approach to social conformity. Psychol
   Bull 71(3):174‚Äì208
Pew Research Center. Global Attitudes Project (2010) Computer and cell phone usage up around
   the world. Global publics embrace social networking. http://pewglobal.org/files/2010/12/Pew-
   Global-Attitudes-Technology-Report-FINAL-December-15-2010.pdf. Accessed 14 Feb 2010
Pfeil U, Zaphiris P (2007) Patterns of empathy in online communication. In: Proceedings of the
   SIGCHI conference on human factors in computing systems, San Jose, pp 919‚Äì928.
Pfeil U, Zaphiris P, Wilson S (2010) The role of message-sequences in the sustainability of an
   online support community for older people. J Comput Mediat Commun 15(2):336‚Äì363.
   doi:10.1111/j.1083-6101.2010.01523.x
Pierce J (2008) New study released by the center for the digital future and AARP shows internet users
   50+ are rapidly closing the digital divide with booming online activity. Center for the digital
   future. http://www.digitalcenter.org/pages/Archive_content.asp?intGlobalId¬º46&intTypeId¬º2.
   Accessed 15 Feb 2011
17   The Elderly and the Internet: How Senior Citizens Deal with Online Privacy                    249

Pierce J (2009) Annual internet survey by the center for digital future finds large increases in use of
   online newspapers. Center for the Digital Future. http://www.digitalcenter.org/pdf/
   2009_Digital_Future_Project_Release_Highlights.pdf. Accessed 25 Oct 2010
Pierce J (2010) World internet project finds large percentage of non-users, and significant gender
   disparities in going online. World internet project. http://www.digitalcenter.org/pdf/
   2010_digital_future_final_release.pdf. Accessed 14 Feb 2010
Rainie L (2010) Internet, broadband, and cell phone statistics. Pew internet & American life
   project. http://www.pewinternet.org/~/media//Files/Reports/2010/PIP_December09_update.
   pdf. Accessed 29 Oct 2010
Saunders EJ (2004) Maximizing computer use along the elderly in rural senior centers. Educ
   Gerontol 30(7):573‚Äì585. doi:10.1080/03601270490466967
Shrewsbury CM (2002) Information technology issues in an era of greater state responsibilities:
   policy concerns for seniors. J Aging Soc Policy 14(3/4):195‚Äì209
Sum S, Mathews RM, Hughes I (2009) Participation of older adults in cyberspace: how Australian
   older adults use the internet. Aust J Ageing 26(4):189‚Äì193
Thibaut JW, Kelley HH (1959) The social psychology of groups. Wiley, New York
Tufekci Z (2008) Can you see me now? Audience and disclosure regulation in online social
   network sites. B Sci Technol Soc 28(1):20‚Äì36. doi:10.1177/0270467607311484
van Eimeren B, Frees B (2010) Ergebnisse der ARD/ZDF-Onlinestudie 2010 [Results of the ARD/
   ZDF-online study 2010]. Media Perspektiven 7‚Äì8:334‚Äì349
Zickuhr K (2010) Generations 2010. Pew internet & American life project. http://www.
   pewinternet.org/~/media//Files/Reports/2010/PIP_Generations_and_Tech10.pdf.              Accessed
   14 Feb 2011
Zukowski T, Brown I (2007) Examining the influence of demographic factors on internet users‚Äô
   information privacy concerns. In: Proceedings of the 2007 annual research conference of the
   South African institute of computer scientists and information technologists on IT research in
   developing countries, SAICSIT, pp 197‚Äì204, Sunshine Coast, South Africa
Chapter 18
Privacy and Gender in the Social Web

Mike Thelwall




18.1    Introduction

The Social Web (e.g., Facebook, Twitter, YouTube, blogs, discussion forums) is
about communication, often interpersonal communication. It embeds itself into the
lives of users and plays many roles, providing entertainment, supporting
friendships, and hosting debates. It is therefore logical to expect some offline
gendered communication styles and issues to recur in Social Web usage patterns
and goals. Even just in terms of technology uptake there can be clear gender
differences: for example, US girls (ages 14‚Äì17) were recently almost twice as
likely to use Twitter than were US boys (Lenhart et al. 2010).
    Online privacy in the Social Web also has a gendered dimension, stemming from
offline concerns. One clear example of this is stalking: women are more likely to be
the victims of this offence (WHOA 2009), and therefore protecting sensitive details
online may be more important for females. There are also purely online phenomena
that disproportionately impact women, such as cyberbullying (Dehue et al. 2008),
and these also give rise to heightened privacy worries. Despite this, there is
relatively little research that focuses on gender and privacy in the Social Web.
This chapter therefore draws together relevant material from a variety of sources.
Table 18.1 summarizes the key findings.
    There are many different definitions and aspects of privacy. This chapter is
concerned with privacy defined as: selective control over who accesses personal
information, including contact information and personal communication, and con-
trol over the contexts in which the information can be used (Altman 1976;
Nissenbaum 2004). This excludes privacy in the sense of seclusion, which is not
relevant to social activities. Control over the contexts in which information is used is
important because of the ease with which web content can be recycled or forwarded.


M. Thelwall (*)
School of Technology, University of Wolverhampton, Wolverhampton, UK
e-mail: mthelwall@wlv.ac.uk

S. Trepte and L. Reinecke (eds.), Privacy Online,                                   251
DOI 10.1007/978-3-642-21521-6_18, # Springer-Verlag Berlin Heidelberg 2011
252                                                                               M. Thelwall


Table 18.1 Key gender-related Social Web privacy differences
Issue                Gender differences
                     Females more concerned about others accessing their personal information
Privacy fears           (Hoy and Milne 2010; Tufekci 2008a)
                     Males most likely to avoid social websites due to privacy concerns
Avoidance               (Youn and Hall 2008)
                     Females most likely to use active strategies: anonymous posts (Madden
Privacy protection      and Smith 2010), inaccurate information (Oomen and Leenes 2008;
    strategies          Youn and Hall 2008), modest photos (Aguiton et al. 2009)
                     Female bloggers more likely to write personal blogs (VieÃÅgas 2005) and
Blogs                   self-disclose (Hollenbaugh 2010), irrespective of privacy concerns
                     Females more likely to join SNSs (Tufekci 2008b), be more active users
                        (Rosen et al. 2010), and open their profiles to more Friends (e.g., in
                        MySpace: Thelwall 2009); females more likely to read SNS privacy
                        policies and alter privacy settings (Hoy and Milne 2010) and have
                        private profiles (Thelwall 2008b). Females more likely to untag
                        pictures (Hoy and Milne 2010). Females less likely to reveal their
                        phone number (Tufekci 2008a) and address (Acquisti and Gross 2006).
Social network sites    Gender differences in types of information reported vary by SNS
    (SNS)               (Kisilevich and Mansmann 2010; Nosko et al. 2010)
                     Females more vulnerable to personal abuse (see e.g., Burgess and Green
                        2009, p. 96-97) but nevertheless create many intimate videos (see e.g.,
YouTube                 Burgess and Green 2009, p. 80; Longhurst 2009)
                     SNS profiles can give SNS status clues (see e.g., Jernigan and Mistree
                        2009). The Social Web offers controlled privacy to ‚Äúcome out‚Äù
                        (Alexander and Losh 2010; Burgess and Green 2009, p. 80), get
LGBT issues             support (Cooper 2010), and find partners (see e.g., Farr 2010)


    This chapter introduces Social Web Gendered Privacy Model, a new theory of
privacy and gender in the Social Web. It then reviews gender-related privacy
concerns and practices in the Social Web, including a section on lesbian, gay,
bisexual, and transgender (LGBT) issues. For stylistic convenience, I use gender
(i.e., learned behavior) and biological sex interchangeably even though the concepts
are different (Money and Ehrhardt 1982) because the overlap between the two
seems sufficient in this context. The primary privacy concerns that this chapter
addresses are the ability to restrict access to personal information, such as home
address or relationship status, and freedom from harassment in the sense of
unwanted intrusions by others. The latter is perhaps a less obvious choice but is
included because harassment is a gendered privacy issue (Allen 1988, pp. 126‚Äì129).



18.2     Social Web Gendered Privacy Model

This section introduces Social Web Gendered Privacy Model, a new theory to
explain gender differences in privacy concerns and practices in the Social Web. It
argues that there are four key gendered components that impact privacy
concerns‚Äìphysical security, harassment, social communication skills, and social
18   Privacy and Gender in the Social Web                                        253


communication needs‚Äìand that the first two explain gender differences in privacy
concerns, whereas all four are needed to explain gender differences in privacy-
related behaviors. The theory argues that women have more offline concerns for
their physical security and more risk of harassment, and that these concerns make
using the Social Web a privacy risk. This translates into caution about using the
Social Web and a need to use privacy-protecting strategies (e.g., identity conceal-
ment, limiting access to information, withholding personal details). Nevertheless,
women have communication needs that are particularly well met by the Social
Web, and the Internet‚Äôs remote access potentially provides protection from physical
threats and harassment. Thus, women have the greatest incentives to use the Social
Web. Overall, the theory predicts that women will use the Social Web more than
men but be more privacy-conscious when using it. They will also tend to use
services that meet their needs if they can use them in a way that does not greatly
threaten their privacy.
    Physical security: Physical security is a greater concern to women, leading to a
greater need for privacy for personal information, such as a home address or
telephone number. This is because within intimate personal relationships, one of
the ultimate sanctions is violence, and although this is not predominantly directed
by men at women, it has a greater effect on women (Magdol et al. 1997). Violence
by men may also be more severe (e.g., beat up rather than punch: Archer 2002).
Women may also be more concerned about hiding information that may provoke
former partners to violence, such as the existence of a new lover or even just
evidence of socializing in mixed gender settings, such as Facebook photographs of
parties, since new partners and jealousy are particular causes of extreme violence
(Campbell et al. 2003). More widely, rape and sexual assault are crimes that
predominantly target, and threaten, women, giving women ongoing physical secu-
rity privacy concerns (Fairchild and Rudman 2008). Moreover, there have been
many media scares about the potential of the Internet to be used by pedophiles.
These scares often involve older men grooming girls online, perhaps hiding their
age, then arranging to meet them offline (see e.g., O‚ÄôConnell 2003). Such stories
may create an atmosphere in which women may worry about the potential for
strangers to contact or physically locate them. Possibly in response to media
pressure, however, MySpace and Facebook have purged large numbers of
convicted sex offenders (BBC 2007; MSNBC 2008). Nevertheless, it seems likely
that environments that make contact with strangers possible, such as chat rooms,
blogs, bulletin boards, and SNSs, will be somewhat associated with risk, particu-
larly for women.
    Physical security concerns are not necessarily a disincentive to using the Social
Web, however, they can also be an incentive to finding web-based (rather than
offline) safe environments for various purposes. For instance, women (and LGBT
groups) may use the web to build bounded communities that are hidden or protected
from outside intrusion.
    Harassment: Historically, much theorizing about privacy and gender concerned
women‚Äôs privacy being invaded through non-physical sexual harassment. For
instance, it now seems accepted that even in public, people (and women in
254                                                                        M. Thelwall


particular) have the right to anonymity in the sense that others should not draw
attention to them in a thoughtless way, particularly if this is systematic harassment
or is intrusive (Allen 1988, pp. 126‚Äì129). The main gender-related forms of non-
physical harassment probably concern inappropriate sexual comments or drawing
attention to personal appearance. Inappropriate sexual comments seem to be a risk
on the Internet since individuals can be anonymous and hence can be offensive with
little risk of being caught. An example of drawing attention to personal appearance
is a website from 1995 that listed homepages of random women and rated them for
attractiveness (Shade 1996). This is not a legal invasion of privacy but is an overt
form of surveillance by drawing attention to women and using their photographs
out of context, thus diminishing privacy in the sense of the right to anonymity.
(A modern gender-neutral version, HotOrNot.com, is based on self-submission and
so is not intrusive.) More generally, women in society seem to be more frequently
evaluated by physical appearance and so even the need to post profile photographs
in social network sites may be potentially off-putting to females.
    Conversely, Internet-based communication can have the advantage of anonymity
or protecting personal appearance from scrutiny. For instance, a social network site
profile picture may not represent its owner or might present them at their best whilst
allowing them to socialize online without worrying about their appearance at the
time. Similarly, relatively ephemeral social websites, like many online discussion
groups and chat sites, are easy to quit if harassment occurs, minimizing the damage
done.
    Communication needs: Women and men tend to use different offline communi-
cation strategies, probably due to socialization into different gender roles in society
(Holmes 1995), and hence have different communication needs. For instance,
females seem to share more personal information with close friends, whereas
males‚Äô friendships tend to focus instead on shared experiences, such as sports,
and banter (Aukett et al. 1988; Elkins and Peterson 1993). In times of stress women
are more likely to desire communication, such as talking to friends or seeking
advice, whereas men are more likely to try to solve problems alone or to avoid
them. For instance, US women are more likely to seek psychiatric help for emo-
tional problems (Kessler et al. 1981) or medical help for health issues (see e.g.,
Galdas 2005). Hence, it seems that women have a greater need to use the Social
Web than men and to share private personal information and problems online.
    Communication needs may not always be satisfactorily resolved online. Women
seem to be disproportionately victims (and perpetrators) of online abuse within
friendship or acquaintanceship circles, as with the case of cyberbullying (Chisholm
2006).
    Communication skills: Women seem to be better at social communication than
men and may therefore get more benefits from it. This skill may provide an incentive
for females to use the Social Web more. For instance, sentiment is a key component
of effective social communication and women are more skilful at detecting sentiment
in offline communication, are also more effective at encoding (Hall 1984) and
decoding (McClure 2000) non-verbal emotional signals, and use positive sentiment
more, such as with smiles (Hall et al. 2000). For online communication, people
18   Privacy and Gender in the Social Web                                          255


replace non-verbal channels, such as facial expression, with textual equivalents, such
as emoticons (see e.g., Fullwood and Martino 2007; Hancock et al. 2007). Women
seem to be more successful at this in open online discussions (e.g., newsgroups),
although in mixed groups men seem to imitate female styles but with less use of
emoticons for ‚Äúsolidarity, support, assertion of positive feelings, and thanks‚Äù (Wolf
2000). Similarly, in an early study of e-mail, women found it to have a stronger sense
of social presence than men did (Gefen and Straub 1997). Some Social Web evidence
that women are better users is that females are disproportionately chosen as Friends
and Top Friends in MySpace (Thelwall 2008b) and give and receive more positive
sentiment than males in MySpace (Thelwall et al. 2010).
    In summary, whilst there are pressures on females to keep them away from the
Social Web to protect their privacy and security, the Social Web can also provide
relatively secure online alternatives to equivalent offline activities, and can poten-
tially fill female-specific social communication needs that are impossible or diffi-
cult to satisfy offline. Moreover, more skilful use of the Social Web by females may
lead to greater incentives to use it. The Social Web Gendered Privacy Model
suggests that the extent to which women use any particular social website will be
largely determined by the strength of these opposing tendencies.



18.3     Privacy Concerns in the Social Web: The Evidence

This section reviews evidence for gender differences in articulated online privacy
concerns related to the Social Web. The next section examines privacy-related
differences in strategies for using the Social Web. The Social Web Gendered
Privacy Model argues that women will be more concerned than men about online
privacy, but this hypothesis is not clearly supported by existing evidence. A survey
of 5,139 Dutch students found no gender differences in general privacy concerns,
although it is not clear whether the responses were specific to Internet-related issues
(Oomen and Leenes 2008). The study sample was self-selected, with a low response
rate to e-mail invitations and other announcements (2.31%) and a low completion
rate for the questionnaire (25%), which may account for the unusual results.
Alternatively, students in the The Netherlands may be an unusual case. In contrast,
an early study of online marketing contexts (i.e., not the Social Web) in the US
found that women were more concerned about privacy than men but men were
more likely to take steps to actively protect their privacy (Sheehan 1999). A later
study of US children found females were more concerned about online privacy than
males: girls provided inaccurate information to protect themselves whereas boys
tended not to register for new websites instead (Youn and Hall 2008). The remain-
der of this section deals with the specific case of SNSs.
   Privacy concerns vary between online contexts, with SNSs appearing to have the
potential to cause the greatest problems. This is due partly to the proliferation of
personal information within them but also due to their powerful facilities for
spreading that information, such as the News Feed feature in Facebook that
256                                                                      M. Thelwall


controversially broadcasts updates on Facebook activities to Friends (Hoadley et al.
2010). US female students seem to be more concerned than males about unwanted
others viewing their SNS profiles (Tufekci 2008a).
   A different type of privacy concern is the fear of intrusive advertising or
marketing strategies. This is a breach of contextual integrity (Nissenbaum 2009)
because information provided in one context is being reused in another. This is
relevant to the web in general (Zimmer 2008) but particularly for SNSs: the
combination of detailed personal information controlled by the company owning
each site and a mass audience with which to perfect marketing strategies makes
behavioral marketing particularly powerful. The consequent risk to privacy seems
to be widely recognized, for instance, leading to the closing down of Facebook
Beacon in September 2009, although it does not seem to have a natural gender
dimension. Nevertheless, a study of US Facebook users found that women were
more concerned about behavioral advertising (e.g., objecting more strongly to
targeted advertising based upon their personal profile information) (Hoy and
Milne 2010) and this aligns with US female students being more concerned than
US male students with government or commercial access to their SNS information
(Tufekci 2008a).




18.4    Privacy Practices in the Social Web

Women are known to typically disclose more information in face to face commu-
nication than men (Dindia 2002), and so this may be expected to extend to online
contexts that have a flavor of interpersonal communication. Before social network
sites, when personal homepages were a major way to express online identities and
privacy concerns may have been lower, women seemed to post more personal
information online than men. A study of adolescents found that girls‚Äô personal
homepages contained information about romantic relationships more often, for
example, as well as referencing friends and family more frequently (Stern 2004).
    Although women seem to be more concerned about online privacy than men,
their response seems to be to adjust security settings, when available, or to take
more precautions, but to continue posting more personal information than men. For
instance, and as a practical privacy step, in public group discussions women seem to
be more likely to make anonymous postings (Madden and Smith 2010). One
experiment also suggests that women prefer to post more modest pictures of
themselves than do men (Aguiton et al. 2009). Another study found opposite
findings for Dutch students, however: men were more likely to use pseudonyms
or anonymous email addresses and more likely to give false information in response
to personal questions (Oomen and Leenes 2008). These practices were especially
associated with younger students. It may also be that women perceive risks but
make a decision that the benefits from loss of privacy outweigh the risks.
18   Privacy and Gender in the Social Web                                          257


18.5     Blog Posts

Blogs are a popular genre with privacy issues related to self-disclosure. Although
prominent blogs are often essentially online newspapers or news filter sites, most
blogs are online personal diaries (Herring et al. 2004), with the typical subject being
‚ÄúMy life‚Äù and the main purpose being ‚Äúto document [] personal experiences and
share them with others‚Äù (McCullagh 2008, p. 9; see also Nardi et al. 2004). The
diary-like nature of most blogs means that they tend to contain personal information
about the author. Blogs are therefore an odd phenomenon in that they are typically
used to publically discuss personal matters that would not be widely broadcast in
other ways and might also be considered to be private (McCullagh 2008). More
generally, the diary format may allow readers to develop an impression of the
identity of the author that is more than just the sum of the individual facts.
   Blogs kept by women seem to give more detailed personal information even
though they are mainly world-readable (exceptions include LiveJournal via its
privacy settings). A study of 525 Taiwanese bloggers found that women posting
frequently were more likely to value self-expression, whereas men were more likely
to value a personal outcome that might arise as a result of blogging (Lu and Hsiao
2009). Other research has shown that females are more likely to self-disclose in
their blogs (Hollenbaugh 2010) and a survey of 486 early bloggers found that the
vast majority of females (92.5%) characterized their blogs as ‚Äúpersonal ramblings‚Äù
in contrast to a much smaller majority (77.5%) of males (VieÃÅgas 2005). This
confirms an earlier study of British bloggers, which found women more likely to
use blogging as a creative outlet, with this more personal aspect to female blogs
perhaps explaining why female bloggers tend to be less prominent than male
bloggers (Pedersen and Macaffee 2007).
   To set the above in context, when males write diary-like blogs, these seem to
have female-like characteristics (Herring and Paolillo 2006), so the key gender
factor may be the choice of type of blog to write (e.g., diary vs. information filter)
rather than style within the type of blog selected. Moreover, the differences may be
less marked or non-existent for younger users (Huffaker and Calvert 2005).


18.6     SNSs

SNSs have gendered privacy concerns related to their use for identity projection and
friendship maintenance. The dominant uses of SNSs seem to be for keeping in
contact with others and for discovering trivial information about them (Donath
2007; Tufekci 2008b). When considering the posting of personal information
online, there are many potential benefits. For instance, posting a personal photo-
graph in Facebook may be seen as a risk but it is an important part of attracting new
Friends. Both men and women are more likely to befriend the opposite sex if they
have an attractive photo in Facebook so there is equal pressure from this perspective
(Wang et al. 2010).
258                                                                       M. Thelwall


    At the most basic level of publication, (US student) women are more likely to
use an SNS than men (Tufekci 2008b). Women seem to publish more personal
content in SNSs, as a number of studies show. An investigation of US students
found that females spent more time maintaining their social network profile and
posted more photographs online (Rosen et al. 2010). For the same demographic,
women seem to report music, books, and religion more than men but to reveal their
phone number much less (Tufekci 2008a). Another study of US university students
found that females were more concerned about general privacy issues than males
but that gender did not seem to be a factor in the decision to join Facebook. Female
students were less likely to post their address, phone number, or sexual orientation,
but there were no gender differences in reports of political affiliation or birthdays
(Acquisti and Gross 2006). A study of Facebook users from Canadian community
or university networks included an examination of gender differences in disclosure
but had contrasting findings. Information revealed was generally not significantly
different between females and males, except that males revealed more information
about their religion and politics (Nosko et al. 2010). Similarly, a study of five
Russian SNSs found that males disclosed more about political views and sexual
information (e.g., orientation, preferences) in response to standard fields than
females, although females tended to reveal more information about non-sexual
aspects, such as religion and marital status (Kisilevich and Mansmann 2010). In
the same study females were much less likely to reveal their current address.
It seems that women may be more active than men in protecting their SNS profile,
perhaps because of heightened concerns and more content posted. For instance,
young US female Facebook users seem to be more careful about friending and
posting personal pictures than males, and are more likely to take the pro-active
measure of untagging a posted picture (Hoy and Milne 2010).
    One study has gone further than those reviewed above in the sense of building
regression models that differentiate between privacy concerns, behaviors, and
gender. It found some evidence that male students shared more information on
Facebook than female students did when privacy concerns and personalization
practices were factored out (Stutzman et al. 2011). In other words, males with the
same level of privacy concerns and practices as females shared more information,
which seems counterintuitive.
    The SNS MySpace has been discussed in the press in the context of risks to
young people, for instance via stalking. A study of adolescent MySpace profiles has
confirmed that a significant minority contain information of potential concern, such
as photographs in bathing suits and evidence of illegal activity, and many included
information that could be used by strangers to identify them, such as their school
name (28%) and phone number (0.3%) (Hinduja and Patchin 2008). This 2006
study may reflect the situation before security issues became more well-known,
however, and unfortunately did not give a gender breakdown of the results. Another
study found no gender differences in the amount of information published in
MySpace profiles that were public, however (Boyle and Johnson 2010). A possible
explanation is that people who joined MySpace for friendship were more likely to
post personal information, so this motivation may have served to partly offset
18   Privacy and Gender in the Social Web                                       259


female tendencies to privacy ‚Äì female MySpace pages tended to be the most vivid
(Boyle and Johnson 2010), however, which suggests implicit personalization.
   In terms of caution with regard to language used, in the US, female MySpace
users have less strong swearing on their profiles but there is no difference for UK
MySpace users (Thelwall 2008a). The number of registered Friends accepted by an
SNS member also has implications for privacy, and it appears that women seem to
have more Friends than do men (e.g., in MySpace: Thelwall 2009).
   Privacy settings can give some control over the important issue of context
(Nissenbaum 2009) in the sense of which Friends will be able to view any particular
content. Many researchers have called for increased control over context settings so
that users can have full control over who can see what (Leenes 2010). Current
privacy settings are relatively simple, however. In terms of gender differences,
women disproportionately select more restrictive privacy settings. For instance, in
MySpace more females than males maintain private profiles (Thelwall 2008b,
2009). Most (71%) US SNS users aged 18‚Äì29 change their privacy settings, so
this is a widespread practice, even though older users are less concerned with the
issue (Madden and Smith 2010). With regard to young US Facebook users, females
are more likely to control their privacy settings to keep personal information away
from non-Friends and from Facebook‚Äôs News Feeds, are more likely to monitor the
personal settings in Facebook, and are more likely to read privacy policies before
joining an SNS (Hoy and Milne 2010).



18.7     YouTube

There are privacy issues associated with posting personal videos on YouTube as
well as with personal information in the profiles of registered members. Although
not its main feature, YouTube has SNS functionality with member personal pages
and Friend-type connections.
   Video is a potentially intrusive technology due to the inclusion of moving
pictures and sound and the cheap availability of portable camcorders. Women
that post videos of themselves seem to be particularly vulnerable to personal
abuse and sexist comments because of an apparent culture of lack of restraint
in the content of YouTube comments and a predominantly young male audience
(see e.g., Burgess and Green 2009, pp. 96‚Äì97).
   There are several Internet-specific phenomena that involve personal or intimate
video and many of these seem to predominantly involve female subjects.
One specifically female genre is the childbirth video: a type that is reasonably
widespread in YouTube and particularly intimate, although often censored for
nudity (Longhurst 2009). Another genre, originating with webcams but spreading
to YouTube, is the bedroom video. For instance, one of the most popular early
YouTube hits was a video of two girls in a bedroom having fun and dancing to a pop
song (http://www.youtube.com/watch?v¬º-_CSo1gOd48, over 31 million views by
September 2010) (Burgess and Green 2009, p. 26). YouTube also hosts a crossover
260                                                                        M. Thelwall


genre, the video log or vlog. These seem to follow blogs in primarily discussing
personal issues. Whilst there seems to be no systematic evidence of gender bias, it
seems likely that the majority are made by females (see e.g., Burgess and Green
2009, p. 80).



18.8    LGBT Issues

The Social Web Gendered Privacy Model probably applies to LGBT people as
much as to females, but with some differences in the details. For example, LGBT
individuals probably have greater personal security concerns than heterosexual men
‚Äì not due to greater risk from intimate relationships or sexual crimes but as potential
targets of hate crime violence from intolerant individuals within society. Moreover,
in some nations homosexuality or homosexual acts are criminal and can even carry
the death penalty. Similarly, the main harassment risks are probably from the insults
or insulting behavior of intolerant individuals, perhaps protected by anonymity. No
evidence is known about LGBT social communication skill levels but this group
has additional communication needs for social support within society (Goodenow
et al. 2008).
   There are many gender-related privacy concerns for LGBT Social Web users
(Cooper and Dzara 2010). Whilst gender is rarely hidden offline, some prefer to
reveal their LGBT status only to trusted friends or others with a similar status. This
can be a problem for SNSs because of the centrality of public lists of Friends: a
person listing several openly gay Friends may be thought to be gay themselves (see
e.g., Jernigan and Mistree 2009). This may discourage some people from using
SNSs and make others reluctant to openly connect with LGBT friends. Within
MySpace, however, many users clearly declare their sexuality as gay, lesbian,
bisexual, or queer (Drushel 2010), but the proportion of SNS users that conceal
or decide not to declare their sexuality is unknown.
   Many have adopted the Web, and YouTube in particular, as a relatively safe
medium through which to ‚Äúcome out‚Äù or defend themselves in terms of gender (e.g.,
for transgender see: Burgess and Green 2009, p. 80). There is a risk of abuse but
also the potential for support and encouragement. For example, one video author
thanked 15 people for their ‚Äúsweet and nice comments‚Äù (URL withheld). The
coming out YouTube video is even a recognized genre (Alexander and Losh 2010).
   The Social Web also allows the creation of private LGBT enclaves that seem to
be particularly valuable for geographically or socially isolated individuals, such as
married women with children realizing that they are lesbians and needing support to
make difficult life decisions (Cooper 2010). Such enclaves can also help sexual
minorities to meet others for offline liaisons safely (see e.g., Farr 2010).
   A controversial issue is the license that the Internet has given for amateur story
sharing, including slash (Berger 2010) and Yaoi manga (McHarry 2010), which
have been criticized in the belief that women invade the collective privacy of gay
men by writing fiction about male-male relationships for personal gratification
18   Privacy and Gender in the Social Web                                          261


(Berger 2010). The fear is that gay relationships may be distorted, creating
unwanted stereotypes.
   Despite the risks to privacy discussed above, particularly for those who conceal
their sexuality, it seems that Web 2.0 is beneficial overall for its ability to connect
people relatively safely, particularly when overcoming geographic isolation. More-
over, there is anecdotal evidence that, particularly in the US, people tend to ‚Äúcome
out‚Äù online first because of greater safety.



18.9     Conclusions

A simple message from Social Web privacy research in many different types of site
is that women tend to be more concerned about privacy and to take more
precautions to protect their privacy in the Social Web, but they also tend to publish
more, including information of a personal nature. In terms of the Social Web
Gendered Privacy Model, it seems that, for females in general, the benefits of
greater social needs and better social communication skills outweigh the greater
physical security and harassment fears and the latter are ameliorated by a wide
range of privacy-protection strategies, such as giving incorrect information or
invoking privacy options. A corollary is that social websites attracting a predomi-
nantly male audience should be seen as unusual and examined for evidence of a
lack of protection of personal information threatening physical security or a lack of
protection from harassment.
   In this chapter, almost all studies reviewed have quite serious sampling
limitations, such as the use of convenience samples, samples of students alone,
snowball sampling, or a particular national group or website. These limitations are
not discussed in detail but the conclusions should be interpreted with caution as a
result.
   The situation for LGBT Social Web users seems to be similar to that for females:
whilst the Social Web creates particular privacy issues, its benefits seem to out-
weigh these threats, with many examples of innovative and positive uses. Women
and LGBT web users are particularly at risk of violations of contextual integrity
because of the need to provide personal information to meet online goals and the
risk of violence or threats if that information is used by unintended others. Never-
theless, these groups also seem to be the ones that gain the most from the Social
Web.
   Finally, the issue of gender and privacy in the Social Web has received little
targeted research and there is a need for systematic investigations into the
perceptions of privacy issues and differences in privacy-related behaviors between
males and females in all types of social website, and for different nationalities and
cultures. This is also true for LGBT Social Web users, about whom there is almost
no quantitative evidence. The results of both of these areas of research should also
give more general insights into why people use the Social Web and the importance
of privacy for decisions about how to use it. This may lead to future Social Web
262                                                                                   M. Thelwall


systems that are more sensitive to privacy issues and to tests of the Social Web
Gendered Privacy Model to see whether there are important factors missing from it
and whether it fits with wider evidence of gender-related privacy issues.




References

Acquisti A, Gross R (2006) Imagined communities: awareness, information sharing, and privacy
   on the facebook. Lect Notes Comput Sci 4258:36‚Äì58
Aguiton C, Cardon D, Castelain A, Fremaux P, Girard H, Granjon F et al (2009) Does showing off
   help to make friends?. In: Proceedings of the third international ICWSM conference, AAAI,
   Menlo Park, pp 10‚Äì17
Alexander J, Losh E (2010) A you tube of one‚Äôs own?: coming out videos as rhetorical action.
   In: Pullen C, Cooper M (eds) LGBT identity and online new media. Routledge, New York,
   pp 37‚Äì50
Allen AL (1988) Uneasy access: privacy for women in a free society. Rowman & Littlefield,
   Totowa
Altman I (1976) Privacy: a conceptual analysis. Environ Behav 8(1):7‚Äì29
Archer J (2002) Sex differences in physically aggressive acts between heterosexual partners: a
   meta-analytic review. Aggression Violent Behav 7(4):313‚Äì351
Aukett R, Ritchie J, Mill K (1988) Gender differences in friendship patterns. Sex Roles
   19(1‚Äì2):57‚Äì66
BBC (2007) MySpace bars 29,000 sex offenders. http://news.bbc.co.uk/2011/hi/technology/
   6914870.stm. Accessed 21 Dec 2010
Berger R (2010) Out and about: slash fic, re-imagined texts, and queer commentaries. In: Pullen C,
   Cooper M (eds) LGBT identity and online new media. Routledge, New York, pp 173‚Äì184
Boyle K, Johnson TJ (2010) MySpace is your space? Examining self-presentation of MySpace
   users. Comput Hum Behav 26(6):1392‚Äì1399
Burgess J, Green J (2009) Youtube: online video and participatory culture. Polity, Cambridge
Campbell JC, Webster D, Koziol-McLain J, Block C, Campbell D, Curry MA et al (2003) Risk
   factors for femicide in abusive relationships: results from a multisite case control study. Am J
   Public Health 93(7):1089‚Äì1097
Chisholm JF (2006) Cyberspace violence against girls and adolescent females. Ann N Y Acad Sci
   1087:74‚Äì89 (Violence and exploitation against women and girls)
Cooper M (2010) Lesbians who are married to men: identity, collective stories, and the internet
   online community. In: Pullen C, Cooper M (eds) LGBT identity and online new media.
   Routledge, New York, pp 75‚Äì86
Cooper M, Dzara K (2010) The facebook revolution: LGBT identity and activism. In: Pullen C,
   Cooper M (eds) LGBT identity and online new media. Routledge, New York, pp 100‚Äì112
Dehue F, Bolman C, Vollink T (2008) Cyberbullying: youngsters‚Äô experiences and parental
   perception. Cyberpsychol Behav 11(2):217‚Äì223
Dindia K (2002) Self-disclosure research: knowledge through meta-analysis. In: Allen M, Preiss
   RW, Gayle BM, Burrell N (eds) Interpersonal communication research: advances through
   meta-analysis. Erlbaum, Mahwah, pp 169‚Äì185
Donath J (2007) Signals in social supernets. J Comput Mediated Commun 13(1). http://jcmc.
   indiana.edu/vol13/issue1/donath.html. Accessed 17 June 2008
Drushel BE (2010) Virtually supportive: self-disclosure of minority sexualities through online
   social networking sites. In: Pullen C, Cooper M (eds) LGBT identity and online new media.
   Routledge, New York, pp 62‚Äì72
Elkins LE, Peterson C (1993) Gender differences in best friendships. Sex Roles 29(7‚Äì8):497‚Äì508
18   Privacy and Gender in the Social Web                                                   263

Fairchild K, Rudman LA (2008) Everyday stranger harassment and women‚Äôs objectification.
   Soc Justice Res 21(3):338‚Äì357
Farr D (2010) A very personal world: advertisement and identity of trans-persons on Craigslist.
   In: Pullen C, Cooper M (eds) LGBT identity and online new media. Routledge, New York,
   pp 87‚Äì99
Fullwood C, Martino OI (2007) Emoticons and impression formation. Vis Popular Cult 19(7):4‚Äì14
Galdas PM, Cheater F, Marshall P (2005) Men and health help-seeking behaviour: literature
   review. J Adv Nurs 49(6):616‚Äì623
Gefen D, Straub DW (1997) Gender differences in perception and adoption of e-mail: an extension
   to the technology acceptance model. MIS Quarterly 21(4):389‚Äì400
Goodenow C, Szalacha L, Westheimer K (2008) School support groups, other school factors, and
   the safety of sexual minority adolescents. Psychol Sch 43(5):573‚Äì589
Hall JA (1984) Nonverbal sex differences: communication accuracy and expressive style. Johns
   Hopkins University Press, Baltimore
Hall JA, Carter JD, Horgan TG (2000) Gender differences in nonverbal communication of
   emotion. In: Fischer A (ed) Gender and emotion: social and psychological perspectives.
   Cambridge University Press, Cambridge, pp 97‚Äì117
Hancock JT, Landrigan C, and Silver C (2007) Expressing emotion in text-based communication.
   In: CHI ‚Äò07: Proceedings of the SIGCHI conference on Human factors in computing systems,
   ACM, New York, pp 929‚Äì932
Herring SC, Paolillo JC (2006) Gender and genre variation in weblogs. J Sociolinguistics
   10(4):439‚Äì459
Herring SC, Scheidt LA, Bonus S, Wright E (2004) Bridging the gap: a genre analysis of weblogs.
   In: Proceedings of the thirty-seventh Hawaii international conference on system sciences
   (HICSS-37), IEEE Press, Los Alamitos
Hinduja S, Patchin JW (2008) Personal information of adolescents on the internet: a quantitative
   content analysis of MySpace. J Adolesc 31(1):125‚Äì146
Hoadley CM, Xu H, Lee JJ, Rosson MB (2010) Privacy as information access and illusory control:
   the case of the facebook news feed privacy outcry. Electron Commerce Res Appl 9(1):50‚Äì60
Hollenbaugh EE (2010) Personal journal bloggers: profiles of disclosiveness. Comput Hum Behav
   26(6):1657‚Äì1666
Holmes J (1995) Women, men and politeness. Longman, New York
Hoy MG, Milne G (2010) Gender differences in privacy-related measures for young adult
   facebook users. J Interactive Advertising 10(2):28‚Äì45
Huffaker DA, Calvert SL (2005) Gender, identity, and language use in teenage blogs. J Comput-
   Mediated Commun 10(2). http://jcmc.indiana.edu/vol10/issue2/huffaker.html. Accessed 31
   July 2010
Jernigan C, Mistree BFT (2009) Gaydar: facebook friendships expose sexual orientation. First
   Monday 14(10). http://firstmonday.org/htbin/cgiwrap/bin/ojs/index.php/fm/article/view/2611/
   2302. Accessed Oct 2009
Kessler R, Brown R, Broman C (1981) Sex differences in psychiatric help-seeking: evidence from
   four large-scale surveys. J Health Soc Behav 22(1):49‚Äì64
Kisilevich S, Mansmann F (2010) Analysis of privacy in online social networks of Runet. SIN10.
   http://infovis.uni-konstanz.de/papers/2010/sin2011p-kisilevich.pdf. Accessed 31 July 2010
Leenes R (2010) Context is everything sociality and privacy in online social network sites.
   In: Bezzi M, Duquenoy P, Fischer-H‚Ç¨   ubner S, Hansen M, Zhang G (eds) Privacy and identity
   management for life. Springer, Berlin, pp 48‚Äì65
Lenhart A, Purcell K, Smith A, Zickuhr K (2010) Social media & mobile internet use among teens
   and young adults. Pew Internet & American Life Project.http://pewinternet.org/Reports/2010/
   Social-Media-and-Young-Adults.aspx. Accessed 5 Feb 2010
Longhurst R (2009) You tube: a new space for birth? Feminist Rev 93(1):46‚Äì63
Lu H-P, Hsiao K-L (2009) Gender differences in reasons for frequent blog posting. Online Info
   Rev 33(1):135‚Äì156
264                                                                                   M. Thelwall


Madden M, Smith A (2010) Reputation management and social media: how people monitor their
   identity and search for others online. Pew Internet & American Life Project, 26 May 2010.
   http://pewinternet.org/Reports/2010/Social-Media-and-Young-Adults.aspx. Accessed 9 Sept
   2010
Magdol L, Moffitt T, Caspi A, Newman D, Fagan J (1997) Gender differences in partner violence
   in a birth cohort of 21-year-olds: bridging the gap between clinical and epidemiological
   approaches. J Consul Clin Psychiatry 65(1):68‚Äì78
McClure EB (2000) A meta-analytic review of sex differences in facial expression processing and
   their development in infants, children, and adolescents. Psychol Bull 126(3):424‚Äì453
McCullagh K (2008) Blogging: self presentation and privacy. Inf Commun Tech Law 17(1):3‚Äì23
McHarry M (2010) Identity unmoored: Yaoi in the West. In: Pullen C, Cooper M (eds) LGBT
   identity and online new media. Routledge, New York, pp 185‚Äì198
Money J, Ehrhardt A (1982) Man and woman: boy and girl. John Hopkins University Press,
   Baltimore
MSNBC (2008). Facebook gives sex offenders the boot. http://www.msnbc.msn.com/id/
   29289048/ns/technology_and_science-security/. Accessed 21 Dec 2010
Nardi BA, Schiano DJ, Gumbrecht M, Swartz L (2004) Why we blog. Commun ACM
   47(12):41‚Äì46
Nissenbaum H (2004) Privacy as contextual integrity. Washington Law Rev 17(1):101‚Äì139
Nissenbaum H (2009) Privacy in context: technology, policy and the integrity of social life.
   Stanford University Press, Stanford
Nosko A, Wooda E, Molemaa S (2010) All about me: disclosure in online social networking
   profiles: the case of FACEBOOK. Comput Hum Behav 26(3):406‚Äì418
O‚ÄôConnell R (2003) A typology of cybersexploitation and on-line grooming practices. JISC. http://
   www.jisc.ac.uk/uploaded_documents/lis_PaperJPrice.pdf. Accessed 8 Sept 2010
Oomen I, Leenes R (2008) Privacy risk perception and privacy protection strategies. In: de Leeuw
   E, Fischer Hubner S, Tseng J, Borking J (eds) Policies and research in identity. Springer,
   Boston, pp 121‚Äì138
Pedersen S, Macaffee C (2007) Gender differences in British blogging. J Comput Mediat Commun
   12(4):1472‚Äì1492
Rosen D, Stefanone MA, Lackaff D (2010) Online and offline social networks: investigating
   culturally-specific behavior and satisfaction. In: Proceedings of the 43 rd Hawaii international
   conference on system sciences, Institute of Electrical and Electronics Engineers,
   New Brunswick. http://www.communication.buffalo.edu/contrib/people/faculty/documents/
   Stefanone_HICSS2010.pdf. Accessed 31 July 2010
Shade LR (1996) Women, the world wide web, and issues of privacy. Feminist Collections
   17(2):33‚Äì35
Sheehan KB (1999) An investigation of gender differences in on-line privacy concerns and
   resultant behaviors. J Interact Mark 13(4):24‚Äì38
Stern SR (2004) Expressions of identity online: prominent features and gender differences in
   adolescents‚Äô World Wide Web home pages. J Broadcasting Electron Media 48(2):218‚Äì243
Stutzman F, Capra R, Thompson J (2011) Factors mediating disclosure in social network sites.
   Comput Hum Behav 27(1):590‚Äì598
Thelwall M (2008a) Fk yea I swear: cursing and gender in a corpus of MySpace pages. Corpora
   3(1):83‚Äì107
Thelwall M (2008b) Social networks, gender and friending: an analysis of MySpace member
   profiles. J Am Soc Info Sci Technol 59(8):1321‚Äì1330
Thelwall M (2009) Social network sites: users and uses. In: Zelkowitz M (ed) Advances in
   computers. Elsevier, Amsterdam, pp 19‚Äì73
Thelwall M, Wilkinson D, Uppal S (2010) Data mining emotion in social network communication:
   gender differences in MySpace. J Am Soc Info Sci Technol 21(1):190‚Äì199
Tufekci Z (2008a) Can you see me now? Audience and disclosure regulation in online social
   network sites. Bull Sci Technol Soc 28(1):20‚Äì36
18   Privacy and Gender in the Social Web                                                  265

Tufekci Z (2008b) Grooming, gossip, facebook and MySpace: what can we learn about these sites
    from those who won‚Äôt assimilate? Inf Commun Soc 11(4):544‚Äì564
VieÃÅgas FB (2005) Bloggers‚Äô expectations of privacy and accountability: an initial survey.
    J Comput Mediated Commun 10(3). http://jcmc.indiana.edu/vol10/issue3/viegas.html.
    Accessed 5 Feb 2010
Wang SS, Moona S-I, Kwona KH, Evansa CA, Stefanone MA (2010) Face off: implications of
    visual cues on initiating friendship on facebook. Comput Hum Behav 26(2):226‚Äì234
WHOA (2009) 2009 Cyberstalking statistics. Haltabuse.org. http://www.haltabuse.org/resources/
    stats/2009Statistics.pdf. Accessed 8 Sept 2010
Wolf A (2000) Emotional expression online: gender differences in emoticon use. Cyberpsychol
    Behav 3(5):827‚Äì833
Youn S, Hall K (2008) Gender and online privacy among teens: risk perception, privacy concerns,
    and protection behaviors. Cyberpsychol Behav 11(6):763‚Äì765
Zimmer M (2008) The gaze of the perfect search engine: google as an infrastructure of
    dataveillance. In: Spink A, Zimmer M (eds) Web search: multidisciplinary perspectives.
    Springer, Berlin, pp 77‚Äì99
Index




A                                               Communication privacy management, 12
Adolescence, 222‚Äì225, 230, 231                  Communication strategies, 254
Age gap, 237                                    Contextual integrity, 48, 50, 53, 54, 198‚Äì202,
Altman, I., 9, 62                                      256, 261
Anonymity, 14, 64, 107, 129, 145, 147,          Costs and rewards, 235, 238, 239
      151, 152, 181, 192, 197, 230,             Credibility, 23, 214
      243‚Äì246, 254, 260                         Crowding, 33, 35, 38, 40‚Äì42
Audience, 19, 20, 22, 29, 30, 35, 39, 52, 64,
      68, 81‚Äì83, 131‚Äì139, 146‚Äì150, 153,
      159, 162, 168‚Äì170, 177, 178, 184,         D
      210‚Äì212, 214, 215, 256, 259, 261          Data protection, 49, 52, 55
Authenticity, 62, 65‚Äì71, 167, 169, 214          Demographic structure, 236
Autonomy, 10, 12, 14, 47, 51, 53, 63, 67,       Development, 12, 21, 36, 38, 49, 51, 57, 63,
      69, 78, 79, 81‚Äì84, 86, 119, 176,                 66, 81, 93, 111, 118‚Äì120, 122, 137,
      180, 183, 186, 199, 211, 233‚Äì225                 159, 163, 165, 176, 206, 209,
                                                       223‚Äì225, 230, 231
                                                Diffusion, 21, 95, 153, 179
B
Blogs, 159‚Äì161, 163, 164, 252, 253, 257
   micro blogs (see Micro blogs)                E
Boundaries, 12‚Äì14, 35, 37‚Äì42, 55, 66, 67,       Emotional relief, 67
       69, 70, 76, 78, 81‚Äì83, 86, 111, 114,
       131, 133, 160, 162, 207, 209, 215,
                                                F
       222, 223, 225
                                                Facebook, 15, 19, 21‚Äì30, 34, 39‚Äì41, 55, 56,
                                                       77‚Äì81, 83, 91, 92, 95, 97, 98, 100,
C                                                      102‚Äì107, 127, 129, 132, 135, 139,
Co-evolution, 91, 92, 98, 104                          162, 177‚Äì179, 182‚Äì184, 191, 211,
Cognitive needs, 150, 151                              221, 237, 244, 245, 251, 253, 256‚Äì258
Communication, 11‚Äì13, 21, 23, 24, 29, 37,       Family, 20, 22, 27, 28, 38, 41, 49, 51, 65,
      40, 41, 48, 56, 63‚Äì67, 69, 70, 72, 76,           66, 82, 119, 136, 146, 148, 152,
      78, 111, 114, 115, 122, 130‚Äì132,                 159, 195, 205, 213, 243, 256
      134‚Äì135, 139, 142 ‚Äì149, 152, 153,         Friends, 22, 24‚Äì28, 35, 39‚Äì41, 55, 56, 58,
      159, 161, 162, 164, 166, 168‚Äì170,                62, 64, 65, 68, 70, 75, 77, 80, 82‚Äì84,
      175, 176, 178, 180‚Äì182, 184‚Äì186,                 86, 92‚Äì94, 96, 97, 100, 102, 103, 105,
      194, 201, 207, 208, 210, 214, 215,               106, 129, 133‚Äì136, 146, 148, 150, 152,
      222‚Äì224, 226‚Äì228, 230, 231, 237,                 153, 162, 166, 168, 177, 183, 229, 242,
      238, 242, 251, 252, 254, 256, 260                254‚Äì257, 259, 260

S. Trepte and L. Reinecke (eds.), Privacy Online,                                          267
DOI 10.1007/978-3-642-21521-6, # Springer-Verlag Berlin Heidelberg 2011
268                                                                                            Index


Friendships, 33, 56, 91, 93, 97, 100, 103‚Äì106,      N
       228, 229, 242, 247, 251, 254                 Network dynamics, 98, 99, 105, 106
                                                    News factor, 206, 207, 211, 213
                                                    Newsworthy, 208, 209, 211, 212
G                                                   Normative approach, 112, 114
Gender, 36, 37, 40, 83, 94, 106, 119, 199, 251,     Norms, 11, 13, 37, 40, 49, 50, 53, 66‚Äì69, 80,
      252, 254, 255, 257, 258, 260, 261                   83, 86, 112, 114, 115, 122, 153, 163,
Gendered privacy concerns, 257                            167‚Äì169, 185, 198, 207, 211, 224
Groups, 10, 14, 15, 24, 27‚Äì29, 35, 36, 39, 40,
      49, 56, 64, 68, 69, 78, 79, 81, 83, 85, 98,   O
      112, 130, 161, 163, 176, 178, 180, 205,       Older generation, 235, 236, 238, 240, 246
      206, 209, 222, 237, 239, 244, 254, 261
                                                    P
                                                    Participatory journalism, 211
H                                                   Peer influence, 95‚Äì98, 100, 104‚Äì106
Harm principle, 52                                  Persistence, 19, 55, 76, 77, 84, 93, 169, 206,
Homophily, 93, 100, 103, 106                               226‚Äì228, 230
                                                    Personal integrative needs, 150‚Äì152
                                                    Physical privacy, 64, 120, 135, 193
I                                                   Privacy
Identity, 11, 15, 19, 23‚Äì25, 34, 35, 38, 47, 63,       behavior, 27, 55, 61, 91‚Äì97, 100‚Äì107,
       64, 67, 76, 78, 80, 81, 83, 86, 116, 131,           143, 176, 183, 184, 187, 213, 235,
       132, 134, 146, 151, 152, 162, 181, 182,             239, 240, 243, 247
       186, 196, 197, 200, 201, 215, 221, 224,         concerns, 22, 23, 28, 64, 92, 96, 115,
       225, 229, 230, 240, 243, 253, 257                   118, 120, 122, 128, 130, 131, 133,
Impression management, 128, 129, 146, 147,                 134, 137, 139, 143, 179, 191, 195,
       151, 185                                            209, 210, 213, 235, 240, 243, 246,
Individuation, 219                                         251, 252, 255, 258, 260
Informational privacy, 48, 50, 61, 62, 64,             differences, 252
       65, 68, 70, 71, 135, 179‚Äì181, 183,              invasion, 50, 52‚Äì54, 92
       186, 187, 214                                   literacy, 51, 57, 58, 211
Informational self-determination, 51, 52               protection, 34, 48, 50, 51, 54‚Äì57, 64, 78,
Information privacy, 15, 35, 49, 206, 222, 241             85, 92, 111, 112, 116‚Äì122
Informed consent, 52, 53, 58, 196                      risk management, 55
Internet adoption rate, 236                            settings, 20, 22‚Äì30, 34, 35, 41, 42, 50, 55,
Interpersonal, 10‚Äì13, 33, 34, 37, 38, 42, 47,              57, 62, 64, 68, 70, 77, 78, 83, 92, 94, 99,
       49, 66, 70, 95, 149, 194, 201, 222,                 100, 102, 105‚Äì107, 143, 178, 180,
       223, 242, 251, 256                                  183‚Äì185, 192, 198, 244, 252, 257, 259
Intimacy, 36, 41, 47, 48, 64, 67, 70, 81‚Äì83,        Privacy-enhancing technology, 49, 50
       92, 135, 145, 146, 148, 152, 153,            Psychological privacy, 61, 62, 64, 68‚Äì71,
       208, 224‚Äì226, 229                                   135, 136
                                                    Public interest, 52, 53, 209, 215
                                                    Public sphere, 47, 112, 162, 200, 206, 208,
                                                           212, 215
J
Journalism, 161
Journalism research, 206, 208, 214, 215             R
Journalists, 167, 179, 209‚Äì215                      Replicability, 76, 77, 84, 206, 226‚Äì230
                                                    Reserve, 12, 143‚Äì145, 147, 151, 152, 182

M                                                   S
Mass media, 149, 150, 207, 208, 211                 Scalability, 76, 77, 84, 206, 226‚Äì230
Micro blogs, 161, 167, 168                          Searchability, 19, 76, 77, 84, 169, 206,
Minimax strategy, 238, 243, 244, 246                       227‚Äì230
  Index                                                                                          269


  Self-disclosure, 24, 35‚Äì38, 61, 64, 69, 71,       Surveillance, 10, 14, 34, 40, 48, 63, 81, 186,
         82, 115, 128, 129, 143‚Äì149, 152,                  206, 213, 254
         153, 160‚Äì162, 167, 168, 181, 207,
         211, 213, 214, 225, 226, 228‚Äì231,
         235, 241, 243‚Äì246, 257                     T
  Self-efficacy, 30, 117, 120, 122, 224             Taxonomy, 48, 176, 177, 180, 181, 186,
  Self-evaluation, 15, 63, 67, 223, 224                    187, 244
  Self-presentation, 23, 24, 67, 86, 127‚Äì129,       Taxonomy of privacy, 48
         131‚Äì139, 185, 225, 226, 228, 229           True self, 53, 66‚Äì68, 132, 134
  Self-restraint, 47‚Äì 58                            Trust, 21, 23, 34, 36, 40, 41, 51, 65, 67, 70,
  Senior citizens, 235, 236, 238, 240, 242,                119, 133, 223, 231, 239, 241
         244‚Äì247                                    Twitter, 41, 83, 139, 160‚Äì162, 165‚Äì167,
  Sex, 39, 116, 213, 230, 245, 252, 253, 257               169, 170, 178, 237, 251
  Sexual self-exploration, 225, 226, 228,
         230, 231
  Sharing, 11, 19, 20, 22, 28, 30, 34‚Äì40, 50,       U
         76‚Äì79, 81, 83, 86, 94, 96, 111, 146,       Users, 20‚Äì25, 28‚Äì30, 34‚Äì36, 38‚Äì42, 54‚Äì58,
         150‚Äì152, 166, 167, 169, 178, 221,                 61, 62, 64, 65, 67‚Äì71, 77, 78, 83, 85,
         231, 239, 260                                     86, 92, 114‚Äì116, 118‚Äì122, 127‚Äì 129,
  Social exchange theory, 235, 238, 243,                   132‚Äì135, 137, 139, 143, 144, 147‚Äì153,
         245, 247                                          159‚Äì162, 165, 166, 169, 170, 175, 176,
  Social integrative needs, 146, 150, 152                  178, 180‚Äì186, 191, 192, 195‚Äì198, 200,
  Social interaction, 11, 12, 15, 35, 41, 67,              202, 211, 221, 231, 235, 237, 239‚Äì247,
         69‚Äì71, 94, 144, 225, 231                          251, 252, 256‚Äì259
  Social media, 16, 22, 24, 38, 39, 42, 54,         Uses and gratifications, 144, 149, 150
         56‚Äì58, 75, 111, 118, 122, 144,
         191, 201, 207, 210‚Äì215, 225,
         235, 237, 247                              W
  Social network sites, 19, 61, 76, 176, 177, 252   Web 2.0, 261
  Social privacy, 61, 63, 64, 69, 70                Well-being, 21, 24, 51, 62, 68
  Social web, 61‚Äì72, 75, 76, 129, 139,              Westin, A., 9, 62, 193
         143‚Äì153, 159, 162, 166, 170, 175,
         176, 200, 215, 221‚Äì223, 225, 226,
         228, 244, 246, 247, 251, 262               Y
  Software, 39, 50, 115, 163, 166‚Äì170, 192, 229     YouTube, 146, 150, 179, 221, 251, 252, 259,
  Solitude, 63, 119, 145, 146, 225                       260




View publication stats
DATA PRIVACY, ETHICS AND PROTECTION
   GUIDANCE NOTE ON BIG DATA FOR
  ACHIEVEMENT OF THE 2030 AGENDA
This document has been approved through the United
Nations Development Group (UNDG) and applies to all
member entities within the UNDG and its working mechanisms.
The approval of this document is based on consensus among
UNDG members and the provisions contained herein apply
to all UNDG entities; FAO, IFAD, ILO, IOM, ITU, OHCHR,
UNAIDS, UNCTAD, UNDESA, UNDP, UNECA, UNECE, UNECLAC,
UNEP, UNESCAP, UNESCO, UNESCWA, UNICEF, UNIDO,
UNFPA, UNHABITAT, UNHCR, UNODC, UN OHRLLS, UNOPS,
UN OSAA, SRSG/CAC, UN Women, UNWTO, WFP, WHO and
WMO.
TA BL E O F
CO NT ENTS
PURPOSE OF THIS GUIDANCE NOTE‚Äã                                    2

PRINCIPLES‚Äã                                                       4

1. LAWFUL, LEGITIMATE AND FAIR USE‚Äã4

2. PURPOSE SPECIFICATION, USE LIMITATION AND PURPOSE COMPATIBILITY‚Äã4

3. RISK MITIGATION AND RISKS, HARMS AND BENEFITS ASSESSMENT‚Äã      4

4. SENSITIVE DATA AND SENSITIVE CONTEXTS‚Äã                         5

5. DATA SECURITY‚Äã5

6. DATA RETENTION AND DATA MINIMIZATION‚Äã6

7. DATA QUALITY‚Äã6

8. OPEN DATA, TRANSPARENCY AND ACCOUNTABILITY‚Äã7

9. DUE DILIGENCE FOR THIRD PARTY COLLABORATORS‚Äã7

DEFINITIONS & NOTES‚Äã8

ADDENDUM A‚Äã                                                       12

HOW DATA ANALYTICS CAN SUPPORT THE SDGs‚Äã                          12

BIBLIOGRAPHY‚Äã14




1   TABLE OF CONTENTS
PU RP O S E O F
T H I S G U IDAN C E N OTE
This document sets out general guidance on data privacy,                                     At the same time, there are legitimate concerns regarding
data protection and data ethics for the United Nations                                       risks associated with handling and processing of big data,
Development Group (UNDG) concerning the use of big                                           particularly in light of the current fragmented regulatory
data, collected in real time by private sector entities as                                   landscape and in the absence of a common set of principles
part of their business offerings1, and shared with UNDG                                      on data privacy, ethics and protection. These concerns
members for the purposes of strengthening operational                                        continue to complicate efforts to develop standardized
implementation of their programmes to support the                                            and scalable approaches to risk management and data
achievement of the 2030 Agenda2. The Guidance Note                                           access. A coordinated approach is required to ensure the
is designed to:                                                                              emergence of frameworks for safe and responsible use of
                                                                                             big data for the achievement of the 2030 Agenda.
‚óè    Establish common principles across UNDG to support
        the operational use of big data for achievement of the                               The guidance described in this document acknowledges
        Sustainable Development Goals (SDGs);                                                and is based on the UN Guidelines for the Regulation
‚óè     Serve as a risk-management tool taking into account                                   of Computerized Personal Data Files, adopted by the
        fundamental human rights; and                                                        UN General Assembly resolution 45/95, and takes into
‚óè      Set principles for obtaining, retention, use and quality                             account both existing international instruments and
        control for data from the private sector.                                            relevant regulations, rules and policies of UNDG member
                                                                                             organizations concerning data privacy and data protection.
 The data revolution was recognized as an enabler of the                                     This Guidance Note is based on standards that have
 Sustainable Development Goals, not only to monitor progress                                 withstood the test of time, reflecting the strength of their
 but also to inclusively engage stakeholders at all levels to                                core values.
 advance evidence-based policies and programmes and to
 reach the most vulnerable.3 The 2030 Agenda asserts that                                    This Guidance Note is designed to support members and
‚ÄúQuality, accessible, timely and reliable disaggregates data will                            partners of the UNDG in establishing efficient and coherent
 be needed to help with the measurement of progress (SGDs)                                   data collaborations.
 and to ensure that no one is left behind. Such data is key to
 decision making.‚Äù4


1   This Guidance Note focuses on the use of big data collected by non-UN parties.           3   Examples of how big data could be used to support the Sustainable Development
     Many of the guiding principles, however, may be applied in cases where UNDG                 Goals can be viewed in Addendum A.
     members collect big data for the purposes of implementing their mandates, for
     example in the form of photographs or videos collected by unmanned aerial vehi-         4   For more detail, see Transforming our World: The 2030 Agenda for Sustainable
     cles (UAVs).                                                                                Development (A/RES/70/1, p. 11), available at https://sustainabledevelopment.un.org/
                                                                                                 post2015/transformingourworld.
2   This Guidance Note supports implementation of the Quadrennial Comprehensive
    Policy Review of the operational activities for development of the UN system             5   The right to privacy is enshrined by the Universal Declaration of Human Rights,
    (A/C.2/71/L.37), particularly in its call for the UN funds, programmes and specialized       Article 12 (UN General Assembly resolution 217 A(III), Paris, France, 10 December
    agencies to strengthen their support to ‚Äúcollect, analyse and increase significantly         1948.; the International Covenant on Civil and Political Rights, Article 17 (General
    the availability of high-quality, timely and reliable disaggregated data‚Ä¶.and in so          Assembly resolution 2200 A(XXI), New York, 19 December 1966, UN Treaty Series,
    doing utilizing national capacities to the fullest extent possible in the context of         vol. 999, No. 14668, p. 171 and vol. 1057, p. 4019); the Convention on the Rights
    United Nations operational activities for development.‚Äù                                      of the Child (art. 16), the International Convention on the Protection of All Migrant
                                                                                                 Workers and Members of Their Families (art. 14); European Convention on Human
                                                                                                 Rights (art. 8); the American Convention on Human Rights (art. 11).




2     DATA PRIVACY, ETHICS AND PROTECTION: GUIDANCE NOTE ON BIG DATA FOR ACHIEVEMENT OF THE 2030 AGENDA
Reaffirming that the right to privacy is a fundamental human                                  mandates as well as their existing regulations, rules and
right and recognizing the social value of data, including the                                 policies concerning data privacy, data protection, data ethics
value of disaggregated SDG indicators with regard to the                                      and data security. It is recommended that designated legal,
implementation of the 2030 Agenda, this document aims to                                      ethics, privacy and security experts be consulted, when
provide a harmonized general framework for accountable,                                       necessary, regarding the implementation of, and compliance
adequately transparent, and responsible data handling                                         with, this Note. Implementing organizations are encouraged
practices across the UNDG and with partners.                                                  to establish a monitoring mechanism for compliance and
                                                                                              implementation of this Note.
This Guidance Note is not a legal document. It provides
only a minimum basis for self-regulation, and therefore                                       Given continuous advances in technology and data, the
may be expanded and elaborated on by the implementing                                         international landscape concerning data privacy and data
organizations.                                                                                protection (including through the relevant work of the UN)
                                                                                              may also change. Accordingly, this note is a living document
Acknowledging the potential risks and harms as well as the                                    and may also evolve over time.
benefits that can result from big data use, this document
goes beyond individual privacy and considers potential
effects on group(s) of individuals. Additionally, this guidance                                 UNDG is grateful to UN Global Pulse for developing
takes into consideration standards of moral and ethical                                         this Guidance Note and acknowledges the
conduct, and recognizes the importance of context when big                                      invaluable contributions of the Global Pulse Privacy
data is being used.                                                                             Advisory Group as well as other private and public
                                                                                                expert stakeholders. Any questions, comments or
It is recommended that this Guidance Note be implemented                                        recommendations regarding this Guidance Note
through more detailed operational guidelines that account                                       should be directed to kit.doco@undg.org.
for the implementation of UNDG member organizations‚Äô




6   Sustainable Development Goal 17 aims to ‚ÄúStrengthen the means of implementation
    and revitalize the global partnership for sustainable development‚Äù. Target 17.18
    notes the need for data disaggregation by income, gender, age, race, ethnicity,
    migratory status, disability, geographic location and other characteristics relevant in
    national contexts (A/70/L.1.).

7   For more information, see the ‚ÄúReport of the Special Rapporteur on the right to priva-
     cy‚Äù, Joseph A. Cannataci, Annex II. A more in-depth look at Open Data and Big Data
     (A/HRC/31/64, p. 24).




3     PURPOSE OF THIS GUIDANCE NOTE
PRINCIPLES

1. LAWFUL, LEGITIMATE AND FAIR USE                                      2.	PURPOSE SPECIFICATION, USE
                                                                            LIMITATION AND PURPOSE COMPATIBILITY
Data access, analysis or other use must be consistent with the
United Nations Charter and in furtherance of the Sustainable            Any data use must be compatible or otherwise relevant, and not
Development Goals.                                                      excessive in relation to the purposes for which it was obtained.
                                                                        The purpose of data use cannot be changed unless there is a
Whether directly or through a contract with a third party data          legitimate basis, as noted in Section 1. The purpose should be
provider, data should be obtained, collected, analysed or other-        legitimate and as narrowly and precisely defined as practically
wise used through lawful, legitimate and fair means. In particular,     possible. Requests or proposals for data access should be
data access (or collection, where applicable), analysis or other        tailored to a specific purpose.
use should be in compliance with applicable laws, including
data privacy and data protection laws, as well as the highest           There must be a legitimate and fair basis for an incompatible
standards of confidentiality and moral and ethical conduct.             deviation from the purpose for which the data was obtained.
                                                                        However, mere difference in purpose does not make a new
Data should always be accessed, analysed or otherwise used              purpose incompatible. In determining compatibility, for example
taking into account the legitimate interests of those individuals       the following criteria could be considered: how deviation from
whose data is being used. Specifically, to ensure that data use         the original purpose may affect an individual(s) or group(s) of
is fair, data should not be used in a way that violates human           individuals; or the type of data used (e.g. public, sensitive or
rights, or in any other ways that are likely to cause unjustified       non-sensitive); or measure taken to safeguard the identity of
or adverse effects on any individual(s) or group(s) of individuals.     individuals whose data is used (e.g. pseudonymization,
It is recommended that legitimacy and fairness of data use is           masking, encryption).
always assessed taking into account risks, harms and benefits
as discussed in Section 6.                                              The purpose of data access (or collection where applicable)
                                                                        should be articulated no later than the time of data access (or
Big data often contains personal data and sensitive data. The           collection where applicable).
use of personal data should be based on one or more of the
following legitimate and fair bases, subject to implementing
UNDG member organizations‚Äô regulations, rules and policies              3.	RISK MITIGATION AND RISKS, HARMS
(including data privacy and data protection policies): (i) adequate         AND BENEFITS ASSESSMENT
consent of the individual whose data is used, (ii) in accordance
with law, (iii) furtherance of international organizational mandates,
(iv) other legitimate needs to protect the vital or best interest of    A risks, harms and benefits assessment that accounts for data
an individual(s) or group(s) of individuals.                            protection and data privacy as well as ethics of data use should
                                                                        be conducted before a new or substantially changed use of
                                                                        data (including its purpose) is undertaken. Appropriate risk
                                                                        mitigation measures should be implemented. Individual(s) or
                                                                        group(s) of individuals should not be exposed to harm, or to
                                                                        undignified or discriminatory treatment, as a consequence of
                                                                        data use by UNDG member organizations.



4   DATA PRIVACY, ETHICS AND PROTECTION: GUIDANCE NOTE ON BIG DATA FOR ACHIEVEMENT OF THE 2030 AGENDA
Any risks, harms and benefits assessment should take into             The risks, harms and benefits assessment can be a tool that
consideration the context of data use, including social,              helps to assist with determination of whether the use of data is
geographic, political and religious factors. Such assessments         legitimate, appropriate or fair.
should account for potential physical, emotional or economic
harms, as well as any harms that could be caused as a result of
infringement of individual(s)‚Äô rights.                                4.	SENSITIVE DATA AND
                                                                          SENSITIVE CONTEXTS
Any risks, harms and benefits assessment should take into con-
sideration the impact that data use may have on an individual(s)
and/or group(s) of individuals, whether legally visible or not and    Stricter standards of data protection should be employed while
whether known or unknown at the time of data use.                     obtaining, accessing, collecting, analysing or otherwise using
                                                                      data on vulnerable populations and persons at risk, children and
An assessment of harms should consider such key factors as: (i)       young people, or any other sensitive data.
the likelihood of occurrence of harms, (ii) potential magnitude of
harms and (iii) potential severity of harms.                          It is important to consider that context can turn non-sensitive
                                                                      data into sensitive data. The context in which the data is used
Additionally, the assessment should take into account the digital     (e.g. cultural, geographic, religious, the political circumstances,
literacy of both potential users of data and those individuals        etc.) may influence the effect of the data analysis on an individ-
whose data is being used.                                             ual(s) or group(s) of individuals, even if the data is not explicitly
                                                                      personal or sensitive.
Where possible, the assessment should be completed by a
diverse team of experts (e.g. legal, ethics and security experts
as well as subject-matter experts) and, where reasonably practi-      5. DATA SECURITY
cal, a representative of the group(s) of individuals who could be
potentially affected.
                                                                      Data security is crucial in ensuring data privacy and data pro-
The risk of harm is much higher for sensitive data, and stricter      tection. Taking into account available technology and cost of
measures for protection should apply if such data is explicit         implementation, robust technical and organizational safeguards
personal data or is reasonably likely to identify an individual(s)    and procedures (including efficient monitoring of data access
or a group(s) of individuals.                                         and data breach notification procedures) should be implemented
                                                                      to ensure proper data management throughout the data lifecycle
Decisions concerning the use of sensitive data should involve         and prevent any unauthorized use, disclosure or breach of
consultation with groups concerned (or their representative)          personal data.
where possible to mitigate any associated risks.
                                                                      Proactively embedding the foundational principles of Privacy by
Additionally, it is important to take into account risks associated   Design and employing privacy enhancing technologies during
with data breaches and vulnerable data security systems, as           every stage of the data life cycle is strongly recommended
noted in Section 3.                                                   as a measure to ensure robust data protection, in an effort to
                                                                      prevent risks to privacy and harms from arising.
Use of data should be based on the principle of proportionality.
In particular, any potential risks and harms should not be            Personal data should be de-identified, where appropriate, using
excessive in relation to the positive impacts (benefits) of data      such methods as aggregation, pseudonymization, or masking,
use. Furthermore, assessing the effect of data on individual          for example, to minimize any potential risks to privacy, and
rights in conjunction with each other is recommended wherever         taking into account the likely occurrence of any potential harms
possible, rather than taking rights in opposition to each other.      associated with data use and non-use. Where appropriate,
                                                                      UNDG member organizations should consider working with
                                                                      data that has been de-identified by third party data providers
                                                                      prior to its disclosure to the UNDG member organizations.



5   PRINCIPLES
Encrypt personal and sensitive data when transferred to or                                6.	DATA RETENTION AND DATA 			
from any network-connected server. No de-identified data                                      MINIMIZATION
should knowingly and purposely be re-identified, unless there
is a legitimate, lawful and fair basis as noted in Section 1. To
minimize the possibility of re-identification, it is recommended                          Data access, analysis or other use should be kept to the minimum
that de-identified data not be analysed or otherwise used by the                          amount necessary to fulfill its purpose, as noted in Section 2.
same individuals who originally de-identified the data.
                                                                                          The amount of data, including its granularity, should be limited
It is important to ensure that the measures taken to protect                              to the minimum necessary. Data use should be monitored to
privacy and ensure data security do not disproportionately                                ensure that it does not exceed the legitimate needs of its use.
compromise the utility of the data for the intended purpose.
                                                                                          Any retention of data8 should have a legitimate and fair basis,
Such measures should be employed in such a way as to                                      including beyond the purposes for which access to the data
maximize the positive impact expected from the data use and                               was originally granted, as specified in Section 1, to ensure that
to fulfill the purposes for which the data was obtained.                                  no extra or just-in-case data set is stored. Any data retention
                                                                                          should be also considered in light of the potential risks, harms
Data access should be limited to authorized personnel, based                              and benefits as discussed in Section 3. The data should be
on the ‚Äúneed-to-know‚Äù principle. Personnel should undergo                                 permanently deleted upon conclusion of the time period
regular and systematic data privacy and data security trainings.                          needed to fulfill its purpose, unless its extended retention is
Prior to data use, vulnerabilities of the security system                                 justified as mentioned in this Section above. Any deletion of
(including data storage, way of transfer, etc.) should be assessed.                       data should be done in an appropriate manner taking into
                                                                                          consideration data sensitivity and available technology.
Data security measures should be assessed in light of the risks,
harms and benefits of data use, including as noted in Section 3.
                                                                                          7. DATA QUALITY
When considering the risks associated with the vulnerability of
data security systems, it is important to consider factors such
as intentional or unintentional unauthorized data leakage or                              All data-related activities should be designed, carried out,
breach: (i) by authorized personnel, (ii) by known third parties                          reported and documented with an adequate level of quality
who have requested or may have access, or may be motivated to                             and transparency. More specifically, to the extent reasonably
get access to misuse the data and information, (iii) by unknown                           possible, data should be validated for accuracy, relevancy,
third parties (e.g. resulting from publishing data sets or the                            sufficiency, integrity, completeness, usability, validity and
results of an analysis).                                                                  coherence, and be kept up to date.

Special attention should be paid when using cloud services,                               Data quality should be carefully considered in light of the risks
especially with regard to the data security setup and physical                            that the use of low quality data for decision-making can create
locations at which data is stored. Usage of non-cloud storage                             for an individual(s) and group(s) of individuals.
should be considered for sensitive data. When third-party cloud
storage providers are used, potential risks and harms associated                          Data quality must be assessed for biases to avoid any adverse
with the use of such cloud storage, as detailed in Section 3,                             effects, where practically possible, including giving rise to
should be both taken into account.                                                        unlawful and arbitrary discrimination.



9   It is important to emphasize that big data generated by the use of social media,      10   Usually, there will be an opportunity to obtain consent if the organization is the
    mobile phones, credit cards, etc. is usually owned by either the original author or        original data collector. However, in situations where data is being obtained from a
    the digital service provider (e.g. social media platform, mobile phone company or          third party data provider, checking whether a third party data provider has obtained
    bank).                                                                                     adequate consent (e.g. directly or indirectly through the online terms of use) or has
                                                                                               another legitimate basis for collecting and sharing the data is recommended when
                                                                                               conducting a due diligence exercise.




6    DATA PRIVACY, ETHICS AND PROTECTION: GUIDANCE NOTE ON BIG DATA FOR ACHIEVEMENT OF THE 2030 AGENDA
Automatic processing of data, including the use of algorithms,      A risks, harms and benefits assessment (noted in Section 3)
without human intervention and domain expertise should be           should be one of the key accountability mechanisms for every
avoided when data is analysed for decision-making that is likely    use of data, and should help determine what other governance
to have any impact on an individual(s) or group(s) of individuals   mechanisms may be needed to monitor compliance. In particular,
to avoid potential harms resulting from low quality of data.        making data open or being transparent about data uses should
                                                                    be considered a separate stage in the data life cycle, and thus it
A periodic assessment of data quality is recommended during         is recommended that a separate assessment, noted in Section
the data life cycle. Furthermore, it is important to establish an   3, is conducted to address risks, harms and benefits related to
internal system of constant data updating and deletion of           that stage. The assessment should help determine the level of
obsolete data, where appropriate and practically possible.          openness and transparency.



8.	OPEN DATA, TRANSPARENCY AND 		 9.	DUE DILIGENCE FOR THIRD PARTY 		
    ACCOUNTABILITY                     COLLABORATORS

Appropriate governance and accountability mechanisms should         Third party collaborators engaging in data use should act in
be established to monitor compliance with relevant law, including   compliance with relevant laws, including privacy laws as well as
privacy laws and the highest standards of confidentiality, moral    the highest standards of confidentiality and moral and ethical
and ethical conduct with regard to data use (including this         conduct. Their actions should be consistent with the United
Guidance Note).                                                     Nations‚Äô global mandate as well as UN regulations, rules and
                                                                    policies. Furthermore, third party collaborators‚Äô actions should
Transparency is a critical element of accountability. Being         adhere to the Guidance Note, including to have a legitimate and
transparent about data use (e.g. publishing data sets or            fair basis for sharing data with UNDG member organizations.9
publishing an organization‚Äôs data use practices or the use of
algorithms) is generally encouraged when the benefits of            It is recommended that a process of due diligence be
transparency are higher than the risks and possible harms.          conducted to evaluate the data practices of any potential third
                                                                    party collaborators.10
Except in cases where there is a legitimate reason not to
do so, at minimum, the existence, nature, anticipated period        Legally binding agreements outlining parameters for data access
of retention and purpose of data use as well as the algorithms      and handling (e.g. data security, data formats, data transmission,
used for processing data should be publicly disclosed and           fusion, analysis, validation, storage, retention, re-use, licensing,
described in a clear and non-technical language suitable for        etc.) should be established to ensure reliable and secure access
a general audience.                                                 to data provided by third party collaborators.

Open data is an important driver of innovation, transparency
and accountability. Therefore, whenever possible, the data
should be made open, unless the risks of making the data open
outweigh the benefits or there are other legitimate bases not to
do so. Disclosure of personal information, even if derived from
public data, should be avoided or otherwise carefully assessed
for potential risks and harms as described in Section 3.




7   PRINCIPLES
D E FI N ITIO NS & N OTES


                        AGGREGATION                                                       ADEQUATE
                        OF DATA                                                           CONSENT



For the purposes of this document, data aggregation means        Consent is adequate, when it is freely given, explicit, informed
a process through which individual level data sets are           and in writing. Adequate consent should be obtained prior
combined to such a format so they cannot be traced back          to data collection or when the purpose of data re-use falls
or linked to an individual. Typically, aggregated data is used   outside of the purpose for which consent was originally
for analytical or statistical purposes to present a summary or   obtained.
determine average results/numbers regarding age, gender,
community preferences, etc.                                      To ensure that consent is informed, it is recommended that
                                                                 as many details about the purpose of data use (e.g. any risks,
In addition, the UN Archives and Records Management              harms and potential positive and negative impacts) should
Section in its ‚ÄúGlossary of Recordkeeping Terms‚Äù defines         be included in the notice when the consent is sought. It is
‚Äúaggregated records‚Äù as referring to ‚Äúaccumulated or collected   important to note that in many instances consent may not be
records that are organized into groupings or series‚Äù. From       adequately informed. Thus, it is important to consider assessing
the International Organization for Migration (IOM), the IOM      the proportionality of risks, harms and benefits of data use
Data Protection Manual also defines ‚Äúaggregate data‚Äù as          even if consent has been obtained.
information, usually summary statistics, which may be compiled
from personal data, but are grouped in a manner to preclude      Consent should be obtained before data is collected or
the identification of individual cases.                          otherwise used, and individuals should have an opportunity
                                                                 to withdraw their consent or object to the use of their data.
                                                                 Checking whether a third party data provider has obtained
                                                                 adequate consent (e.g. directly or indirectly through the
                                                                 online terms of use) or has another legitimate basis for
                                                                 sharing the data is recommended.

                                                                 While there may be an opportunity to obtain consent at the
                                                                 time of data collection, re-use of data often presents difficulties
                                                                 for obtaining consent (e.g. in emergencies where you may
                                                                 no longer be in contact with the individuals concerned). In
                                                                 situations where it is not possible or reasonably practical to
                                                                 obtain informed consent, as a last resort, data experts may




8   DATA PRIVACY, ETHICS AND PROTECTION: GUIDANCE NOTE ON BIG DATA FOR ACHIEVEMENT OF THE 2030 AGENDA
still consider using such data for the best or vital interest of        ‚Äúa paradigm for enabling the collection, storage, management,
an individual(s) or group(s) of individuals (e.g. to save their         analysis and visualization, potentially under real-time constraints,
life, reunite families, etc.). In such instances, any decision to       of extensive datasets with heterogeneous characteristics‚Äù.
proceed without consent must be based on an additional
detailed assessment of risks, harms and benefits to justify
such action and must be found fair, lawful, legitimate and
in accordance with the principle of proportionality (e.g. any                                     DE-IDENTIFICATION
potential risks and harms should not be excessive in relation                                     (ANONYMIZATION)
to the expected benefits of data use).
                                                                                                  OF DATA


                                                                        For the purposes of this document, de-identification shall
                           BIG                                          mean a process of using all reasonable means to convert
                           DATA                                         personal data into anonymous data, such as that it cannot
                                                                        be traced back or linked to an individual(s) or group(s) of
                                                                        individuals. There are many different methods that could be
                                                                        used to de-identify data. Examples include data aggregation,
There are many definitions of big data. UN Global Pulse                 masking, pseudonymization and k-anonymity.
in its report ‚ÄúBig data for development: Challenges and
opportunities‚Äù takes a traditional approach, defining big data          De-identified data may be stripped of all personal identifiers
as ‚Äúa massive volume of both structured and unstructured                such as name, date of birth, exact location, etc.); however,
data that is so large that it‚Äôs difficult to process with traditional   as noted in the UN Global Pulse Data Innovation Risk
database and software techniques. The characteristics which             Assessment Tool guidance, this data, although not directly or
broadly distinguish big data are sometimes called the ‚Äò3                explicitly identifying or singling out an individual or group(s)
V‚Äôs‚Äô: more volume, more variety and higher rates of velocity.‚Äù          of individuals, can still be linked to an individual(s) or
The report provides examples of such data, including data               group(s) of individuals with the use of adequate technology,
from sensors used to gather climate information, posts to               skills and intent and thus may require the same level of
social media sites, digital pictures and videos posted online,          protection as explicit personal data.
transaction records of online purchases, and from cell phone
GPS signals.

There are many types of big data with potential utility for
development. This document applies specifically to data
                                                                                                  DIGITAL
collected by the private sector in real time and that may                                         LITERACY
be used for the observation of human behaviour by UNDG,
thus affecting the decision-making process with regard to
the individual(s) or group(s) of individuals. Usually, such data
would be owned by an original author (e.g. a social media               For the purposes of this document, digital literacy means
user) or a digital service provider (e.g. a social media platform,      how people understand the data they work with or share,
a mobile phone company, a bank, etc.).                                  including how much they are aware of the positive and
                                                                        negative impacts of data use and sharing. Digital literacy
The International Telecommunication Union (ITU), in its                 concerns both actors who are using the data and those
recommendation on ‚ÄúBig data ‚Äì Cloud computing based                     whose data is being used.
requirements and capabilities‚Äù defines big data as




9    DEFINITIONS & NOTES
                        ENCRYPTION                                                        PERSONAL
                                                                                          DATA


In the glossary of the UN Archives and Records Management        For the purposes of this document, personal data means data,
Section, encryption is defined as a ‚Äúsecurity procedure that     in any form or medium, relating to an identified or identifiable
translates electronic data in plain text into a cipher code by   individual who can be identified, directly or indirectly, by means
means of either a code or a cryptographic system in order        reasonably likely to be used, including where an individual can
to render it incomprehensible without the aid of the original    be identified by linking the data to other information reasonably
code or cryptographic system‚Äù.                                   available. Personal data is defined by many regional and
                                                                 national instruments and can also be referenced as personal
                                                                 information or personally identifiable information.

                        GROUP(S) OF                              Personal data can be made private by its owner by restricting
                        INDIVIDUALS                              access to it or made public by its owner (e.g. shared publicly
                                                                 on social media). While sharing (and oversharing) personal
                                                                 details about oneself and others on social networks has
                                                                 become more common, such publicly available information
                                                                 remains personal and it can pose risks to those individuals
For the purposes of this document, reference to a group(s) of    represented in the data.
individuals also includes ‚Äúlegal‚Äù invisible (known or unknown)
group(s) of individuals, as adapted from the human-rights
based approach to data of the Office of the United Nations
High Commissioner for Human Rights (OHCHR).
                                                                                          PRIVACY


                        MASKING
                                                                 A report of the Special Rapporteur to the Human Rights
                                                                 Council (A/HRC/23/40) defines privacy as ‚Äúthe presumption that
                                                                 individuals should have an area of autonomous development,
                                                                 interaction and liberty, a ‚Äòprivate sphere‚Äô with or without
For the purposes of this document, masking means a               interaction with others, free from State intervention and from
de-identification technique whereby the original personal        excessive unsolicited intervention by other individuals‚Äù. While
information collected from social media, such as comments,       the majority of literature and legislature concentrates on ‚Äúthe
photos and videos, is altered to such extent that it cannot      right to privacy‚Äù, in another such report (A/HRC/31/64), it has
be traced back or linked to an individual(s) or group(s)         been noted that there is currently no internationally accepted
of individuals.                                                  definition of privacy.




10   DATA PRIVACY, ETHICS AND PROTECTION: GUIDANCE NOTE ON BIG DATA FOR ACHIEVEMENT OF THE 2030 AGENDA
                        PRIVACY BY
                        DESIGN                                                                      RE-IDENTIFICATION


Privacy by Design11 is an approach that promotes technology         For the purposes of this document, re-identification means a
design and engineering to incorporate privacy into the              process by which de-identified (anonymized) data becomes
design process from the start. The concept includes seven           re-identifiable again and thus can be traced back or linked
guiding principles on privacy and security.                         to an individual(s) or group(s) of individuals. As noted in
                                                                    UN Global Pulse Data Innovation Risk Assessment Tool
                                                                    guidance, to determine whether an individual(s) or group(s)
                                                                    of individuals is identifiable, consider all of the means
                                                                    reasonably likely to be used to single out an individual(s)
                        PSEUDONYMIZATION                            or group(s) of individuals. Factors to consider regarding
                                                                    whether it is reasonably likely that an individual(s) or group(s) of
                                                                    individuals can be re-identified include the required expertise,
                                                                    costs and amount of time required for re-identification, and
                                                                    reasonably and commercially available technology.
For the purposes of this document, pseudonymization
means modifying personal data by removing or substituting
all direct identifiers (e.g. in many instances name, address,
date of birth, etc.) with other unique identifiers (e.g. in many
instances hashing algorithms, ID numbers, etc.) in such a way                                       SENSITIVE DATA
that it is still possible to distinguish a unique individual in a
data set. Masking is a type of pseudonymization.


                                                                    For the purposes of this document, sensitive data should be
                                                                    considered as any data related to (i) racial or ethnic origin,
                                                                    (ii) political opinions, (iii) trade union association, (iv) religious
                                                                    beliefs or other beliefs of a similar nature, (v) physical or
                                                                    mental health or condition (or any genetic data), (vi) sexual
                                                                    orientation and other related activities, (vii) the commission
                                                                    or alleged commission of any offence, (viii) any information
                                                                    regarding judicial proceedings, (ix) any financial data, (x)
                                                                    children and (xi) an individual(s) or group(s) of individuals that
                                                                    face any risks of harm (e.g. physical, emotional, economic).




                                                                    11   The approach was developed by Dr. Ann Cavoukian. A summary is available at
                                                                         https://www.ipc.on.ca/wp-content/uploads/Resources/7foundationalprinciples.pdf




11   DEFINITIONS & NOTES
      BIG
AD D ENDU M A
     DATA                                                                                    BIG
                                                                                             DATA
                                             NO POVERTY                                                              CLEAN WATER


      SDGs
HOW DATA A N A LYT I C S CA N S UPPO RT   THE patterns
                                      Spending   SDGSon                                                              AND SANITATION
                                      mobile phone services can                                                Sensors connected to

                                                                                             SDGs
                                                                                                                                    Spend
                                      provide proxy indicators                                                 water pumps can track
                                                                                                                                    mobil
                                      of income levels                                                         access to clean waterprovid
                                                                                                                                         of inc
                                                                             ZERO HUNGER                               AFFORDABLE AND
                                                                      Crowdsourcing or tracking                        CLEAN ENERGY


                       BIG
                                                                      of food prices listed online           Smart metering allowsCrowd
                                                                      can help monitor food                  utility companies to of foo
     How data science                                                 security in near real-time             increase or restrict thecan h
                                                                       How data science                      flow of electricity, gassecur
     and analytics can
                       DATA
     contribute to sustainable
                                                                       andGOOD HEALTH AND
                                                                                  analytics
                                                                          WELL-BEING
                                                                          NO    POVERTY               can    or water to reduce waste
                                                                                                             and ensureCLEANadequate
                                                                                                                                WATER


                       SDGs
                                                                   Mapping the
                                                                   Spending         movement
                                                                                patterns   on of             supplyAND       SANITATION
                                                                                                                        at peak   periods
                                                                       contribute               to
                                                                                     users cancan
                                                                   mobile phone services
                                                                                                     sustainable
                                                                                                             Sensors connected to mobil
                                                                                                                                          Mapp
     development                                                   helpdevelopment
                                                                         predict
                                                                   provide   proxythe  spread
                                                                                    indicators                         DECENTcan
                                                                                                             water pumps          WORK
                                                                                                                                     trackAND p
                                                                                                                                          help
                                                                      infectious
                                                                   of income        diseases
                                                                                levels                       accessECONOMIC          GROWTH
                                                                                                                         to clean water   of inf
                                                                                                             Patterns in global postal
                                                                          QUALITY
                                                                          ZERO HUNGER  EDUCATION             traffic AFFORDABLE         AND
                                                                                                                       can provide indicato
                                                                   Citizen  reportingorcan
                                                                   Crowdsourcing          tracking           such asCLEAN       ENERGY
                                                                                                                          economic    growth,
                                                                                                                                          Citize
                                                                   reveal
                                                                   of foodreasons     for online
                                                                            prices listed                    remittances,
                                                                                                             Smart      meteringtrade  and
                                                                                                                                   allows    GD
                                                                                                                                          revea
                                                                   student
                                                                   can helpdrop-out
                                                                               monitor rates
                                                                                         food                utility companies to         stude
     How data science                                              security in near real-time                increase  INDUSTRY,
                                                                                                                           or restrict the
                                                                          GENDER EQUALITY                    flow ofINNOVATION        AND
                                                                                                                        electricity, gas
     and analytics can                                                    GOOD
                                                                   Analysis         HEALTH AND
                                                                              of financial                   or water  INFRASTRUCTURE
                                                                                                                          to reduce waste Analy
                                                                          WELL-BEING
                                                                   transactions    can reveal                Dataensure
                                                                                                             and      from GPS    devicestrans
                                                                                                                              adequate
     contribute to sustainable                                     the spending
                                                                   Mapping          patterns of
                                                                               the movement                  can be at
                                                                                                             supply      used  forperiods
                                                                                                                            peak
                                                                                                                                          the sp
                                                                                                                                   trafficand d
                                                                   and  different
                                                                   mobile  phoneimpacts
                                                                                     users can               control and to improveof eco
     development                                                   of economic
                                                                   help  predict shocks     on
                                                                                   the spread                public DECENT
                                                                                                                        transportWORK men AND a
                                                                   men  and women
                                                                   of infectious    diseases                           ECONOMIC GROWTH
                                                                                                             Patterns in global postal
                                                                          QUALITY EDUCATION                  traffic can provide indicato
                                                                   Citizen reporting can                     such as economic growth,
                                                                                                www.unglobalpulse.org
                             www.unglobalpulse.org                 reveal reasons for           @UNGlobalPulseremittances,
                                                                                                                  2017          trade and GD
                             @UNGlobalPulse 2017                   student drop-out rates
                                                                                                                       INDUSTRY,
                                                                          GENDER EQUALITY                              INNOVATION AND
12    DATA PRIVACY, ETHICS AND PROTECTION: GUIDANCE NOTE ON BIG DATA FOR ACHIEVEMENT OF THE 2030 AGENDA
                                                                   Analysis of financial                               INFRASTRUCTURE
                                                                   transactions can reveal                   Data from GPS devices
                  NO POVERTY                    CLEAN WATER                       REDUCED INEQUALITY             LIFE BELOW WATER
           Spending patterns on                 AND SANITATION             Speech-to-text analytics       Maritime vessel tracking
           mobile phone services can      Sensors connected to             on local radio content         data can reveal illegal,
           provide proxy indicators       water pumps can track            can reveal discrimination      unregulated and unreported
           of income levels               access to clean water            concerns and support           fishing activities
                                                                           policy response
                  ZERO HUNGER                     AFFORDABLE AND                                                 LIFE ON LAND
           Crowdsourcing or tracking              CLEAN ENERGY                    SUSTAINABLE CITIES      Social media monitoring
           of food prices listed online   Smart metering allows                   AND COMMUNITIES         can support disaster
           can help monitor food          utility companies to             Satellite remote sensing       management with
           security in near real-time     increase or restrict the         can track encroachment         real-time information
                                          flow of electricity, gas         on public land or spaces       on victim location,
                  GOOD HEALTH AND         or water to reduce waste         such as parks and forests      effects and strength
                  WELL-BEING              and ensure adequate                                             of forest fires or haze
ble        Mapping the movement of        supply at peak periods                  RESPONSIBLE
           mobile phone users can                                                 CONSUMPTION AND                PEACE, JUSTICE
           help predict the spread                DECENT WORK AND                 PRODUCTION                     AND STRONG
           of infectious diseases                 ECONOMIC GROWTH          Online search patterns or             INSTITUTIONS
                                          Patterns in global postal        e-commerce transactions        Sentiment analysis of
                  QUALITY EDUCATION       traffic can provide indicators   can reveal the pace            social media can reveal
           Citizen reporting can          such as economic growth,         of transition to energy        public opinion on effective
           reveal reasons for             remittances, trade and GDP       efficient products             governance, public service
           student drop-out rates                                                                         delivery or human rights
                                                INDUSTRY,                         CLIMATE ACTION
                  GENDER EQUALITY               INNOVATION AND             Combining satellite imagery,         PARTNERSHIPS
           Analysis of financial                INFRASTRUCTURE             crowd-sourced witness                FOR THE GOALS
           transactions can reveal        Data from GPS devices            accounts and open data can     Partnerships to enable the
           the spending patterns          can be used for traffic          help track deforestation       combining of statistics,
           and different impacts          control and to improve                                          mobile and internet data can
           of economic shocks on          public transport                                                provide a better and real-
           men and women                                                                                  time understanding of today‚Äôs
                                                                                                          hyper-connected world




      13      ADDENDUM A
BIBLIOGRAPHY

Asia-Pacific Economic Cooperation           International Committee of the Red          Organization of American States (1969).
(2005) APEC Privacy Framework.              Cross (2015). ICRC Code of Conduct on       American Convention on Human Rights,
December. Singapore: APEC Secretariat.      Data Protection. November. Geneva,          ‚ÄúPact of San Jose‚Äù, Costa Rica. 22
                                            Switzerland: ICRC.                          November. Washington, DC: OAS.
Cavoukian, Ann (2011). Privacy by Design:                                               (art. 11).
The 7 Foundational Principles. January.     International Committee of the Red Cross
Ontario, Canada: Information and Privacy    (2016). ICRC Rules on Personal Data         United Nations (1989). Convention on the
Commissioner of Ontario.                    Protection. January. Geneva, Switzerland:   Rights of the Child. Treaty Series 1577
                                            ICRC.                                       (1989): 3 (art. 16).
Council of Europe (1981). Convention
for the Protection of Individuals with      International Conference of Data            United Nations Archives and Records
regard to Automatic Processing of           Protection and Privacy Commissioners,       Management Section. Glossary of
Personal Data. 28 January. ETS No. 108.     Resolution on Data Protection and           Recordkeeping Terms.
Strasbourg, Austria.                        International Organizations.
                                                                                        United Nations Children‚Äôs Fund (2007).
(1953). European Convention for             International Organization for Migration    The Paris Principles: Principles and
the Protection of Human Rights and          (2010). IOM Data Protection Manual.         Guidelines on Children Associated
Fundamental Freedoms, as amended            Geneva, Switzerland: IOM.                   with Armed Forces or Armed Groups.
by Protocols Nos. 11 and 14. 4 November.                                                February. New York: UNICEF.
ETS 5 (art 8).                              International Organization for
                                            Standardization. Online Collection:         United Nations Data Revolution Group
Economic Community of West African          Information Security Management             (2014). A World that Counts: Mobilising
States (2010). Supplementary Act A/         Systems. Geneva, Switzerland: ISO.          the data revolution for sustainable
SA.1/01/10 on Personal Data Protection                                                  development. The UN Secretary-General‚Äôs
within ECOWAS. 16 February.                 International Telecommunication Union       Independent Expert Advisory Group
                                            (2015). Recommendation Y.3600. Big          on a Data Revolution for Sustainable
European Union (2016). Regulation (EU)      data ‚Äì Cloud computing based require-       Development. November.
2016/679 of the European Parliament         ments and capabilities. 6 November.
and of the Council of 27 April 2016 on                                                  United Nations Development Programme
the protection of natural persons with      Organisation for Economic Co-operation      and United Nations Global Pulse
regard to the processing of personal        and Development (1980). OECD                (2016). A Guide to Data Innovation for
data and on the free movement of such       Guidelines on the Protection of Privacy     Development ‚Äì From ideas to proof-of-
data, and repealing Directive 95/46/        and Transborder Flows of Personal Data.     concept. New York: UN.
EC (General Data Protection Regulation)     23 September. Paris: OECD.
(Text with EEA relevance).




14   DATA PRIVACY, ETHICS AND PROTECTION: GUIDANCE NOTE ON BIG DATA FOR ACHIEVEMENT OF THE 2030 AGENDA
United Nations Economic Commission            Big data for development: Challenges          United Nations Statistical Commission
for Europe (2014). The Role of Big            and opportunities, p. 13.                     (2015). Report of the Global Working
Data in the Modernisation of Statistical                                                    Group on Big Data for Official Statistics.
Production. Geneva, Switzerland: UNECE.       Principles. Data Privacy and Data             17 December. E/CN.3/2016/6.
                                              Protection Principles.
United Nations General Assembly (2016).                                                     United Nations, Human Rights Council
Quadrennial comprehensive policy              Privacy Tools. Risks, Harms, Benefits         (2016). Report of the Special Rapporteur
review of operational activities for devel-   Assessment.                                   on the right to privacy, Joseph A.
opment of the United Nations system. 28                                                     Cannataci. 8 March. A/HRC/31/64, pp.
October. A/C.2/71/L.37.                       United Nations High Commissioner for          6, 10. Annex II. A more in-depth look at
                                              Human Rights (2016). A Human Rights-          Open Data & Big Data.
A/70/L.1 (2015). Transforming Our World:      Based Approach to Data: Leaving No
the 2030 Agenda for Sustainable               One Behind in the 2030 Development            Report of the Special Rapporteur on the
Development. 18 September.                    Agenda, Guidance Note to Data                 promotion and protection of the right to
                                              Collection and Disaggregation. 19             freedom of opinion and expression, Frank
A/RES/68/261 from 29 January 2014             February. Geneva, Switzerland; UNHCR.         La Rue. 17 April. A/HRC/23/40, p. 6.
(2014). Fundamental Principles of Official
Statistics. 3 March.                          (2015). Policy on the Protection of           World Food Programme (2017). WFP
                                              Personal Data of Persons of Concern           Guide to Personal Data Protection and
A/RES/45/158 (1990). International            to UNHCR. May. Geneva, Switzerland:           Privacy. 22 February. Rome: WFP.
Convention on the Protection of the           UNHCR.
Rights of All Migrant Workers and                                                           World Health Organization (2016).
Members of their Families (art. 14). 18       United Nations Human Rights Committee         Guidance on Good Data and Record
December.                                     (1988). CCPR General Comment No 16:           Management Practices. WHO Technical
                                              Article 17 (Right to Privacy), The Right to   Report Series, No. 996. Annex 5.
A/RES/45/95 (1990).Guidelines for the         Respect of Privacy, Family, Home and
regulation of computerized personal data      Correspondence, and Protection of
files. 14 December.                           Honour and Reputation. 8 April.

2200/A (XXI) (1966).International             United Nations Information Security
Covenant on Civil and Political Rights. 19    Special Interest Group (June 2013). Use
December. UN Treaty Series, vol. 999,         of Cloud Computing in the UN System,
No. 14668, p. 171 and vol. 1057, p. 4019      Recommendations for Risk Mitigation.
(art. 17).
                                              United Nations International Law
217 A (III) (1948). The Universal             Commission (2006). Report on the
Declaration of Human Rights. 10               work of the fifty-eight session (2006).
December. Paris, France. (art. 12).           Annex IV. Protection of Personal Data in
                                              Transborder Flow of Information.




15   BIBLIOGRAPHY
The United Nations Development Group (UNDG) unites
the 32 UN funds, programmes, specialized agencies,
departments, and offices that play a role in development.
Since 2008, the UNDG has been one of the three pillars
of the UN System Chief Executives Board for Coordination,
the highest-level coordination forum of the United
Nations system.

At the regional level, six Regional UNDG Teams play
a critical role in driving UNDG priorities by supporting
UN Country Teams with strategic priority setting,
analysis and advice.

At the country level, 131 UN Country Teams serving
165 countries and territories work together to increase
the synergies and joint impact of the UN system.

The Development Operations Coordination Office (DOCO)
is the secretariat of the UNDG, bringing together the
UN development system to promote change and innovation
to deliver together on sustainable development.




@UNDGDOCO WWW.UNDG.ORG
Data privacy as a
strategic priority
Enabling growth and innovation by
using information governance to
effectively manage data privacy risk
The imperative
Data privacy as to
                a strategic
                   become cloud
                            priority
                                  native




2
                                                                                Data privacy as a strategic priority




Introduction
In a world where the value and volume of data are growing exponentially, data
privacy has emerged as a board-level issue and potential source of competitive
advantage‚Äînot just a compliance requirement. But the devil is in the details.
Without a comprehensive and effective program for information governance
(IG), data privacy remains a compliance challenge and a potential reputation
time bomb.

Companies today face increasing pressure from regulators and the marketplace
to improve how they collect, use, store and delete personal information (PI),
and how they manage data privacy. And the pressure will only increase as
innovations such as the Internet of Things (IoT), mobile, and big data‚Äîas well
as always-on virtual assistants‚Äîgenerate more and more data and insights
about everything people say and do. Under certain circumstances, consumers
now have the right to access and delete their data, and they can opt out of
sale of their PI, further driving the need for strong information governance
and management.

Until now, many companies' data privacy efforts have revolved around the
specific privacy-related regulations and requirements for their industries,
to which the standard response was a mix of narrowly-focused initiatives
designed to satisfy those specific requirements without comprehensively
tackling larger problems. However, that scattered, one-off approach to data
privacy may no longer be good enough.

Driven by the rising importance and visibility of the data privacy issue‚Äîas
well as by sweeping data regulations such as the European Union's General
Data Protection Regulation (GDPR) and the California Consumer Privacy Act
(CCPA)‚Äîcompanies can benefit from a more comprehensive and coordinated
approach to IG. Such an approach helps a business efficiently and effectively
tackle the full range of data-related challenges‚Äîincluding data privacy. It does
so by reconciling and rationalizing the overlapping and conflicting regulatory
requirements and then addressing them in a coordinated manner that avoids
duplication and gaps. It can also help the business use its superior data
privacy capabilities as a strategic differentiator in an increasingly digital and
competitive marketplace.



                                                                                                                  3
Data privacy as a strategic priority




                                                        Regulatory compliance
                                                        and beyond
                                                        Regulatory requirements and business drivers are prompting companies to focus more
                                                        attention and resources on managing data privacy risk:

                                                        ‚Ä¢‚Ä¢ Regulatory requirements: Data privacy and cybersecurity rules not only require
                                                           the protection of customer data, they impose obligations to assure the data‚Äôs quality,
                                                           completeness, and governance‚Äîincluding limited acquisition and use, as well as
                                                           appropriate retention and disposition. Today, a number of states are considering
                                                           legislation similar to the CCPA (Figure 1). These regulations span every aspect of a
                                                           company‚Äôs interactions with its customers.

                                                        ‚Ä¢‚Ä¢ Business drivers: Companies are seeking competitive advantages in the marketplace by
                                                           better mining existing information and taking advantage of non-traditional sources and
                                                           uses of data, advanced analytics, artificial intelligence, and new ways of interacting with
                                                           customers, such as digitization.
                                                        A comprehensive and coordinated IG program can enable companies to more effectively
                                                        address both the regulatory requirements and the business drivers.

Figure 1: Similar legislative initiative to the CCPA1
                                                                                                                                                       Data Privacy and Consumer
                                                                                  Chicago Data Collection and
                                                                                                                                                                Protection (SB 110)                      Consumer Data
                                                                                        Protection Ordinance
                                                                                                                                                                                                         Privacy (SB 120)
                                                         Protect Personal
                                                    Information (HB 457)
                Washington
                                                                                               HB 1138
       Privacy Act (HB 1854)
                                                                                               HB 1485
                                                   WA

                                                                                                                                                                                                      ME
    Health Information                                                       MT                       ND
Protection Act (SB 703)
                                                                                                                         MN                                                                 VT
                                              OR
                                                              ID                                                                                                                                 NH
               CCPA                                                                                  SD                                 WI                                                       MA        Insurance Consumer
                                                                                                                                                                                      NY
                                                                                                                                                   MI                                            CT RI     Protection Sales Act
                                                                                  WY                                                                                                                       (HB 5903)

                                                                                                                              IA                                                PA                         Right to Privacy
                                                   NV                                                NE                                                                                    NJ              Act (SB 1108)
                                                                                                                                                            OH                       MD
                                                                                                                                             IL   IN                                        DE        SB A4640
      Protection for Consumer                                       UT                                                                                                           DC
                                                                                     CO                                                                                                               SB 2834
     Data Privacy (HB 18-1128)                                                                                                                                        WV
                                          CA                                                                   KS                                                                VA
                                                                                                                                   MO                  KY
          Insurance Information                                                                                                                                                 NC                    Online Consumer
              Practices (SB 1113)                                                                                                                 TN                                                  Protection Act (SB 613)
    Personal Information Access;                                                                                    OK
              Websites (HB 2259)                               AZ                  NM
                                                                                                                                   AR                                      SC


                                                                                                                                             MS   AL             GA
                                                                                                                                                                                                Cybersecurity Care and
                                                                                                                                                                                                Disposal (HB 2793)
                                         AK                                                                                        LA
                                                                                                          TX
                                                        Consumer Information                                                                                           FL
             HI                                            Privacy Act (SB 176)

                  Identifying Information
                  Relating to Privacy (SB 418)           Texas Consumer Privacy Act (HB 4390)
                                                         Texas Privacy Protection Act (HB 4518)




                                                        Status as of 07/22/19

                                                             No consumer or data privacy action to note                                                 Bill became Law

                                                             Bill introduced and/or passed by House or Senate                                           Bill failed to pass ‚Äì reintroduction possible
                                                             (includes ‚ÄúIn Committee‚Äù)
4
                                                                                                Data privacy as a strategic priority




These require more than just tougher information security; they require a comprehensive
and effective IG program‚Äîalong with the infrastructure necessary to collect only the
required information and to retain it no longer than necessary. For example, section 500.13
of the New York Department of Financial Services (NY DFS) Cybersecurity Regulation states
that companies "shall include policies and procedures for the secure disposal on a periodic
basis of any Nonpublic Information that is no longer necessary for business operations
or other legitimate business purposes, except where such information is required to be
retained by law or regulation." (See sidebar on data minimization).




Data minimization: More isn‚Äôt always better

One consequence of the complex and expanding set of data-related regulations is that many companies
have begun to rethink their traditional posture of ‚Äúkeeping everything.‚Äù They‚Äôre taking steps instead
to determine what information is needed, how it‚Äôs protected and used, and how long to keep it. This
emerging focus on data minimization is quickly rising to the top of the IG agenda, driven in significant
respect by regulations including the GDPR and NY DFS Cybersecurity Regulations.
Until recently, the prevailing mindset about data was ‚Äúmore is better.‚Äù But companies and regulators are
now recognizing that it‚Äôs possible to have too much of a good thing. Like a cluttered garage, collecting and
retaining too much data creates a whole host of problems, including:

‚Ä¢‚Ä¢ Cost: The more data you have, the more it costs to store it.
‚Ä¢‚Ä¢ Security: The more data you have, the harder it is to secure‚Äîand the greater the potential risk of a
   security breach.
‚Ä¢‚Ä¢ Reduced effectiveness: When you have too much data, it‚Äôs harder to find what you actually need.
‚Ä¢‚Ä¢ Compliance: The traditional "keep everything" approach now violates some of the new and emerging
   data regulations.

Companies need to actively determine what information to collect and keep, and how long to keep it. This
will require careful analysis and reconciliation of the various regulatory requirements‚Äîmany of which are
overlapping or conflicting‚Äîas well as careful consideration of the company's information needs. It will
also require new policies and system capabilities, including data-driven disposition and retention, that can
help the company efficiently and effectively handle the day-to-day task of data minimization without losing
valuable information or causing unnecessary risk.




                                                                                                                                  5
Data privacy as a strategic priority




                                       Are your data privacy capabilities
                                       up to the task?
                                       Thoughtful questions about the state of data privacy risk at your organization can help start
                                       the conversation and drive toward the right answers. Here are some key questions your
                                       organization should be asking:

                                       ‚Ä¢‚Ä¢ Are we being proactive in identifying and complying with all the laws and regulations that
                                          govern data capture, use, retention, security, and disposal at our company?

                                       ‚Ä¢‚Ä¢ Do we have an adequate information governance foundation in place that allows us to deal
                                          with current and upcoming data privacy challenges, such as consumer access and deletion
                                          requests, and consumers opting out of sale?
                                       ‚Ä¢‚Ä¢ Do we know what information we have, how complete and accurate it is, where it is, and
                                          how it‚Äôs used and protected?
                                       ‚Ä¢‚Ä¢ Do we have the appropriate leadership, structure, capabilities, resources, and support to
                                          address these risks comprehensively‚Äîin the context of our business model and goals?
                                       ‚Ä¢‚Ä¢ Do we receive and retain the necessary information to support key business decisions
                                          and actions?
                                       ‚Ä¢‚Ä¢ Have we organized the compliance and privacy functions to best support and oversee
                                          our business and operations?
                                       ‚Ä¢‚Ä¢ How do our IG program and capabilities align to industry standards and peer organizations?




                                       Key process areas for managing
                                       data privacy risks
                                       Complying with the regulations can be difficult and complex, requiring companies to assess
                                       a wide range of activities (strategy, people, process, and technology), and to build diverse
                                       capabilities and tools in four key process areas (records management, privacy/compliance,
                                       crisis management/cyber, and IG). These capabilities and tools encompass:

                                       ‚Ä¢‚Ä¢ Data inventory: Companies need to know the type and source of data collected, stored,
                                          and used‚Äîand how accurate and complete it is. Inventories should be risk‚Äëranked to
                                          reflect inherent risk and quantify business needs for the data.

                                       ‚Ä¢‚Ä¢ Classification: Companies need to define the types of data collected and retained‚Äîand
                                          which data is personal versus public‚Äîin a manner that‚Äôs compliant with privacy regulations
                                          and that clearly classifies individuals impacted by the information to ensure customer
                                          access requests are properly addressed.

                                       ‚Ä¢‚Ä¢ Third-party relationships: Companies need a comprehensive inventory of third-
                                          party relationships (and of the data collected, stored, or shared with third parties) to
                                          implement programs that properly address issues related to data quality, use, privacy,
                                          and security. Contracts should be created or amended to hold these third parties to
                                          new privacy standards.




6
                                                                                                     Data privacy as a strategic priority




‚Ä¢‚Ä¢ Portability and erasure: Companies must manage customer requests that involve
   moving or eliminating personal information.

‚Ä¢‚Ä¢ Data security: Companies need to implement and maintain reasonable security
   procedures and practices. They also need to respond effectively to data breaches.

‚Ä¢‚Ä¢ Consent: Companies need management tools capable of handling consumer requests in a
   timely manner, including specific authentication and permissions for cross-affiliate marketing.

‚Ä¢‚Ä¢ Oversight and monitoring: Companies must implement programs that are comprehensive
   and strong, yet flexible enough to adapt to continued changes and ongoing regulatory/
   business implementations. Such programs can benefit from increased focus on training and
   change management procedures to ensure they‚Äôre properly implemented through the three
   lines of defense, which can help avoid regulatory enforcement, fines, and penalties.




Data privacy and reputation risk

The increased number of laws and regulations is the vanguard of a paradigm shift in which the general
population is growing more concerned about their private data. Recent headlines have shone a spotlight on
the potential misuse of consumer data, and the reality is that any organization collecting data about
consumers‚Äîespecially if they share the data with vendors or third parties‚Äîmay be at risk of having their
data misused. Significant reputation damage can result from misused data and/or data breaches.
Organizations should understand and prepare for the reputational risks that extend beyond non-compliance
with the myriad of data privacy laws and regulations.




                                                                                                                                       7
Data privacy as a strategic priority




                                             The three lines of defense
                                             In designing and implementing their approaches to IG, companies should assign accountability
                                             using the three lines of defense model.
                                             To advance the effectiveness of this model, some organizations are placing the privacy
                                             function and its resources squarely within the compliance organization. After all, data privacy
                                             represents a critical, if not one of the most critical, compliance risk for these organizations.
                                             And tying the privacy and compliance functions together promotes oversight, clarity of roles
                                             and responsibilities, effective management of regulatory matters and relationships, and
                                             timely reporting to senior leaders and the board.




Figure 2: Typical roles and responsibilities across the lines of defense in privacy organizations


                                             ‚Ä¢‚Ä¢ Owns privacy risks and overall accountability for protecting personal data against current and emerging
    1st line of defense




                                               risks across the data lifecycle
                                             ‚Ä¢‚Ä¢ Operationalizes control procedures to effectively align with privacy policies and applicable laws
                                               and regulations
                                             ‚Ä¢‚Ä¢ Implements privacy program within the lines of business
                              Lines of
                                             ‚Ä¢‚Ä¢ Conducts periodic assessments to assess effectiveness of privacy controls
                          business/privacy
                              liaisons       ‚Ä¢‚Ä¢ Tracks and reports performance of privacy controls




                                             ‚Ä¢‚Ä¢ Establishes governance, policies, oversight, and accountability for privacy
                                             ‚Ä¢‚Ä¢ Develops core privacy program components such as processes, tools, templates, guidance, and privacy
    2nd line of defense




                                               notices for the lines of business to use in building controls
                                             ‚Ä¢‚Ä¢ Establishes privacy risk assessment requirements and performs second line risk-based monitoring
                                               and enforcement of privacy controls to ensure compliance
                                             ‚Ä¢‚Ä¢ Develops and implements competency-based training and awareness for privacy related policies
                               Global          and standards
                           privacy office    ‚Ä¢‚Ä¢ Reports performance of privacy program to the board; risk committees; and privacy, security, and data
                           & compliance        governance councils
                                             ‚Ä¢‚Ä¢ Manages communication with supervisory authorities/regulators on privacy and data protection
    3rd line of defense




                                             ‚Ä¢‚Ä¢ Evaluates, on an independent basis, the effectiveness of privacy controls to provide assurance
                                             ‚Ä¢‚Ä¢ Regularly reports control effectveness results and challenges to senior leader and the audit committee




                           Internal audit




8
                                                                                                  Data privacy as a strategic priority




Practical lessons learned
Implementing a comprehensive and coordinated approach to IG can be challenging and
time-consuming. Here are some leading practices to keep in mind:

‚Ä¢‚Ä¢ Establish an enterprise governance strategy: Create the vision, driven by executive
   leadership, to help ensure the program‚Äôs success. Verify that the governance strategy
   supports and aligns with corporate strategy and growth objectives. Make sure line-of-
   business and functional leadership approves and supports the program prior to execution.

‚Ä¢‚Ä¢ Deliver value quickly: Ensure information assets deliver business value. Set realistic
   goals and expectations with program leadership. Maintain a straightforward, no-nonsense
   approach; simplicity is usually better. Measure success at the appropriate stages to confirm
   program alignment.

‚Ä¢‚Ä¢ Build a foundation with the end in mind: Establish an extendable IG model to
   support expanding business needs. The IG operating model framework should be able
   to support multiple types of data and customer interactions, and it must bring together
   all the relevant stakeholders.

‚Ä¢‚Ä¢ Don't boil the ocean: Perform sufficient due diligence during the planning phase to
   isolate and prioritize the top data issues/risks. Focus on determining the most cost-
   effective solutions and develop a comprehensive plan for resolution. Ensure solutions
   don‚Äôt negatively affect customer-facing processes but improve them.

‚Ä¢‚Ä¢ Establish business ownership and accountability for data: Develop clearly defined
   roles and responsibilities and drive process improvements to gain efficiencies with lines
   of business and functional areas. Establish ownership within business management and
   mandate accountability for data quality.

‚Ä¢‚Ä¢ Treat information governance as a program, not a project: IG is a journey, and
   adoption must be institutionalized in the organization‚Äôs culture. IG requires executive
   sponsorship and support to be effective; a bottom-up approach won‚Äôt work. Policies and
   standards will need to be approved by the IG governance structure, and resources will
   need to be made available to test adherence and measure quality.




                                                                                                                                    9
Data privacy as a strategic priority




                                       Getting started
                                       Although the task of organizing and implementing IG can seem daunting, the end results
                                       are worth the attention and effort. In addition to enabling compliance with data privacy
                                       regulations, IG can pay significant business dividends‚Äîparticularly when accomplished through
                                       careful planning and execution, collaboration with all key stakeholders, and strong executive
                                       sponsorship. Here are some considerations for getting started:

                                       ‚Ä¢‚Ä¢ Assess the current state of IG capabilities across the enterprise
                                       ‚Ä¢‚Ä¢ Develop a vision for IG tailored to the organization's data privacy risks and requirements, as
                                          well as its business strategy and goals
                                       ‚Ä¢‚Ä¢ Craft a multiyear roadmap, with priority on high-impact initiatives
                                       ‚Ä¢‚Ä¢ Develop, fund, staff, and roll out the IG program organization
                                       ‚Ä¢‚Ä¢ Select and begin to implement IG tools
                                       ‚Ä¢‚Ä¢ Consider an experiment or pilot to understand value and opportunity


                                       Ultimately, the path to effective management of data privacy risk through IG starts by making it a
                                       high priority within your organization. Are you ready to take that critical first step?




                                       Endnotes
                                       1.   Sources for CCPA similar initiatives: https://adexchanger.com/privacy/bevy-of-ccpa-amendments-pass-california-
                                            assembly-next-stop-the-senate/
                                            https://iapp.org/news/a/ccpa-update-senate-committee-pares-back-amendments/
                                            https://www.dataprotectionreport.com/2019/06/nevada-new-york-and-other-states-follow-californias-ccpa/
                                            https://www.lexology.com/library/detail.aspx?g=b9b1a955-c0e4-4f65-a33b-e7af82224477
                                            https://www.cyberadviserblog.com/wp-content/uploads/sites/18/2019/06/State-Privacy-Law-Tracker-06-19.pdf




10
                                                                          Data privacy as a strategic priority




Contacts
Jay Cohen                            Tim Cercelle
Managing director                    Managing director
Deloitte Risk & Financial Advisory   Deloitte Risk & Financial Advisory
Deloitte & Touche LLP                Deloitte & Touche LLP
jaycohen@deloitte.com                tcercelle@deloitte.com


Rich Vestuto                         Niels Aafjes
Managing director                    Senior manager
Deloitte Risk & Financial Advisory   Deloitte Risk & Financial Advisory
Deloitte Transactions and            Deloitte & Touche LLP
Business Analytics LLP               niaafjes@deloitte.com
rvestuto@deloitte.com




                                                                                                           11
This publication contains general information only and Deloitte is not, by means
of this publication, rendering accounting, business, financial, investment, legal,
tax, or other professional advice or services. This publication is not a substitute
for such professional advice or services, nor should it be used as a basis for any
decision or action that may affect your business. Before making any decision or
taking any action that may affect your business, you should consult a qualified
professional adviser. Deloitte shall not be responsible for any loss sustained by
any person who relies on this publication.

About Deloitte
Deloitte refers to one or more of Deloitte Touche Tohmatsu Limited, a UK
private company limited by guarantee (‚ÄúDTTL‚Äù), its network of member firms,
and their related entities. DTTL and each of its member firms are legally
separate and independent entities. DTTL (also referred to as ‚ÄúDeloitte Global‚Äù)
does not provide services to clients. In the United States, Deloitte refers to one
or more of the US member firms of DTTL, their related entities that operate
using the ‚ÄúDeloitte‚Äù name in the United States and their respective affiliates.
Certain services may not be available to attest clients under the rules and
regulations of public accounting. Please see www.deloitte.com/about to learn
more about our global network of member firms.

Copyright ¬© 2019 Deloitte Development LLC. All rights reserved.
